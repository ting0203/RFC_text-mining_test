{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " host\n",
      " network\n",
      " user\n",
      " data\n",
      " imp\n",
      " file\n",
      " protocol\n",
      " server\n",
      " message\n",
      " pdp\n",
      " 10\n",
      " system\n",
      " socket\n",
      " bbn\n",
      " connection\n",
      " telnet\n",
      " may\n",
      " nic\n",
      " tip\n",
      " site\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1969-1973\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "index_name = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   \n",
    "        f = open(path + '\\\\' + i, 'r', encoding='utf-8')\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        index_name.append(i)                            # 输出所有的csv文件\n",
    "        f.close()\n",
    "        \n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "\n",
    "stopwords_word = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [',','|','.','--',':','(',')','rfc','page','[',']','+','-+']\n",
    "stopwords_word.extend(newStopWords)\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=stopwords_word, token_pattern=\"(?u)\\\\b\\\\w\\\\w+\\\\b\", smooth_idf=True, norm='l2')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=index_name)\n",
    "\n",
    "# df_tfidf.to_csv('D:/Desktop/測試用/Result.csv',na_rep='NA')\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :20]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " host\n",
      " network\n",
      " user\n",
      " data\n",
      " imp\n",
      " file\n",
      " protocol\n",
      " server\n",
      " message\n",
      " pdp\n",
      " 10\n",
      " system\n",
      " socket\n",
      " bbn\n",
      " connection\n",
      " telnet\n",
      " may\n",
      " nic\n",
      " tip\n",
      " site\n",
      " command\n",
      " process\n",
      " meeting\n",
      " link\n",
      " control\n",
      " would\n",
      " ucla\n",
      " use\n",
      " one\n",
      " number\n",
      " character\n",
      " mail\n",
      " ftp\n",
      " transfer\n",
      " ncp\n",
      " information\n",
      " time\n",
      " 1973\n",
      " terminal\n",
      " line\n",
      " bit\n",
      " computer\n",
      " 1971\n",
      " messages\n",
      " nwg\n",
      " input\n",
      " group\n",
      " graphics\n",
      " 360\n",
      " mit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1969-1973\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "index_name = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   \n",
    "        f = open(path + '\\\\' + i, 'r', encoding='utf-8')\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        index_name.append(i)                            # 输出所有的csv文件\n",
    "        f.close()\n",
    "        \n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "\n",
    "stopwords_word = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [',','|','.','--',':','(',')','rfc','page','[',']','+','-+']\n",
    "stopwords_word.extend(newStopWords)\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=stopwords_word, token_pattern=\"(?u)\\\\b\\\\w\\\\w+\\\\b\", smooth_idf=True, norm='l2')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=index_name)\n",
    "\n",
    "# df_tfidf.to_csv('D:/Desktop/測試用/Result.csv',na_rep='NA')\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :50]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " host\n",
      " data\n",
      " message\n",
      " imp\n",
      " rfc\n",
      " protocol\n",
      " user\n",
      " command\n",
      " network\n",
      " new\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1974-1978\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "stopwords_word = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [',','|','.','--',':','(',')','rfc','page']\n",
    "stopwords_word.extend(newStopWords)\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=stopwords_word, token_pattern=\"(?u)\\\\b\\\\w\\\\w+\\\\b\", smooth_idf=True, norm='l2')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " accepted\n",
      " host\n",
      " protocol\n",
      " data\n",
      " message\n",
      " mail\n",
      " 10\n",
      " internet\n",
      " rfc\n",
      " postel\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1979-1983\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " protocol\n",
      " rrr\n",
      " network\n",
      " data\n",
      " page\n",
      " host\n",
      " address\n",
      " internet\n",
      " message\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1984-1988\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "str6 = \" \".join(documents)  \n",
    "documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " protocol\n",
      " internet\n",
      " object\n",
      " network\n",
      " information\n",
      " ip\n",
      " page\n",
      " message\n",
      " mib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1989-1993\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "#         print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " internet\n",
      " protocol\n",
      " object\n",
      " address\n",
      " server\n",
      " page\n",
      " ip\n",
      " information\n",
      " type\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1994-1998\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " internet\n",
      " object\n",
      " protocol\n",
      " page\n",
      " message\n",
      " standards\n",
      " track\n",
      " type\n",
      " server\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\1999-2003\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " page\n",
      " standards\n",
      " ietf\n",
      " document\n",
      " protocol\n",
      " message\n",
      " track\n",
      " information\n",
      " type\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\2004-2008\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " document\n",
      " ipv6\n",
      " page\n",
      " address\n",
      " message\n",
      " protocol\n",
      " server\n",
      " section\n",
      " standards\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\2009-2013\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " document\n",
      " page\n",
      " http\n",
      " server\n",
      " section\n",
      " et\n",
      " al\n",
      " ipv6\n",
      " network\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\2014-2018\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "    textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "        textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rfc\n",
      " document\n",
      " ietf\n",
      " https\n",
      " org\n",
      " 10\n",
      " server\n",
      " www\n",
      " info\n",
      " section\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\Desktop\\\\測試用\\\\RFC-all\\\\2019-2020\"          # 设置路径\n",
    "dirs = os.listdir(path)                    # 获取指定路径下的文件\n",
    "endtext = \"\"\n",
    "documents = []\n",
    "for i in dirs:                             # 循环读取路径下的文件并筛选输出\n",
    "    if os.path.splitext(i)[1] == \".txt\":   # 筛选csv文件\n",
    "        text = open(path + '\\\\' + i, 'r', encoding='utf-8').read()\n",
    "        #print(text)\n",
    "        documents.append(text)\n",
    "        #print(text)\n",
    "        #print(i)                            # 输出所有的csv文件\n",
    "\n",
    "# str6 = \" \".join(documents)  \n",
    "# documents = [str6]\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值\n",
    "\n",
    "tfidf = vectorizer.fit_transform(documents)\n",
    "#toarray()可看到词频矩阵的结果\n",
    "# df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names(), index=['text'])\n",
    "# print(df_tfidf)\n",
    "\n",
    "true_k = 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(tfidf)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]  #以每列為基準，將數值由大排到小的位置做陣列\n",
    "\n",
    "terms = vectorizer.get_feature_names()  #get_feature_names()可看到所有文本的关键字\n",
    "# print(terms)\n",
    "for i in range(true_k):\n",
    "#     textcloud = []\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "#         print(ind)\n",
    "        print(' %s' % terms[ind])\n",
    "#         textcloud.append(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
