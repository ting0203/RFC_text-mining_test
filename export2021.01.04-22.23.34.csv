Document Title,中文標題,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Scalability analysis of semantics based distributed document clustering algorithms,基於語義的分佈式文檔聚類算法的可擴展性分析,N. Shah; S. Mahajan,"IT Department, DJSCE, Vile Parle (West), Mumbai, India; Institute of Computer Science, M.E.T., Bandra (west), Mumbai, India","2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",23-Apr-18,2017,,,763,768,"Document clustering is an unsupervised learning paradigm. It provides efficient representation and visualization of the documents; thus helps in easy navigation. The importance of document clustering emerges from the massive volume of textual documents created. Here, semantics based distributed document clustering using Hadoop and MapReduce is proposed. Distributed version of two well-known algorithms, K-Means and Bisecting K-Means, is implemented; testing and comparison of both these algorithms is done for scalability and stability on single-node and multi-node Hadoop cluster. Also, scalability of the cluster setup and both the distributed algorithms is verified. The scalability of the algorithms is based on the Speedup, Scaleup, and Sizeup parameters.",,978-1-5090-6106-8,10.1109/ICICICT1.2017.8342660,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342660,Document Clustering;Distributed Document Clustering;K-Means for Distributed Document Clustering;Bisecting K-Means for Distributed Document Clustering;Scalability of Distributed Document Clustering algorithms,Semantics;Scalability;Clustering algorithms;Partitioning algorithms;Stability analysis;Data mining;Databases,data handling;data visualisation;distributed algorithms;document handling;parallel processing;pattern clustering;unsupervised learning,multinode Hadoop cluster;distributed algorithms;scalability analysis;document clustering;unsupervised learning paradigm;textual documents;distributed version;document visualization;MapReduce;single-node Hadoop cluster,,,,23,,23-Apr-18,,,IEEE,IEEE Conferences
Text document clustering on the basis of inter passage approach by using K-means,通過使用k-means基於跨通道方法進行文本文檔聚類,R. K. Mishra; K. Saini; S. Bagri,"Computer Science and Engineering, Manav Rachna College of Engineering, Faridabad, India; Computer Science and Engineering, Manav Rachna College of Engineering, Faridabad, India; Computer Science and Engineering, Manav Rachna College of Engineering, Faridabad, India","International Conference on Computing, Communication & Automation",6-Jul-15,2015,,,110,113,"Document clustering usually deals with clustering of documents that revolve around a single topic. To achieve more efficient clustering results, it is important to consider the fact that a document may deal with more than one topic. Our research work proposes a new inter-passage based clustering technique which will cluster the segment of the documents on the basis of similarities. The input will be the collection of documents consisting of multi topic segments taken from web. SentiWordNet has been used to calculate the segment score of the segments within the documents. Based upon the segment score segment based clustering is performed on the intra-document level. Once we are done with intra-document segment based clustering then k-means approach is applied to the entire collection of documents to perform inter-document clustering in which the similar segments of various documents will be clustered under a single cluster. Our proposed technique would help in efficient organization of multi topic documents into their corresponding clusters.",,978-1-4799-8890-7,10.1109/CCAA.2015.7148354,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148354,Document Clustering;Intra Document Clustering;Inter Document Clustering;Sentiment analysis;SentiWordNet,Clustering algorithms;Automation;Computer science;Organizations;Information retrieval;Cleaning;Partitioning algorithms,learning (artificial intelligence);pattern clustering;text analysis,text document clustering;inter passage approach;K-means clustering;document similarity;SentiWordNet;segment score segment based clustering;intra-document segment based clustering;inter-document clustering,,8,,6,,6-Jul-15,,,IEEE,IEEE Conferences
A Fast Spectral Method to Solve Document Cluster Ensemble Problem,一種解決文件集群集合問題的快速譜法,S. Xu; Z. Lu; G. Gu,"Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin; Coll. of Inf. & Commun. Eng., Harbin Eng. Univ., Harbin; Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin",2008 International Multi-symposiums on Computer and Computational Sciences,23-Jan-09,2008,,,180,183,"The critical problem in cluster ensemble is how to combine clusterers to yield a final superior clustering result. In this paper, we introduce a spectral method to solve document cluster ensemble problem. Since spectral clustering inevitably needs to compute the eigenvalues and eigenvectors of a matrix, for large scale document datasets, itpsilas computationally intractable. By using algebraic transformation to similarity matrix we get a feasible algorithm. Experiments on TREC and Reuters document sets show that our spectral algorithm yields better clustering results than other typical cluster ensemble techniques without high computational cost.",,978-0-7695-3430-5,10.1109/IMSCCS.2008.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4760320,clustering analysis;cluster ensemble;spectral clustering;document clustering,Clustering algorithms;Partitioning algorithms;Eigenvalues and eigenfunctions;Computational efficiency;Educational institutions;Large-scale systems;Pattern analysis;Machine learning algorithms;Matrix decomposition;Diversity reception,document handling;eigenvalues and eigenfunctions;matrix algebra;pattern clustering,fast spectral method;document cluster ensemble problem;spectral clustering;large scale document datasets;algebraic transformation;TREC document sets;Reuters document sets,,2,,10,,23-Jan-09,,,IEEE,IEEE Conferences
An Improved Document Clustering Approach with Multi-Viewpoint Based on Different Similarity Measures,基於不同相似措施的多視點改進了文檔聚類方法,A. Gunta; R. Dubey,"RGPV: Computer science, sis tech, SAGAR Institute of Science & Technology, Bhopal, India; RGPV: Computer science, sis tech, SAGAR Institute of Science & Technology, Bhopal, India",2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS),10-Mar-19,2018,,,152,157,"Electronic information such as online newspapers, journals, conference proceedings, Web sites, e-mails, etc. They are-growing very fast in extremely large amount. Using all this electronic information controlling, indexing or searching is not possible for human and for search engines also for such a huge amount of large data. Therefore, automatic document organization become a critical issue. With the help of document clustering methods, we can understand data distribution or we can preprocess data for other applications. For an instance, Search engine can produce results more effectively and efficiently if a search engine uses documents those are clustered to search an item or data. Document clustering is an automatic clustering operation and also it is a technique of an unsupervised learning. It combines related documents in one cluster and unrelated documents in different clusters so each cluster consist of documents that are related to one another within the same clusters and are unrelated to documents belonging to other cluster. For applying any clustering methods, it is necessary to calculate similarity measure. The similarity measure is used to find out the degree of closeness or degree of similarity of the target objects. In this paper, we introduce document clustering on Multiview point-based similarity measure and two related document clustering methods. The existing document clustering dissimilarity/similarity measure uses only a single viewpoint, which is the origin that means it uses only one reference point, while the ours use many different viewpoints of references.",,978-1-5386-2842-3,10.1109/ICCONS.2018.8663209,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663209,text mining;document clustering;information extraction,Clustering algorithms;Conferences;Control systems;Clustering methods;Data mining;Euclidean distance;Length measurement,document handling;pattern clustering;search engines;unsupervised learning,electronic information controlling;indexing;automatic document organization;data distribution;automatic clustering operation;document clustering approach;search engines;multiview point-based similarity measure;document clustering dissimilarity measure;unsupervised learning,,,,12,,10-Mar-19,,,IEEE,IEEE Conferences
Efficient phrase-based document indexing for Web document clustering,基於基於短語的Web文檔群集的文檔索引,K. M. Hammouda; M. S. Kamel,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada; Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada",IEEE Transactions on Knowledge and Data Engineering,24-Aug-04,2004,16,10,1279,1296,"Document clustering techniques mostly rely on single term analysis of the document data set, such as the vector space model. To achieve more accurate document clustering, more informative features including phrases and their weights are particularly important in such scenarios. Document clustering is particularly useful in many applications such as automatic categorization of documents, grouping search engine results, building a taxonomy of documents, and others. This article presents two key parts of successful document clustering. The first part is a novel phrase-based document index model, the document index graph, which allows for incremental construction of a phrase-based index of the document set with an emphasis on efficiency, rather than relying on single-term indexes only. It provides efficient phrase matching that is used to judge the similarity between documents. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. The second part is an incremental document clustering algorithm based on maximizing the tightness of clusters by carefully watching the pair-wise document similarity distribution inside clusters. The combination of these two components creates an underlying model for robust and accurate document similarity calculation that leads to much improved results in Web document clustering over traditional methods.",1558-2191,,10.1109/TKDE.2004.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1324634,Index Terms- Web mining;document similarity;phrase-based indexing;document clustering;document structure;document index graph;phrase matching.,Indexing;Data mining;Clustering methods;Information retrieval;Taxonomy;Clustering algorithms;Text mining;Web mining;Artificial intelligence;Data models,Internet;indexing;document handling;data mining;pattern clustering,single term analysis;document data set;vector space model;automatic categorization;search engine;phrase-based document index model;document index graph;phrase matching;pair-wise document similarity distribution;Web document clustering;Web mining;document structure,,164,,45,,24-Aug-04,,,IEEE,IEEE Journals
A comparison of two suffix tree-based document clustering algorithms,基於後綴的文檔聚類算法的比較,M. Rafi; M. Maujood; Murtaza Munawar Fazal; Syed Muhammad Ali,"National University of Computer & Emerging Sciences, NU-FAST, Karachi, Pakistan; National University of Computer & Emerging Sciences, NU-FAST, Karachi, Pakistan; National University of Computer & Emerging Sciences, NU-FAST, Karachi, Pakistan; National University of Computer & Emerging Sciences, NU-FAST, Karachi, Pakistan",2010 International Conference on Information and Emerging Technologies,9-Nov-10,2010,,,1,5,"Document clustering as an unsupervised approach extensively used to navigate, filter, summarize and manage large collection of document repositories like the World Wide Web (WWW). Recently, focuses in this domain shifted from traditional vector based document similarity for clustering to suffix tree based document similarity, as it offers more semantic representation of the text present in the document. In this paper, we compare and contrast two recently introduced approaches to document clustering based on suffix tree data model. The first is an Efficient Phrase based document clustering, which extracts phrases from documents to form compact document representation and uses a similarity measure based on common suffix tree to cluster the documents. The second approach is a frequent word/word meaning sequence based document clustering, it similarly extracts the common word sequence from the document and uses the common sequence/ common word meaning sequence to perform the compact representation, and finally, it uses document clustering approach to cluster the compact documents. These algorithms are using agglomerative hierarchical document clustering to perform the actual clustering step, the difference in these approaches are mainly based on extraction of phrases, model representation as a compact document, and the similarity measures used for clustering. This paper investigates the computational aspect of the two algorithms, and the quality of results they produced.",,978-1-4244-8003-6,10.1109/ICIET.2010.5625688,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625688,Document Clustering;Feature Extraction;Document Representation Model;Suffix Tree;Similarity Measure,Clustering algorithms;Entropy;Partitioning algorithms;Semantics;Computational modeling;Algorithm design and analysis;Databases,data mining;document handling;Internet;pattern clustering;tree data structures,two suffix tree based document clustering algorithms;unsupervised approach;document repositories;World Wide Web;WWW;text presentation;document representation;word meaning sequence,,4,,12,,9-Nov-10,,,IEEE,IEEE Conferences
Document Clustering Method Using Weighted Semantic Features and Cluster Similarity,使用加權語義特徵和群集相似性的文檔聚類方法,S. Park; D. U. An; C. I. Cheon,"Adv. Grad. Educ. Center of Jeonbuk for Electron. & Inf. Technol.-BK21, Chonbuk Nat. Univ., Jeonju, South Korea; Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea; Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea",2010 Third IEEE International Conference on Digital Game and Intelligent Toy Enhanced Learning,13-May-10,2010,,,185,187,"In this paper, a document clustering method that use the weighted semantic features and cluster similarity is introduced to cluster meaningful topics from document set. The proposed method can improve the quality of document clustering because it can avoid clustering the documents whose similarities with topics are high but are meaningless between cluster and document by using the weighted semantic features. Besides, it uses cluster similarity to remove dissimilarity documents in clusters and avoid the biased inherent semantics of the documents to be reflected in clusters by NMF (non-negative matrix factorization). The experimental results demonstrate that the proposed method has better performance than other document clustering methods.",,978-1-4244-6434-0,10.1109/DIGITEL.2010.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463764,Document clustering;semantic feature;cluster similarity;NMF,Clustering methods;Matrix decomposition;Data mining;Games;Sun;Educational technology;Tree graphs;Machine learning;Learning systems;Feature extraction,document handling;matrix algebra;pattern clustering,document clustering method;weighted semantic features;cluster similarity;nonnegative matrix factorization;NMF,,4,,18,,13-May-10,,,IEEE,IEEE Conferences
Malay document clustering using complete linkage clustering technique with Cosine Coefficient,馬來文檔群集使用具有餘弦係數的完整鏈接聚類技術,N. Abd Rahman; Z. Abu Bakar; N. S. S. Zulkefli,"Department of Computer Science, Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Department of Computer Science, Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Department of Computer Science, Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia",2015 IEEE Conference on Open Systems (ICOS),11-Jan-16,2015,,,103,107,"Finding useful and relevant information is a very challenging task to the user. The retrieval system usually responded with a long listed documents which are not necessarily relevant to the user's need. Document clustering is a special technique that can sort out the documents effectively so that documents in the same cluster are similar to each other and documents in different cluster are dissimilar to each other. This paper focuses on document clustering for Malay test collection. It consists of 2028 Malay translated Hadith documents from book Sahih Bukhari. This paper presents the results using Complete Linkage Clustering algorithm with Cosine Coefficient on Malay translated Hadith documents. The evaluation of the experiments uses Recall (R), Precision (P) and Effectiveness (E) measure. The experiments is conducted on 100 clusters, 50 clusters and 20 clusters. It shows that the smaller the size of clusters, Recall (R) will increase, but Precision (P) will decrease. Results for Effectiveness (E) measure compared to the non-clustered documents show that applying clustering algorithm will improved the effectiveness of searching process. For this experiment 20 clusters is rather effective compared to the others.",,978-1-4673-9434-5,10.1109/ICOS.2015.7377286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377286,Malay Information Retrieval;Document Clustering;Hierarchical Agglomerative Clustering;Complete Linkage;Cosine Similarity,Clustering algorithms;Couplings;Conferences;Open systems;Partitioning algorithms;Search engines;Indexes,document handling;information retrieval;natural language processing;pattern clustering,Malay document clustering;complete linkage clustering technique;cosine coefficient;information retrieval system;Malay test collection;searching process,,1,,19,,11-Jan-16,,,IEEE,IEEE Conferences
A novel algorithm for automatic document clustering,一種用於自動文檔聚類的新算法,R. Agrawal; M. Phatak,"MAEER'S MIT, Pune, India; MAEER'S MIT, Pune, India",2013 3rd IEEE International Advance Computing Conference (IACC),13-May-13,2013,,,877,882,"Internet has become an indispensible part of today's life. World Wide Web (WWW) is the largest shared information source. Finding relevant information on the WWW is challenging. To respond to a user query, it is difficult to search through the large number of returned documents with the presence of today's search engines. There is a need to organize a large set of documents into categories through clustering. The documents can be a user query or simply a collection of documents. Document clustering is the task of combining a set of documents into clusters so that intra cluster documents are similar to each other than inter cluster documents. Partitioning and Hierarchical algorithms are commonly used for document clustering. Existing partitioning algorithms have the limitation that the number of clusters has to be given as input and the clustering result depends on this input. If the number of clusters is not known, results are not acceptable. In this paper, we have developed a novel algorithm which generates number of clusters automatically for any unknown text dataset and clusters the documents appropriately based on Cosine Similarity between them. We have also detected zero clustering issue in partitioning algorithm and solved it using our novel algorithm.",,978-1-4673-4529-3,10.1109/IAdCC.2013.6514342,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514342,Document clustering;Zero Clustering;TF-IDF;Threshold;Cosine similarity,Clustering algorithms;Vectors;Partitioning algorithms;Algorithm design and analysis;Computational modeling;Feature extraction;Silicon,document handling;pattern clustering;query processing;search engines,automatic document clustering algorithm;Internet;World Wide Web;information source;user query;search engines;document collection;partitioning algorithm;hierarchical algorithm;cosine similarity;zero clustering issue,,6,,13,,13-May-13,,,IEEE,IEEE Conferences
Fuzzy clustering of web documents using equivalence relations and fuzzy hierarchical clustering,使用等價關係和模糊分層聚類的Web文檔模糊群集,S. Kumar; M. Kathuria; A. K. Gupta; M. Rani,"Department of Computer Science & Engineering, YMCA University of Science and Technology, Faridabad-121006, India; Department of Computer Science & Engineering, YMCA University of Science and Technology, Faridabad-121006, India; Department of Computer Science & Engineering, YMCA University of Science and Technology, Faridabad-121006, India; Department of Computer Science & Engineering, YMCA University of Science and Technology, Faridabad-121006, India",2012 CSI Sixth International Conference on Software Engineering (CONSEG),10-Nov-12,2012,,,1,5,"The conventional clustering algorithms have difficulties in handling the challenges posed by the collection of natural data which is often vague and uncertain. Fuzzy clustering methods have the potential to manage such situations efficiently. Fuzzy clustering method is offered to construct clusters with uncertain boundaries and allows that one object belongs to one or more clusters with some membership degree. In this paper, an algorithm and experimental results are presented for fuzzy clustering of web documents using equivalence relations and fuzzy hierarchical clustering.",,978-1-4673-2177-8,10.1109/CONSEG.2012.6349496,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6349496,Information Retrieval;Clustering;Fuzzy Clustering;Document Clustering;Web Mining;Search Engine,Clustering algorithms;Web mining;Clustering methods;Information retrieval;Algorithm design and analysis;Euclidean distance,data mining;document handling;fuzzy logic;Internet;learning (artificial intelligence);pattern clustering,Web documents;equivalence relations;fuzzy hierarchical clustering;uncertain boundaries;unsupervised learning;Web mining,,2,,17,,10-Nov-12,,,IEEE,IEEE Conferences
Algorithm for clustering web documents,群集Web文檔的算法,Y. Stekh; F. M. E. Sardieh; M. Lobur; M. Dombrova,"CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., 79013, UKRAINE; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., 79013, UKRAINE",2010 Proceedings of VIth International Conference on Perspective Technologies and Methods in MEMS Design,1-Jul-10,2010,,,187,187,The classical clastering algorithms work with vector based document models. In this paper we introduce graph-based representation of web documents and clustering algorithms that based on graph representation of web documents.,,978-966-2191-11-0,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5499290,Web document;clustering algorithm;maximum common subgraph,Clustering algorithms;Clustering methods;Web mining;Extraterrestrial measurements;Computer aided manufacturing;CADCAM,document handling;graph theory;Internet;pattern clustering,Web document clustering;classical clustering algorithms;vector based document models;graph based representation,,,,3,,1-Jul-10,,,IEEE,IEEE Conferences
A new hybrid approach for document clustering,一種新的文檔聚類混合方法,O. Ismael,"Department of Information Systems, Faculty of Computers and Information, Cairo University, Giza, Egypt.",2017 13th International Computer Engineering Conference (ICENCO),12-Feb-18,2017,,,291,296,"K-means algorithm is a well-known clustering algorithm due to its simplicity. Unfortunately, the output of k-means depends on the initialization of cluster centroids. In this paper, we propose a new hybrid approach for document clustering which uses the outputs of single pass clustering (SPC) as an initialization for k-means algorithm. We aim to get the advantages of careful seeding with single pass clustering and the benefits of k-means algorithm. The experimental results state that the proposed approach outperforms traditional k-means algorithm in both unsupervised and supervised evaluation measures especially when the number of required clusters is increased.",2475-2320,978-1-5386-4266-5,10.1109/ICENCO.2017.8289803,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8289803,document clustering;single pass clustering;K-means;MapReduce,Clustering algorithms;Task analysis;Programming;Complexity theory;Big Data;Algorithm design and analysis;Software algorithms,document handling;pattern clustering,K-means algorithm;SPC;supervised evaluation measures;unsupervised evaluation measures;single pass clustering;cluster centroids;clustering algorithm;document clustering;hybrid approach;required clusters,,1,,21,,12-Feb-18,,,IEEE,IEEE Conferences
Clustering Web Retrieval Results Accompanied by Removing Duplicate Documents,群集Web檢索結果拆除重複文檔伴隨,X. Li; Q. Yang; L. Zeng,"Sch. of Electr. & Electron. Eng., North China Electr. Power Univ., Baoding, China; Geophys. Exploration Technol. Lab., Center for Hydrogeology & Environ. Geol., Baoding, China; Sch. of Electr. & Electron. Eng., North China Electr. Power Univ., Baoding, China",2010 International Conference on Web Information Systems and Mining,13-Jan-11,2010,1,,259,261,"Since keyword-based search engine usually return large amount of results in which there are many unrelated documents and many documents with same content, automatic clustering technology is used to classify the retrieval results. While there are large amount of Web retrieval results, the clustering process usually costs long time and the clusters are not friendly to users since there are still many documents with same content. This paper proposed an improved clustering method by removing the duplicate documents from retrieval results. The removal operation is executed first in initial partition stage during clustering. Then it is executed again after the initial partition stage to remove the duplicate documents thoroughly. We proposed an efficient removal method in this stage. At last, we made experiment to verify our method.",,978-1-4244-8438-6,10.1109/WISM.2010.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5662322,clustering;web retrieval result;duplicate documents;k-means,Search engines;Frequency measurement;Clustering methods;Clustering algorithms;Heuristic algorithms;Fingerprint recognition;Support vector machine classification,document handling;Internet;pattern clustering;search engines,duplicate documents;keyword based search engine;unrelated documents;automatic clustering;Web retrieval results;clustering process,,1,,12,,13-Jan-11,,,IEEE,IEEE Conferences
Application of LSA space's dimension character in document multi-hierarchy clustering,LSA空間維度在文檔多層次群集中的應用,Yun-Feng Liu; Huan Qi; Xiang-En Hu; Zhi-Qiang Cai; Jian-Min Dai; Li Zhu,"Inst. of Syst. Eng., Huazhong Univ. of Sci. & Technol., Hubei, China; Inst. of Syst. Eng., Huazhong Univ. of Sci. & Technol., Hubei, China; NA; NA; NA; NA",2005 International Conference on Machine Learning and Cybernetics,7-Nov-05,2005,4,,2384,2389 Vol. 4,"In LSA space, dimensions corresponding to bigger singular values reflect the general concept of language elements, while dimensions corresponding to smaller singular values reflect particular concept of language elements. On this basis, different dimensions of LSA space are adopted for document clustering under various concept granularities. In addition, in the LSA-based algorithm of document clustering, better clustering results are obtained by taking the row vectors of document self-indexing matrix as the objects to be clustered, instead of the document vectors with low dimensionality.",2160-1348,0-7803-9091-1,10.1109/ICMLC.2005.1527343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1527343,Latent Semantic Analysis;Document Self-indexing Matrix;Document Multi-hierarchy Clustering;Concept Granularity,Matrix decomposition;Clustering algorithms;Space technology;Natural languages;Singular value decomposition;Frequency;Computer aided instruction;Systems engineering and theory;Intelligent systems;Text analysis,document handling;indexing;data mining;pattern clustering,LSA space dimension character;latent semantic analysis;document multihierarchy clustering;language elements;document self-indexing matrix;document vectors,,,,9,,7-Nov-05,,,IEEE,IEEE Conferences
Application of Genetic Algorithm in Document Clustering,遺傳算法在文檔聚類中的應用,W. Jian-Xiang; L. Huai; S. Yue-hong; S. Xin-Ning,"Dept. of Inf. Sci., Nanjing Coll. for Population Programme Manage., Nanjing, China; NA; NA; NA",2009 International Conference on Information Technology and Computer Science,4-Aug-09,2009,1,,145,148,"By researching all kinds of methods for document clustering, we put forward a new dynamic method based on genetic algorithm (GA). K-means is a greedy algorithm, which is sensitive to the choice of cluster center and very easily results in local optimization. Genetic algorithm is a global convergence algorithm, which can find the best cluster centers easily. Among the traditional document clustering methods, the document similar matrix is a sparse matrix. In this paper, we propose some new formulas improved on the traditional method. Then, we make some improvement on genetic algorithm. All individuals are encoded by floating-point number and the sum of mean square deviation of intra-class distance is adopted as the objective function. The steps of the algorithm are given in detail. The experimental results show that the accuracy of GA can reach over 98 percent and generate better clustering result than K-means.",,978-0-7695-3688-0,10.1109/ITCS.2009.269,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5190037,Genetic Algorithm;Document Clustering;Optimal Cluster Center,Genetic algorithms;Clustering algorithms;Computer science;Greedy algorithms;Clustering methods;Sparse matrices;Information technology;Application software;Sun;Information science,document handling;genetic algorithms;greedy algorithms;number theory;pattern clustering;sparse matrices,document clustering;genetic algorithm;greedy algorithm;local optimization;global convergence algorithm;document similar matrix;sparse matrix;encoding;floating-point number;mean square deviation;intra-class distance;objective function,,6,,13,,4-Aug-09,,,IEEE,IEEE Conferences
Document clustering by fuzzy c-mean algorithm,通過模糊C均值算法進行文檔聚類,Thaung Thaung Win; Lin Mon,"University of Computer Studies, Mandalay, UCSM Yangon, Myanmar; University of Computer Studies, Mandalay, UCSM Yangon, Myanmar",2010 2nd International Conference on Advanced Computer Control,17-Jun-10,2010,1,,239,242,"Clustering documents enable the user to have a good overall view of the information contained in the documents. Most classical clustering algorithms assign each data to exactly one cluster, thus forming a crisp partition of the given data, but fuzzy clustering allows for degrees of membership, to which a data belongs to different clusters. In this system, documents are clustered by using fuzzy c-means (FCM) clustering algorithm. FCM clustering is one of well-know unsupervised clustering techniques. However FCM algorithm requires the user to pre-define the number of clusters and different values of clusters corresponds to different fuzzy partitions. So the validation of clustering result is needed. PBM index and F-measure are used for cluster validity.",,978-1-4244-5848-6,10.1109/ICACC.2010.5487022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487022,Fuzzy c-mean algorithm;Clulster Validity;Document Clustering;PBMindex,Clustering algorithms;Partitioning algorithms;Fuzzy sets;Clustering methods;Fuzzy systems;Spatial databases;Algorithm design and analysis;Topology;Navigation;Information retrieval,document handling;fuzzy set theory;pattern clustering,document clustering;fuzzy c-mean algorithm;unsupervised clustering techniques;fuzzy partitions;PBM index;F-measurement,,1,,7,,17-Jun-10,,,IEEE,IEEE Conferences
Semantic Structural Similarity for Clustering XML Documents,群集XML文檔的語義結構相似性,T. Kim; J. Lee; J. Song,"Sch. of Comput. Sci. & Eng., Inha Univ., Incheon; Sch. of Comput. Sci. & Eng., Inha Univ., Incheon; Sch. of Comput. Sci. & Eng., Inha Univ., Incheon",2008 International Conference on Convergence and Hybrid Information Technology,9-Sep-08,2008,,,552,557,"The amount of XML documents is increasing rapidly. In order to analyze the information represented in XML documents efficiently, researches on XML document clustering are actively in progress. The key issue is how to devise the similarity measure between XML documents to be used for clustering. Since XML documents have hierarchical structure, it is not appropriate to cluster them by using a general document similarity measure. Previous works on similarity measure for XML document clustering have no consideration for the semantic information as they consider only the structural information. In this paper, we propose the novel similarity measure that concurrently considers both structural and semantic information of XML document. Our experiments show that the proposed method improve accuracy on the clustering from the semantic point of view, compared to the previous works.",,978-0-7695-3328-5,10.1109/ICHIT.2008.183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4622883,XML document similarity;XML document Clustering;Semantic Structural Similarity,XML;Books;Data mining;Clustering algorithms;Music;Accuracy;Distance measurement,document handling;pattern clustering;XML,semantic structural similarity;XML document clustering;data representation,,1,,23,,9-Sep-08,,,IEEE,IEEE Conferences
An adaptive affinity propagation document clustering,自適應親和傳播文檔集群,Y. He; Q. Chen; X. Wang; R. Xu; X. Bai; X. Meng,"Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China; Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China; Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China; Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China; Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China; Dept. of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, P.R. China",2010 The 7th International Conference on Informatics and Systems (INFOS),6-May-10,2010,,,1,7,"The standard affinity propagation clustering algorithm suffers from one limitation that it is hard to know the value of the parameter 聶preference聶 which can yield an optimal clustering solution. To overcome this limitation, in this paper we proposes an adaptive affinity propagation method. The method first finds out the range of 聶preference聶, then searches the space of 聶preference聶 to find a good value which can optimize the clustering result. We apply the method to document clustering and compare it with the standard affinity propagation and K-Means clustering method in real data sets. Experimental results show that our proposed method can get better clustering result.",,978-1-4244-5828-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461817,Affinity propagation;adaptive clustering;document clustering;vector space model,Clustering algorithms;Clustering methods;Helium;Computer science;Optimization methods;Gene expression;Self organizing feature maps;Upper bound;Standards development,document handling;pattern clustering,adaptive affinity propagation;document clustering;preference parameter;K-means clustering,,,,11,,6-May-10,,,IEEE,IEEE Conferences
Formal concept analysis support for web document clustering based on social tagging,基於社交標記的Web文檔聚類的正式概念分析支持,Chunping Ouyang; Xiaohua Yang; Xiaoyun Li; Zhiming Liu,"School of computer science and technology, University of south china, Hengyang, China, 421001; School of computer science and technology, University of south china, Hengyang, China, 421001; School of computer science and technology, University of south china, Hengyang, China, 421001; School of computer science and technology, University of south china, Hengyang, China, 421001",2012 2nd International Conference on Uncertainty Reasoning and Knowledge Engineering,4-Oct-12,2012,,,304,307,"Web document clustering is one of the most important research branches of Clustering Analyzing. The objective of web document clustering is to meet the need of retrieving web document efficiently from massive information in Internet. Recently social tagging is the important form of document organization in web 2.0, and the tagging as a document descriptor is used to improve the effectiveness of web searching. But a web document usually belongs to various category of tagging, which may lead to the difficulty of browsing web document based on single tagging. This paper explores the use of Formal Concept Analysis (FCA) as mathematical tool to analyze the social tagging of web document, and presents a model for web document clustering based on tagging semantic. Furthermore, taking community web site Douban as an example, the model is applied to allow users to tag and serendipitously browse web document using Formal Concept Analysis.",,978-1-4673-1460-2,10.1109/URKE.2012.6319573,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319573,formal concept analysis;social tagging;document clustering,Tagging;Context;Lattices;Semantics;Educational institutions;Analytical models,document handling;formal concept analysis;information retrieval;Internet;pattern clustering,formal concept analysis support;Web document clustering;social tagging;clustering analysis;Web document retrieval;Internet;document organization;Web 2.0;document descriptor;Web searching;FCA;semantic tagging;community Web site Douban;Web document browsing,,1,,14,,4-Oct-12,,,IEEE,IEEE Conferences
An evaluation of Hadoop cluster efficiency in document clustering using parallel K-means,使用並行K均值評估Hadoop集群在文檔集群中的效率,T. H. Sardar; Z. Ansari; A. Khatun,"Dept. of CSE, P.A. College of Engineering, Kolkata; Dept. of CSE and Dean (Research), P.A. College of Engineering, Kolkata; Tata Consultancy Services Ltd, Kolkata",2017 IEEE International Conference on Circuits and Systems (ICCS),29-Mar-18,2017,,,17,20,"One of the significant data mining techniques is clustering. Due to digitalization and globalization of each work space, large datasets are being generated rapidly. Such large dataset clustering is a challenge for traditional sequential clustering algorithms as it requires large execution time to cluster such datasets. Distributed parallel architectures and algorithms are thus helpful to achieve performance and scalability requirement of clustering large datasets. In this study, we design and experiment a parallel k-means algorithm using MapReduce programming model and compared the result with sequential k-means for clustering varying size of document dataset. The result demonstrates that proposed k-means obtains higher performance and outperformed sequential k-means while clustering documents.",,978-1-5090-6480-9,10.1109/ICCS1.2017.8325954,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325954,Clustering;Data Mining;MapReduce;Hadoop;Distributed Computing,Clustering algorithms;Conferences;Circuits and systems;Data mining;Programming;File systems;Containers,data mining;document handling;parallel programming;pattern clustering,Hadoop cluster efficiency;document clustering;parallel k-means;dataset clustering;execution time;scalability requirement;MapReduce programming model;document dataset;clustering documents;data mining techniques;sequential clustering algorithms,,,,17,,29-Mar-18,,,IEEE,IEEE Conferences
Documents clustering using quantum clustering algorithm,使用量子聚類算法群集文檔,R. Bhagawati; S. R. Laskar; B. Swain,"Department of Computer Science & Engineering, Assam University, Silchar, India; Department of Computer Science & Engineering, Assam University, Silchar, India; Department of Computer Science & Engineering, Assam University, Silchar, India","2016 International Conference on Microelectronics, Computing and Communications (MicroCom)",28-Jul-16,2016,,,1,4,"In the domain of Quantum Information processing (QIP), quantum clustering approach plays a vital role for processing information. The quantum clustering technique is basically used for providing effective information navigation and browsing mechanism. There are many classical approach of clustering the documents like k-means clustering algorithm, hierarchical clustering algorithm, information bottleneck clustering algorithm and many more. In this paper we proposed a clustering method which is based on quantum theory. For clustering the documents, such that which provides an agreeable result in comparison of existing classical approach.",,978-1-4673-6621-2,10.1109/MicroCom.2016.7522562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7522562,Clustering;Document Clustering;Quantum Clustering,Clustering algorithms;Algorithm design and analysis;Mathematical model;Standards;Computer science;Semantics;Wave functions,pattern clustering;quantum computing;quantum theory,documents clustering;quantum clustering algorithm;quantum information processing;QIP;information navigation;browsing mechanism;k-means clustering algorithm;hierarchical clustering algorithm;information bottleneck clustering algorithm;quantum theory,,2,,19,,28-Jul-16,,,IEEE,IEEE Conferences
Clustering Algorithm Based on Semantic Distance for XML Documents,基於語義距離的聚類算法XML文檔,L. Yang; J. Gu; H. Chen,"Coll. of Inf. Sci. & Eng., Wuhan Univ. of Sci. & Technol., Wuhan, China; NA; Coll. of Inf. Sci. & Eng., Wuhan Univ. of Sci. & Technol., Wuhan, China",2009 First International Workshop on Database Technology and Applications,18-Aug-09,2009,,,549,552,"As the information grows exponentially, it has become a new and basic requirement to reduce the querying area efficiently and accurately for information querying. This paper proposes a semantic distance based clustering algorithm for XML documents. It discusses the algorithm in two steps. Firstly, it forms some DTD clusters with all heterogeneous DTD documents by using the global semantic dictionary. Secondly, it computes the semantic distance between XML documents which corresponded certain DTD cluster, then build some finally XML clusters according threshold value given beforehand. Users can locate document cluster and query within this area without extending all over XML documents, and the querying results satisfying the users' requirements can be returned rapidly. The experiments show that this algorithm has good categorization function, and can facilitate information querying.",2167-194X,978-0-7695-3604-0,10.1109/DBTA.2009.134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207698,Semantic distance;Documents clustering;Heterogeneous,Clustering algorithms;XML;Databases;Heuristic algorithms;Dictionaries;Educational institutions;Application software;Information science;Data engineering;Computer science,document handling;pattern clustering;XML,clustering algorithm;semantic distance;XML document;global semantic dictionary;DTD cluster;document type definition,,,,15,,18-Aug-09,,,IEEE,IEEE Conferences
Text Document Clustering Based on the Modifying Relations,基於修改關係的文本文檔群集,T. Weixin; Z. Fuxi,"Comput. Sch., Wuhan Univ., Wuhan; Comput. Sch., Wuhan Univ., Wuhan",2008 International Conference on Computer Science and Software Engineering,22-Dec-08,2008,1,,256,259,"Text document clustering plays an important role in the modern knowledge management. This paper addresses the task of developing an effective and efficient method of clustering the text document. To meet this requirement, we first extract the modifying relations (MR) from the sentences and then organize them as feature set for representing the document. A novel similarity measure is proposed on the basis of MR-vectors in this paper. We use agglomerative hierarchical clustering algorithm in the experimental work and compare the results with other previous studies.",,978-0-7695-3336-0,10.1109/CSSE.2008.1545,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721737,Text mining;clustering;modifying relation;document representaion,Clustering algorithms;Clustering methods;Computer science;Software engineering;Educational institutions;Information technology;Knowledge management;Data mining;Text mining;Algorithm design and analysis,data structures;document handling;pattern clustering;text analysis,text document clustering;knowledge management;modifying relations;document representation;agglomerative hierarchical clustering algorithm,,2,,12,,22-Dec-08,,,IEEE,IEEE Conferences
Term Clustering and Confidence Measurement in Document Clustering,文檔聚類中的術語聚類和置信度測量,K. Csorba; I. Vajk,"Department of Automation and Applied Informatics, Budapest University of Technology and Economics, 1111 Budapest, Goldmann Gy. T矇r 3., Hungary, kristof@aut.bme.hu; Department of Automation and Applied Informatics, Budapest University of Technology and Economics, 1111 Budapest, Goldmann Gy. T矇r 3., Hungary, vajk@aut.bme.hu",2006 IEEE International Conference on Computational Cybernetics,12-Feb-07,2006,,,1,6,"A novel topic based document clustering technique is presented in the paper for situations, where there is no need to assign all the documents to the clusters. Under such conditions the clustering system can provide a much cleaner result by rejecting the classification of documents with ambiguous topic. This is achieved by applying a confidence measurement for every classification result and by discarding documents with a confidence value less than a predefined lower limit. This means that our system returns the classification for a document only if it feels sure about it If not, the document is marked as ""unsure"". Beside this ability the confidence measurement allows the use of a much stronger term filtering, performed by a novel, supervised term cluster creation and term filtering algorithm, which is presented in this paper as well.",,1-4244-0071-6,10.1109/ICCCYB.2006.305694,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4097655,document clustering;term cluster creation;confidence;supervised learning,Automation;Informatics;Filtering algorithms;Supervised learning;Information filtering;Information filters;Paper technology;Performance evaluation;Feature extraction;Frequency,classification;document handling;information filtering;learning (artificial intelligence);pattern clustering,document clustering system;confidence measurement;ambiguous topic;document classification;supervised term cluster creation;supervised term filtering algorithm,,1,,5,,12-Feb-07,,,IEEE,IEEE Conferences
A Document Clustering Algorithm for Web Search Engine Retrieval System,一種文檔集群算法，用於Web搜索引擎檢索系統,H. Yang,"Sch. of Software, Yunnan Univ., Kunming, China","2010 International Conference on e-Education, e-Business, e-Management and e-Learning",18-Mar-10,2010,,,383,386,"As the number of available Web pages grows, it is become more difficult for users finding documents relevant to their interests. Clustering is the classification of a data set into subsets (clusters), so that the data in each subset (ideally) share some common trait - often proximity according to some defined distance measure. It can enable users to find the relevant documents more easily and also help users to form an understanding of the different facets of the query that have been provided for web search engine. A popular technique for clustering is based on K-means such that the data is partitioned into K clusters. In this method, the groups are identified by a set of points that are called the cluster centers. The data points belong to the cluster whose center is closest. Existing algorithms for k-means clustering are slow and do not scale to large number of data points and converge to different local minima based on the initializations. A fast greedy k-means algorithm can attack both these drawbacks, but it is a limitation when the algorithm is used for large number of data points, So we introduce an efficient method to compute the distortion for this algorithm. The experiment results show that the fast greedy algorithm is superior to other method and can help users to find the relevant documents more easily than by relevance ranking.",,978-1-4244-5681-9,10.1109/IC4E.2010.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432478,Document clustering;fast greedy k-means;Search engine,Clustering algorithms;Web search;Search engines;Databases;Information retrieval;Electronic learning;Software algorithms;Web pages;Partitioning algorithms;Greedy algorithms,document handling;greedy algorithms;pattern clustering;query processing;relevance feedback;search engines;Web sites,document clustering;Web search engine retrieval system;Web page;data set classification;query;k-means clustering;cluster center;fast greedy k-means algorithm;relevance ranking,,5,,8,,18-Mar-10,,,IEEE,IEEE Conferences
Document Clustering based on mutual information and PCA subspace,基於相互信息和PCA子空間的文檔群集,Jiangfeng Yang; Zheng Ma,"Department of Communication and Information Engineering, University of Electronic Science and Technology of China, UESTC, Cheng Du, China; Department of Communication and Information Engineering, University of Electronic Science and Technology of China, UESTC, Cheng Du, China","2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",5-Sep-11,2011,,,2983,2986,"Mutual information is a criterion widely used in statistical language modeling word associations and feature selection. Principle component analysis (PCA) is a statistical technique for unsupervised dimension reduction. K-means clustering is commonly used data clustering for unsupervised learning tasks. In this paper, we first select features from native feature space by mutual information to reduce data dimension, then decompose the covariance matrix of new word-document matrix via Singular Value Decomposition (SVD), finally, K-means is used to cluster in PCA subspace. The clustering result is analyzed comprehensively under different condition.",,978-1-4577-0536-6,10.1109/AIMSEC.2011.6011352,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011352,document clustering;PCA subspace;dimension reduction,Principal component analysis;Covariance matrix;Accuracy;Mutual information;Clustering algorithms;Training;Eigenvalues and eigenfunctions,document handling;pattern clustering;principal component analysis,document clustering;mutual information;PCA subspace;statistical language modeling word associations;feature selection;principle component analysis;unsupervised dimension reduction;K-means clustering;data clustering;unsupervised learning task;data dimension;covariance matrix;word-document matrix;singular value decomposition,,,,8,,5-Sep-11,,,IEEE,IEEE Conferences
A New Multimedia Documents Clustering Approach Based on Feature Patterns Similarity,基於特徵模式相似性的新多媒體文檔聚類方法,K. Pushpalatha; V. S. Ananthanarayana,"Dept. of Comput. Sci. & Eng., Sahyadri Coll. of Eng. & Manage., Mangalore, India; Department of Information TechnologyNational Institute of Technology KarnatakaSurathkal, Mangalore,India-575025",2017 IEEE International Symposium on Multimedia (ISM),1-Jan-18,2017,,,296,299,"With the rapid advances in digital technology, the multimedia documents have been growing ubiquitously. The analysis of this huge repository of multimedia documents requires efficient organization of documents. Multimedia document clustering organizes the multimedia documents with common multimedia topics. The important step of multimedia document clustering is computing the similarity between multimedia documents. The multimodal objects of multimedia documents are described by the feature patterns. Hence, the feature patterns of multimedia objects play the major role in computing the similarity of multimedia documents. In this paper, we propose an feature pattern similarity based clustering approach for multimedia documents. Experimental results show that the proposed clustering approach clusters the multimedia documents efficiently and outperform the competitive methods.",,978-1-5386-2937-6,10.1109/ISM.2017.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241618,Multimedia Document;Clustering;Feature Patterns;Similarity;Multimodal,Multimedia communication;Entropy;Flickr;Multimedia computing;Feature extraction;Clustering algorithms;Correlation,document handling;multimedia computing;pattern clustering,multimedia documents clustering approach;feature patterns similarity;multimodal objects;multimedia objects,,,,20,,1-Jan-18,,,IEEE,IEEE Conferences
Bilingual topic taxonomy generation based on bilingual documents clustering,基於雙語文件聚類的雙語主題分類生成,C. Zhang,"Dept. of information management, Nanjing University of Science and Technology, Nanjing, 210094",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1889,1895,"Bilingual taxonomy is one of key components of multilingual Ontology. In this paper, affinity propagation clustering algorithm is used to cluster bilingual documents collection and generate bilingual topic taxonomy. Two bilingual topic taxonomy generation methods, i.e. bilingual documents clustering before or after text feature reconstruction, are described. Dataset in two domains are tested and result shows that: according to net similarity, the result of documents clustering after feature reconstruction is better than that before feature reconstruction.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016948,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016948,Topic taxonomy generation;Bilingual documents clustering;Parallel corpus;Ontology learning;Multilingual Ontology,Taxonomy;Feature extraction;Entropy;Clustering algorithms;Dictionaries;Ontologies;Law,document handling;linguistics;ontologies (artificial intelligence);pattern clustering,bilingual topic taxonomy generation;bilingual documents clustering;multilingual ontology;affinity propagation clustering algorithm;before text feature reconstruction;after text feature reconstruction;bilingual documents collection,,2,,20,,12-Sep-11,,,IEEE,IEEE Conferences
Semi-supervised document clustering using Seeds affinity propagation and consensus algorithm in multi-domain settings,使用種子親和力傳播和多域設置共識算法的半監督文檔聚類,R. Radha; T. T. Mirnalinee; T. E. Trueman,"Dept of Computer Science & Engg, Anna University of Technology Chennai, India; Dept of Computer Science & Engg, SSN College of Engg, Chennai, India; Dept of Computer Science & Engg, Anna University of Technology Chennai, India",2012 International Conference on Recent Trends in Information Technology,31-May-12,2012,,,85,90,"Domain adaptation is the process of transferring the knowledge to a different domain from a source domain but they are related. In this paper, we first apply `Consensus Regularization' based algorithm to merge multiple source domain to a single source domain. Then we propose multi-domain adaptation in document clustering using Seeds affinity propagation and Consensus Regularization Algorithm. A semi-supervised document clustering algorithm, called Seeds Affinity Propagation (SAP) is applied based on an effective clustering algorithm Affinity Propagation (AP). The labeled and unlabeled documents are preprocessed through various processes such as stop words removal, word stemming and finding word frequency and given as the input. After pre-processing, structured documents are obtained. Tri-set Computation, a feature extraction technique is used to find out the features through Co-feature set, Unilateral feature set and Significant Co-feature set methods. Then calculate the similarity measure of the documents and assigning the label to the documents if they are matched. Finally clustered documents are obtained through seeds affinity propagation via similarity measurement. Further the performance of the algorithm can be evaluated and improved.",,978-1-4673-1601-9,10.1109/ICRTIT.2012.6206802,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6206802,Multi-domain adaptation;Consensus Regularization;Seeds Affinity Propagation (SAP);Tri set Computation;Document Clustering,Clustering algorithms;Feature extraction;Entropy;Algorithm design and analysis;Convergence;Computer science;Educational institutions,document handling;learning (artificial intelligence);merging;pattern clustering,semi-supervised document clustering;seeds affinity propagation;multidomain setting;domain adaptation process;consensus regularization based algorithm;domain merging;single source domain;multiple source domain;clustering algorithm affinity propagation;stop words removal process;word stemming process;word frequency finding process;tri-set computation technique;feature extraction technique;co-feature set method;unilateral feature set method;significant co-feature set method;similarity measurement,,1,,8,,31-May-12,,,IEEE,IEEE Conferences
Proposal of interactive document clustering system based on coordinated multiple views,基於協調多視圖的交互式文檔聚類系統提案,T. Tonegawa; Y. Takama,"Graduate School of System Design, Tokyo Metropolitan University, 6-6 Asahigaoka, Hino, 191-0065, Japan; Graduate School of System Design, Tokyo Metropolitan University, 6-6 Asahigaoka, Hino, 191-0065, Japan",2014 IEEE 7th International Workshop on Computational Intelligence and Applications (IWCIA),18-Dec-14,2014,,,19,22,"This paper proposes an interactive document clustering system based on coordinated multiple views (CMV). Recently, the amount of documents has become extremely large, which makes it difficult for a user to under-stand relationship between documents and topics mentioned in a set of documents. In order to support a user, interactive clustering is a promising approach. When designing an interactive clustering system, it should be considered that a user has to examine clustering result from various levels such as clusters, documents, and words. The proposed system divides documents into four levels and displays each level in parallel with different views. By designing coordination among those views, a user can understand document information from various levels and perform interactive clustering efficiently.",1883-3977,978-1-4799-4770-6,10.1109/IWCIA.2014.6987729,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987729,coordinated multiple views (CMV);interactive document clustering;visualization,Data visualization;Prototypes;Text mining;Clustering algorithms;Information retrieval;Artificial intelligence,data visualisation;document handling;interactive systems;pattern clustering,interactive document clustering system;coordinated multiple views;CMV,,1,,14,,18-Dec-14,,,IEEE,IEEE Conferences
A Wordsets based document clustering algorithm for large datasets,基於Wordsets的大型數據集文檔聚類算法,A. Sharma; R. Dhir,"Department of Computer Science & Engineering, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India; Department of Computer Science & Engineering, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India",2009 Proceeding of International Conference on Methods and Models in Computer Science (ICM2CS),22-Jan-10,2009,,,1,7,"Document clustering is an important tool for applications such as search engines and document browsers. It enables the user to have a good overall view of the information contained in the documents. The well-known methods of document clustering, however, do not really address the special problems of text document clustering: very high dimensionality of the document, very large size of the datasets and understandability of the cluster description. Also there is a strong need of hierarchical document clustering where clustered documents can be browsed according to the increasing specificity of topics. Frequent Itemset Hierarchical Clustering (FIHC) is a novel data mining algorithm for hierarchical grouping of text documents. The approach does not give reliable clustering results when the number of frequent sets of terms is large. In this paper we propose WDC (Wordsets-based Clustering), an efficient clustering algorithm based closed words sets. WDC uses a hierarchical approach to cluster text documents having common words. WDC found scalable, effective and efficient when compared with existing clustering algorithms like K-means and its variants.",,978-1-4244-5051-0,10.1109/ICM2CS.2009.5397962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5397962,document clustering;Clustering algorithm;Wordsets based Clustering;Hierarchical document clustering,Clustering algorithms;Itemsets;Partitioning algorithms;Robustness;Computer science;Data engineering;Application software;Search engines;Data mining;Databases,data mining;pattern clustering;text analysis,Wordsets based document clustering algorithm;search engines;document browsers;document information;cluster description;hierarchical document clustering;topic specificity;frequent itemset hierarchical clustering;data mining;hierarchical text document grouping,,8,,24,,22-Jan-10,,,IEEE,IEEE Conferences
A New Glowworm Swarm Optimization Based Clustering Algorithm for Multimedia Documents,多媒體文檔基於發光蟲群優化的一種新的聚類演算法,K. Pushpalatha; V. S. Ananthanarayana,"Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Surathkal, India; Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Surathkal, India",2015 IEEE International Symposium on Multimedia (ISM),28-Mar-16,2015,,,262,265,"Due to the explosion of multimedia data, the demand for the sophisticated multimedia knowledge discovery systems has been increased. The multimodal nature of multimedia data is the big barrier for knowledge extraction. The representation of multimodal data in a unimodal space will be more advantageous for any mining task. We initially represent the multimodal multimedia documents in a unimodal space by converting the multimedia objects into signal objects. The dynamic nature of the glowworms motivated us to propose the Glowworm Swarm Optimization based Multimedia Document Clustering (GSOMDC) algorithm to group the multimedia documents into topics. The better purity and entropy values indicates that the GSOMDC algorithm successfully clusters the multimedia documents into topics. The goodness of the clustering is evaluated by performing the cluster based retrieval of multimedia documents with better precision values.",,978-1-5090-0379-2,10.1109/ISM.2015.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442338,Multimedia Document;Multimodal;Glowworm Swarm Optimization;Unified Representation;Clustering,Multimedia communication;Clustering algorithms;Heuristic algorithms;Particle swarm optimization;Entropy;Algorithm design and analysis;Media,data mining;document handling;information retrieval;multimedia computing;pattern clustering,glowworm swarm optimization based clustering algorithm;multimedia knowledge discovery systems;knowledge extraction;multimodal multimedia documents;GSOMDC algorithm;cluster based retrieval,,1,,17,,28-Mar-16,,,IEEE,IEEE Conferences
Comparison of Algorithms for Document Clustering,文檔聚類演算法的比較,M. Gupta; A. Rajavat,"Dept. of Comput. Sci., SVITS, Indore, India; Dept. of Comput. Sci., SVITS, Indore, India",2014 International Conference on Computational Intelligence and Communication Networks,26-Mar-15,2014,,,541,545,"Clustering is ""the method of organizing objects into groups whose members are related in some way"". A cluster is therefore a collection of objects which are coherent internally, but clearly dissimilar to the objects belonging to other clusters. Document clustering is used in many fields such as data mining and information retrieval. Thus, the main goals of this paper are to identify the comparison of the performance of criterion function in the context of partition clustering approach, k means, and agglomerative hierarchical approach. By comparing all this we establish right clustering algorithm to produce qualitative clustering of real world document. And also modify existing algorithm to establish right algorithm which we try to make more efficient than existing algorithms which we are study in this paper.",,978-1-4799-6929-6,10.1109/CICN.2014.123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065543,Document Clustering;BIRCH;Kmeans;Support Vector Model;Matrix Representation,Clustering algorithms;Vectors;Partitioning algorithms;Algorithm design and analysis;Entropy;Clustering methods;Computer science,data mining;document handling;information retrieval;pattern clustering,document clustering;object organization;data mining;information retrieval,,2,,12,,26-Mar-15,,,IEEE,IEEE Conferences
Clustering homogeneous XML documents using weighted similarities on XML attributes,使用 XML 屬性上的加權相似性對同質 XML 文件進行聚類,N. K. Nagwani; A. Bhansali,"Department of CS&E, NIT, Raipur, India; Department of IT, OPJIT, Raigarh, India",2010 IEEE 2nd International Advance Computing Conference (IACC),1-Mar-10,2010,,,369,372,"XML (eXtensible Markup Language) have been adopted by number of software vendors today, it became the standard for data interchange over the web and is platform and application independent also. A XML document is consists of number of attributes like document data, structure and style sheet etc. Clustering is method of creating groups of similar objects. In this paper a weighted similarity measurement approach for detecting the similarity between the homogeneous XML documents is suggested. Using this similarity measurement a new clustering technique is also proposed. The method of calculating similarity of document's structure and styling is given by number of researchers, mostly which are based on tree edit distances. And for calculating the distance between document's contents there are number of text and other similarity techniques like cosine, jaccord, tf-idf etc. In this paper both of the similarity techniques are combined to propose a new distance measurement technique for calculating the distance between a pair of homogeneous XML documents. The proposed clustering model is implemened using open source technology java and is validated experimentally. Given a collection of XML documents distances between documents is calculated and stored in the java collections, and then these distances are used to cluster the XML documents.",,978-1-4244-4790-9,10.1109/IADCC.2010.5422926,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422926,XML Clustering;Weighted Similarity;XML Documents Similarity,XML;Java;Clustering algorithms;Software standards;Application software;Weight measurement;Distance measurement;Software measurement;Software testing;Information retrieval,document handling;pattern clustering;XML,homogeneous XML documents;extensible markup language;weighted similarities;XML attributes;XML clustering;similarity measurement,,2,,13,,1-Mar-10,,,IEEE,IEEE Conferences
Clustering OCR-ed texts for browsing document image database,為瀏覽文件影像資料庫而聚類 OCR-ed 文本,K. Tsuda; S. Senda; M. Minoh; K. Ikeda,"Dept. of Inf. Sci., Kyoto Univ., Japan; NA; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,171,174 vol.1,"Document clustering is a powerful tool for browsing throughout a document database. Similar documents are gathered into several clusters and a representative document of each cluster is shown to users. To make users infer the content of the database from several representatives, the documents must be separated into tight clusters, in which documents are connected with high similarities. At the same time, clustering must be fast for user interaction. We propose an O(n/sup 2/) time, O(n) space cluster extraction method. It is faster than the ordinal clustering methods, and its clusters compare favorably with those produced by Complete Link for tightness. When we deal with OCR-ed documents, term loss caused by recognition faults can change similarities between documents. We also examined the effect of recognition faults to the performance of document clustering.",,0-8186-7128-9,10.1109/ICDAR.1995.598969,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598969,,Image databases;Clustering methods;Information science;Frequency;Merging;Clustering algorithms;Object detection;Information retrieval;Optical character recognition software,document image processing;visual databases;optical character recognition;word processing;human factors;interactive systems;feature extraction,OCR text clustering;document image database browsing;document clustering;user interaction;cluster extraction method;ordinal clustering methods;Complete Link;term loss;recognition faults,,2,,6,,6-Aug-02,,,IEEE,IEEE Conferences
An Improved XML Document Clustering Using Path Feature,使用路徑功能改進的 XML 文件群集,J. Yuan; X. Li; L. Ma,"Dept. of Electron. & Commun. Eng., North China Electr. Power Univ., Baoding; Dept. of Electron. & Commun. Eng., North China Electr. Power Univ., Baoding; Dept. of Electron. & Commun. Eng., North China Electr. Power Univ., Baoding",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,2,,400,404,"Extensible markup language (XML) documents clustering is useful to XML application such as XML search engine. The element tags and their position in the document's hierarchy provide valuable information to clustering XML documents. XML path can represent both element tags and their position information. Since common Xpath represents only parts of the XML structural, using common Xpath as XML structural representation is not always efficient to XML clustering, especially when those documents are with dissimilar structure. In this paper, we use all paths less than or equal to length L as feature vectors for XML documents. Since the feature vector matrix is usually sparse, we use bipartite graph to express association relation among XML documents and path features. Based on this idea, we improved the path-based XML clustering algorithm. Experiments are described to demonstrate its efficiency.",,978-0-7695-3305-6,10.1109/FSKD.2008.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666147,XML clustering;path;bipartite graph,XML;Tellurium;Clustering methods;Sparse matrices;Bipartite graph;Search engines;Fuzzy systems;Knowledge engineering;Power engineering and energy;Educational institutions,document handling;pattern clustering;XML,XML document clustering;path feature;extensible markup language;XML documents clustering;XML search engine;common Xpath;XML structural representation;XML clustering;bipartite graph;association relation,,4,,14,,5-Nov-08,,,IEEE,IEEE Conferences
Improved Document Clustering using k-means algorithm,使用 k-means 演算法改進文件聚類,P. Bide; R. Shedge,"Dept. Computer Engg, Ramrao Adik Institute of Technology, Navi Mumbai, India; Dept. of Computer Engg, Ramrao Adik Institute of Technology, Navi Mumbai, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",27-Aug-15,2015,,,1,5,"Searching for similar documents has a crucial role in document management. Because of tremendous increase in documents day by day, it is very essential to segregate these documents in proper clusters. Faster categorization of documents is required in forensic investigation but analysis of these documents is very difficult. So, there is a need to separate multiple collections of documents into similar ones through clustering. Specifying number of clusters is mandatory in existing partitioning algorithms and the output is totally dependent on given input. Over clustering is the major problem in document clustering. The proposed algorithm takes input as Keywords found after extraction and solves the problem of over clustering by dividing the documents into small groups using Divide and Conquer Strategy. In this paper, an Improved Document Clustering algorithm is given which generates number of clusters for any text documents and uses cosine similarity measures to place similar documents in proper clusters. Experimental results showed that accuracy of proposed algorithm is high compare to existing algorithm in terms of F-Measure and time complexity.",,978-1-4799-6085-9,10.1109/ICECCT.2015.7226065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226065,Document Clustering;Divide and Conquer;Cosine Similarity;Tf-Idf;Threshold,Clustering algorithms,digital forensics;divide and conquer methods;pattern clustering;text analysis,k-means algorithm;similar document searching;document management;document categorization;forensic investigation;partitioning algorithms;divide and conquer strategy;document clustering algorithm;text documents;cosine similarity measures,,4,,15,,27-Aug-15,,,IEEE,IEEE Conferences
Clustering GML documents using maximal frequent induced subtrees,使用最大頻繁誘發子樹對 GML 文檔進行聚類,Y. Zhu; G. Ji; Qin-hong Sun,"Department of computer foundation teaching, Sanjiang University, Nanjing 210012, China; Department of computer, Nanjing Normal University, 210097, China; Department of computer foundation teaching, Sanjiang University, Nanjing 210012, China",2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery,9-Sep-10,2010,5,,2265,2269,"An algorithm, TBCClustering, is presented in the paper for clustering GML documents using maximal frequent induced subtree patterns. TBCClustering mines the maximal frequent induced subtrees by using the structural information of GML documents, it can get the best minimum support automatically, and then chooses a set of subtree patterns to form the optimistic clustering features. Finally it uses CLOPE algorithm to cluster the GML documents by clustering features without giving the number of clusters. Experiment results have shown that TBCClustering is more effective and efficient than PBClustering.",,978-1-4244-5934-6,10.1109/FSKD.2010.5569321,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569321,Clustering;GML document mining;Induced subtree;Maximal frequent subtree,Clustering algorithms;XML;Databases;Algorithm design and analysis;Data mining;Encoding;Computers,data mining;document handling;pattern clustering;trees (mathematics),clustering GML document;maximal frequent induced subtree;TBCClustering;structural information;optimistic clustering feature;CLOPE algorithm;PBClustering,,1,,17,,9-Sep-10,,,IEEE,IEEE Conferences
A Comparative Study of Selective Cluster Ensemble for Document Clustering,文檔聚類選擇性集群組合的比較研究,S. Xu; J. Gao; X. Xu; X. Li; H. Yu,"Sch. of Inf. Eng., Yancheng Inst. of Technol., Yancheng, China; Sch. of Inf. Eng., Yancheng Inst. of Technol., Yancheng, China; Sch. of Inf. Eng., Yancheng Inst. of Technol., Yancheng, China; Sch. of Inf. Eng., Yancheng Inst. of Technol., Yancheng, China; Sch. of Comput. Sci. & Eng., Jiangsu Univ. of Sci. & Technol., Zhenjiang, China",2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics,23-Nov-15,2015,1,,308,311,"Spherical K-means algorithm (SKM) has been widely used for document clustering. Yet it is prone to getting stuck at local optimums. Recently, cluster ensemble has shown to be an effective method in improving the performance of single clustering algorithm such as SKM by combining multiple clustering solutions resulted by SKM. This paper describes and compares three graph partitioning algorithms and four hierarchy clustering algorithms for document clustering, both theoretically and empirically, using a selective cluster ensemble framework. Our experimental results over several document datasets show that, in terms of normalized mutual information, (a) The hierarchy clustering algorithms produce better clustering results than graph partitioning algorithms (b) on most datasets, the best results are arrived when the size of ensemble is between 40 and 60.",,978-1-4799-8646-0,10.1109/IHMSC.2015.268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334710,cluster analysis;selective cluster ensemble;normalized mutual information,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Chlorine;Mutual information;Classification algorithms;Image segmentation,document handling;graph theory;pattern clustering,document clustering;spherical K-means algorithm;SKM;single clustering algorithm;graph partitioning algorithm;hierarchy clustering algorithm,,,,13,,23-Nov-15,,,IEEE,IEEE Conferences
Document grouping with concept based discriminative analysis and feature partition,使用基於概念的區分分析和特徵分區進行文檔分組,S. Kajapriya; K. N. V. Shankar,"Department of Computer Science & Engineering, V.S.B Engineering College, Karur, India; Department of Computer Science & Engineering, V.S.B Engineering College, Karur, India",International Conference on Information Communication and Embedded Systems (ICICES2014),9-Feb-15,2014,,,1,4,Clustering is one of the most important techniques in machine learning and data mining responsibilities. Similar documents are grouped by performing clustering techniques. Similarity measure is used to determine transaction associations. Hierarchical clustering method produces tree structured results. Partition based clustering model produces the results in grid format. Text documents are formless data values with high dimensional attributes. Document clustering group the unlabeled text documents into meaningful clusters. Traditionally clustering methods need cluster count (K) before the document grouping process. Clustering accuracy decreases drastically with reference to the unsuitable cluster count. Document word features are automatically partitioned into two groups discriminative words and non-discriminative words. But only discriminative words are useful for grouping documents. The contribution of nondiscriminative words confuses the clustering process and leads to poor cluster solutions. The variational inference algorithm is used to infer the document collection structure and partition of document words at the same time. Dirichlet Process Mixture (DPM) model is used to partition documents. DPM clustering model utilizes both the data likelihood and the clustering property of the Dirichlet Process (DP). Dirichlet Process Mixture Model for Feature Partition (DPMFP) is used to discover the latent cluster structure based on the DPM model. DPMFP clustering model is performed without requiring the no. of clusters as input. The Discriminative word identification process is enhanced with the labeled document analysis mechanism. The concept relationships are analyzed with Ontology support. Semantic weight analysis is used for the document similarity measure. This method increases the scalability with the support of labels and concept relations for dimensionality cutback process.,,978-1-4799-3834-6,10.1109/ICICES.2014.7033763,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033763,Dirichlet Process Mixture Model;Document Clustering;Feature Partition;Semi-Supervised;Database management;Text mining,Semantics;Educational institutions;Feature extraction;Inference algorithms;Partitioning algorithms;Text mining;Clustering methods,data mining;inference mechanisms;learning (artificial intelligence);mixture models;ontologies (artificial intelligence);pattern clustering;text analysis;variational techniques,concept based discriminative analysis;machine learning;data mining;similarity measure;transaction associations;hierarchical clustering method;partition based clustering model;document clustering accuracy;unlabeled text documents;document word features;discriminative words;nondiscriminative words;document collection structure;document word partition;DPM clustering model;data likelihood;clustering property;Dirichlet process mixture model for feature partition;DPMFP clustering model;latent cluster structure;discriminative word identification process;document analysis mechanism;dimensionality cutback process,,,,6,,9-Feb-15,,,IEEE,IEEE Conferences
A Document Clustering Method Based on One-Dimensional SOM,基於一維 SOM 的文檔聚類方法,Y. Yu; P. He; Y. Bai; Z. Yang,"Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin; NA; NA",Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008),23-May-08,2008,,,295,300,"In this paper a new method is presented and used in clustering document collections. This method is based on the one-dimensional arrays of Self-Organizing Map network (1-D SOM array). The main idea of this method is to obtain the clustering results by calculating the distances between every two adjacent MSPs (the most similar prototype to the input vector ) of well trained 1-D SOM. The process is simple, easy to understand and unnecessary to give the number of clusters beforehand. The experimental results show that this method works well in clustering document collection.",,978-0-7695-3131-1,10.1109/ICIS.2008.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529835,Self-Organizing Map(SOM);document clustering;data mining,Clustering methods;Neurons;Prototypes;Sequences;Neural networks;Frequency;Text analysis;Clustering algorithms;Information science;Computer science,document handling;pattern clustering;self-organising feature maps,document clustering method;1D SOM array;self-organizing map network;most similar prototype,,1,,7,,23-May-08,,,IEEE,IEEE Conferences
An Improved Hierarchical K-Means Algorithm for Web Document Clustering,改進的 Web 文件聚類的分層 K-means 演演算法,Y. Liu; Z. Liu,"Sch. of Comput. Sci. & Technol., Xidian Univ., Xian; Sch. of Comput. Sci. & Technol., Xidian Univ., Xian",2008 International Conference on Computer Science and Information Technology,12-Sep-08,2008,,,606,610,"In order to conquer the major challenges of current Web document clustering, i.e. huge volume of documents, high dimensional process, we proposed a simple agglomerative hierarchical k-means clustering (SAHKC) algorithm based on H-K (hierarchical k-means) algorithm, and a new model was used in this paper to describe the Web document, named as multiple feature vector space model (MFVSM). Experimental results indicate that: the MFVSM is helpful in improving the quality of clustering result, and compare with the H-K algorithm, the SAHKC algorithmpsilas running time reduce nearly 30%, however, the average precision of clustering result only reduce about 10%.",,978-0-7695-3308-7,10.1109/ICCSIT.2008.152,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624939,web document clustering;vector space time (VSM);K-Means,Clustering algorithms;Algorithm design and analysis;Optimized production technology;Signal processing algorithms;Partitioning algorithms;Complexity theory;Correlation,document handling;Internet;pattern clustering,Web document clustering;simple agglomerative hierarchical k-means clustering;multiple feature vector space model,,6,,7,,12-Sep-08,,,IEEE,IEEE Conferences
XML Documents Clustering Research Based on Weighted Cosine Measure,基於加權餘弦測量的 XML 文檔聚類研究,W. Li; X. Li; Y. Zhao,"NA; NA; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China",2010 Fifth International Conference on Frontier of Computer Science and Technology,16-Sep-10,2010,,,95,100,"Recently, a large amount of work has been done in XML data mining. However, most of the existing work focuses on the snapshot XML data, while XML data is dynamic in practical application. In order to mine knowledge hidden in the frozen structures (FS) which are not changed or very little changed during the historical changing process of an XML document, we present a method for clustering XML documents via FS. Also, a novel algorithm called weighted cosine measure (WCM) improved from the traditional algorithm has been proposed, and using which we can calculate the similarity between two clusters. Otherwise, we propose a method using the agglomerative hierarchical during the cluster process. Experiments results on our new algorithm indicate that the proposed solution performs significantly. XML documents can be effectively clustered and the results of using the WCM are better than using the traditional cosine measure. Then, XML documents in each cluster have similar structures not often changed.",2159-631X,978-1-4244-7779-1,10.1109/FCST.2010.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575647,data mining;document clustering;XML;frozen structure;weighted cosine measure;agglomerative hierarchical method,XML;Clustering algorithms;Weight measurement;Algorithm design and analysis;Merging;Companies;Data mining,data mining;document handling;pattern clustering;XML,XML documents clustering research;weighted cosine measurement;XML data mining;frozen structures;WCM,,2,,10,,16-Sep-10,,,IEEE,IEEE Conferences
Information retrieval using fuzzy c-means clustering and modified vector space model,使用模糊 c-means 聚類和修改向量空間模型進行資訊檢索,Chandrani Ray Chowdhury; Prachet Bhuyan,"School of Computer Engineering, KIIT University, Bhubaneswar, Orissa, India; School of Computer Engineering, KIIT University, Bhubaneswar, Orissa, India",2010 3rd International Conference on Computer Science and Information Technology,7-Sep-10,2010,1,,696,700,"This paper presents a method to improve the performance of Information Retrieval System (IRS) by increasing the no of relevant documents retrieved. There are several types of uncertainty and fuzziness associated with IRS like search term uncertainty, relevance uncertainty involved in retrieving of irrelevant documents. The aim of this paper is to eliminate different types of uncertainty and increase the chance of retrieving relevant documents. In the framework a method is proposed which first calculate query and document cluster similarity which not only retrieve the documents matching query terms as well as similar to retrieved documents by calculating the query and cluster similarity. This helps to reduce search term uncertainty and tries to reduce the fuzziness associated with document relevance in two steps. First modification is made in general term frequency-inverse document frequency (tf-idf) scoring mechanism to give importance of informativeness of a document contents and secondly calculating query and document summary overlap. All the above information is used to measure the document relevant score. Finally retrieved documents are filtered by Pearson correlation coefficient between query vector and document vector to find out only those documents correlated with query. In experiment standard NPL test collection prepared by Vaswani and Cameron at the National Physical Laboratory in England was used. After full implementation of above methodology it was found that proposed work is better in comparison with existing methods.",,978-1-4244-5540-9,10.1109/ICCSIT.2010.5564542,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564542,Information Retrieval;Clustering;Fuzzy cmeans;Document cluster;Document summary;document frequency;Correlation ratio,Correlation;Discrete wavelet transforms,correlation methods;document handling;fuzzy set theory;information filtering;information retrieval systems;pattern clustering;relevance feedback;search problems;vectors,fuzzy c-means clustering;modified vector space model;information retrieval system;IRS;relevant document retrieval;irrelevant documents;document cluster similarity;search term uncertainty;fuzziness;document relevance;term frequency-inverse document frequency;tf-idf scoring mechanism;document summary;Pearson correlation coefficient;query vector;document vector;standard NPL test collection;National Physical Laboratory,,1,,7,,7-Sep-10,,,IEEE,IEEE Conferences
A concept driven document clustering using WordNet,使用 WordNet 的概念驅動文檔群集,S. R. Kolhe; S. D. Sawarkar,"Department of Computer Engineering, Datta Meghe College of Engineering, Navi Mumbai, Airoli; Department of Computer Engineering, Datta Meghe College of Engineering, Navi Mumbai, Airoli",2017 International Conference on Nascent Technologies in Engineering (ICNTE),15-Jun-17,2017,,,1,5,"Recently Information Technology is used extensively for wide range of application for example solutions enabled through e-commerce to different web based information system. This usage has lead to development of large textual data base. Mostly this information data is stored in unstructured text. This large data developed has lead to the need of its systematic clustering for easy data retrieval organization and summarization, typically called as data mining. Many techniques such as document clustering, web based result clustering and information visualization are developed by the researchers based on various algorithms such as k-means, k-medoid, Bisecting k-means, Suffix Tree Clustering (STC), Hierarchical clustering, Lingo and nature inspired optimization algorithm like clustering using Ant colony optimization, Fireflies and so on. Although these techniques are widely used, these are suffered from inconsistencies in cluster content and inconsistencies in cluster description. Therefore this paper presents a novel approach termed as `Semantic Lingo' based on concept of the text data. This concept driven approach is executed on Wordnet. The developed approach identifies the predominant notion and automatically generate clusters based on these notion. The standard available dataset ODP, AMBIENT are used in the paper for validation of the proposed concept. The data clusters obtained from the proposed concept has shown high precision level and recall.",,978-1-5090-2794-1,10.1109/ICNTE.2017.7947888,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947888,Web Document clustering;Concepts;Semantic;Word Net;cluster labels,Semantics;Clustering algorithms;Computers;Matrix decomposition;Resource management;Algorithm design and analysis;Art,ant colony optimisation;data visualisation;database management systems;information retrieval;Internet;pattern clustering;text analysis,concept driven document clustering;WordNet;information technology;e-commerce;Web based information system;large textual data base development;unstructured text;systematic clustering;data retrieval organization;data retrieval summarization;Web based result clustering;data mining;information visualization;k-means algorithm;k-medoid algorithm;bisecting k-means algorithm;suffix tree clustering algorithm;STC algorithm;hierarchical clustering algorithm;Lingo algorithm;nature inspired optimization algorithm;ant colony optimization;fireflies;semantic lingo;text data;ODP dataset;AMBIENT dataset;automatic cluster generation,,1,,14,,15-Jun-17,,,IEEE,IEEE Conferences
Incremental document clustering using cluster similarity histograms,使用聚類相似性直方圖進行增量文檔聚類,K. M. Hammouda; M. S. Kamel,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada; Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),27-Oct-03,2003,,,597,601,"Clustering of large collections of text documents is a key process in providing a higher level of knowledge about the underlying inherent classification of the documents. Web documents, in particular, are of great interest since managing, accessing, searching, and browsing large repositories of Web content requires efficient organization. Incremental clustering algorithms are always preferred to traditional clustering techniques, since they can be applied in a dynamic environment such as the Web. An incremental document clustering algorithm is introduced, which relies only on pair-wise document similarity information. Clusters are represented using a cluster similarity histogram, a concise statistical representation of the distribution of similarities within each cluster, which provides a measure of cohesiveness. The measure guides the incremental clustering process. Complexity analysis and experimental results are discussed and show that the algorithm requires less computational time than standard methods while achieving a comparable or better clustering quality.",,0-7695-1932-6,10.1109/WI.2003.1241276,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241276,,Histograms;Clustering algorithms;Clustering methods;Design engineering;Knowledge engineering;Systems engineering and theory;Electronic mail;Content management;Algorithm design and analysis;Web sites,Web sites;Internet;content-based retrieval;computational complexity;document handling;pattern clustering,document clustering algorithm;cluster similarity histogram representation;text document;Web document;pair-wise document similarity information;statistical representation;complexity analysis,,14,,8,,27-Oct-03,,,IEEE,IEEE Conferences
Skew detection of document images by focused nearest-neighbor clustering,通過聚焦近鄰聚類對文檔圖像進行傾斜檢測,Xiaoyi Jiang; H. Bunke; D. Widmer-Kljajo,"Dept. of Comput. Sci., Bern Univ., Switzerland; NA; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,629,632,"Describes an algorithm to estimate the skew angle of document images. It utilizes the nearest-neighbor clustering paradigm. In contrast to earlier approaches, the local clustering process is focused on a subset of plausible neighbors. The proposed skew detection algorithm is potentially usable for any feature points that reveal the dominant orientation of document images in their entirety. Experimental results using connected components and pass codes as features are presented to show the general usefulness of the proposed algorithm.",,0-7695-0318-7,10.1109/ICDAR.1999.791866,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791866,,Focusing;Detection algorithms;Histograms;Computer science;Identity-based encryption;Clustering algorithms;Ear;Text analysis;Image analysis;Humans,document image processing;pattern clustering;codes,document image skew detection;focused nearest-neighbor clustering;skew angle estimation algorithm;local clustering process;plausible neighbors;feature points;dominant orientation;connected components;pass codes,,14,,7,,6-Aug-02,,,IEEE,IEEE Conferences
An Improved Approach for Web Document Clustering,Web 文檔群集的改進方法,V. Madaan; R. Kumar,"Computer Science and Engineering NITTTR, Chandigarh, India; Computer Science and Engineering NITTTR, Chandigarh, India","2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)",1-Jul-19,2018,,,435,440,"With an ever increasing volume of data, terabytes of text data is being generated everyday in the form of semi structured, unstructured and structured data via various sources. Efficiency of clusters is the major concern when dealing with application areas of clustering like information retrieval. In this paper K-means clustering approach is applied based on similarity measure among the documents. Further optimization technique specifically bio- inspired is used to intelligently explore and re-adjust the documents from the pre-organized clusters followed by a classification approach for the evaluation and validation of the clusters.",,978-1-5386-4119-4,10.1109/ICACCCN.2018.8748351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8748351,Document Clustering;Similarity;K-means;Optimization Algorithm;Classification;Neural Networks,Clustering algorithms;Optimization;Classification algorithms;Particle swarm optimization;Artificial neural networks;Matlab,document handling;information retrieval;Internet;optimisation;pattern clustering,web document;text data;information retrieval;similarity measure;optimization technique;K-means clustering approach,,,,14,,1-Jul-19,,,IEEE,IEEE Conferences
Frequent term based text document clustering: A new approach,頻繁基於術語的文本文檔聚類：新方法,M. Kumar; D. K. Yadav; V. K. Gupta,"Department of Information and Technology, BBDNITM, Lucknow, India; Department of Computer Science, MNNIT, Allahabad Allahabad, India; Department of Computer Science, BBDNIIT, Lucknow",2015 International Conference on Soft Computing Techniques and Implementations (ICSCTI),13-Jun-16,2015,,,11,15,"Document clustering is used to organize the documents into groups. VSM (Vector Space Model) is a technique used to represent the document as a vector. Working with VSM to cluster the documents is easier. The main problem with text documents clustering is very high dimensionality of data. A term in the document represents a dimension. To reduce the dimensions of the document vector space, it is preprocessed. The main techniques involved are stemming and term filtering for dimensions reduction of document vectors. After dimensions reduction, term frequency vectors corresponding to each document are generated, where each cell in the term frequency vector represents frequencies of a term. Using proposed method in the paper, each pair of term frequency vectors are compared to find out the similarity value between every two corresponding documents. In this way, three similarity matrices minimum_match, maximum_match and average_match are generated which are further used in various clustering techniques to produce clusters. Clusters produced using proposed approach are compared with that of clusters produced based on cosine similarity in terms of F-measure. Higher values of F-measure for clusters produced using proposed method shows that proposed algorithm is better.",,978-1-4673-6792-9,10.1109/ICSCTI.2015.7489630,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489630,K-means;VSM;Vector Space Model;data mining;classification;clustering;document clustering;term frequency;cosine similarity;F-measure;inverse document frequency,Clustering algorithms;Data mining;Filtering;Computer science;Standards;Time-frequency analysis;Semantics,data reduction;matrix algebra;pattern clustering;text analysis;unsupervised learning;vectors,term frequency vector;text document clustering;vector space model;VSM;data dimension reduction;similarity matrix;unsupervised learning,,3,,17,,13-Jun-16,,,IEEE,IEEE Conferences
A Transfer Learning Algorithm for Document Categorization Based on Clustering,基於聚類的文檔分類轉移學習算法,W. Sun; Q. Xu,"Coll. of Mech. Electron. & Inf. Eng., China Univ. of Min. & Technol.(Beijing), Beijing, China; Coll. of Mech. Electron. & Inf. Eng., China Univ. of Min. & Technol.(Beijing), Beijing, China",2012 International Conference on Computer Science and Electronics Engineering,23-Apr-12,2012,2,,528,531,"Traditional machine learning and data mining have achieved significant success in many knowledge engineering areas including classification, regression clustering and so on, but a major assumption in them is that the training and test data must be in the same feature space and follow the same distribution. However, in real applications, this assumption couldn't be satisfied for ever. In this case, the role of transfer learning can be highlight, because transfer learning does not make the same distributional assumptions as the traditional machine learning, and reduces the dependencies of the target task and training data, has a wider migration of knowledge. In this paper we will propose a transfer learning algorithm for document categorization based on clustering. We describe the main idea and the step of the algorithm. Then use experiment to test the algorithm and compare the algorithm with no-transfer algorithm. the experiment demonstrate that the algorithm we proposed in this paper is better than the others in some extent.",,978-0-7695-4647-6,10.1109/ICCSEE.2012.132,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188085,transfer learning;machine learning;clustering;document categorization,Clustering algorithms;Machine learning;Training data;Classification algorithms;Humans;Finance;Art,data mining;document handling;learning (artificial intelligence);pattern classification;pattern clustering,transfer learning algorithm;document categorization;clustering;machine learning;data mining;knowledge engineering;training data;knowledge migration;document classification,,,,10,,23-Apr-12,,,IEEE,IEEE Conferences
An Evaluation of the Formal Concept Analysis-Based Document Vector on Document Clustering,基於形式概念分析的文檔聚類中文檔向量的評估,J. Jehng; S. Chou; C. Cheng; J. Heh,"Inst. of Human Resource Manage., Nat. Central Univ., Jhongli, Taiwan; Dept. of Inf. Manage., Nat. Central Univ., Jhongli, Taiwan; Dept. of Inf. Manage., Nat. Central Univ., Jhongli, Taiwan; Dept. of Inf. & Comput. Eng., Chung Yuan Christian Univ., Jhongli, Taiwan",2011 International Conference on Computational Science and Its Applications,21-Jul-11,2011,,,207,210,"In conventional approaches, documents are represented by the vector whose dimensionalities are equivalent to the terms extracted from a document set. These approaches, called bag-of-term approaches, ignore the conceptual relationships between terms such as synonyms, hypernyms and hyponyms. In the past, researches have applied thesauri such as Word Net to solve this problem. However, thesauri such as Word Net are developed more for general purposes and are limited in specific domain. Therefore, an automatically built ontology for terms is desired. In our previous study, we proposed a method which applies formal concept analysis (FCA), an automatic ontology building method, to extract the term relationships from a document set, and then apply the extracted information as the ontology of terms to represent the documents as concept vectors. In order to evaluate the usability and effectiveness of the proposed method for information retrieval related applications, we employed the concept vectors generated for the documents to the document clustering. In this study, we apply bisecting k-means clustering and hierarchical agglomerative clustering as the platforms with which to evaluate our method.",,978-1-4577-0142-9,10.1109/ICCSA.2011.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959620,Formal concept analysis;Document clustering;Document vector;Concept vector;Term Ontology,Ontologies;Lattices;Context;Information retrieval;Thesauri;Data mining;Clustering algorithms,document handling;information retrieval;ontologies (artificial intelligence);pattern clustering;vectors,formal concept analysis;document vector;document clustering;bag-of-term approach;automatic ontology building method;term relationship extraction;information retrieval;bisecting k-means clustering;hierarchical agglomerative clustering,,1,,23,,21-Jul-11,,,IEEE,IEEE Conferences
An approach for document clustering using PSO and K-means algorithm,使用PSO和K-means算法的文檔聚類方法,R. Chouhan; A. Purohit,"Department of Computer Engineering, S.G.S.I.T.S, Indore; Department of Information Technology, S.G.S.I.T.S, Indore",2018 2nd International Conference on Inventive Systems and Control (ICISC),28-Jun-18,2018,,,1380,1384,"The world wide web also known as WWW is the source for the largest shared information. To effectively organize, summarize and navigate through the information on the web in a fast and high quality manner, document clustering algorithms are needed. Various clustering algorithms are proposed by the researchers in which the K-means is widely used partitioning clustering algorithm which is easy for implementation, has fast convergence property in local area, and takes less time for execution. But major drawback of this method is its random choice of initial cluster centroids. To overcome this problem, an approach for document clustering using Particle Swarm Optimization (PSO) method is proposed in this paper. PSO method is applied before K-means for finding the optimal points in the search space and these points are used as initial cluster centroids for K-means algorithm to find final clusters of documents. Results of clustering algorithms are tested on four different document datasets. The outcome shows that the most efficient clustering results are generated than traditional K-means algorithm.",,978-1-5386-0807-4,10.1109/ICISC.2018.8399034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399034,PSO;Vector Space Model;Cluster Centroid;K-means;Document Clustering,Clustering algorithms;Partitioning algorithms;Mathematical model;Convergence;Heuristic algorithms;Entropy;Conferences,particle swarm optimisation;pattern clustering,document clustering algorithms;partitioning clustering algorithm;PSO method;convergence property;cluster centroids;particle swarm optimization method;document datasets,,5,,11,,28-Jun-18,,,IEEE,IEEE Conferences
Document Clustering Based on Fuzzy Rough Set,基於模糊粗糙集的文檔聚類,Z. Peng; L. Zhishu; C. Yang; H. Zhiguo,"Coll. of Comput. Sci., Sichuan Univ., Chengdu; Coll. of Comput. Sci., Sichuan Univ., Chengdu; Coll. of Comput. Sci., Sichuan Univ., Chengdu; NA",2009 International Conference on Communication Software and Networks,19-Jun-09,2009,,,701,703,"High dimension and hard describing of cluster are the primary problems of clustering algorithm based on vector model. The conditional probability combined with rough set is introduced in this paper, and a information retrieval model based on fuzzy rough set is proposed, finally its document clustering algorithm is designed in detail. Especially the description and updating of cluster is studied in this paper.",,978-0-7695-3522-7,10.1109/ICCSN.2009.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5076945,rough set;fuzzy set;information retrieval;document clustering,Fuzzy sets;Information retrieval;Clustering algorithms;Educational institutions;Algorithm design and analysis;Computer science;Software algorithms;Set theory;Rough sets;Design methodology,document handling;fuzzy set theory;information retrieval;pattern clustering;rough set theory,document clustering;fuzzy rough set;vector model;conditional probability;information retrieval model,,,,4,,19-Jun-09,,,IEEE,IEEE Conferences
Textual Document Clustering Using Topic Models,使用主題模型的文本文檔聚類,X. Sun,"Knowledge Grid Group, Inst. of Comput. Technol., Beijing, China","2014 10th International Conference on Semantics, Knowledge and Grids",24-Nov-14,2014,,,1,4,"Document clustering is to group documents according to a certain semantic features defined on the document set for measuring the similarities between two documents. The keyword models such as the TFIDF model of document have been widely used as features for document clustering. But it lacks of semantic structure, which limit its further usage in document analysis. Topic model has been developed to discover multiple probabilistic distributions over the vocabulary, which can be seen as different topic dimensions of the document set. It has a richer semantic structure than the TFIDF models. Using topic model to cluster documents, one can obtain the not only the document ids of clusters but also the topic of the clusters and the global document set. There are two major ways to use the topic models in document clustering: one is based on the basic topic model and the other is based on new cluster-oriented topic models. In this paper, we evaluate the basic clustering performance of these two types of methods. We proposed several simple clustering methods based on the basic topic model and compare them with the cluster-oriented topic model and other major clustering methods. The experimental results show that the simple method can achieve the comparable clustering accuracy and recall rate to those latest models and algorithms.",,978-1-4799-6715-5,10.1109/SKG.2014.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964656,document clustering;probabilistic topic model;Latent Dirichlet Allocation (LDA),Clustering algorithms;Probabilistic logic;Clustering methods;Semantics;Measurement;Computational modeling;Vocabulary,pattern clustering;probability;text analysis,textual document clustering;topic models;semantic features;keyword models;TFIDF model;document analysis;multiple probabilistic distributions,,3,,15,,24-Nov-14,,,IEEE,IEEE Conferences
The influence of word normalization in English document clustering,字標準化在英語文檔聚類中的影響,P. Han; S. Shen; D. Wang; Y. Liu,"School of Information Management, Nanjing University, Nanjing, China; School of Information Management, Nanjing University, Nanjing, China; School of Information Management, Nanjing University, Nanjing, China; Institute of Command Automation, The PLA University of Technology & Science, Nanjing, China",2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE),20-Aug-12,2012,2,,116,120,"Stemming or lemmatization method is a key step in English document processing. Based on three clustering algorithms and two evaluation functions, the paper makes a comprehensive study about two stemming algorithms and one lemmatization algorithm. According to the experimental result, it shows that the performance is not remarkable, compared with Snowball stemmer and Stanford lemmatization, Porter stemmer can make a better performance in entropy and purity.",,978-1-4673-0089-6,10.1109/CSAE.2012.6272740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272740,stemming;lemmatization;document clustering,Clustering algorithms;Entropy;Classification algorithms;Dictionaries;Partitioning algorithms;Educational institutions,document handling;natural language processing;pattern clustering;word processing,word normalization;English document clustering;lemmatization method;stemming method;English document processing;evaluation functions;Snowball stemmer;Stanford lemmatization;Porter stemmer,,4,,11,,20-Aug-12,,,IEEE,IEEE Conferences
Concept Based Document Clustering Using K Prototype Algorithm,K原型算法的基於概念的文檔聚類,S. Pasarate; R. Shedge,"Department of Computer Engineering, Ramrao Adik Institute of Technology, Nerul, Navi Mumbai; Department of Computer Engineering, Ramrao Adik Institute of Technology, Nerul, Navi Mumbai","2018 International Conference on Control, Power, Communication and Computing Technologies (ICCPCCT)",13-Dec-18,2018,,,579,583,"Internet is a wide network of unstructured data such as blogs, tweets, mails, files. The retrieval of correct data is necessary. Data clustering is the process of putting together data into groups which are coherently similar. Clustering minimizes search time as similar documents are in the same cluster. Named entity recognition method allocates noun entities to different sections named as person, location etc. Latent dirichlet allocation treats documents as mixture of topics, and works as generative model. We have used Reuters 21578 dataset, self created dataset, News article dataset, web dataset for processing. The proposed system consists of preprocessing web document data for removing unwanted data. Next is the feature extraction phase through named entity recognition method and topic modeling approach(LDA). Feature extraction shrinks data dimensionality. K-prototype clustering algorithm approach performs better for clustering as it takes into consideration number of mismatches for categorical data. The execution time and space utilized by K-prototype algorithm is better than Fuzzy clustering algorithm.",,978-1-5386-0796-1,10.1109/ICCPCCT.2018.8574332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574332,clustering;feature extraction;k prototype algorithm,Clustering algorithms;Feature extraction;Semantics;Prototypes;Computers;Data mining;Information retrieval,document handling;feature extraction;fuzzy set theory;information retrieval;Internet;pattern clustering;text analysis;Web sites,document clustering;search time;fuzzy clustering algorithm;news article dataset;LDA;categorical data;K-prototype clustering algorithm approach;data dimensionality;topic modeling approach;feature extraction phase;unwanted data;web document data;web dataset;self created dataset;Reuters 21578 dataset;generative model;latent dirichlet allocation;noun entities;named entity recognition method;data clustering;correct data;unstructured data;wide network;K prototype algorithm,,,,23,,13-Dec-18,,,IEEE,IEEE Conferences
The Formal Concept Analysis of the Documents Clusters,文件集群的正式概念分析,H. Shi; Q. Hua; P. Zhang,"School of Mathematics and Physics, North China Electric Power University, Boading 071003, China. E-MAIL: shihuifeng_mcm@yahoo.com.cn; Faculty of Mathematics and Computer Science, Hebei University, Boading 071002, China; School of Mathematics and Physics, North China Electric Power University, Boading 071003, China",2007 International Conference on Machine Learning and Cybernetics,29-Oct-07,2007,6,,3381,3385,"In this paper, an method based on formal concept analysis for explaining the document clustering result is presented. In order to decrease the number of objects in formal concept analysis, the documents is clustered by high dimensional sparse clustering approach. The clusters are considered as the objects of formal context; The attributes set is the characters set of centroid vectors. The concept lattice of this formal context is builded by the Ganter algorithm. From the line graph of the concept lattice with respect to the formal context, we can explain that why some documents are clustered, and find the content of those documents in the each cluster.",2160-1348,978-1-4244-0972-3,10.1109/ICMLC.2007.4370732,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370732,Document cluster;formal concept analysis;concept lattice;high dimensional sparse cluster,Text analysis;Machine learning;Cybernetics,data analysis;document handling;graph theory;lattice theory;pattern clustering;set theory;vectors,formal concept analysis;documents clustering;high dimensional sparse clustering approach;centroid vector characters set;concept lattice;Ganter algorithm;line graph,,1,,12,,29-Oct-07,,,IEEE,IEEE Conferences
Efficient Semisupervised MEDLINE Document Clustering With MeSH-Semantic and Global-Content Constraints,高效的半體驗Medline文檔集群與網格語義和全局內容約束,J. Gu; W. Feng; J. Zeng; H. Mamitsuka; S. Zhu,"Shanghai Key Laboratory of Intelligent Information Processing and the School of Computer Science , Fudan University, Shanghai, China; School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji, Japan; Shanghai Key Laboratory of Intelligent Information Processing and the School of Computer Science , Fudan University, Shanghai, China",IEEE Transactions on Cybernetics,15-Jul-13,2013,43,4,1265,1276,"For clustering biomedical documents, we can consider three different types of information: the local-content (LC) information from documents, the global-content (GC) information from the whole MEDLINE collections, and the medical subject heading (MeSH)-semantic (MS) information. Previous methods for clustering biomedical documents are not necessarily effective for integrating different types of information, by which only one or two types of information have been used. Recently, the performance of MEDLINE document clustering has been enhanced by linearly combining both the LC and MS information. However, the simple linear combination could be ineffective because of the limitation of the representation space for combining different types of information (similarities) with different reliability. To overcome the limitation, we propose a new semisupervised spectral clustering method, i.e., SSNCut, for clustering over the LC similarities, with two types of constraints: must-link (ML) constraints on document pairs with high MS (or GC) similarities and cannot-link (CL) constraints on those with low similarities. We empirically demonstrate the performance of SSNCut on MEDLINE document clustering, by using 100 data sets of MEDLINE records. Experimental results show that SSNCut outperformed a linear combination method and several well-known semisupervised clustering methods, being statistically significant. Furthermore, the performance of SSNCut with constraints from both MS and GC similarities outperformed that from only one type of similarities. Another interesting finding was that ML constraints more effectively worked than CL constraints, since CL constraints include around 10% incorrect ones, whereas this number was only 1% for ML constraints.",2168-2275,,10.1109/TSMCB.2012.2227998,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374265,Biomedical text mining;document clustering;semisupervised clustering;spectral clustering,Bioinformatics;Genomics;Vectors;Indexing;Educational institutions;Thesauri;Clustering algorithms,document handling;learning (artificial intelligence);medical information systems;pattern clustering,MeSH-semantic constraints;global-content constraints;biomedical document clustering;local-content information;global-content information;MEDLINE collections;medical subject heading-semantic information;MS information;LC information;semisupervised MEDLINE spectral document clustering method;SSNCut;LC similarities;must-link constraints;document pairs;MS similarities;cannot-link constraints;CL constraints;GC similarities;MEDLINE records,Cluster Analysis;Data Mining;MEDLINE;Medical Subject Headings;Semantics;Supervised Machine Learning,28,,36,,4-Dec-12,,,IEEE,IEEE Journals
Bengali Document Clustering Using Word Movers Distance,使用單詞移動距離的孟加拉文文檔聚類,A. Ahmad; M. Ruhul Amin; F. Chowdhury,"Department of Computer Science & Engineering, Search Engine Pipilika, Shahjalal University of Science & Technology, Sylhet, Bangladesh; Department of Computer Science & Engineering, Search Engine Pipilika, Shahjalal University of Science & Technology, Sylhet, Bangladesh; Department of Computer Science & Engineering, Search Engine Pipilika, Shahjalal University of Science & Technology, Sylhet, Bangladesh",2018 International Conference on Bangla Speech and Language Processing (ICBSLP),2-Dec-18,2018,,,1,6,"In this paper, we propose a pipeline architecture for Bengali Document clustering and apply it for clustering Bengali news documents using different clustering algorithms. Our goal is to cluster news from different online newspapers according to the topic describing the identical stories. We used Word Movers Distance (WMD), a relatively new algorithm to measure document distances, which is based on vector representation of words. Later, we conduct Bengali document clustering using several other algorithms, namely K-means, Hierarchical Clustering Algorithm (HCA) and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN). We also evaluate the clusters against a manually prepared set of clusters, which we consider as the ground truth. Our experiment shows that, HCA performs best with a Fl-score of 92%, which is the most similar to the number of clusters and cluster members compared to the ground truth. We also released a live working demo where the program collects the recent news from popular online Bengali newspapers and create clusters according to the news story.",,978-1-5386-8207-4,10.1109/ICBSLP.2018.8554598,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554598,,Clustering algorithms;Computational modeling;Task analysis;Linear programming;Encyclopedias;Electronic publishing,document handling;learning (artificial intelligence);natural language processing;pattern clustering,cluster members;Bengali news documents;cluster news;document distances;Bengali document clustering;hierarchical clustering algorithm;word movers distance;clustering algorithms;online Bengali newspapers;K-means clustering;HCA;WMD;hierarchical density-based spatial clustering of applications with noise;HDBSCAN,,1,,18,,2-Dec-18,,,IEEE,IEEE Conferences
Document clustering using ant colony algorithm,使用蟻群算法的文檔聚類,E. Nagarajan; K. Saritha; G. MadhuGayathri,NA; NA; NA,2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC),19-Oct-17,2017,,,459,463,"In unusual years, merit to the pick up the novel of the documents everywhere the World Wide Web and libraries, has duty bound us to crowd the documents. The main desire of clustering is to group the documents based on the semantics. Document clustering is a well-known application in data mining. It contains applications a well-known as extracting the documents consisting of description of similar semantics. The concept of finding similar semantics helps in clustering the documents. Clustering is a move and intensely complicated research area for obtaining the relevant flea in ear in late applications. This campaign boot by the same token be experienced as unsupervised technique. It deals mutually with more number of documents. Clustering is also represented a tool that can be used to accumulate homogeneous type of semantic documents. The Ant algorithms were stimulated by observations of trustworthy ant colonies. In this algorithm, the force of ant is around random. Because of the randomness reaction of the ant, the efficiency of algorithm can be increased. The ant uses the work of genius as picking up and dropping down the documents based similarity outlay of the documents. This outlay is obtained by the cosine similarity of the documents per the inverse document frequency and normalized order frequency. Then the clustered documents are compared mutually the contrasting clustering techniques.",,978-1-5090-6400-7,10.1109/ICBDACI.2017.8070884,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070884,Normalized order frequency;Inverse document frequency;Cosine similarity;Ant Colony Algorithm,Handheld computers;Big Data;Computational intelligence,ant colony optimisation;data mining;document handling;Internet;pattern clustering,document clustering;similar semantics;semantic documents;inverse document frequency;clustered documents;data mining;unsupervised technique;ant colony algorithm;cosine similarity;normalized order frequency,,,,8,,19-Oct-17,,,IEEE,IEEE Conferences
Hierarchical document clustering based on cosine similarity measure,基於餘弦相似度度量的分層文檔聚類,S. K. Popat; P. B. Deshmukh; V. A. Metre,"D.Y. Patil College of Engineering, Akurdi, Pune, MH, India; D.Y. Patil College of Engineering, Akurdi, Pune, MH, India; D.Y. Patil College of Engineering, Akurdi, Pune, MH, India",2017 1st International Conference on Intelligent Systems and Information Management (ICISIM),30-Nov-17,2017,,,153,159,"Clustering is one of the prime topics in data mining. Clustering partitions the data and classifies the data into meaningful subgroups. Document clustering is a set of the document into groups such that two groups show different characteristics with respect to likeness. In this paper, an experimental exploration of similarity based method, HSC for measuring the similarity between data objects particularly text documents is introduced. It also provides an algorithm which has an incremental approach and evaluates cluster likeness between documents that leads to much improved results over other traditional methods. It also focuses on the selection of appropriate similarity measure for analyzing similarity between the documents.",,978-1-5090-4264-7,10.1109/ICISIM.2017.8122166,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122166,Cluster;Document cluster;Similarity,Clustering algorithms;Weight measurement;Algorithm design and analysis;Classification algorithms;Size measurement;Text categorization,data mining;pattern clustering;text analysis,hierarchical document clustering;cosine similarity measure;prime topics;data mining;clustering partitions;meaningful subgroups;experimental exploration;similarity based method;cluster likeness;appropriate similarity measure;text documents;similarity analysis;data objects;incremental approach,,5,,19,,30-Nov-17,,,IEEE,IEEE Conferences
Effectiveness and Efficiency for Document Clustering in Biomedicine,生物醫學文獻聚類的有效性和效率,K. Seki; M. S. Ortiz; J. Mostafa,"Konan University,Faculty of Intelligence and Informatics,Kobe,Japan; Carolina Health Informatics Program, University of North Carolina,Chapel Hill,USA; School of Information and Library Science, University of North Carolina,Chapel Hill,USA",2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),6-Feb-20,2019,,,1620,1623,"It is crucial for biomedical information retrieval, or clinical decision support in particular, to discover relevant biomedical/clinical information buried in scientific publications. At present, typical search interface is based on keywords as queries and returns a ranked list of documents, which is suited for finding simple factoids but not ideal for more complex information needs required for clinical decision support. A search interface deemed more suitable for this kind of tasks is cluster-based browsing, where retrieved documents are topically grouped for more intuitive exploration. To adopt this model, however, one needs to consider not only the effectiveness but also the efficiency of clustering framework as clustering is a computationally costly operation. As a first step toward a cluster-based browsing information exploration, this paper empirically studies representative feature selection/extraction methods and clustering algorithms for their effectiveness and efficiency.",,978-1-7281-1867-3,10.1109/BIBM47256.2019.8983328,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983328,information retrieval;document representation;clustering;effectiveness;efficiency,,decision support systems;document handling;feature extraction;feature selection;information retrieval;medical information systems;pattern clustering;user interfaces,scientific publications;search interface;clinical decision support;cluster-based browsing;documents retrieval;information exploration;clustering algorithms;document clustering;biomedical information retrieval;feature selection;feature extraction,,,,14,,6-Feb-20,,,IEEE,IEEE Conferences
Fuzzy?Clustering?of Text?Documents?Using Na?ve Bayesian Concept,使用樸素貝葉斯概念對文本進行模糊聚類,R. S. Roy; D. Toshniwal,"Indian Inst. of Technol. Roorkee, Roorkee, India; Indian Inst. of Technol. Roorkee, Roorkee, India","2010 International Conference on Recent Trends in Information, Telecommunication and Computing",6-May-10,2010,,,55,59,"Clustering organizes text in an unsupervised fashion. In this paper, we propose an algorithm for the fuzzy clustering of text documents using the naive Bayesian concept. Fuzzy clustering implies that the text documents are assigned to multiple clusters, ranked in descending order of probability. The Vector Space Model is used to represent our dataset as a term-weight matrix. In any natural language, semantically linked terms tend to co-occur in documents. Hence, the co-occurrences of pairs of terms in the term-weight matrix are observed. This information is used to build a term-cluster matrix where each term may belong to multiple clusters. The naive Bayesian concept is used to uniquely assign each term to a single term-cluster. The documents are assigned to multiple clusters using mean computations. The proposed algorithm has been validated using benchmark datasets available on the Internet. Our results show that the proposed scheme has a significantly better running time as compared to traditional algorithms.",,978-1-4244-5957-5,10.1109/ITC.2010.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460613,Text Clustering;Co-occurrence Probability;Naive Bayesian Concept;Cluster Conditional Independence;Fuzzy Clustering,Bayesian methods;Clustering algorithms;Probability;Databases;Current measurement;Position measurement;Telecommunication computing;Natural languages;Internet;Writing,Bayes methods;document handling;fuzzy set theory;Internet;matrix algebra;pattern clustering;text analysis,fuzzy clustering;text documents;Naive Bayesian concept;vector space model;term-weight matrix;natural language;benchmark datasets;Internet,,1,,5,,6-May-10,,,IEEE,IEEE Conferences
Relevance feedback versus web search document clustering,相關性反饋與Web搜索文檔聚類,M. Alam; K. Sadaf,"Dept. of Computer Science, JMI, New Delhi, INDIA; Dept. of Computer Science, JMI, New Delhi, INDIA",2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom),4-May-15,2015,,,1665,1669,"The performance of an IR system is deteriorated by factors including short and vague queries put up by the users, ever increasing volume of documents on the web, users not knowing their exact information need etc. Relevance feedback (RF) and web search document clustering are techniques to improve the performance of an Information Retrieval (IR) system. Relevance feedback provides a method to get more relevant search result from an IR system using documents that are marked relevant by the user as a feedback to reformulate query. This refined query is then used to retrieve the documents. In document clustering approach, the search result is divided into thematic groups where documents of one group are similar to each other and dissimilar to the documents of other groups. This paper presents a report on the effectiveness of relevance feedback technique as compared to document clustering in context of web information retrieval and why document clustering is the most preferred approach.",,978-9-3805-4416-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100530,Relevance Feedback;Document Clustering;IR;web search,Information retrieval;Engines,document handling;Internet;pattern clustering;query formulation;query processing;relevance feedback,Web search document clustering approach;IR system;Web documents;information need;query reformulation;relevance feedback technique;Web information retrieval,,,,17,,4-May-15,,,IEEE,IEEE Conferences
Document clustering optimization with synonym dictionary check function,具有同義詞詞典檢查功能的文檔聚類優化,A. Amalia; M. S. Lydia; S. D. Fadilla; M. Huda; D. Gunawan,"Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Faculty of Computer Science and Information Technology, Universitas Sumatera Utara, Medan, Indonesia",2017 International Conference on Electrical Engineering and Informatics (ICELTICs),11-Jan-18,2017,,,286,291,"Document clustering is an unsupervised process to group documents into the suitable category based on document's similarity. Before being clustered, generally set of documents are transformed into bag-of-words that was produced by preprocessing steps. Preprocessing is the initial steps that usually composed by case folding, tokenizing, stemming and filtering processes. But this traditionally bag-of-words representation have many drawbacks, such as the identification of synonym terms, high dimension and ineffective computation time. To resolve these problems, we propose the addition of functions in preprocessing stage. These additional functions are LSA, TF-IDF and the combination of TF-IDF/LSA function. Furthermore, these functions will be combined with synonym dictionary check function. The research uses 100 computer science documents in Bahasa Indonesia. The clustering process was implemented with K- Means clustering algorithm. According to the results, the addition of these functions will improved cluster accuracy compared to traditional preprocessing steps in document clustering. The experiment results also showed that the cluster accuracy was increasing while these functions were combined with synonym dictionary check function. The best accuracy achieved by the combination of TF-IDF/LSA with synonym dictionary check function in preprocessing stage. On the other hand, the addition of synonym dictionary check function has improved the completion time for clustering process as well. However, the addition of other functions such as TF-IDF, LSA or TF-IDF will cause longer completion time for clustering process.",,978-1-5386-2934-5,10.1109/ICELTICS.2017.8253285,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253285,document clustering;preprocessing;bag-of-words;document representation;synonym,Dictionaries;Semantics;Clustering algorithms;Electrical engineering;Informatics;Filtering;Matrix decomposition,computer science;document handling;natural language processing;pattern clustering;text analysis,clustering process;cluster accuracy;synonym dictionary check function;preprocessing stage;document clustering optimization;group documents;TF-IDF/LSA function;computer science documents;unsupervised process;document similarity;Bahasa Indonesia;k- means clustering algorithm,,1,,18,,11-Jan-18,,,IEEE,IEEE Conferences
WAF-based document clustering algorithm,基於WAF的文檔聚類算法,Yang Luo; Guang Chen; Yongtian Zhang,"School of Information and Communication, Beijing University of Posts and Telecommunications, China; School of Information and Communication, Beijing University of Posts and Telecommunications, China; School of Information and Communication, Beijing University of Posts and Telecommunications, China",Proceedings of 2011 International Conference on Computer Science and Network Technology,12-Apr-12,2011,1,,14,16,"This paper proposes a novel document cluster algorithm based on Word Activation Forces (WAFs), a type of newly presented statistics.. A matrix of WAFs captures the information of terms occurrence and co-occurrence in a document, reflecting the underlying semantics that have not ever been considered from the current document representations. Its main consideration is that the same word in different documents may form disparate relation net which can be used to gain the similarities of the documents. Experimental evaluations on the dataset of the CLP2010 show that our proposed method is efficient and accurate for documents clustering.",,978-1-4577-1587-7,10.1109/ICCSNT.2011.6181899,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181899,Document representation;Document clustering;WAF,Clustering algorithms,pattern clustering;statistical analysis;text analysis;word processing,WAF-based document clustering algorithm;word activation forces;document cooccurrence;document representations;CLP2010;WAF matrix,,,,8,,12-Apr-12,,,IEEE,IEEE Conferences
Application of xie-beni-type validity index to fuzzy co-clustering models based on cluster aggregation and pseudo-cluster-center estimation,謝貝尼型有效性指標在基於聚類聚集和偽聚類中心估計的模糊聚類模型中的應用,M. Muranishi; K. Honda; A. Notsu,"Graduate School of Engineering, Osaka Prefecture University, Sakai, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, Japan",2014 14th International Conference on Intelligent Systems Design and Applications,26-Mar-15,2014,,,34,38,"In k-Means-type clustering, cluster validation is an important problem, where the most plausible solution supported by several validity indices is selected from results with various parameter settings. Xie-Beni index is a popular validity index in FCM clustering, which measures the plausibility level of fuzzy partitions by considering partition quality and geometrical features. In this research, the applicability of a Xie-Beni-type co-cluster validity index is studied with several fuzzy co-clustering models such as cluster aggregation models (FCCM and Fuzzy CoDoK) and pseudo-cluster-center models (FSKWIC and SCAD2), and is demonstrated in a document clustering application.",2164-7151,978-1-4799-7938-7,10.1109/ISDA.2014.7066274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066274,co-clustering;document clustering;validity index;Xie-Beni-type validity index,Indexes;Standards;Estimation;Optimization;Artificial intelligence,document handling;fuzzy set theory;pattern clustering,pseudo-cluster-center estimation;Xie-Beni-type co-cluster validity index;fuzzy co-clustering models;cluster aggregation models;pseudo-cluster-center models;FCCM;Fuzzy CoDoK;FSKWIC;SCAD2;document clustering,,1,,12,,26-Mar-15,,,IEEE,IEEE Conferences
One way to cluster documents by meaning using semantic-ball,一種使用語義球通過含義對文檔進行聚類的方法,C. Seo,Korea,2012 IEEE 13th International Conference on Information Reuse & Integration (IRI),17-Sep-12,2012,,,724,726,"Semantic technology dealing with meaning is difficult to process meaning as intended by people due to it is flexibility, instability, and even uncertainty. Moreover, there is no explicit definition of how to sense meaning and expression by computer. To overcome these barriers, this paper suggests Semantic-ball, which is a co-occurrence of different word-set, and based on assumptions that Semantic-ball has emergence self-organization attribution, group relationship not pair relationship, and specificity. Therefore this paper focuses on finding Semantic-ball to solve semantic problems. Approach to find Semantic-ball is based on conventional statistical ways, the FP Growth algorithm(Frequency Pattern Growth algorithm)[1], and the bottom-up method. While the previous researches have been based on the definition which used to pin down meaning to solve semantic problems, this paper does not bring any definition. Moreover, Semantic-ball is made only using computing ability automatically. Thus, Semantic-ball helps to sense word and cluster documents by meaning.",,978-1-4673-2284-3,10.1109/IRI.2012.6303082,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303082,Semantic-ball;Clustering documents by meaning;specificity;FP Growth Algorithm;without any definition;Emergence Self-organization,Semantics;Clustering algorithms;Hidden Markov models;Statistical analysis;Computers;Frequency domain analysis;Uncertainty,document handling;pattern clustering;set theory;statistical analysis,document clustering;semantic-ball;meaning processing;self-organization attribution group relationship;statistical analysis;FP growth algorithm;frequency pattern growth algorithm;bottom-up method;automatic computing ability;word set sensing,,,,5,,17-Sep-12,,,IEEE,IEEE Conferences
An effective implementation of Social Spider Optimization for text document clustering using single cluster approach,使用單一聚類方法對文本文檔聚類有效實施Social Spider Optimization,T. R. Chandran; A. V. Reddy; B. Janet,"Dept. of Computer Applications, NIT, Trichy, India; Dept. of Computer Applications, NIT, Trichy, India; Dept. of Computer Applications, NIT, Trichy, India",2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT),27-Sep-18,2018,,,508,511,"Since the last decade, researchers have been concentrating on high quality clustering methods for the effective analysis of the growing data. Algorithms inspired by nature have become part of a widely used class of computing methods, because of their ability to adjust to variety of conditions. These nature inspired algorithms have been frequently used for solving complex, real-world optimization problems. In recent years, they have emerged as powerful optimization techniques. These algorithms inspired by the cooperative behavior of social animals have the ability to provide better solutions for clustering problems. Social Spider Optimization (SSO) is a population based stochastic optimization algorithm. It has been used to solve many complicated optimization problems. It simulates the behavior of social spiders. In this paper, a new and more effective implementation of SSO (ESSOSC) for solving text document clustering problem is specified by using single cluster implementation for each spider.",,978-1-5386-1974-2,10.1109/ICICCT.2018.8473332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473332,Nature inspired algorithms;Text document Clustering;Social spider optimization;Social Spiders;K-means,Clustering algorithms;Heuristic algorithms;Optimization;Classification algorithms;Sociology;Statistics;Patents,evolutionary computation;optimisation;pattern clustering;stochastic processes;text analysis,single cluster approach;high quality clustering methods;computing methods;real-world optimization problems;social animals;clustering problems;Social Spider Optimization;stochastic optimization algorithm;complicated optimization problems;social spiders;text document clustering problem;complex world optimization problems,,,,20,,27-Sep-18,,,IEEE,IEEE Conferences
Topic detection for emergency events based on FCM document clustering,基於FCM文檔聚類的突發事件主題檢測,Tian Gao; Junping Du; Su Wang; Liping Chen,"Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China",2010 3rd IEEE International Conference on Broadband Network and Multimedia Technology (IC-BNMT),31-Jan-11,2010,,,1181,1185,"This paper discusses the usage of document clustering methods for topic detection of emergencies. Its main contribution is to apply the named entity of event-based framework to extract the feature terms of Web documents, exploit the TF-IDF method to weight the Web document characteristics of emergencies, and finally detect the hot topics through the FCM clustering algorithm. This method can reduce the redundancy feature terms of Web documents for emergencies effectively, and explore the internal structure and connections of the original data. It can also decrease the feature dimensions to improve the intelligibility of document data and the accuracy of topic detection to a large extent. Experimental results show that the FCM clustering method can achieve the topic cluster aggregation in the Web document sets, receive excepted topics of the Internet information sources timely, and monitor its related reports.",,978-1-4244-6772-3,10.1109/ICBNMT.2010.5705276,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705276,document clustering;Topic Detection;Emergency events;Feature extraction,Cybernetics,document handling;emergency services;feature extraction;Internet;pattern clustering,topic detection;emergency events;FCM document clustering;Web documents;TF-IDF method;hot topics;redundancy feature terms;internal structure;internal connections;Internet information sources;feature extraction,,1,,8,,31-Jan-11,,,IEEE,IEEE Conferences
Multi-document summarization using sentence clustering,使用句子聚類的多文檔摘要,V. K. Gupta; T. J. Siddiqui,"Samsung India Software Operation, Bangalore, India; Dept. of Electronics & Communication, University of Allahabad, Allahabad, India",2012 4th International Conference on Intelligent Human Computer Interaction (IHCI),21-Mar-13,2012,,,1,5,"This paper presents an approach to query focused multi document summarization by combining single document summary using sentence clustering. Both syntactic and semantic similarity between sentences is used for clustering. Single document summary is generated using document feature, sentence reference index feature, location feature and concept similarity feature. Sentences from single document summaries are clustered and top most sentences from each cluster are used for creating multi-document summary. We observed an average F-measure of 0.33774 on DUC 2002 multi-document dataset, which is comparable to three best performing systems reported on the same dataset.",,978-1-4673-4369-5,10.1109/IHCI.2012.6481826,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481826,Multi document summarization;sentence clustering method;feature extraction;DUC-2002,Semantics;Feature extraction;Indexes;Syntactics;Cancer;Vectors;Information retrieval,document handling;grammars;pattern clustering;pattern matching,multidocument summarization;sentence clustering;single document summary;semantic similarity;syntactic similarity;document feature;sentence reference index feature;location feature;concept similarity feature;F-measure;multidocument dataset,,13,,13,,21-Mar-13,,,IEEE,IEEE Conferences
Dynamic peer-to-peer distributed document clustering and cluster summarization,動態對等分佈式文檔聚類和聚類匯總,S. Meena,"Department of Information Technology, Rajalakshmi Engineering College, Chennai-602105, India",International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011),2-Feb-12,2011,,,815,819,"The main objective of this paper is to provide cluster summarization of huge text document. Mining process includes the sharing of large scale amount of data from various sources, which gets concluded at the mined data. In distributed data mining, adopting aflat node distribution model can affect scalability, modularity, flexibility which are being overcome by using dynamic peer to peer document clustering and cluster summarization. The Dynamic P2P document clustering and cluster summarization (DP2PCS) architecture is based upon bonus words and stigma words. For document clustering applications, the system summarizes the distributed document clusters using a distributed key-phrase extraction algorithm, thus providing interpretation of the clusters. Document summarization is used for fast information retrieval in less time. Compared to existing system the dynamic nature of proposed system facilitates a scalable cluster wherein the peers may join or leave the group at will. The summarization process on an average reduces the original documents content by 63 percentage based on the word count.",,978-9-38043-000-3,10.1049/cp.2011.0478,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143427,Distributed data mining;distributed document clustering;hierarchical peer-to-peer networks;document summarization,,data mining;information retrieval;pattern clustering;peer-to-peer computing;text analysis,dynamic peer-to-peer distributed document clustering;text document cluster summarization;distributed data mining process;flat node distribution model;bonus words;stigma words;distributed key-phrase extraction algorithm;information retrieval,,,,,,2-Feb-12,,,IET,IET Conferences
Font adaptive word indexing of modern printed documents,字體自適應詞索引現代印刷文檔,S. Marinai; E. Marino; G. Soda,"Dipt. di Sistemi e Inf., Univ. di Firenze, Italy; Dipt. di Sistemi e Inf., Univ. di Firenze, Italy; Dipt. di Sistemi e Inf., Univ. di Firenze, Italy",IEEE Transactions on Pattern Analysis and Machine Intelligence,19-Jun-06,2006,28,8,1187,1199,"We propose an approach for the word-level indexing of modern printed documents which are difficult to recognize using current OCR engines. By means of word-level indexing, it is possible to retrieve the position of words in a document, enabling queries involving proximity of terms. Web search engines implement this kind of indexing, allowing users to retrieve Web pages on the basis of their textual content. Nowadays, digital libraries hold collections of digitized documents that can be retrieved either by browsing the document images or relying on appropriate metadata assembled by domain experts. Word indexing tools would therefore increase the access to these collections. The proposed system is designed to index homogeneous document collections by automatically adapting to different languages and font styles without relying on OCR engines for character recognition. The approach is based on three main ideas: the use of self organizing maps (SOM) to perform unsupervised character clustering, the definition of one suitable vector-based word representation whose size depends on the word aspect-ratio, and the run-time alignment of the query word with indexed words to deal with broken and touching characters. The most appropriate applications are for processing modern printed documents (17th to 19th centuries) where current OCR engines are less accurate. Our experimental analysis addresses six data sets containing documents ranging from books of the 17th century to contemporary journals",1939-3539,,10.1109/TPAMI.2006.162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642655,Clustering;digital libraries;document image retrieval;heuristic oversegmentation;holistic word representation;modern documents;self organizing map.,Indexing;Optical character recognition software;Web search;Search engines;Content based retrieval;Web pages;Software libraries;Image retrieval;Assembly;Character recognition,character recognition;character sets;digital libraries;document image processing;image retrieval;indexing;pattern clustering;self-organising feature maps;word processing,font adaptive word-level indexing;modern printed documents;OCR engines;word position retrieval;Web search engines;Web pages;textual content;digital libraries;digitized documents;document images;domain experts;index homogeneous document collections;font styles;character recognition;self organizing maps;unsupervised character clustering;vector-based word representation;word aspect-ratio;run-time alignment;query word,"Abstracting and Indexing as Topic;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Libraries, Digital;Natural Language Processing;Pattern Recognition, Automated;Publishing;Reproducibility of Results;Semantics;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique;User-Computer Interface;Vocabulary, Controlled",33,,31,,19-Jun-06,,,IEEE,IEEE Journals
Efficient pre-processing for enhanced semantics based distributed document clustering,基於增強語義的分佈式文檔聚類的高效預處理,S. Mahajan; N. Shah,"Institute of Computer Science, M.E.T., Bandra (west), Mumbai, India; IT Department, DJSCE, Vile Parle (West), Mumbai, India",2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),31-Oct-16,2016,,,338,343,"Document clustering, one of the traditional data mining techniques, is an unsupervised learning paradigm where clustering methods try to identify inherent groupings of the text documents, so that a set of clusters is produced in which clusters exhibit high intra-cluster similarity and low inter-cluster similarity. The importance of document clustering emerges from the massive volumes of textual documents created. For distributed document clustering, we experimented on Hadoop and MapReduce open source platforms. We tested on two datasets consisting of 20000 (20-NewsGroup) and 21578 (Reuters) documents. We focused on efficient pre-processing wherein we applied syntactic and semantics tagging to the dataset. Here, we have suggested syntactic and semantic parsing i.e. POS tagging, and word sense disambiguation for better understanding of synonymous words and hyponymy words using WordNet. Porter's Stemming algorithm is revised and compared with traditional algorithm. We also suggest different order of pre-processing steps. We performed experiments to generate various clusters with different count of iterations for refining the clusters. Also, for monitoring the scale-up of Hadoop we added nodes and compared the time taken for clustering.",,978-9-3805-4421-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724283,Document Clustering;Semantic Document Clustering;Pre-processing for document clustering;Distributed Document Clustering,Semantics;Entropy;Tagging;Clustering algorithms;Algorithm design and analysis;Syntactics;Databases,data mining;natural language processing;parallel processing;pattern clustering;public domain software;text analysis;unsupervised learning,enhanced semantics based distributed document clustering;unsupervised learning;intracluster similarity;intercluster similarity;textual documents;Hadoop platform;MapReduce open source platform;syntactic tagging;semantic parsing;semantic tagging;syntactic parsing;POS tagging;word sense disambiguation;synonymous words;hyponymy words;WordNet;Porter's stemming algorithm;data mining,,1,,16,,31-Oct-16,,,IEEE,IEEE Conferences
Optimization of Document Clustering Using UNL Document Vector Generation and Swarm Intelligence,使用UNL文檔向量生成和群智能優化文檔聚類,V. A. Metre; S. K. Popat; P. B. Deshmukh,"D. Y. Patil, College of Engineering, Department of Computer Engg, Akurdi, Pune, India; D. Y. Patil, College of Engineering, Department of Computer Engg, Akurdi, Pune, India; D. Y. Patil, College of Engineering, Department of Computer Engg, Akurdi, Pune, India","2017 International Conference on Computing, Communication, Control and Automation (ICCUBEA)",13-Sep-18,2017,,,1,6,"World Wide Web, the largest shared information source has a remarkable amount of text documents, which makes the document clustering as one of the ideal areas of research these days. To navigate, summarize and retrieve the information effectively, document clustering can facilitate the automatic document organization. To attain the basic objective of document clustering, i.e. clustered documents should have high intra-similarity rate and low inter-similarity rate to other clusters, several techniques are proposed. The basic categorization of document clustering techniques is done into two: partitional and hierarchical techniques. However, the partitional clustering techniques are extremely popular in document clustering area. The K-means inspired algorithms are the most efficient and fast partitional clustering algorithms, which seeks to divide documents collection into separate groups to look for the optimized value of clustering. Cluster grouping techniques frequently experience scalability, high dimensionality, and inaccurate cluster labels issues. This paper modifies the clustering scheme, using Universal Networking Language (UNL) generative feature vector, Subtractive Clustering approach combined with Boundary Restricted Particle Swarm Optimization (BR-APSO) algorithm for efficient document clustering. The proposed method not only compares but analyses the existing document clustering methodologies and improves entropy and purity rates.",,978-1-5386-4008-1,10.1109/ICCUBEA.2017.8463860,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463860,Text clustering;Vector Space Model;Particle Swarm Optimization;Universal Networking Language,Clustering algorithms;Partitioning algorithms;Semantics;Mathematical model;Particle swarm optimization;Dimensionality reduction;Matrix converters,document handling;entropy;Internet;particle swarm optimisation;pattern clustering;swarm intelligence,UNL document vector generation;text documents;automatic document organization;document clustering techniques;Swarm Intelligence;partitional clustering algorithms;subtractive clustering approach;Universal Networking Language;Boundary Restricted Particle Swarm Optimization;World Wide Web,,2,,22,,13-Sep-18,,,IEEE,IEEE Conferences
Single-Document Summarization Using Sentence Embeddings and K-Means Clustering,使用句子嵌入和K均值聚類的單文檔摘要,S. Agarwal; N. K. Singh; P. Meel,"Department of Computer Science, Delhi Technological University, Delhi, 110042; Department of Computer Science, Delhi Technological University, Delhi, 110042; Department of Information Technology, Delhi Technological University, Delhi, 110042","2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)",1-Jul-19,2018,,,162,165,"This paper proposes a novel method for extractive single document summarization using K-Means clustering and Sentence Embeddings. Sentence embeddings were processed by K-Means algorithm into a number of clusters depending on the required summary size. Sentences in a given cluster contained similar information, and the most appropriate sentence was picked and included in the summary for each cluster by a ridge regression sentence scoring model. Experimental ROUGE score evaluation of summaries of various lengths for the DUC 2001 dataset demonstrated the effectiveness of the approach.",,978-1-5386-4119-4,10.1109/ICACCCN.2018.8748762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8748762,Single document summarization;sentence embeddings;k-means clustering,Clustering algorithms;Feature extraction;Measurement;Natural language processing;Machine learning;Rocks;Data mining,document handling;learning (artificial intelligence);pattern clustering,single-document summarization;sentence embeddings;ridge regression sentence scoring model;DUC 2001 dataset;ROUGE score evaluation;K-means algorithm;single document summarization,,,,16,,1-Jul-19,,,IEEE,IEEE Conferences
Novel Hybrid Document Clustering Algorithm Based on Ant Colony and Agglomerate,基於蟻群算法的混合文檔聚類新算法,X. Wang; J. Shen; H. Tang,"Inst. of Comput. Applic. Technol., Hangzhou Dianzi Univ., Hangzhou, China; Inst. of Comput. Applic. Technol., Hangzhou Dianzi Univ., Hangzhou, China; Inst. of Comput. Applic. Technol., Hangzhou Dianzi Univ., Hangzhou, China",2009 Second International Symposium on Knowledge Acquisition and Modeling,28-Dec-09,2009,3,,65,68,"In this paper, ant colony algorithm was improved from two aspects, then a novel hybrid ant colony and agglomerate document clustering algorithm, hybrid-AC&A, has been proposed based on ant colony model and agglomerate clustering algorithms. Firstly, Compact algorithm was applied while ant dropping its load. Secondly, evaluate function based schedule algorithm was applied while ant obtains load. Finally, agglomerate clustering algorithm was integrated into the iteration procedure of ant colony clustering algorithm. The performance of Hybrid-AC&A is compared with other clustering methods, the experimental results denote that the proposed algorithm not only inherits the intrinsic advantages of ant colony model clustering algorithm, but also improves the aspect of time efficiency. Computational result on real documents collection shows it is much more efficient than other mentioned algorithms.",,978-0-7695-3888-4,10.1109/KAM.2009.182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362451,Ant Colony;Agglomerate;Document Clustering,Clustering algorithms;Clustering methods;Biological system modeling;Ant colony optimization;Knowledge acquisition;Computer applications;Algorithm design and analysis;Scattering;Density measurement;Convergence,document handling;optimisation,hybrid document clustering algorithm;ant colony algorithm;agglomerate clustering algorithm;function based schedule algorithm,,2,,10,,28-Dec-09,,,IEEE,IEEE Conferences
An ontology-based and domain specific clustering methodology for financial documents,基於本體的，特定領域的財務文檔聚類方法,C. Kulathunga; D. D. Karunaratne,"University of Colombo School of Computing, Sri Lanka; University of Colombo School of Computing, Sri Lanka",2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer),15-Jan-18,2017,,,1,8,"Financial documents play an important role in modern financial analysis and information retrieval tasks. In order to accomplish various investigational needs, financial organizations continuously search for accurate and meaningful unsupervised document classification techniques. Nevertheless, unsupervised document categorization or document clustering is a challenging problem studied by many scientists. Incorporating semantic knowledge from an ontology into document clustering has been extensively studied and it has provided enhanced clustering performances. The incorporated semantic knowledge is generally used for identifying the correct meanings of the ambiguous words in the documents. Most of the proposed methodologies were experimented on general document datasets and most of the few available domain specific clustering studies were constrained to specific domains where complete domain ontologies are available. Although financial domain has several domain ontologies, none of them are complete and suitable for semantic document clustering. In this context, our study proposes a document clustering methodology for financial documents which adapts WordNet ontology to the financial domain to serve as an external knowledge source. This study empirically shows that nouns are relatively prevalent and more important for document clustering rather than other terms in a document. Afterwards, a subset of nouns is identified as most important for the clustering, based on their frequency distribution within the main noun list. We developed a word sense disambiguation technique which uses ontological knowledge for noun disambiguation. Finally, nouns in each document are disambiguated with the proposed word sense disambiguation technique, associated with tf-idf weights and clustered. On the basis of the empirical results of this research, it can be concluded that the proposed methodology can significantly enhance the clustering performance compared to no disambiguation and pure WordNet based disambiguation approaches.",2472-7598,978-1-5386-2444-9,10.1109/ICTER.2017.8257786,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257786,Financial document clustering;WordNet based clustering;Resnik similarity;Word sense disambiguation,Ontologies;Biomedical measurement;Semantics;Feature extraction;Clustering methods;Length measurement,document handling;financial data processing;information retrieval;ontologies (artificial intelligence);pattern clustering,complete domain ontologies;semantic document clustering;financial documents;word sense disambiguation technique;domain specific clustering methodology;financial organizations;unsupervised document categorization;financial analysis;information retrieval;semantic knowledge;domain specific clustering;unsupervised document classification techniques,,1,,26,,15-Jan-18,,,IEEE,IEEE Conferences
Document clustering using GIS visualizing and EM clustering method,使用GIS可視化和EM聚類方法進行文檔聚類,T. Dogdas; S. Akyokus,"Dogus University, Istanbul, Turkey; Dogus University, Istanbul, Turkey",2013 IEEE INISTA,15-Aug-13,2013,,,1,4,This paper uses expectation-maximization clustering algorithm and a simple multidimensional projection method for visualization and data reduction. The multidimensional data is projected into a 2D Cartesian coordinate system. We run EM and K-Means algorithms on the transformed data. The system uses Microsoft Spatial Data Base Engine as a GIS tool for visualization. We used Expectation-Maximization (EM) and K-Means clustering algorithms of the Microsoft Analysis Services. The simple multidimensional projection method used in this paper tries to preserve the similarity relationships in original datasets.,,978-1-4799-0661-1,10.1109/INISTA.2013.6577647,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577647,GIS;Clustering;Performance optimization,Iris;Clustering algorithms;Data visualization;Algorithm design and analysis;Geographic information systems;Spatial databases;Data mining,data reduction;data visualisation;document handling;expectation-maximisation algorithm;geographic information systems;pattern clustering,document clustering;GIS visualizing method;EM clustering method;expectation-maximization clustering algorithm;multidimensional data projection method;data reduction;2D Cartesian coordinate system;k-means clustering algorithms;Microsoft spatial data base engine;similarity relationship preservation;geographic information system;Microsoft analysis services,,,,14,,15-Aug-13,,,IEEE,IEEE Conferences
A semantic ontology-based document organizer to cluster elearning documents,基於語義本體的文檔組織者，對學習文檔進行聚類,S. Alaee; F. Taghiyareh,"Electrical and Computer Engineering Department, University of Tehran, Tehran, Iran; Electrical and Computer Engineering Department, University of Tehran, Tehran, Iran",2016 Second International Conference on Web Research (ICWR),23-Jun-16,2016,,,1,7,"Document clustering is a useful technique to organize large sets of documents into meaningful groups. The usefulness is appreciated by labeling the clusters with relevant words that describe their associated documents. The traditional approach for document clustering, i.e. bag-of-words representation, often ignores the semantic relations between terms. Hence, ontology-based document clustering is proposed. In the context of e-Learning, the richer annotation of learning materials, via the use of appropriate ontologies is a way to deal with the reusability and remix of learning objects. Through providing a semantic infrastructure that will explicitly declare the semantics and relations between concepts used in labeling learning objects, the desired quality in the learning offer can be ensured. This paper proposes an ontology-based document clustering approach based on two-step clustering algorithm and compares its performance with the conventional clustering. Ontology is introduced through defining a weighting scheme that integrates traditional scheme, i.e. co-occurrences of words, with weights of relations between words in ontology. Our experimental evaluations are performed on ICVL (International Conference on Virtual Learning) paper collection as dataset with e-Learning domain ontology as the background knowledge. The ontology was implemented by us through a different research. The results show that inclusion of ontology increases the clustering quality.",,978-1-5090-2166-6,10.1109/ICWR.2016.7498438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498438,Document Clustering;Ontology-based Clustering;eLearning;Ontology Generation;Semantic Relation;eLearning Concept,Ontologies;Semantics;Electronic learning;Syntactics;Clustering algorithms;Data mining;Natural language processing,computer aided instruction;ontologies (artificial intelligence);pattern clustering,semantic ontology-based document organizer;e-Learning document clustering;ontology-based document clustering;semantic infrastructure;labeling learning objects;two-step clustering algorithm;ICVL;international conference on virtual learning;e-Learning domain ontology,,1,,34,,23-Jun-16,,,IEEE,IEEE Conferences
Document clustering algorithm based on NMF and SVDD,基於NMF和SVDD的文檔聚類算法,Ziqiang Wang; Qingzhou Zhang; Xia Sun,"School of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; School of Information Science and Engineering, Henan University of Technology, Zhengzhou, China; School of Information Science and Engineering, Henan University of Technology, Zhengzhou, China","2010 Second International Conference on Communication Systems, Networks and Applications",30-Sep-10,2010,1,,192,195,"Document clustering is one of the most important research areas of data mining due to its wide application in many fields. To efficiently cope with this problem, a novel document clustering algorithm based on nonnegative matrix factorization (NMF) and support vector data description (SVDD) is proposed in this paper. Experimental results on two well-known document data sets demonstrate the effectiveness of the proposed document clustering algorithm.",,978-1-4244-7478-3,10.1109/ICCSNA.2010.5588684,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5588684,document clustering;nonnegative matrix factorization;support vector data description;data mining,Accuracy;Indexes;World Wide Web;Bioinformatics,computer software;data mining;document handling;matrix decomposition;pattern clustering;support vector machines,document clustering algorithm;NMF;SVDD;data mining;nonnegative matrix factorization;support vector data description;document data set,,,,11,,30-Sep-10,,,IEEE,IEEE Conferences
A GPU-based Harmony K-means Algorithm for document clustering,用於文檔聚類的基於GPU的Harmony K-means算法,Zhanchun Gao; Enxing Li; Yanjun Jiang,"Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, China",IET International Conference on Information Science and Control Engineering 2012 (ICISCE 2012),6-Mar-14,2012,,,1,4,"Document clustering is one of the most important tasks in text mining. In clustering algorithms, high-dimensional vector is usually used to represent a document which causes that the algorithms are often computationally expensive. On the other hand, Graphic Processing Unit (GPU) is increasingly important in parallel computing due to its powerful parallel capacity and high bandwidth. This paper implements a GPU-based Harmony K-means Algorithm (HKA) with NVIDIA's Compute Unified Device Architecture (CUDA), and uses it for document clustering. In our experiment, our GPU-based program can acquire a maximum 20 times speedup in contrast with CPU-based program.",,978-1-84919-641-3,10.1049/cp.2012.2426,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755805,document clustering;Harmony search;K-means;GPU;parallel computing,,data mining;document handling;graphics processing units;parallel architectures;parallel programming;pattern clustering;vectors,CUDA;compute unified device architecture;HKA;parallel computing;graphic processing unit;high-dimensional vector;text mining;document clustering;harmony k-means algorithm;GPU,,,,,,6-Mar-14,,,IET,IET Conferences
A Text Document Clustering Method Based on Weighted BERT Model,基於加權BERT模型的文本文檔聚類方法,Y. Li; J. Cai; J. Wang,"School of Information and Communication Engineering, Communication University of China,Beijing,China; Key Laboratory of Media Audio &Video (Communication University of China), Ministry of Education,Beijing,China; School of Information and Communication Engineering, Communication University of China,Beijing,China","2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",4-May-20,2020,1,,1426,1430,"Traditional text document clustering methods represent documents with uncontextualized word embeddings and vector space model, which neglect the polysemy and the semantic relation between words. This paper presents a novel text document clustering method to deal with these problems. Firstly, pre-trained language representation model Bidirectional Encoder Representations from Transformers (BERT) is utilized to generate sentence embeddings. Then, two sentence-level weighting schemes based on named entity are designed to enhance the performance. Finally, the k-means clustering algorithm is applied to find groups of similar documents. Experimental results on four datasets indicate that the proposed weighted method achieves higher accuracy than unweighted average method. Friedman tests conducted separately with F1 score and Adjusted Rand Index (ARI) values both validate better overall performance of our proposed method.",,978-1-7281-4390-3,10.1109/ITNEC48623.2020.9085059,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085059,text mining;document clustering;transformer;named entity,,document handling;pattern clustering;text analysis,ARI;Adjusted Rand Index;k-means clustering algorithm;Bidirectional Encoder Representations from Transformers;sentence-level weighting schemes;sentence embeddings;language representation model;vector space model;uncontextualized word embeddings;weighted BERT model;text document clustering method,,,,12,,4-May-20,,,IEEE,IEEE Conferences
Text Document Clustering: Issues and Challenges,文本文檔聚類：問題與挑戰,M. Afzali; S. Kumar,"Department of Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India; Department of Computer Science and Engineering, Manav Rachna International Institute of Research and Studies, Faridabad, India","2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",11-Oct-19,2019,,,263,268,"The text document clustering has become one of the foremost research areas in the field of data mining. The exponential growth of the textual data in today's digital world has bound the organizations to confront several challenges in organizing and browsing process. Although laborious work is done to make the process of clustering the text documents effortless, yet there are certain difficulties that emerge while dealing with a high volume of text documents. In this paper, the problems and challenges that come across while clustering a huge amount of text data are discussed and explained in detail.",,978-1-7281-0211-5,10.1109/COMITCon.2019.8862247,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862247,Text mining;data mining;text document clustering;partitional;Hierarchical;K-means clustering algorithm,Clustering algorithms;Sparse matrices;Data mining;Task analysis;Prediction algorithms;Partitioning algorithms;Big Data,data mining;pattern clustering;text analysis,text document clustering;data mining;textual data,,1,,33,,11-Oct-19,,,IEEE,IEEE Conferences
Effects of Similarity Metrics on Document Clustering,相似度指標對文檔聚類的影響,K. Taghva; R. Veni,"Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, NV, USA; Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, NV, USA",2010 Seventh International Conference on Information Technology: New Generations,1-Jul-10,2010,,,222,226,"Document clustering or unsupervised document classification is an automated process of grouping documents with similar content. A typical technique uses a similarity function to compare documents. In the literature, many similarity functions such as dot product or cosine measures are proposed for the comparison operator. In these papers, we evaluate the effects of many similarity functions on k-mean clustering algorithm. Based on our analysis, we conclude that Chi-Square works best for the document collection with efficiency around 80% followed by Canberra and Euclidean distances with 70%. The results also indicate that the distance metrics like Bray-Curtis, Variational and Trigonometric function didn't produce good results.",,978-1-4244-6271-1,10.1109/ITNG.2010.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501469,Clustering;distance function;k-mean;unsupervised learning,Clustering algorithms;Partitioning algorithms;Euclidean distance;Information retrieval;Information technology;Computer science;Convergence;Unsupervised learning,document handling;geometry;pattern clustering;software metrics,similarity metrics;document clustering;unsupervised document classification;dot product;cosine measures;similarity functions;k-mean clustering algorithm;Chi-Square;Euclidean distances;Canberra distances,,3,,10,,1-Jul-10,,,IEEE,IEEE Conferences
A New Hierarchical Document Clustering Method,一種新的分層文檔聚類方法,G. Kou; Y. Peng,"Sch. of Manage. & Econ., Univ. of Electron. Sci. & Technol. of China, Chengdu, China; Sch. of Manage. & Econ., Univ. of Electron. Sci. & Technol. of China, Chengdu, China","2009 Fifth International Joint Conference on INC, IMS and IDC",13-Nov-09,2009,,,1789,1792,"The advances in digital data collection and storage technologies during the last two decades allow companies and organizations store up huge amounts of electronic documents. Large collections of electronic text present opportunities and challenges. How to assist users to find the most relevant documents from vast text collections efficiently is one of the challenges. This study proposes a hierarchical clustering method to efficiently label documents that satisfy users' information needs. An experiment was conducted to examine the proposed method and the results shown that the clustering method is effective and efficient, in terms of both objective and subjective measures.",,978-1-4244-5209-5,10.1109/NCM.2009.126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331371,information retrieval;document clustering;hierachical algorithm;text mining;k-means,Clustering methods;Information retrieval;Clustering algorithms;Technology management;Content addressable storage;Text mining;Abstracts;Conference management;Unsupervised learning;Statistics,information needs;information retrieval;information storage;pattern clustering;text analysis,hierarchical document clustering method;digital data collection technologies;digital data storage technologies;electronic documents;document labeling;user information needs,,1,,23,,13-Nov-09,,,IEEE,IEEE Conferences
A novel approach to aggregation of web documents by semantic suffix tree and self organizing feature map methods,一種通過語義後綴樹和自組織特徵映射方法聚合Web文檔的新方法,R. Thilagavathy; R. Sabitha,"CSE Department, Sathyabama University, Chennai-119, India; IT Department, Jeppiaar Engineering College, Chennai-119, India",2017 Third International Conference on Science Technology Engineering & Management (ICONSTEM),18-Jan-18,2017,,,87,92,"The amount of information available on the internet became enormously high because of the rapid development in the electronic medium. Now days, managing and retrieving useful information from the web becomes the very tedious job. Web document clustering is a methodology which tries to group the available web documents in a meaningful way. There are lot of issues are identified in this area like, heterogeneous sources of information, no universal model, dynamicity of information, the huge volume of data, etc. most of the existing document clustering models follows the ?bag-of-words??document representation. The proposed model presents an incremental analysis of web documents based on semantic suffix tree which analyses web documents by means of concepts to improve the cluster quality. The concepts are evaluated at three levels, a sentence level analysis, a document level analysis and domain level analysis. The proposed model also uses self organizing feature map (SOFM) to group similar documents together in a cluster and organize similar clusters close together unlike most other clustering methods. A set of experiments using the proposed model on different data sets have been conducted. The experiment result shows a significant improvement in the cluster quality.",,978-1-5090-4855-7,10.1109/ICONSTEM.2017.8261262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261262,text mining;web document clustering;term frequency;conceptual term frequency;concept-based similarity measure;suffix tree clustering;SOFM,Semantics;Clustering algorithms;Frequency measurement;Analytical models;Text analysis;Text mining,data mining;information retrieval;Internet;pattern clustering;self-organising feature maps;text analysis;tree data structures,semantic suffix tree;web document clustering;bag-of-words document representation;cluster quality;sentence level analysis;document level analysis;domain level analysis;self organizing feature map;information retrieval;self-organizing feature map methods;document clustering models;web document analysis;SOFM,,,,23,,18-Jan-18,,,IEEE,IEEE Conferences
A Hybrid Strategy for Clustering Data Mining Documents,數據挖掘文檔聚類的混合策略,Y. Peng; G. Kou; Y. Shi; Z. Chen,"University of Nebraska at Omaha; Thomson Legal & Regulatory, R&D, 610 Opperman Drive, Eagan; 3Chinese Academy of Sciences; University of Nebraska at Omaha",Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06),15-Jan-07,2006,,,838,842,"With the increase in the number of electronic documents, it is hard to manually organize, analyze and present these documents efficiently. Document clustering, which automatically groups similar or related documents together, has been used in practical applications to understand the contents and structures of documents. Although a variety of methods and algorithms have been proposed, it is still a challenging task to generate meaningful document clusters. This paper uses an approach that combines quantitative and qualitative methods in order to create high-quality clusters for a collection of data mining and knowledge discovery (DMKD) publications. The quantitative method extracts a list of noun/noun phrases from the DMKD documents and uses an optimization procedure from CLUTO toolkit to assign documents to clusters. The qualitative method uses grounded theory to identify major categories of the documents to improve the comprehensibility of resultant clusters. The results demonstrate that the strategy produces more meaningful clusters than single-term k-way clustering algorithm in terms of internal metrics and human assessment",2375-9259,0-7695-2792-2,10.1109/ICDMW.2006.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4063742,Document clustering;Hard clustering;Soft;clustering;Optimization algorithm;Data mining;Grounded theory,Data mining;Clustering algorithms;Partitioning algorithms;Humans;Classification algorithms;Organizing;Information retrieval;Databases;Educational institutions;Information science,data mining;document handling;pattern clustering,data mining;document clustering;hard clustering;soft clustering;knowledge discovery;electronic documents;CLUTO toolkit;k-way clustering;internal metrics;human assessment,,1,,19,,15-Jan-07,,,IEEE,IEEE Conferences
Dual Fuzzy-Possibilistic Coclustering for Categorization of Documents,用於文檔分類的雙重模糊可能性聚類,W. Tjhi; L. Chen,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Div. of Inf. Eng., Nanyang Technol. Univ., Singapore",IEEE Transactions on Fuzzy Systems,27-May-09,2009,17,3,532,543,"In this paper, we develop a new soft model dual fuzzy-possibilistic coclustering (DFPC) for document categorization. The proposed model targets robustness to outliers and richer representations of coclusters. DFPC is inspired by an existing algorithm called possibilistic fuzzy C-means (PFCM) that hybridizes fuzzy and possibilistic clustering. It has been shown that PFCM can perform effectively for low-dimensional data clustering. To achieve our goal, we expand this existing idea by introducing a novel PFCM-like coclustering model. The new algorithm DFPC preserves the desired properties of PFCM. In addition, as a coclustering algorithm, DFPC is more suitable for our intended high-dimensional application: document clustering. Besides, the coclustering mechanism enables DFPC to generate, together with document clusters, fuzzy-possibilistic word memberships. These word memberships, which are absent in the existing PFCM model, can play an important role in generating useful descriptions of document clusters. We detail the formulation of the proposed model and provide an extensive analytical study of the algorithm DFPC. Experiments on an artificial dataset and various benchmark document datasets demonstrate the effectiveness and potential of DFPC.",1941-0034,,10.1109/TFUZZ.2008.924332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4505351,Coclustering;document clustering;fuzzy clustering;information retrieval;possibilistic clustering;text mining;Fuzzy clustering;possibilistic clustering;co-clustering;document clustering;information retrieval;text mining,Clustering algorithms;Robustness;Robust stability;Phase change materials;Algorithm design and analysis;Information retrieval;Text mining,classification;document handling;fuzzy set theory;pattern clustering;possibility theory,dual fuzzy-possibilistic coclustering;document categorization;possibilistic fuzzy c-means;low-dimensional data clustering;document clustering;fuzzy-possibilistic word memberships,,22,,28,,30-Apr-08,,,IEEE,IEEE Journals
Comparative analysis of similarity measures in document clustering,文檔聚類中相似性度量的比較分析,Kavitha Karun A; M. Philip; K. Lubna,"Department of Computer Science and Engineering, Rajagiri School of Engineering & Technology, Kochi-39, Kerala, India; Department of Computer Science and Engineering, Rajagiri School of Engineering & Technology, Kochi-39, Kerala, India; Department of Computer Science and Engineering, Rajagiri School of Engineering & Technology, Kochi-39, Kerala, India","2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)",2-Jun-14,2013,,,857,860,"Rapid breakthrough in science and technology paved way for the accumulation of bulk of data. Extracting useful and meaningful data from this gargantuan amount of data is a tedious process. This has resulted in the development of efficient Data mining methods to discover interesting unknown knowledge from a large amount of data. Document mining or Text mining refers to data mining techniques to extract interesting and nontrivial information and knowledge from unstructured text. Document clustering is an effective Text mining method which classifies similar documents in to a group. Similarity measures play a key role in clustering documents. In this, a comparative study on the effect of various similarity measures in clustering documents in the same data set is done.",,978-1-4673-6126-2,10.1109/ICGCE.2013.6823554,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823554,Clusters;Document Clustering;similarity measures,Vectors;Euclidean distance;Correlation coefficient;Clustering algorithms;Text mining;Computational modeling,data mining;information retrieval;pattern classification;pattern clustering;text analysis,comparative analysis;similarity measures;document clustering;data mining methods;document mining;text mining;information extraction,,2,,8,,2-Jun-14,,,IEEE,IEEE Conferences
Browser with Clustering of Web Documents,具有Web文檔群集的瀏覽器,R. Tetali; J. Bose; T. Arif,"WMG Group, Samsung Res. Inst. India Bangalore, Bangalore, India; WMG Group, Samsung Res. Inst. India Bangalore, Bangalore, India; WMG Group, Samsung Res. Inst. India Bangalore, Bangalore, India","2013 2nd International Conference on Advanced Computing, Networking and Security",16-Jan-14,2013,,,164,168,"Accessing relevant information quickly, given limited time and space, is a major issue in Web browsers, especially those in mobile devices. In this paper we propose a framework for grouping similar Web documents in a browser based on similar content of the browsed pages. This grouping can help reduce clutter and enable the user to access relevant Web information quickly. The algorithm we used for clustering is MajorClust, a document similarity algorithm based on tokenizing the words in the document and then determining a cosine similarity measure to estimate the distance between the words. The entire algorithm for clustering is implemented inside the browser without the need of an external Web server. We have implemented and tested the algorithm on a mobile browser and obtained accurate finer clustering of Web pages when compared to Alexa's sub-categories.",2377-2514,978-0-7695-5127-2,10.1109/ADCONS.2013.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714157,document clustering;Web browser;MajorClust;intelligent browsing;Web history,Clustering algorithms;Browsers;Engines;Web pages;Mobile communication;History;Mobile handsets,Internet;mobile computing;online front-ends;pattern clustering;text analysis;Web sites,Web documents clustering;Web browsers;Web information;MajorClust;document similarity algorithm;document words tokenizing;cosine similarity measure;mobile browser;Web pages clustering,,2,,11,,16-Jan-14,,,IEEE,IEEE Conferences
Evaluation of knowledge acquisition from document clustering based on information retrieval scales,基於信息檢索量表的文檔聚類知識獲取評估,S. Ochikubo; K. Komiya; F. Saitoh; S. Ishizu,"Graduate School of Science and Engineering, Aoyama Gakuin University, Sagamihara, Japan; Department of Industrial Engineering and System Engineering, Aoyama Gakuin University, Sagamihara, Japan; Department of Industrial Engineering and System Engineering, Aoyama Gakuin University, Sagamihara, Japan; Department of Industrial Engineering and System Engineering, Aoyama Gakuin University, Sagamihara, Japan",2017 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),12-Feb-18,2017,,,220,224,"Twitter is becoming one of the most important social sensors for observing the reputation and trends of events and things in the real world. Also the impression and reputation of enterprises on the public, information available on Twitter is effective in influencing opinions. In this study, we attempted to classify companies using tweets that included hash tags that corresponded to each company from language resources related to the companies accumulated on Twitter. However, there are differences in the number of tweets by companies, which may affect the performance of clustering. Therefore, by comparing TF-IDF which is a conventional method and BM25 considered in document length, it is confirmed whether difference in performance of companies clustering occurs. The collected tweets were weighted by information retrieval scale, and clustering result was evaluated by entropy. As a result, the peripheral method of BM 25 was shown to be effective in document clustering.",2157-362X,978-1-5386-0948-4,10.1109/IEEM.2017.8289884,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8289884,BM25;Contractor Classification;Document Clustering;Twitter Hashtag,Companies;Entropy;Twitter;Mathematical model;Tools;Feature extraction,business data processing;document handling;information retrieval;knowledge acquisition;pattern clustering;social networking (online),knowledge acquisition;document clustering;hash tags;language resources;TF-IDF;BM25;document length;information retrieval scale;tweets;Twitter;social sensors;companies,,,,11,,12-Feb-18,,,IEEE,IEEE Conferences
Clustering Algorithm Comparison of Search Results Documents,搜索結果文檔的聚類算法比較,David; R. R. Kosala,"STMIK Pontianak, Pontianak, Indonesia; Universitas Bina Nusantara, Jakarta, Indonesia",2018 6th International Conference on Cyber and IT Service Management (CITSM),28-Mar-19,2018,,,1,6,"Document clustering is one of the popular studies of data mining. This research focused on creation of the application system of document clustering of search results documents through clustering algorithms of Ant Colony Optimization, Forgy and ISODATA. Created applications were used to group and ease search results documents. Clustered documents were articles of journals, theses, thesis proposals, and ebooks. Indexing and searching the documents apply Apache Lucene, the search engine. Ant Colony Optimization algorithm was compared to partitioning clustering of Forgy and ISODATA. Comparison was on examination of processing time of clustering, variance, and the sum of squared errors. Experiments of groups of documents and datasets were conducted. To conclude, clustering results of the three methods show identical variance and produce high intraclass similarity and low interclass similarity. Also, in comparison to others, clustering through algorithm of Ant Colony Optimization takes the most time.",,978-1-5386-5434-7,10.1109/CITSM.2018.8674246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674246,Document Clustering;Ant Colony Optimization;Forgy;ISODATA,Clustering algorithms;Mathematical model;Ant colony optimization;Linear programming;Clustering methods;Partitioning algorithms;Proposals,ant colony optimisation;data mining;document handling;pattern clustering;search engines,search results documents;document clustering;clustered documents;search engine;Ant Colony Optimization algorithm;partitioning clustering;clustering algorithm;ISODATA,,,,20,,28-Mar-19,,,IEEE,IEEE Conferences
Restructuring web search results by generating feedback session and clustering pseudo documents,通過生成反饋會話和聚類偽文檔來重組Web搜索結果,B. G. Salve; R. B. Wagh,"Department of Computer Engineering, North Maharashtra University, SES's R. C. Patel Institute of Technology, Shirpur, India; Department of Computer Engineering, North Maharashtra University, SES's R. C. Patel Institute of Technology, Shirpur, India","2015 Conference on Power, Control, Communication and Computational Technologies for Sustainable Growth (PCCCTSG)",7-Jul-16,2015,,,299,303,"Restructuring web search results is the best solution for ambiguous queries being entered to the search engine. When ambiguous queries are entered to the search engine gives multiple results for same query, so user don't get specific and accurate information about what they really want, so it becomes difficult for a user to get specific information related to the submitted keyword. For this reason a new criterion is used in which feedback sessions are first generated from user clicked through logs. Using Feedback session a pseudo documents are generated by calculating TF-IDF (Term Frequency Inverse Data Frequency) vectors for each URL in clicked through logs. Then k-means clustering algorithm is applied and these pseudo documents are clustered and user search goals are generated and restructuring is done through user search goal and user gets specific information fast and correctly. Then the performance of each user search goal is calculated by using CAP metric. These metrics shows how correct restructuring is done. On the basis of this metrics we conducted our experiments on 1720 queries from which 100 most ambiguous queries were separately considered. We observed that the results were high in performance of restructure and less risk.",,978-1-4673-6890-2,10.1109/PCCCTSG.2015.7503926,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503926,queries;user search goal;pseudo documents;classified average precision;k-means clustering;TF-IDF vector,Uniform resource locators;Search engines;Web search;Measurement;Clustering algorithms;Computers;Rivers,document handling;Internet;pattern clustering;query processing;search engines,Web search;feedback session;pseudodocument clustering;search engine;TF-IDF;term frequency inverse data frequency;k-means clustering algorithm;CAP metric,,,,10,,7-Jul-16,,,IEEE,IEEE Conferences
High performance in minimizing of term-document matrix representation for document clustering,在最小化用於文檔聚類的術語文檔矩陣表示方面的高性能,L. Muflikhah; B. Baharudin,"Computer and Information Sciences Departement of Universiti Teknologi PETRONAS, Malaysia; Computer and Information Sciences Departement of Universiti Teknologi PETRONAS, Malaysia",2009 Innovative Technologies in Intelligent Systems and Industrial Applications,28-Aug-09,2009,,,225,229,"Document clustering usually involves high dimensional term space, which makes it difficult for organizing data into a small number of meaningful clusters. Clustering based on similar terms without considering the content or meaning is often unsatisfactory as it ignores the relationship between important terms that do not co-occur literally. In this paper, we propose to integrate the latent semantic indexing (LSI) concept to our document clustering. This involves the use of singular value decomposition (SVD) which creates a new abstract and uses a way of finding pattern document collection in matrix representation, so that it can identify between the terms and documents which are similar. By using various numbers of patterns (rank) of SVD, the proposed method is applied to cluster documents using the fuzzy C-means algorithm. The results of the experiment show that the performance of document clustering to be better when applied to the LSI method.",,978-1-4244-2886-1,10.1109/CITISIA.2009.5224207,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5224207,,Large scale integration;Matrix decomposition;Singular value decomposition;Clustering algorithms;Information retrieval;Space technology;Intelligent systems;Organizing;Indexing;Natural language processing,document handling;fuzzy set theory;indexing;matrix algebra;pattern clustering;singular value decomposition,matrix representation;document clustering;high dimensional term space;latent semantic indexing;singular value decomposition;pattern document collection;fuzzy C-means algorithm,,2,,7,,28-Aug-09,,,IEEE,IEEE Conferences
Document Clustering Description Based on Combination Strategy,基於組合策略的文檔聚類描述,C. Zhang,"Inst. of Sci. & Tech. Inf. of China, Beijing, China","2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)",17-Feb-10,2009,,,1084,1088,"Document clustering description is a problem of labeling the clustered results of document collection clustering. It can help users determine whether one of the clusters is relevant to users' information require. Therefore, labeling a clustered set of documents is an important and challenging work in document clustering applications. The DCF (description comes first) method can generate document clustering description. For the clustering description base on DCF is generate before document clustering, there is 'semantic interval' between clustering description and cluster central vector. So, it contradicts to the intuition of 'first clustering, second description', and decreases the readability of clustering description. A method based on combination strategy, i.e. combination of the DCF and DCL (description comes last) is proposed to solve the problem of the weak readability of clustering description in this paper. Experimental results show that the method is effective, and the method is used to describe the search result clustering.",,978-1-4244-5544-7,10.1109/ICICIC.2009.178,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412632,,Labeling;Clustering algorithms;Data mining;Frequency;Information management,document handling;pattern clustering,document clustering description;combination strategy;document collection clustering;description comes first method;cluster central vector;description comes last method,,3,,21,,17-Feb-10,,,IEEE,IEEE Conferences
Text Document Clustering: The Application of Cluster Analysis to Textual Document,文本文檔聚類：聚類分析在文本文檔中的應用,V. S. Reddy; P. Kinnicutt; R. Lee,"Dept. of Comput. Sci., Central Michigan Univ., Mount Pleasant, MI, USA; Dept. of Comput. Sci., Central Michigan Univ., Mount Pleasant, MI, USA; Dept. of Comput. Sci., Central Michigan Univ., Mount Pleasant, MI, USA",2016 International Conference on Computational Science and Computational Intelligence (CSCI),20-Mar-17,2016,,,1174,1179,"Gathering the most relevant data for one's need, from the huge collection of data in the internet is a work of great difficult. To make it easier, we propose an application called text clustering, which is an automatic grouping of text documents into clusters, so that documents within a cluster defines the similarity between them, but they are not similar to documents in other clusters. Most of existing text clustering algorithms uses the traditional vector space model, which treats documents as group of words while the word sequences in the documents are ignored and the meaning of natural languages strongly depends on them. Our first objective is to implement a clustering algorithm in java, named Clustering based on Frequent Word Sequences. The frequent word sequences can provide compact and valuable information about the text documents. Our second objective is to use an association rule miner[13] to find the frequent two-word sets that satisfy the minimum support using Apriori Algorithm[2,5]. Our results will show that the finally compact documents will be more accurate and precise than the regular method documents.",,978-1-5090-5510-4,10.1109/CSCI.2016.0222,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881515,clustering;text;word sequence;group of words;apriori algorithm;space model;efficiency,Clustering algorithms;Algorithm design and analysis;Databases;Merging;Unified modeling language;Internet,data mining;document handling;Java;pattern clustering;text analysis,text document clustering;cluster analysis;clustering algorithm;Java;frequent word sequences;association rule miner;apriori algorithm,,3,,19,,20-Mar-17,,,IEEE,IEEE Conferences
Clustering document images using a bag of symbols representation,使用一袋符號表示對文檔圖像進行聚類,E. Barbu; P. Heroux; S. Adam; E. Trupin,"Lab. PSI, Univ. de Rouen, Mont-Saint-Aignan, France; Lab. PSI, Univ. de Rouen, Mont-Saint-Aignan, France; Lab. PSI, Univ. de Rouen, Mont-Saint-Aignan, France; Lab. PSI, Univ. de Rouen, Mont-Saint-Aignan, France",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,1216,1220 Vol. 2,"Document image classification is an important step in document image analysis. Based on classification results we can tackle other tasks such as indexation, understanding or navigation in document collections. Using a document representation and an unsupervised classification method, we may group documents that from the user point of view constitute valid clusters. The semantic gap between a domain independent document representation and the user implicit representation can lead to unsatisfactory results. In this paper, we describe document images based on frequent occurring symbols. This document description is created in an unsupervised manner and can be related to the domain knowledge. Using data mining techniques applied to a graph based document representation we find frequent and maximal subgraphs. For each document image, we construct a bag containing the frequent subgraphs found in it. This bag of ""symbols"" represents the description of a document. We present results obtained on a corpus of 60 graphical document images.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.75,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575736,,Text analysis;Image analysis;Layout;Image classification;Navigation;Data mining;Information retrieval;XML;Postal services,document image processing;image classification;image representation;data mining,document image clustering;symbols representation;document image classification;document image analysis;unsupervised classification;independent document representation;domain knowledge;data mining;graph-based document representation;graphical document images,,10,,16,,16-Jan-06,,,IEEE,IEEE Conferences
Dynamic Fluzzy Clustering Algorithm for Web Documents Mining,Web文檔挖掘的動態模糊聚類算法,Q. Luo,"Dept. of Comput. Sci., Weinan Teachers Coll., Weinan, China",2010 International Conference on Computational Intelligence and Security,20-Jan-11,2010,,,64,67,"This paper first studies the methods of web documents mining and text clustering, and summaries the fuzzy clustering algorithms and similarity measure functions, then proposes a modified similarity function which can solve the problems of feature selection and feature extraction in high-dimensional space. Finally, this paper puts forward to a dynamic fluzzy clustering algorithm(DCFCM) by combining the proposed similarity function with approximated C-mediods. The experiments show that DCFCM can effectively improve he precision of web documents clustering, the method is feasible in web documents mining.",,978-1-4244-9114-8,10.1109/CIS.2010.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5696233,text mining;document clustering;similarity measure function;fuzzy clustering,Clustering algorithms;Classification algorithms;Heuristic algorithms;Partitioning algorithms;Data models;Catalogs;Data mining,data mining;feature extraction;Internet;pattern clustering;text analysis,dynamic fluzzy clustering algorithm;Web documents mining;text clustering;similarity measure functions;feature selection;feature extraction;approximated C-mediods,,,,8,,20-Jan-11,,,IEEE,IEEE Conferences
A Fuzzy Approach to Clustering of Text Documents Based on MapReduce,基於MapReduce的文本文檔模糊聚類方法。,H. Zongzhen; Z. Weina; Liyue; D. Xiaojuan; Y. fan,"Dept. of Comput. Sci., Yunnan Univ., Kunming, China; Dept. of Comput. Sci., Yunnan Univ., Kunming, China; NA; Dept. of Comput. Sci., Yunnan Univ., Kunming, China; Dept. of Comput. Sci., Yunnan Univ., Kunming, China",2013 International Conference on Computational and Information Sciences,24-Oct-13,2013,,,666,669,"This paper discusses text clustering based on a parallel computing platform called Hadoop. According to the concept of fuzzy set, this paper presents a fuzzy clustering approach for document categorization. Furthermore, a parallel text clustered framework based on MapReduce was designed according to the proposed text clustering procedure.",,978-0-7695-5004-6,10.1109/ICCIS.2013.181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643097,Text document clustering;MapReduce;Hadoop;Fuzzy approach;Parallel computing;Distributed computing,Clustering algorithms;Training;Data mining;Algorithm design and analysis;Information entropy;Educational institutions;Programming,fuzzy set theory;parallel programming;pattern clustering;text analysis,fuzzy approach;text document clustering;MapReduce;Hadoop parallel computing platform;fuzzy set concept;fuzzy clustering approach;document categorization;text clustering procedure,,1,,11,,24-Oct-13,,,IEEE,IEEE Conferences
Document clustering using gravitational ensemble clustering,使用重力集成聚類的文檔聚類,A. H. Sadeghian; H. Nezamabadi-pour,Department of Electrical Engineering Shahid Bahonar University of Kerman; Department of Electrical Engineering Shahid Bahonar University of Kerman,2015 The International Symposium on Artificial Intelligence and Signal Processing (AISP),15-Jun-15,2015,,,240,245,"Text Mining is a field that is considered as an extension of data mining. In the context of text mining, document clustering is used to set apart likewise documents of a collection into the identical category, called cluster, and divergent documents to distinctive groups. Since every dataset has its own characteristics, finding an appropriate clustering algorithm that can manage all kinds of clusters, is a big challenge. Clustering algorithms has theirs unique approaches for computing the number of clusters, imposing a structure on the data, and attesting the out coming clusters. The idea of combining different clustering is an effort to overwhelm the faults of single algorithms and further enhance their executions. On the other hand, inspired by the gravitational law, different clustering algorithms have been introduced that each one attempted to cluster complex datasets. Gravitational Ensemble Clustering (GEC) is an ensemble method that employs both the concepts of gravitational clustering and ensemble clustering to reach a better clustering result. This paper represents an application of GEC to the problem of document clustering. The proposed method uses a modification of the original GEC algorithm. This modification tries to produce a more varied clustering ensemble using new parameter setting. The GEC algorithm is assessed using document datasets. Promising results of the presented method were obtained in comparison with competing algorithms.",,978-1-4799-8818-1,10.1109/AISP.2015.7123481,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123481,Data mining;Data Clustering;Document clustering;Gravitational clustering;Quality measures,Clustering algorithms;Partitioning algorithms;Data mining;Algorithm design and analysis;Entropy;Clustering methods,data mining;pattern clustering;text analysis,document clustering;gravitational ensemble clustering;text mining;data mining;gravitational law,,3,,27,,15-Jun-15,,,IEEE,IEEE Conferences
Text extraction from color documents-clustering approaches in three and four dimensions,從彩色文檔中提取文本-3維和4維聚類方法,T. Perroud; K. Sobottka; H. Bunke,"Inst. of Comput. Sci. & Appl. Math., Bern Univ., Switzerland; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,937,941,"Colored paper documents often contain important text information. For automating the retrieval process, identification of text elements is essential. In order to reduce the number of colors in a scanned document, color clustering is usually done first. In this article two histogram-based color clustering algorithms are investigated. The first is based on the RGB color space exclusively, while the second takes spatial information into account, in addition to the colors. Experimental results have shown that the use of spatial information in the clustering algorithm has a positive impact. Thus the automatic retrieval of text information can be improved. The proposed methods for clustering are not restricted to document images. They can also be used for processing Web or video images, for example.",,0-7695-1263-1,10.1109/ICDAR.2001.953923,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953923,,Clustering algorithms;Computer science;Information retrieval;Books;Mathematics;Machine assisted indexing;Data mining;Color;Histograms;Marine vehicles,document image processing;information retrieval;image colour analysis;optical character recognition,text extraction;color documents;text retrieval;document scanning;histogram-based color clustering;RGB color space;spatial information;experimental results;document image processing;Web images;video images;information retrieval;OCR,,13,,7,,7-Aug-02,,,IEEE,IEEE Conferences
Toward Part-Based Document Image Decoding,邁向基於零件的文檔圖像解碼,W. Song; S. Uchida; M. Liwicki,"Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan; DFKI, Kaiserslautern, Germany",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,266,270,"Document image decoding (DID) is a trial to understand the contents of a whole document without any reference information about font, language, etc. Typically, DID approaches assume the correct segmentation of the document and some a priori knowledge about the language or the script. Unfortunately, this assumption will not hold if we deal with various documents, such as documents with various sized fonts, camera-captured documents, free-layout documents, or historical documents. In this paper, we propose a part-based character identification method where no segmentation into characters is necessary and no a priori information about the document is needed. The approach clusters similar key points and groups frequent neighboring key point clusters. Then a second iteration is performed, i.e., the groups are again clustered and optionally pairs frequent group clusters are detected. Our first experimental results on multi font-size documents look already very promising. We could find nearly perfect correspondences between characters and detected group clusters.",,978-0-7695-4661-2,10.1109/DAS.2012.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195376,Document image decoding;part-based,Decoding;Feature extraction;Character recognition;Image segmentation;Robustness;Text analysis;Vectors,cameras;document image processing;image coding;image segmentation,part-based document image decoding;document segmentation;camera-captured documents;free-layout documents;historical documents;part-based character identification method;neighboring keypoint clusters;multifont-size documents,,,,7,,7-May-12,,,IEEE,IEEE Conferences
Incremental multi-document summarization: An incremental clustering based approach,增量多文檔摘要：一種基於增量聚類的方法,J. John; S. Asharaf,"Indian Institute of Information Technology and Management - Kerala, Trivandrum, India; Indian Institute of Information Technology and Management - Kerala, Trivandrum, India",2014 International Conference on Data Science & Engineering (ICDSE),4-Dec-14,2014,,,136,139,"Documents which are published both online and offline are considered to be the primary source of information. Astonishing growth of documentation and communication systems tends to flood these pools of information sources with enormous amount of documents. In such a scenario, it is critical to envisage algorithms and methodologies that can convert these huge collection of documents to their best possible form of summaries. It will help to cater to the information hunters who needs only the abstract summaries in a fully digestible form. Different methods which can perform this task can be compared on the basis of quality of summary that it generates and the amount of processing power that it demands. Existing methods are capable of generating summaries incrementally (update summaries as and when new documents are added to the pool). But the inability to keep summaries unaffected by the order in which new documents are added to the pool and the need to process whole set of documents (together with those which are already summarized)each time the summary needs to be updated, pulls them back from their potential applications. We propose a mechanism that can overcome these difficulties and generate update summaries in an affordable way.",,978-1-4799-5460-5,10.1109/ICDSE.2014.6974625,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974625,Incremental multi-document summarization;order-independent summarization;leader algorithm;incremental clustering,Clustering algorithms;Zinc;Feature extraction;Sugar;Standards;Computers;Algorithm design and analysis,abstracting;document handling;pattern clustering,incremental multidocument summarization;incremental clustering;update summaries,,,,6,,4-Dec-14,,,IEEE,IEEE Conferences
Large Scale Document Categorization With Fuzzy Clustering,帶有模糊聚類的大規模文檔分類,J. Mei; Y. Wang; L. Chen; C. Miao,"College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Fuzzy Systems,4-Oct-17,2017,25,5,1239,1251,"Clustering documents into coherent categories is a very useful and important step for document processing and understanding. The introducing of fuzzy set theory into clustering provides a favorable mechanism to capture overlapping among document clusters. Document dataset is commonly represented as a collection of high-dimensional vectors, which may not be able to fit into memory entirely, when the dataset is large and with a very high dimensionality. However, most of the existing fuzzy clustering approaches deal with small and static datasets. Some of them may have a good scalability but they are only effective for low dimensional data. The study presented in this paper is about new efforts on fuzzy clustering of large-scale and high-dimensional data-especially suitable for document categorization. To consider both large scale and high dimensionality into the problem formulation, our key idea is to incorporate document-tailored fuzzy clustering into a scheme, which is effective for dealing with a large-scale problem. We first identified three representative schemes in fuzzy clustering for handling large-scale data, namely sampling extension, single pass, and divide ensemble. The limitation of fuzzy C-means (FCM)-based approaches for a large document clustering are then investigated. Based on the study, we propose new approaches by incorporating each of hyperspherical FCM and fuzzy coclustering with the three scale-up schemes, respectively. This enables our new approaches to maintain effectiveness for high-dimensional data with an extended scalability. Extensive experimental studies with real-world large document datasets have been conducted and the results demonstrate that the proposed approaches perform consistently better over existing ones in document categorization.",1941-0034,,10.1109/TFUZZ.2016.2604009,National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555373,Document clustering;fuzzy clustering;fuzzy coclustering (FCoC);hyperspherical clustering;large data,Scalability;Data models;Computational modeling;Clustering algorithms;Mathematical model;Fuzzy set theory;Indexes,document handling;fuzzy set theory;pattern clustering,fuzzy set theory;document dataset;high-dimensional vectors;low dimensional data;high-dimensional data;document clustering;fuzzy coclustering;scale-up schemes;large-scale document categorization;fuzzy clustering approaches;sampling extension;single pass;divide ensemble;fuzzy C-means-based approach;FCM-based approach;hyperspherical FCM,,9,,38,Traditional,29-Aug-16,,,IEEE,IEEE Journals
EDSC: Efficient document subspace clustering technique for high-dimensional data,EDSC：高維數據的有效文檔子空間聚類技術,Radhika K R; Pushpa C N; Thriveni J; Venugopal K R,"Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore, India; Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore, India; Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore, India; Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore, India",2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT),18-Jul-16,2016,,,222,226,"With the advancement in the pervasive technology, there is a spontaneous rise in the size of the data. Such data are generated from various forms of resources right from individual to organization level. Due to the characteristics of unstructured or semi-structuredness in data representation, the existing data analytics approaches are not directly applicable which leads to curse of dimensionality problem. Hence, this paper presents an Efficient Document Subspace Clustering (EDSC) technique for high-dimensional data that contributes to the existing system with respect to identification by eliminating the redundant data. The discrete segmentation of data points are used to explicitly expose the dimensionality of hidden subspaces in the clusters. The outcome of the proposed system was compared with existing system to find the effective document clustering process for high-dimensional data. The processing time of EDSC for subspace clustering is reduced by 50% as compared to the existing system.",,978-1-5090-0082-1,10.1109/ICCTICT.2016.7514582,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514582,Cluster Analysis;High-Dimensional Data;Subspace Clustering,Clustering algorithms;Databases;Mathematical model;Algorithm design and analysis;Data analysis;Redundancy;Organizations,data analysis;document handling;pattern clustering,hidden subspace dimensionality;discrete data point segmentation;curse-of-dimensionality problem;data analytics approaches;data representation;semistructuredness characteristics;unstructured data characteristics;pervasive technology;high-dimensional data;efficient document subspace clustering technique;EDSC,,1,,11,,18-Jul-16,,,IEEE,IEEE Conferences
A Brief Survey on Meta-Heuristic Approaches for Web Document Clustering,Web文檔聚類的元啟發式方法概述,M. S. Behniwal; A. Bhasin; S. Jangra,"Dept. of Comput. Applic., Punjab Tech. Univ., Kapurthala, India; Dept. of Comput. Applic., Punjab Tech. Univ., Kapurthala, India; Dept. of Comput. Applic., Guru Teg Bahadur Coll., Sangrur, India",2018 4th International Conference on Computing Sciences (ICCS),13-Jan-19,2018,,,98,101,"Internet is a gigantic information resource, which is growing rapidly as more and more data are being added to the World Wide Web. It is now becoming ever harder to search useful information from the web. It has been also been found that HTML tags which have particular meanings could be used to enhance the performance of IR system. Still, the bulk of data on the web is speedily growing. Clustering can be used to play a key role in organizing such a bulky amount of documents into groups. However, due to restrictions in prevailing clustering techniques, scientists started to use meta-heuristic algorithms for the document clustering problem. This paper provides a brief survey of the available literature on a web search in which HTML tags have been used in information retrieval and Meta-heuristics approaches used in web document clustering.",,978-1-5386-8025-4,10.1109/ICCS.2018.00024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611042,"Internet, HTML, IR, Clustering, Meta-heuristics.",Clustering algorithms;Web search;Particle swarm optimization;Genetic algorithms;Computer science;Optimization,document handling;hypermedia markup languages;information resources;information retrieval;Internet;pattern clustering,World Wide Web;HTML tags;meta-heuristic algorithms;information retrieval;information resource;clustering techniques;Web search;Web document clustering;Internet,,,,23,,13-Jan-19,,,IEEE,IEEE Conferences
A feature point clustering approach to the segmentation of form documents,特徵點的聚類方法表單文檔分割,Kuo-Chin Fan; Jeng-Ming Lu; Jing-Yuh Wang,"Inst. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Chung-Li, Taiwan; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,623,626 vol.2,"Among various kinds of documents, forms are one of the important types. The prerequisite for form optical character recognition (Form OCR) is the extraction of characters from form documents. The authors present a clustering based technique for extracting characters from form documents. In this method, they treat the character extraction process as a pattern clustering problem. The feasibility of the novel method is demonstrated through experimenting various kinds of forms. Experimental results reveal the feasibility of the novel method.",,0-8186-7128-9,10.1109/ICDAR.1995.601973,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601973,,Character recognition;Feature extraction;Optical character recognition software;Data mining;Image segmentation;Clustering algorithms;Pattern clustering;Image storage;Optical devices;Storage automation,optical character recognition;feature extraction;document image processing;image segmentation,form document segmentation;feature point clustering approach;form optical character recognition;character extraction;pattern clustering problem,,,,7,,6-Aug-02,,,IEEE,IEEE Conferences
Web Document Clustering Technique Using Case Grammar Structure,使用案例語法結構的Web文檔聚類技術,K. P. Supreethi; E. V. Prasad,"JNTUCEA, Anantapur; NA",International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007),7-Jan-08,2007,2,,98,102,"Most of the documents clustering techniques rely on single term analysis of the document data set, such as the Vector space model. More informative features including phrases and their weights are particularly important to achieve more accurate document clustering. Document clustering is particularly useful in many applications such as automatic categorization of documents, grouping search engine results, building taxonomy of documents and others. The motivation behind the work in this paper is that we believe that document clustering should be based not only on single word analysis, but on phrases as well. Phrase based analysis means that the similarity between documents should be based on matching phrases rather than on single words only. In this paper, we propose a system for Web clustering based on two key concepts. The first is the use of weighted phrases as an essential constituent of documents. Similarity between documents will be based on matching phrases and their weights. The second concept is the incremental clustering of documents to maximize the tightness of clusters by carefully watching the similarity distribution inside each cluster.",,0-7695-3050-8,10.1109/ICCIMA.2007.245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426677,,Text analysis;Clustering methods;HTML;Computational intelligence;Functional analysis;Search engines;Taxonomy;Indexing;Graph theory;System analysis and design,document handling;grammars;Internet;pattern clustering;search engines,Web document clustering technique;grammar structure;single term analysis;vector space model;automatic document categorization;search engine;single word analysis;phrase based analysis;Web clustering,,5,,8,,7-Jan-08,,,IEEE,IEEE Conferences
Biomedical document clustering using ontology based concept weight,使用基於本體的概念權重的生物醫學文檔聚類,S. Logeswari; K. Premalatha,"CSE dept, Bannari Amman Institute of Technology, Sathyamangalam, India; CSE dept, Bannari Amman Institute of Technology, Sathyamangalam, India",2013 International Conference on Computer Communication and Informatics,21-Feb-13,2013,,,1,4,Conventional document clustering techniques are mainly based on the existence of keywords and the number of occurrences of it. Most of the term frequency based clustering techniques consider the documents as bag-of-words and ignore the important relationships between the words in the document. Phrase based clustering techniques also capture only the order in which the words occur in a sentence rather than the semantics behind the words. Hence a concept based clustering technique is proposed in this paper. It uses Medical Subject Headings MeSH ontology for concept extraction and concept weight calculation based on the identity and synonymy relationships. K-means algorithm is used for clustering the documents based on the semantic similarity and the results are analyzed.,,978-1-4673-2907-1,10.1109/ICCCI.2013.6466273,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466273,Document Clustering;Ontology;semantic similarity;concept weight,Semantics;Ontologies;Indexing;Data mining;Vectors;Abstracts;Computers,data mining;medical computing;ontologies (artificial intelligence);pattern clustering;text analysis,biomedical document clustering;ontology based concept weight;document clustering technique;keywords;term frequency based clustering technique;bag-of-words;word relationship;phrase based clustering technique;word semantics;concept based clustering technique;medical subject headings;MeSH ontology;concept extraction;concept weight calculation;identity relationship;synonymy relationship;K-means algorithm;semantic similarity,,6,,12,,21-Feb-13,,,IEEE,IEEE Conferences
Boosting Discrimination Information Based Document Clustering Using Consensus and Classification,使用共識和分類促進基於歧視信息的文檔聚類,A. M. Sheri; M. A. Rafique; M. T. Hassan; K. N. Junejo; M. Jeon,"Department of Computer Software Engineering, Military College of Signals, National University of Sciences and Technology, Pakistan; Department of Computer Sciences, Quaid-e-Azam University, Islamabad, Pakistan; Department of Software Engineering, School of Systems and Technology, University of Management and Technology, Lahore, Pakistan; Ibex CX, Pakistan; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea",IEEE Access,25-Jun-19,2019,7,,78954,78962,"Adequate choice of term discrimination information measure (DIM) stipulates guaranteed document clustering. Exercise for the right choice is empirical in nature, and characteristics of data in the documents help experts to speculate a viable solution. Thus, a consistent DIM for the clustering is a mere conjecture and demands intelligent selection of the information measure. In this work, we propose an automated consensus building measure based on a text classifier. Two distinct DIMs construct basic partitions of documents and form base clusters. The consensus building measure method uses the clusters information to find concordant documents and constitute a dataset to train the text classifier. The classifier predicts labels for discordant documents from earlier clustering stage and forms new clusters. The experimentation is performed with eight standard data sets to test efficacy of the proposed technique. The improvement observed by applying the proposed consensus clustering demonstrates its superiority over individual results. Relative Risk (RR) and Measurement of Discrimination Information (MDI) are the two discrimination information measures used for obtaining the base clustering solutions in our experiments.",2169-3536,,10.1109/ACCESS.2019.2923462,"Institute for Information and communications Technology Promotion; Korea Government (MSIP), (Development of global multi-target tracking and event prediction techniques based on real-time large-scale video analysis); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737935,Consensus clustering;discrimination information;document clustering;evidence combination;knowledge reuse;mining methods and algorithms;text mining,Clustering algorithms;Buildings;Clustering methods;Licenses;Data mining;Internet;Unsupervised learning,information retrieval;learning (artificial intelligence);pattern classification;pattern clustering;text analysis,automated consensus building measure;text classifier;base clusters;consensus building measure method;concordant documents;discordant documents;consensus clustering;discrimination information measures;base clustering solutions;document clustering;intelligent selection;DIM;cluster information;relative risk;RR;measurement of discrimination information;MDI,,,,53,CCBY,17-Jun-19,,,IEEE,IEEE Journals
A concept based clustering model for document similarity,基於概念的文檔相似度聚類模型,G. Veena; N. K. Lekha,"Dept. of Computer Science and Application Amrita Vishwa Vidyapeetham Kollam, India; Dept. of Computer Science Amrita Vishwa Vidyapeetham Kollam, India",2014 International Conference on Data Science & Engineering (ICDSE),4-Dec-14,2014,,,118,123,A lot of research work has been done in the area of concept mining and document similarity in past few years. But all these works were based on the statistical analysis of keywords. The major challenge in this area involves the preservation of semantics of the terms or phrases. Our paper proposes a graph model to represent the concept in the sentence level. The concept follows a triplet representation. A modified DB scan algorithm is used to cluster the extracted concepts. This cluster forms a belief network or probabilistic network. We use this network for extracting the most probable concepts in the document. In this paper we also proposes a new algorithm for document similarity.,,978-1-4799-5460-5,10.1109/ICDSE.2014.6974622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974622,concept mining model;document similarity;concept based Extended DB scan algorithm,Semantics;Nanofluidics;Clustering algorithms;Probability;Accuracy;Nanomaterials;Analytical models,belief networks;document handling;graph theory;pattern clustering,concept based clustering model;document similarity;graph model;triplet representation;DBSCAN algorithm;belief network;probabilistic network,,10,,16,,4-Dec-14,,,IEEE,IEEE Conferences
Balanced Word Clusters for Interpretable Document Representation,平衡的詞簇，可解釋的文檔表示,M. Wrzalik; D. Krechel,RheinMain University of Applied Sciences; RheinMain University of Applied Sciences,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),7-Nov-19,2019,5,,103,109,"We present Bag-of-Balanced-Concepts (BOBC), a document representation method for fuzzy and interpretable similarity estimation based on word clusters. For this purpose, a k-medoid variant is proposed, which iteratively resamples small clusters to introduce a tendency towards balanced cluster sizes. The necessary inter-word similarities for clustering are computed using GloVe or word2vec word embeddings. In this way, words that often share contexts tend to appear in the same clusters. Those clusters are used to represent documents as normalized probability distributions. Various distance measures acting as document dissimilarity estimators have been evaluated on five datasets. The impact of clustering parameters, input word vectors, and inverse document frequency weighting has been examined in our experiments. Furthermore, a comparison with document similarity estimation baselines has been performed. We demonstrate that, on average, our approach outperforms cosine similarity of both weighted Bag-of-Words vectors (TF-IDF and BM25) and word embedding centroids (Word Centroid Distance).",,978-1-7281-5054-3,10.1109/ICDARW.2019.40089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892983,Document Representation;Word Clustering;Inter Document Similarity;Bag-of-Concepts;Word Embeddings,Estimation;Histograms;Clustering algorithms;Frequency estimation;Probability distribution;Data models;Task analysis,fuzzy set theory;pattern clustering;probability;text analysis,inverse document frequency weighting;bag-of-words vectors;interpretable document representation;document representation;fuzzy similarity estimation;k-medoid variant;word2vec word embeddings;normalized probability distributions;document similarity estimation;word centroid distance;balanced word clusters;bag-of-balanced-concepts;inter-word similarities;GloVe,,1,,28,,7-Nov-19,,,IEEE,IEEE Conferences
Clustering Algorithm Using Quoting Relation of Documents,基於文檔報價關係的聚類算法,H. Jeong,"Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju",2008 Second International Conference on Future Generation Communication and Networking Symposia,10-Apr-09,2008,2,,29,34,"There is increasing interest in clustering, owing to improvements in computer capacity and expanding accessible information, circa 1990s. This paper aims to increase cohesion of clustering borrowing from techniques of pagerank showing the links among pages by assigning numerical values to the relevance of pages linked in Web pages. There are cases that need to be referenced in other documents because of any index in academic information. The greater the frequency of referenced academic information, the higher the probability that the referenced index of the document is relevant. Relations among documents referencing each other, are calculated via literature citing each document. The method of increasing the cohesion of clustering involves controlling the distance between two documents based on the values.",,978-1-4244-3430-5,10.1109/FGCNS.2008.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813515,Clustering;quoting relation,Clustering algorithms;Conferences,document handling;information retrieval;Internet;pattern classification;pattern clustering;probability;unsupervised learning,cohesion clustering algorithm;quoting relation;Web page;academic information;probability;document referencing index;unsupervised learning classification,,,,8,,10-Apr-09,,,IEEE,IEEE Conferences
Clustering web documents based on Multiclass spectral clustering,基於多類光譜聚類的Web文檔聚類,X. He; J. Wang; Z. Zhang; Y. Cai,"Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1466,1471,"Multiclass spectral clustering is a clustering method which has been successfully applied in image segmentation and many other aspects. In this paper, Multiclass spectral clustering is used to cluster web documents including both English and Chinese pages. Through experiments, we found that Multiclass spectral clustering can be well used in web document clustering, and the method not only works well to cluster English web documents but also works well to cluster Chinese web documents clustering. We applied our method to a web search engine, and users can get the suitable results easily by just selecting the desirable classes.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6017004,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6017004,Multiclass spectral clustering;web document clustering;search engine,Clustering algorithms;Classification algorithms;Sun;Indexes;Search engines;Buildings;Machine learning,document image processing;image segmentation;Internet;natural language processing;pattern clustering;search engines,English Web document clustering;multiclass spectral clustering;image segmentation;English pages;Chinese pages;Web search engine,,2,,19,,12-Sep-11,,,IEEE,IEEE Conferences
User Interest Modeling for P2P Document Sharing Systems Based on K-Medoids Clustering Algorithm,基於K-Medoids聚類算法的P2P文檔共享系統用戶興趣建模,C. Qin; Z. Yang; H. Liu,"Sch. of Econ. & Manage., Xidian Univ., Xi'an, China; Sch. of Econ. & Manage., Xidian Univ., Xi'an, China; Sch. of Econ. & Manage., Xidian Univ., Xi'an, China",2014 Seventh International Joint Conference on Computational Sciences and Optimization,16-Oct-14,2014,,,576,578,"User interest modeling is an important way for P2P document sharing systems to improve the level of information service such as personalized information retrieval and document recommendation. Based on K-medoids clustering, the paper presents a method of user interest modeling for P2P document sharing systems. Staring from the perspective of the shared document, the proposed approach creates the initial user interest model with k-mediods clustering algorithm. Then, combining with the related results of user's historical queries, the initial user interest model is improved and complete user interest model is obtained.",,978-1-4799-5372-1,10.1109/CSO.2014.113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923751,P2P document sharing system;user interest model;k-medoids clustering algorithm,Vectors;Clustering algorithms;Information services;Peer-to-peer computing;Information retrieval;Computational modeling;Educational institutions,pattern clustering;peer-to-peer computing;query processing;user modelling,user interest modeling;P2P document sharing systems;information service level;personalized information retrieval;document recommendation;k-mediods clustering algorithm;user historical queries,,,,5,,16-Oct-14,,,IEEE,IEEE Conferences
Improving Logo Spotting and Matching for Document Categorization by a Post-Filter Based on Homography,基於單應性的後置過濾器改進徽標分類和匹配以進行文檔分類,V. P. Le; M. Visani; C. D. Tran; J. Ogier,"Lab. L3I, La Rochelle Univ., La Rochelle, France; Lab. L3I, La Rochelle Univ., La Rochelle, France; NA; Lab. L3I, La Rochelle Univ., La Rochelle, France",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,270,274,"Digital document categorization based on logo spotting and recognition has raised a great interest in the research community because logos in documents are sources of information for categorizing documents with low costs. In this paper, we present an approach to improve the result of our method for logo spotting and recognition based on key point matching and presented in our previous paper [7]. First, the key points from both the query document images and a given set of logos (logo gallery) are extracted and described by SIFT, and are matched in the SIFT feature space. Secondly, logo segmentation is performed using spatial density-based clustering. The contribution of this paper is to add a third step where homography is used to filter the matched key points as a post-processing. And finally, in the decision stage, logo classification is performed by using an accumulating histogram. Our approach is tested using a well-known benchmark database of real world documents containing logos, and achieves good performances compared to state-of-the-art approaches.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628626,logo spotting;homography;pattern recognition;document analysis,Matched filters;Histograms;Accuracy;Clustering algorithms;Feature extraction;Text analysis;Image segmentation,document image processing;image matching;image recognition;image retrieval;image segmentation,real world documents;logo classification;spatial density-based clustering;logo segmentation;SIFT;query document images;key point matching;document categorization;logo matching;logo spotting and matching;logo recognition;logo spotting;Digital document categorization,,7,,14,,15-Oct-13,,,IEEE,IEEE Conferences
Review of Web Document Clustering algorithms,Web文檔聚類算法綜述,S. K. Sahu; S. Srivastava,"Dept. of Computer Science, Utkal University, Bhubaneswar, Odisha, India; University School of Information & Communication Technology, GGSIP University, Delhi, India",2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),31-Oct-16,2016,,,1153,1155,"Users of search engines are fond of accurate and fast results. Web is now overloaded with lots of pages or documents dealing with same topic. Thus they are often forced to surf through the large and irrelevant set of results. This has forced the IR community to explore such document clustering techniques capable of providing fast and accurate results. Even after being a very effective solution, clustering is yet not deployed on the major search engines. This paper will articulate the requirements of Web Document Clustering and reports on the clustering methods belonging in this domain. The focus of ours is on; these methods create their clusters based on the characters or individual terms rather than showcasing them as a single phrase with a meaning and sequence of words. Paper will be reporting general term based and phrase based techniques and will provide conclusion based on their individual efficiency to work with their key methods.",,978-9-3805-4421-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724445,Clustering;DIG;HAC;K-means;K-NN;NLP;STD;STC;VSD,Clustering algorithms;Search engines;Indexes;Matrix decomposition;Web pages;Algorithm design and analysis;Conferences,document handling;information retrieval;Internet;pattern clustering;search engines,Web document clustering algorithms;search engines;IR community;term-based techniques;phrase-based techniques,,,,15,,31-Oct-16,,,IEEE,IEEE Conferences
Using element and document profile for information clustering,使用元素和文檔配置文件進行信息聚類,J. Lai; B. Soh,"Dept. of Comput. Sci. & Comput. Eng., La Trobe Univ., Australia; Dept. of Comput. Sci. & Comput. Eng., La Trobe Univ., Australia","IEEE International Conference on e-Technology, e-Commerce and e-Service, 2004. EEE '04. 2004",19-Apr-04,2004,,,503,506,"The tremendous growth in the amount of information available and the number of visitors to Web sites in the recent years poses some key challenges for information filtering and retrieval. Web visitors not only expect high quality and relevant information, but also wish that the information be presented in an as efficient way as possible. The traditional filtering methods, however, only consider the relevant values of document. These conventional methods fail to consider the efficiency of documents retrieval. In this paper, we propose a new algorithm to calculate an index called document similarity score based on elements of the document. Using the index, document profile will be derived. Any documents with the similarity score above a given threshold are clustered. Using these pre-clustered documents, information filtering and retrieval can be made more efficient.",,0-7695-2073-1,10.1109/EEE.2004.1287354,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287354,,Information filtering;Information filters;Information retrieval;Clustering algorithms;Search engines;Internet;Computer science;Web sites;Books;Conference proceedings,search engines;information filters;information retrieval;Web sites;document handling;pattern clustering,document profile;information clustering;Web sites;information filtering;information retrieval;Web visitors;document retrieval;document similarity score;search engine,,3,,7,,19-Apr-04,,,IEEE,IEEE Conferences
Clustering Documents using the Document to Vector Model for Dimensionality Reduction,使用文檔到矢量模型對文檔進行聚類以減少維數,R. Radu; I. R?dulescu; C. Truic?; E. Apostol; M. Mocanu,"Faculty of Automatic Control and Computers, University Politehnica of Bucharest,Computer Science and Engineering Department,Bucharest,Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest,Computer Science and Engineering Department,Bucharest,Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest,Computer Science and Engineering Department,Bucharest,Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest,Computer Science and Engineering Department,Bucharest,Romania; Faculty of Automatic Control and Computers, University Politehnica of Bucharest,Computer Science and Engineering Department,Bucharest,Romania","2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",1-Jul-20,2020,,,1,6,"The TF-IDF model is the most common way of representing documents in the vector space. However, its results are highly dimensional, posing problems to the classic clustering algorithms due to the curse of dimensionality. Recent word embeddings based techniques can reduce the documents representations dimensionality while also preserving the semantic relationships between words. In this paper, we analyze the accuracy of four different classical clustering algorithms (K-Means, Spherical K-Means, LDA, and DBSCAN) in combination with the Document to Vector model.",,978-1-7281-7166-1,10.1109/AQTR49680.2020.9129967,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9129967,text clustering;document embeddings;text preprocessing;clustering evaluation,,document handling;pattern clustering;text analysis;vectors,clustering documents;Vector model;dimensionality reduction;TF-IDF model;vector space;classic clustering algorithms;recent word embeddings;documents representations dimensionality;different classical clustering algorithms,,1,,24,,1-Jul-20,,,IEEE,IEEE Conferences
Document Clustering in Personal Dataspace,個人數據空間中的文檔聚類,D. Liu; D. Yang; T. Nie; Y. Kou; D. Shen,"Dept. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Dept. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Dept. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Dept. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Dept. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China",2010 Seventh Web Information Systems and Applications Conference,23-Sep-10,2010,,,9,12,"In Personal Dataspace (PDS), documents containing a lot of useful information play an important role in our daily work. However, it is difficult to manage the information in these documents efficiently. In this paper, we first extract some frequent terms from documents, and then cluster these documents based on the terms. Thus users can query the documents based on their contents conveniently. The experiments demonstrate the accuracy and efficiency of the key techniques in our approach.",,978-1-4244-8440-9,10.1109/WISA.2010.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581382,dataspaces;document clustering;FP-tree,Clustering algorithms;Data mining;Contracts;Algorithm design and analysis;Databases;Classification algorithms;Merging,database management systems;document handling;information management;knowledge acquisition;query processing,document clustering;personal dataspace;information management;frequent term extraction,,,,13,,23-Sep-10,,,IEEE,IEEE Conferences
Forensic Analysis of financial document using Dempster Shafer approach,使用Dempster Shafer方法對財務文件進行司法鑑定,S. Shejale; S. Bharne; V. Jethani,"Dept.of Computer Engineering, Ramrao Adik Inst. of Technology, Nerul, Navi Mumbai, India; Dept.of Computer Engineering, Ramrao Adik Inst. of Technology, Nerul, Navi Mumbai, India; Dept.of Computer Engineering, Ramrao Adik Inst. of Technology, Nerul, Navi Mumbai, India",2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT),16-Mar-17,2016,,,527,531,"Now a day's many of crimes are related to financial domain so forensic analysis of such documents is required. Due to digitization many of documents for investigation is faster. If analyzer analyzes the document manually it will time consuming and tedious task so, we follow the approach which will specify the clustering algorithm to document for forensic analysis of seize system which will help the police for the investigation purpose. We tend to illustrate the planned approach by ending intensive experimentation with six well-known cluster algorithms (K-means, K-medoids, Single Link, Complete Link, Average Link, and Cluster-based Similarity Partitioning Algorithm) applied to 5 real-world datasets obtained from computers confiscated in real-world investigations. For resolving the reported case the forensic analysis of that document is required and such document is store in devices where police or another authority take devices by warrant or legal right. The proposed system facilitates domain work towards automated forensic analysis of financial document in order to faster the investigation process.",,978-1-5090-2080-5,10.1109/ICACDOT.2016.7877641,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877641,Clustering;Dempster-Shafer Theory;digital forensic;financial document analyzer,Forensics;Text analysis;Computers;Clustering algorithms;Text mining;Classification algorithms;Algorithm design and analysis,data analysis;document handling;financial data processing;inference mechanisms;pattern clustering;uncertainty handling,forensic analysis;financial document;Dempster Shafer approach;financial domain;clustering algorithm;k-means algorithm;k-medoids algorithm;single link algorithm;complete link algorithm;average link algorithm;cluster-based similarity partitioning algorithm,,1,,20,,16-Mar-17,,,IEEE,IEEE Conferences
Search Results Clustering Algorithm Based on the Suffix Tree,基於後綴樹的搜索結果聚類算法,D. Wang; L. Liu; J. Dong; J. Zheng,"Sch. of Math. & Comput. Sci., Ningxia Univ., Yinchuan, China; Sch. of Math. & Comput. Sci., Ningxia Univ., Yinchuan, China; Nat. Eng. Res. Center for Inf. Technol. in Agric., Beijing, China; Sch. of Math. & Comput. Sci., Ningxia Univ., Yinchuan, China",2015 2nd International Conference on Information Science and Control Engineering,11-Jun-15,2015,,,456,460,"The STC algorithm clusters the documents based on shared phrases and it is a linear time algorithm. Directed against the insufficiency of the existing STC algorithm such as the quality of clustering results and the screening of the clustering labels, the paper improves STC algorithm, respectively perfecting the choice of the base cluster, the similarity calculation formula used to merge the base clusters and the scoring function for the clustering labels. Finally entropy is taken as the evaluation criterion for the clustering results. Compared with the original algorithm there are a better effect which is attested by experiments and more readability, descriptive and distinguishable clustering labels.",,978-1-4673-6850-6,10.1109/ICISCE.2015.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120646,suffix tree;search result clustering;document clustering;clustering algorithm,Clustering algorithms;Entropy;Search engines;Data mining;Mathematical model;Bismuth;Algorithm design and analysis,computational complexity;document handling;information retrieval;pattern clustering;trees (mathematics),search result clustering algorithm;STC algorithm;shared phrases;linear time algorithm;similarity calculation formula;base cluster merging;scoring function;clustering labels;entropy;distinguishable clustering labels;descriptive clustering labels;suffix tree,,,,12,,11-Jun-15,,,IEEE,IEEE Conferences
XML documents clustering algorithm based on cluster core and LSPX,基於集群核心和LSPX的XML文檔聚類算法,D. Zhao; H. Fu; H. Ren; M. Wei; J. Chu,"College of Computer cience and Technology, Wuhan University of Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System Wuhan, China; College of Computer cience and Technology, Wuhan University of Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System Wuhan, China; College of Computer cience and Technology, Wuhan University of Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System Wuhan, China; College of Computer cience and Technology, Wuhan University of Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System Wuhan, China; College of Computer cience and Technology, Wuhan University of Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System Wuhan, China",2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA),8-Feb-18,2017,,,1027,1032,"Due to the large number of XML data and information and its advantage of simplicity, semi-structure, extensibility and self-description, clustering XML document has become a hot issue in XML data mining. LSPX model is a kind of XML data structure representation model, which is simple in construction process and short in time. The incremental clustering algorithm based on this model and the similarity calculation gets high time efficiency and good clustering effect. But the weakness of sensitivity to the order of input appeared in traditional incremental clustering. In order to further improve the clustering efficiency of XML documents, in this paper, a new XML clustering algorithm (CO-LSPX) is proposed, which is based on the cluster core and LSPX. The experimental result shows that the proposed method can increase efficiency of clustering, reduce the time consumption greatly, as well as mute the sensitivity of input data order on the basis of ensuring the quality of clustering results.",2158-2297,978-1-5090-6161-7,10.1109/ICIEA.2017.8282990,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282990,cluster core;incremental clustering;LSPX model,XML;Clustering algorithms;Data models;Algorithm design and analysis;Data structures;Feature extraction;HTML,data mining;data structures;document handling;pattern clustering;XML,time efficiency;clustering effect;incremental clustering;XML document clustering algorithm;CO-LSPX algorithm;input data order;XML clustering algorithm;clustering efficiency;incremental clustering algorithm;XML data structure representation model;LSPX model;XML data mining;cluster core,,,,12,,8-Feb-18,,,IEEE,IEEE Conferences
CFTDISM:Clustering Financial Text Documents Using Improved Similarity Measure,CFTDISM：使用改進的相似性度量來聚類財務文本文檔,P. Srikanth; D. Deverapalli,"Department of IT, VNR Vignana Jyothi Institute of Engineering & Technology, Hyderabad, India; Faculty of Information Technology, Shri Vishnu Engineering College for Woman, Bhimavaram",2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC),8-Nov-18,2017,,,1,4,Financial Text Documents is one of the research domain build variety of applications. Mostly text processing techniques solve financial text problems. Dimensionality reduction process one the major challenging in text processing. Text features retained it helps to performing defines text clustering. Text clustering of similarity measure treats as the similarity between two text documents. The Main objective this paper defines and design suitable similarity measure motived from [18]-[21] and proposed similarity measure improves the previous measure [34]. This processes representation of financial system features initialized process based defined as Clusters.,2473-943X,978-1-5090-6621-6,10.1109/ICCIC.2017.8524466,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8524466,Financial Text Documents;Clustering;Financial System Features;Information Gain and Clustering Algorithm,Clustering algorithms;Text mining;Feature extraction;Gain measurement;Conferences;Dimensionality reduction,financial data processing;pattern clustering;text analysis,dimensionality reduction process;text processing;financial system;improved similarity measure;research domain;financial text problems;financial text documents clustering;text features,,,,37,,8-Nov-18,,,IEEE,IEEE Conferences
Hierarchical Dirichlet Multinomial Allocation Model for Multi-Source Document Clustering,多源文檔聚類的分層Dirichlet多項式分配模型,R. Huang; W. Xu; Y. Qin; Y. Chen,"College of Computer Science and Technology, Guizhou University, Guiyang, China; College of Computer Science and Technology, Guizhou University, Guiyang, China; College of Computer Science and Technology, Guizhou University, Guiyang, China; College of Computer Science and Technology, Guizhou University, Guiyang, China",IEEE Access,22-Jun-20,2020,8,,109917,109927,"Mining a document structure from multiple data sources in terms of their underlying topics has become an important task of document clustering. The traditional document clustering approach cannot be applied directly to the multi-source document clustering problem. There are three typical difficulties: 1) The topics of different data sources are related but not the same. 2) Usually, each data source has its own focus on topics. 3) The number of clusters of the data sources are not necessarily the same and are not known beforehand. In this paper, based on our previous research, we design a novel multi-source document clustering model, namely, the hierarchical Dirichlet multinomial allocation (HDMA) model, to solve all the above problems. The HDMA model is investigated with a two-step hierarchical topic generation process. Topics are learnt to share their general characteristics across data source, while at the same time preserve the local characteristics of the data source. Each data source is applied with an exclusive topic partition to learn the source-level topic emphasis. A Gibbs sampling algorithm is then used to learn the number of clusters for each data source as well as the parameters of the HDMA model at the same time. Experimental results demonstrate that the HDMA model is effective.",2169-3536,,10.1109/ACCESS.2020.3002107,Joint Funds of the National Natural Science Foundation of China; Major Research Program of National Natural Science Foundation of China; Major Special Science and Technology Projects of Guizhou Province; Key Projects of Science and Technology of Guizhou Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115621,Document clustering;multi-source document clustering;Dirichlet distribution;Gibbs sampling,Clustering algorithms;Data models;Resource management;Partitioning algorithms;Clustering methods;Social networking (online);Licenses,data mining;document handling;learning (artificial intelligence);Markov processes;Monte Carlo methods;pattern clustering,hierarchical dirichlet multinomial allocation model;HDMA model;two-step hierarchical topic generation process;source-level topic emphasis;multiple data sources;multisource document clustering problem;Gibbs sampling,,,,20,CCBY,12-Jun-20,,,IEEE,IEEE Journals
Segmentation of color documents by line oriented clustering using spatial information,使用空間信息通過線定向聚類對彩色文檔進行分割,M. Worring; L. Todoran,"Intelligent Sensory Inf. Syst., Amsterdam Univ., Netherlands; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,67,70,"In this contribution we introduce a new method for global segmentation of color documents with a structure based on text frames and pictures. It is based on an extensive analysis of the expected shape of clusters in RGB-color space. The method provides an improved segmentation over k-means based clustering, and gives a proper basis for indexing and layout analysis. Results are promising.",,0-7695-0318-7,10.1109/ICDAR.1999.791726,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791726,,Shape;Indexing;Image segmentation;Clustering algorithms;Text analysis;Image coding;Tiles;Morphology;Color;Intelligent systems,document image processing;indexing;image colour analysis;image segmentation,color documents segmentation;line oriented clustering;spatial information;global segmentation;text frames;pictures;RGB-color space;k-means based clustering;indexing;layout analysis,,6,,6,,6-Aug-02,,,IEEE,IEEE Conferences
A SOM-based document clustering using phrases,使用短語的基於SOM的文檔聚類,J. Bakus; M. F. Hussin; M. Kamel,"Dept. of Syst. Design Eng, Waterloo Univ., Ont., Canada; NA; NA","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",5-Jun-03,2002,5,,2212,2216 vol.5,"Most of the existing techniques for document clustering rely on a ""bag of words"" document representation. Each word in the document is considered as a separate feature, ignoring the word order. We investigate the use of phrases rather than words as document features for the document clustering. We present a phrase grammar extraction technique, and use the extracted phrases as the features in a self-organizing map based document clustering algorithm. We present clustering results using the REUTERS corpus and show an improvement in clustering performance using both entropy and F-measure evaluation measures.",,981-04-7524-1,10.1109/ICONIP.2002.1201886,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201886,,Data mining;Clustering algorithms;Machine learning;Merging;Computer science;Automatic control;Entropy;Internet;Information retrieval;Organizing,self-organising feature maps;document handling;pattern clustering;natural languages,SOM-based document clustering;bag of words;document representation;document features;phrase grammar extraction technique;self-organizing map;REUTERS corpus;F-measure evaluation measures,,11,,21,,5-Jun-03,,,IEEE,IEEE Conferences
Document Clustering Using Concept Space and Cosine Similarity Measurement,使用概念空間和余弦相似度度量的文檔聚類,L. Muflikhah; B. Baharudin,"Dept. of Comput. & Inf. Sci., Univ. Teknol. Petronas, Tronoh, Malaysia; Dept. of Comput. & Inf. Sci., Univ. Teknol. Petronas, Tronoh, Malaysia",2009 International Conference on Computer Technology and Development,28-Dec-09,2009,1,,58,62,"Document clustering is related to data clustering concept which is one of data mining tasks and unsupervised classification. It is often applied to the huge data in order to make a partition based on their similarity. Initially, it used for Information Retrieval in order to improve the precision and recall from query. It is very easy to cluster with small data attributes which contains of important items. Furthermore, document clustering is very useful in retrieve information application in order to reduce the consuming time and get high precision and recall. Therefore, we propose to integrate the information retrieval method and document clustering as concept space approach. The method is known as Latent Semantic Index (LSI) approach which used Singular Vector Decomposition (SVD) or Principle Component Analysis (PCA). The aim of this method is to reduce the matrix dimension by finding the pattern in document collection with refers to concurrent of the terms. Each method is implemented to weight of term-document in vector space model (VSM) for document clustering using fuzzy c-means algorithm. Besides reduction of term-document matrix, this research also uses the cosine similarity measurement as replacement of Euclidean distance to involve in fuzzy c-means. And as a result, the performance of the proposed method is better than the existing method with f-measure around 0.91 and entropy around 0.51.",,978-0-7695-3892-1,10.1109/ICCTD.2009.206,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359952,data mining;document clustering;LSI;fuzzy c-means;cosine similarity,Extraterrestrial measurements;Information retrieval;Clustering algorithms;Data mining;Large scale integration;Euclidean distance;Clustering methods;Space technology;Information science;Principal component analysis,data mining;document handling;fuzzy set theory;matrix algebra;pattern classification;pattern clustering;principal component analysis;singular value decomposition;vectors,document clustering;concept space;cosine similarity measurement;data clustering concept;data mining tasks;unsupervised classification;information retrieval;data attributes;latent semantic index approach;singular vector decomposition;principle component analysis;matrix dimension;document collection;vector space model;fuzzy c-means algorithm;term-document matrix;Euclidean distance,,32,,18,,28-Dec-09,,,IEEE,IEEE Conferences
Web Document Clustering by Using Automatic Keyphrase Extraction,使用自動關鍵詞提取的Web文檔聚類,J. Han; T. Kim; J. Choi,"Hanyang Univ., Ansan; Hanyang Univ., Ansan; Hanyang Univ., Ansan",2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops,14-Jan-08,2007,,,56,59,"In most traditional techniques of document clustering, the number of total clusters is not known in advance and the cluster that contain the target information cannot be determined since the semantic nature is not associated with the cluster. The well-known K-means clustering algorithm partially solves these problems by allowing users to specify the number of clusters. However, if the pre-specified number of clusters is modified, the precision of each result also changes. To solve this problem, this paper proposes a new clustering algorithm based on the Kea keyphrase extraction algorithm which returns several keyphrases from the source documents by using some machine learning techniques. In this paper, documents are grouped into several clusters like K-means, but the number of clusters is automatically determined by the algorithm with some heuristics using the extracted keyphrases. Our Kea-means clustering algorithm provides easy and efficient ways to extract test documents from massive quantities of resources.",,0-7695-3028-1,10.1109/WI-IATW.2007.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427539,Kea-means ClusteringKeyphrasesK-meansClustering,Clustering algorithms;Machine learning algorithms;Data mining;Testing;Partitioning algorithms;Intelligent agent;Machine learning;Heuristic algorithms;Conferences;Computer science,document handling;feature extraction;Internet;learning (artificial intelligence);pattern clustering,Web document clustering;automatic keyphrase extraction;K-means clustering algorithm;machine learning techniques;Kea-means clustering algorithm,,7,,10,,14-Jan-08,,,IEEE,IEEE Conferences
Automatic Creation Technologies of Declarative Tools for Clustering Media Documents,用於群集媒體文檔的聲明性工具的自動創建技術,V. Zakharov; A. Krassovitskiy; Z. Meirambekkyzy; I. Ualiyeva; A. Khoroshilov; A. Khoroshilov,"Federal Research Center for Computer Science and Control, Russian Academy of Sciences; Institute of Information and Computer Technologies; Institute of Information and Computer Technologies; Institute of Information and Computer Technologies; Federal Research Center for Computer Science and Control, Russian Academy of Sciences; Federal Research Center for Computer Science and Control, Russian Academy of Sciences",2019 International Conference on Engineering Technologies and Computer Science (EnT),13-May-19,2019,,,39,42,"The article describes the methods of identifying the conceptual content structure of the dataset of documents for the clustering. It was found that in the automatic extraction of key text concepts it is necessary to use the criteria of semantic significance of words and phrases obtained on the basis of syntactic, statistical and semantic methods. The syntactic criteria are based on the definition of the syntactic role of words and phrases in the text dataset. We accent on those elements of sentences that forms its semantic (predicate-actant) structure. In this research four methods of automatic identification of key text concepts have been elaborated, their comparative analysis is carried out and the technology of automatic creation of declarative means for text clustering of media is developed. The precision assessment of document clustering with and without declarative methods is conducted on test dataset.",,978-1-7281-1915-1,10.1109/EnT.2019.00013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8711885,"document clustering, automated text processing, formal text description, linguistic software, declarative tools, key text concepts",Dictionaries;Semantics;Syntactics;Tools;Clustering methods;Computer science;Media,pattern clustering;text analysis,clustering media documents;conceptual content structure;automatic extraction;semantic significance;semantic methods;text dataset;text clustering;predicate-actant structure;text concepts;declarative tools;automatic creation technologies;declarative methods,,,,8,,13-May-19,,,IEEE,IEEE Conferences
Novel similarity measure for document clustering based on topic phrases,基於主題短語的文檔聚類新相似度度量,A. E. ELdesoky; M. Saleh; N. A. Sakr,"Dept. of Computer and System, Mansoura University, Egypt; Dept. of Computer and System, King AbdulAziz University, KSA; Dept. of Computer and System, Mansoura University, Egypt",2009 International Conference on Networking and Media Convergence,2-May-09,2009,,,92,96,"Document clustering is a subset of the data clustering field which categorizes large set of documents into similar and related groups. In the traditional vector space model (VSM) researchers have considered the unique word which occurs in the document set as the candidate feature. Recently a new trend which considered the phrase to be a more informative feature has taken place; the matter which contributes in improving the document clustering accuracy and effectiveness. This paper proposes a new approach for computing the similarity measure of the traditional VSM by considering the topic phrases of the document as the constituting terms for the VSM instead of the traditional term ldquowordrdquo and applying the new approach to the Buckshot method, which is a mix of the Hierarchical Agglomerative Clustering (HAC) algorithm and the K-means partitioning algorithm. Such a mechanism may raise the effectiveness of the clustering by increasing the evaluation metrics values.",,978-1-4244-3776-4,10.1109/ICNM.2009.4907196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907196,,Clustering algorithms;Partitioning algorithms;Clustering methods;Frequency;Organizing;Humans;Information retrieval;Text mining;Natural language processing;Taxonomy,document handling;pattern clustering,similarity measure;document clustering;topic phrases;vector space model;Buckshot method;hierarchical agglomerative clustering algorithm;k-means partitioning algorithm,,3,,17,,2-May-09,,,IEEE,IEEE Conferences
A real time clustering method using document index graph,使用文檔索引圖的實時聚類方法,N. Akthar; M. V. Ahamad; A. U. S. Khan,"Department of Computer Engineering, ZHCET, Aligarh Muslim University, 202002, India; Department of Computer Engineering, ZHCET, Aligarh Muslim University, 202002, India; Department of Computer Engineering, ZHCET, Aligarh Muslim University, 202002, India",2014 International Conference on Data Mining and Intelligent Computing (ICDMIC),13-Nov-14,2014,,,1,7,"From a previous survey, 45% of users did not get what they are actually looking for in the web using any search engine. Suppose, you have a million of text file in your server or in your computer, then there is a need to categorize them on the basis of their content in a very efficient way. As a result, IR (Information Retrieval) tool has been developed, it provides a more effective ways for users to categorize relevant data. Most of the clustering algorithm like Vector Space Model considers only single words but it is not incremental so it can't be applied on-line and another algorithm, STC, involves `trie' concept to identify shared phrases suitable to apply on-line, but the main problem is, it doesn't work for large number of data set. In this paper, we have introduced DIGE clustering algorithm which generates the clusters based on the common phrases and also on the single terms. DIGE clustering algorithm based on the DIG model for the representation of documents. The construction of DIG model is incremental, so DIGE is also capable to produce cluster using online document and also it doesn't occupy much memory, so also applicable for offline.",,978-1-4799-4674-7,10.1109/ICDMIC.2014.6954222,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954222,Clustering;Document Index Graph;Suffix Tree Clustering;Phrase Cluster;Incremental Algorithm;Web-Snippets,Clustering algorithms;Rivers;Merging;Vectors;Search engines;Algorithm design and analysis;Indexes,document handling;graph theory;information retrieval;Internet;pattern clustering,online document;document representation;DIG model;DIGE clustering algorithm;information retrieval tool;IR tool;search engine;document index graph;real time clustering method,,,,21,,13-Nov-14,,,IEEE,IEEE Conferences
Ontology Based Document Clustering - An Efficient Hybrid Approach,基於本體的文檔聚類-一種高效的混合方法,E. K. Jasila; N. Saleena; K. A. A. Nazeer,"National Institute of Technology,Dept.of Computer Science&Engineering,Calicut,India; National Institute of Technology,Dept.of Computer Science&Engineering,Calicut,India; National Institute of Technology,Dept.of Computer Science&Engineering,Calicut,India",2019 IEEE 9th International Conference on Advanced Computing (IACC),30-Jan-20,2019,,,153,157,"Recent research results show that ontology can be used to improve the accuracy of document clustering. Previous studies mainly focused on the preprocessing part of text document using ontology. In this paper, we propose a hybrid approach, concentrating on both the preprocessing task as well as the clustering algorithm. This is with an objective of reducing the number of features and execution time, eliminate synonymous problems and enhance the accuracy of clustering. Cosine similarity is used as similarity measure. The preprocessing part uses a WordNet Ontology based feature extraction method. In clustering, the initial centroids are found by applying the Red Black Tree based sorting method. The data points are allocated to the suitable clusters using a novel approach, by maintaining the path of similarity between data points and nearest cluster centroids. Experimental results on some of the existing clustering algorithms with cosine similarity are compared with our novel clustering technique. Results show that the proposed hybrid approach executes better on the Newsgroup dataset with considerable improvements in dimensionality reduction, running time, and accuracy.",2473-3571,978-1-7281-4392-7,10.1109/IACC48062.2019.8971594,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971594,Document clustering;Text clustering;Ontology;WordNet;Red Black Tree,,feature extraction;ontologies (artificial intelligence);pattern clustering;sorting;text analysis;tree searching,WordNet Ontology;nearest cluster centroids;cosine similarity;text document preprocessing;similarity measure;ontology based document clustering;feature extraction;Red Black Tree based sorting,,,,24,,30-Jan-20,,,IEEE,IEEE Conferences
A novel cluster analysis for gene-miRNA interactions documents using improved similarity measure,使用改進的相似性度量進行基因-miRNA相互作用文檔的新型聚類分析,P. Srikanth; N. Rajasekhar,"Department of Information Technology VNR, Vignana Jyothi Institute of Engineering & Technology, Hyderabad, India; Faculty of Information Science & Engineering, Dayananda Sagar College of Engineering, Bangalore, India",2016 International Conference on Engineering & MIS (ICEMIS),17-Nov-16,2016,,,1,7,"In the present period of time, the bioinformatics involves in the collection of the discovery of files, which are similar to binary files and records based on function of codes. Researchers, medical experts, and doctors have construed a tool by using medical and biological files together in sequential order. Bioinformatics is a collection of many-to-many relational data repositories, which develops to examine the functions of different code patterns. Clustering and classification of gene-protein miRNA interaction to except the file based on built matrix and sequential matrix. The main perspective of this paper is to initiate clustering of documents for the set of different files consisting of text files of geneprotein target interaction is related to the files based on applying on new existing similar measure. The function scope is which something exists based on finding similarity among two files or any documents like gene miRNA interaction data files. Typically to build a matrix as n ? n files or documents. A similarity function and designing of clustering algorithm is discussed in this paper. These processes carried out with feature sets and clusters to identify gene-miRNA predicted data and gene-miRNA interaction data.",,978-1-5090-5579-1,10.1109/ICEMIS.2016.7745383,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745383,gene-miRNA predicted interactions;text document files;features;gain and clustering,Proteins;Protein engineering;Text mining;Splicing;Clustering algorithms;Mice;Rats,bioinformatics;pattern classification;pattern clustering;proteins;relational databases;RNA,cluster analysis;gene-miRNA interactions documents;improved similarity measure;bioinformatics;binary files;medical files;biological files;relational data repositories;code patterns;sequential matrix;geneprotein target interaction;gene miRNA interaction data files,,4,,35,,17-Nov-16,,,IEEE,IEEE Conferences
Exploiting Wikipedia Knowledge for Conceptual Hierarchical Clustering of Documents,利用Wikipedia知識進行文檔的概念層次聚類,G. Spanakis; G. Siolas; A. Stafylopatis,NA; NA; NA,The Computer Journal,18-Jan-18,2012,55,3,299,312,"In this paper, we propose a novel method for conceptual hierarchical clustering of documents using knowledge extracted from Wikipedia. The proposed method overcomes the classic bag-of-words models disadvantages through the exploitation of Wikipedia textual content and link structure. A robust and compact document representation is built in real-time using the Wikipedia application programmer's interface, without the need to store locally any Wikipedia information. The clustering process is hierarchical and extends the idea of frequent items by using Wikipedia article titles for selecting cluster labels that are descriptive and important for the examined corpus. Experiments show that the proposed technique greatly improves over the baseline approach, both in terms of F-measure and entropy on the one hand and computational cost on the other.",1460-2067,,10.1093/comjnl/bxr024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130321,document clustering;document representation;Wikipedia knowledge;conceptual clustering,,,,,3,,,,18-Jan-18,,,OUP,OUP Journals
Hierarchically Distributed Peer-to-Peer Document Clustering and Cluster Summarization,分層分佈的對等文檔聚類和聚類匯總,K. M. Hammouda; M. S. Kamel,"University of Waterloo, Waterloo; University of Waterloo, Waterloo",IEEE Transactions on Knowledge and Data Engineering,24-Mar-09,2009,21,5,681,698,"In distributed data mining, adopting a flat node distribution model can affect scalability. To address the problem of modularity, flexibility and scalability, we propose a Hierarchically-distributed Peer-to-Peer (HP2PC) architecture and clustering algorithm. The architecture is based on a multi-layer overlay network of peer neighborhoods. Supernodes, which act as representatives of neighborhoods, are recursively grouped to form higher level neighborhoods. Within a certain level of the hierarchy, peers cooperate within their respective neighborhoods to perform P2P clustering. Using this model, we can partition the clustering problem in a modular way across neighborhoods, solve each part individually using a distributed K-means variant, then successively combine clusterings up the hierarchy where increasingly more global solutions are computed. In addition, for document clustering applications, we summarize the distributed document clusters using a distributed keyphrase extraction algorithm, thus providing interpretation of the clusters. Results show decent speedup, reaching 165 times faster than centralized clustering for a 250-node simulated network, with comparable clustering quality to the centralized approach. We also provide comparison to the P2P K-means algorithm and show that HP2PC accuracy is better for typical hierarchy heights. Results for distributed cluster summarization match those of their centralized counterparts with up to 88% accuracy.",1558-2191,,10.1109/TKDE.2008.189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4626955,Clustering;Text mining;Data mining;Abstracting methods;Distributed systems;Clustering;Text mining;Data mining;Abstracting methods;Distributed systems,Peer to peer computing;Data mining;Distributed decision making;Scalability;Clustering algorithms;Partitioning algorithms;Distributed computing;Network topology;Communication system traffic control;Nonhomogeneous media,data mining;distributed processing;document handling;pattern clustering;peer-to-peer computing,hierarchically distributed peer-to-peer document clustering;distributed data mining;flat node distribution;multilayer overlay network;higher level neighborhoods;distributed k-means variant;distributed document clusters;distributed keyphrase extraction algorithm;distributed cluster summarization,,32,,32,,19-Sep-08,,,IEEE,IEEE Journals
A fuzzy-based algorithm for Web document clustering,基於模糊的Web文檔聚類算法,M. Friedman; A. Kandel; M. Schneider; M. Last; B. Shapira; Y. Elovici; O. Zaafrany,"Dept. of Phys., Nucl. Res. Center-Negev, Beer-Sheva, Israel; NA; NA; NA; NA; NA; NA","IEEE Annual Meeting of the Fuzzy Information, 2004. Processing NAFIPS '04.",27-Sep-04,2004,2,,524,527 Vol.2,"Most existing methods of document clustering are based on a model that assumes a fixed-size vector representation of key terms or key phrases within each document. This assumption is not realistic in large and diverse document collections such as the World Wide Web. We propose a new fuzzy-based document clustering method (FDCM), to cluster documents that are represented by variable length vectors. Each vector element consists of two fields. The first is an identification of a key phrase (its name) in the document and the second denotes a frequency associated with this key phrase within the particular document. A new averaging method is defined for the cluster centroid calculating, and a membership function is developed for relating new documents to existing clusters. The proposed approach is described in detail and we show how it is implemented in a real world application from the area of Web monitoring.",,0-7803-8376-1,10.1109/NAFIPS.2004.1337355,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1337355,,Clustering algorithms;Clustering methods;Frequency;Information systems;Systems engineering and theory;Computer science;Electronic mail;Educational institutions;Web sites;Monitoring,pattern clustering;Internet;fuzzy logic;document handling,fuzzy-based algorithm;Web document clustering;vector representation;World Wide Web;Web monitoring,,5,,10,,27-Sep-04,,,IEEE,IEEE Conferences
An Overview of Clustering Models with an Application to Document Clustering,聚類模型概述及其在文檔聚類中的應用,I. Pauletic; L. N. Prskalo; M. B. Bakaric,"Department of Informatics, University of Rijeka, Radmile Matej?i?, Rijeka, 2, 51000, Croatia; Department of Informatics, University of Rijeka, Radmile Matej?i?, Rijeka, 2, 51000, Croatia; Department of Informatics, University of Rijeka, Radmile Matej?i?, Rijeka, 2, 51000, Croatia","2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",11-Jul-19,2019,,,1659,1664,"This paper presents an overview of selected clustering models and shows an application of K-Means algorithm to document clustering. In the introductory part, the definitions of basic concepts and common characteristics of clustering models are described. Then an overview of clustering models is given. The methods of clustering, basic characteristics, visualization and possible input data for each algorithm are presented. The authors also explain the assessment of each algorithm taking into consideration measures such as Rand index, homogeneity completeness, V-measure and Silhouette coefficient. Furthermore, the paper describes the application of the K-Means algorithm to document clustering showing the final result and elaborating the procedures applied when clustering the documents.",2623-8764,978-953-233-098-4,10.23919/MIPRO.2019.8756868,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756868,clustering;clustering algorithms;K-Means;Mean-shift;hierarchical clustering;DBSCAN;Birch;clustering evaluation,Clustering algorithms;Indexes;Classification algorithms;Performance evaluation;Convergence;Labeling;Machine learning algorithms,document handling;pattern clustering,clustering models;document clustering;k-means algorithm;visualization;Rand index;V-measure;Silhouette coefficient,,1,,13,,11-Jul-19,,,IEEE,IEEE Conferences
Hybrid Bisect K-Means Clustering Algorithm,混合二等分K均值聚類算法,K. Murugesan; J. Zhang,"Dept. of Comput. Sci., Univ. of Kentucky, Lexington, KY, USA; Dept. of Comput. Sci., Univ. of Kentucky, Lexington, KY, USA",2011 International Conference on Business Computing and Global Informatization,25-Aug-11,2011,,,216,219,"In this paper, we present a hybrid clustering algorithm that combines divisive and agglomerative hierarchical clustering algorithm. Our method uses bisect K-means for divisive clustering algorithm and Unweighted Pair Group Method with Arithmetic Mean (UPGMA) for agglomerative clustering algorithm. First, we cluster the document collection using bisect K-means clustering algorithm with the value K', which is greater than the total number of clusters, K. Second, we calculate the centroids of K' clusters obtained from the previous step. Then we apply the UPGMA agglomerative hierarchical algorithm on these centroids for the given value, K. After the UPGMA finds K clusters in these K' centroids, if two centroids ended up in the same cluster, then all of their documents will belong to the same cluster. We compared the goodness of clusters generated by bisect K-means and the proposed hybrid algorithms, measured on various cluster evaluation metrics. Our experimental results shows that the proposed method outperforms the standard bisect K-means algorithm.",2378-895X,978-1-4577-0788-9,10.1109/BCGIn.2011.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003884,Bisect K-means;hybrid algorithm;document clustering,Clustering algorithms;Partitioning algorithms;Entropy;Hybrid power systems;Complexity theory;Computer science;Measurement,document handling;pattern clustering,agglomerative hierarchical clustering algorithm;unweighted pair group method;arithmetic mean;document clustering;UPGMA agglomerative hierarchical algorithm;k-means algorithm,,10,,16,,25-Aug-11,,,IEEE,IEEE Conferences
Clustering Ontology-enriched Graph Representation for Biomedical Documents based on Scale-Free Network Theory,基於無標度網絡理論的生物醫學文檔富集本體豐富圖表示,I. Yoo; X. Hu,"College of Information Science and Technology, Drexel University, Philadelphia, PA 19104 USA. e-mail: potence@drexel.edu; College of Information Science and Technology, Drexel University, Philadelphia, PA 19104 USA. e-mail: potence@drexel.edu",2006 3rd International IEEE Conference Intelligent Systems,23-Apr-07,2006,,,851,858,"In this paper we introduce a novel document clustering approach that solves some major problems of traditional document clustering approaches. Instead of depending on traditional vector space model, this approach represents documents as graphs using domain knowledge in ontology because graphs can represent the semantic relationships among the concepts in documents. Based on scale-free network theory, our approach generates a model for each document cluster from the ontology-enriched graph representation by identifying k high density subgraphs capturing the core semantic relationship information about each document cluster. Using these k high density subgraphs, each document is assigned to a proper document cluster. Our extensive experimental results on MEDLINE articles show that our approach outperforms two leading document clustering algorithms, BiSecting K-means and CLUTO's vcluster. Moreover, our approach provides a meaningful explanation for document clustering through generated models. This explanation helps users to understand clustering results and documents as a whole",1941-1294,1-4244-0195-X,10.1109/IS.2006.348532,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4155539,document clustering;graph clustering;ontology;scale-free network,Ontologies;Network theory (graphs);Clustering algorithms;Neoplasms;Vocabulary;Information retrieval;Nearest neighbor searches;Text mining;Engineering profession;Educational institutions,complex networks;document handling;medical administrative data processing;network theory (graphs);ontologies (artificial intelligence);pattern clustering,document clustering;ontology-enriched graph representation;biomedical documents;scale-free network theory;domain knowledge;semantic relationships;high density subgraphs;MEDLINE articles;graph clustering,,2,,35,,23-Apr-07,,,IEEE,IEEE Conferences
Unsupervised document clustering based on keyword clusters,基於關鍵字聚類的無監督文檔聚類,Hsi-Cheng Chang; Chiun-Chieh Hsu; Yi-Wen Deng,"Dept. of Electron. Eng., Hwa Hsia Coll. of Technol. & Commerce, Taipei, Taiwan; NA; NA","IEEE International Symposium on Communications and Information Technology, 2004. ISCIT 2004.",11-Apr-05,2004,2,,1198,1203 vol.2,"Due to the explosion growth of digital information, automatic document clustering or categorization has been an important research topic. Since document clustering has high dimension, the magnitude of the representation features will influence the efficiency and effect of the clustering and the precision of the clustering results. This paper presents an unsupervised document clustering method based on partitioning a weighted undirected graph. It initially discovers a set of tightly relevant keyword clusters that are disposed throughout the feature space of the collection of documents, and further clusters the documents into document clusters by using these keyword clusters. The experimental results show that the proposed approach can efficiently produce higher quality document clustering as compared with several well-known document clustering algorithms.",,0-7803-8593-4,10.1109/ISCIT.2004.1413908,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1413908,,Clustering methods;Information retrieval;Clustering algorithms;Organizing;Educational institutions;Business;Cities and towns;Information management;Explosions;Internet,pattern clustering;document handling;relevance feedback;graph theory,keyword clusters;weighted undirected graph partitioning;tightly relevant keyword clusters;automatic document categorization;unsupervised document clustering,,1,,20,,11-Apr-05,,,IEEE,IEEE Conferences
An Efficient Spectral Method for Document Cluster Ensemble,一種高效的文檔簇集成譜方法,S. Xu; Z. Lu; G. Gu,"Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin; Coll. of Inf. & Commun. Eng., Harbin Eng. Univ., Harbin; Coll. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin",2008 The 9th International Conference for Young Computer Scientists,12-Dec-08,2008,,,808,813,"Cluster ensemble techniques have been recently shown to be effective in improving the accuracy and stability of single clustering algorithms. A critical problem in cluster ensemble is how to combine multiple clusterers to yield a final superior clustering result. In this paper, we present an efficient spectral graph theory-based ensemble clustering method feasible for large scale applications such as document clustering. Since the EigenValue Decomposition (EVD) of Laplacian is formidable for large document sets, we first transform it to a Singular Value Decomposition (SVD) problem, and then an equivalent EVD is performed. Experiments show that our spectral algorithm yields better clustering results than other cluster ensemble techniques without high computational cost.",,978-0-7695-3398-8,10.1109/ICYCS.2008.228,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709078,clustering analysis;cluster ensemble;spectral clustering;document clustering,Clustering algorithms;Partitioning algorithms;Laplace equations;Computational efficiency;Educational institutions;Eigenvalues and eigenfunctions;Pattern analysis;Machine learning algorithms;Bagging;Boosting,document handling;eigenvalues and eigenfunctions;graph theory;pattern clustering;singular value decomposition,document cluster ensemble;cluster ensemble techniques;spectral graph theory;ensemble clustering method;eigenvalue decomposition;singular value decomposition problem,,6,,31,,12-Dec-08,,,IEEE,IEEE Conferences
Extended ACO based document clustering with hybrid distance metric,具有混合距離度量的基於擴展ACO的文檔聚類,K. Subhadra; M. Shashi; A. Das,"Department of CSE, GITAM University, Visakhapatnam, India; Department of CS&SE, Andhra University, Visakhapatnam, India; Database Administrator, S&P Capital IQ, Hyderabad, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",27-Aug-15,2015,,,1,6,"Large amount of high dimensional data has to be handled often to solve problems that arise in the field of information retrieval. This paper deals with the problem of grouping similar documents into clusters and then retrieving the required document with respect to the user's query. This work introduces a novel approach of document clustering which combines swarm intelligence techniquebased on the brood behavior of ants with standard clustering approaches. The main idea behind this paper is to apply nature inspired algorithm, Ant Colony Optimization (ACO) Algorithm for limited number of iterations followed by medoidbased post pruning. The proposed method has been applied to the dataset formed by collecting 10000 documents on various topics from the standard data repository Wikipedia. The experimental results proved that the proposed method achieved better clustering results in terms of precision and recall and in a very less time.",,978-1-4799-6085-9,10.1109/ICECCT.2015.7226090,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226090,Clustering;swarm intelligence;optimization;purity;recall,Measurement;Complexity theory;Encyclopedias;Electronic publishing;Internet,ant colony optimisation;document handling;pattern clustering;query processing;swarm intelligence,ACO based document clustering;hybrid distance metric;high dimensional data;information retrieval;similar documents grouping;document retrieval;user query;swarm intelligence technique;ants behavior;nature inspired algorithm;ant colony optimization;ACO algorithm;medoid based post pruning;data repository;Wikipedia,,1,,13,,27-Aug-15,,,IEEE,IEEE Conferences
A survey on methodologies used for semantic document clustering,語義文檔聚類方法的調查,A. Gupta; J. Gautam; A. Kumar,"Department of Computer Science & Engineering, JSS Academy of Technical Education, Noida; Department Department of Computer Science & Engineering, JSS Academy of Technical Education, Noida; Department of Computer Science & Engineering, JSS Academy of Technical Education, Noida","2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)",21-Jun-18,2017,,,671,675,"Document clustering is a traditional technique, and is used in multiple fields like data mining, information retrieval, knowledge discovery from data, pattern recognition etc. Large volumes of textual data being created in the modern world have resulted in the rise in importance of document clustering techniques. Although various document-clustering techniques have been studied in recent years, clustering quality still remains an area of concern. Particularly, majority of the present document clustering methods do not account for the semantic relationships and as a result give unsatisfactory clustering results. Semantic relationships consider the context of the usage of the term and do not solely rely on its isolated meaning. In the recent years, a lot of effort has gone into applying semantics to document clustering. This paper presents a survey of various research papers that have been studied and highlights the merits and demerits of each clustering algorithm. This will give a direction to future research in a more focused manner.",,978-1-5386-1887-5,10.1109/ICECDS.2017.8389521,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8389521,document clustering;evaluation measures;semantic;wordnet;word sense disambiguation,Semantics;Clustering algorithms;Genetic algorithms;Data analysis;Partitioning algorithms;Entropy;Computer science,document handling;information retrieval;pattern clustering;text analysis,textual data;document-clustering techniques;clustering quality;semantic relationships;unsatisfactory clustering results;clustering algorithm;semantic document clustering;information retrieval,,2,,29,,21-Jun-18,,,IEEE,IEEE Conferences
A hierarchical feature decomposition clustering algorithm for unsupervised classification of document image types,用於文檔圖像類型無監督分類的分層特徵分解聚類算法,D. Curtis; V. Kubushyn; E. A. Yfantis; M. Rogers,"Univ. of Nevada, Las Vegas; Univ. of Nevada, Las Vegas; Univ. of Nevada, Las Vegas; Univ. of Nevada, Las Vegas",Sixth International Conference on Machine Learning and Applications (ICMLA 2007),25-Feb-08,2007,,,423,428,"In a system where medical paper document images have been converted to a digital format by a scanning operation, understanding the document types that exists in this system could provide for vital data indexing and retrieval. In a system where millions of document images have been scanned, it is infeasible to expect a supervised based algorithm or a tedious (human based) effort to discover the document types. The most sensible and practical way is an unsupervised algorithm. Many clustering techniques have been developed for unsupervised classification. Many rely on all data being presented at once, the number of clusters to be known, or both. The algorithm presented in this paper is a two-threshold based technique relying on a hierarchical decomposition of the features. On a subset of document images, it discovered document types at an acceptable level and confidentially classified unknown document images.",,978-0-7695-3069-7,10.1109/ICMLA.2007.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457267,,Classification algorithms;Clustering algorithms;Optical character recognition software;Biomedical imaging;Indexing;Character recognition;Focusing;Feature extraction;Machine learning;Application software,classification;document image processing;indexing;information retrieval;medical information systems;pattern clustering;unsupervised learning,hierarchical feature decomposition clustering algorithm;unsupervised classification;document image types;medical paper document images;digital format;data indexing;data retrieval;supervised based algorithm;clustering techniques,,2,,18,,25-Feb-08,,,IEEE,IEEE Conferences
The research on document clustering of network hot topics,網絡熱點話題的文檔聚類研究,Lin Tang; Wei Si,"Computer and Information Engineering College, Chifeng University, China; Computer and Information Engineering College, Chifeng University, China",2016 2nd IEEE International Conference on Computer and Communications (ICCC),11-May-17,2016,,,2472,2475,"There are endless of unexpected events and hot topics on the internet, facing with massive resources from different channels, the method to find hot topics and then present to the users is the key technology to discover hot topics. How to effectively organize these resources and obtain valuable data from it is the primary problem to be solved, however, clustering is an effective solution to this problem.",,978-1-4673-9026-2,10.1109/CompComm.2016.7925143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7925143,hot topics;network;document clustering,Analytical models;Classification algorithms;Silicon;Information filters;Biological system modeling;Standards,document handling;information retrieval;Internet;pattern clustering,document clustering;network hot topics;unexpected events;Internet,,,,10,,11-May-17,,,IEEE,IEEE Conferences
Application of Multi-Level Classifiers and Clustering for Automatic Word Spotting in Historical Document Images,多層分類器和聚類在歷史文獻圖像自動詞點識別中的應用,R. F. Moghaddam; M. Cheriet,"Synchromedia Lab. for Multimedia Commun. in Telepresence, Ecole de Technol. Super., Montreal, QC, Canada; Synchromedia Lab. for Multimedia Commun. in Telepresence, Ecole de Technol. Super., Montreal, QC, Canada",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,511,515,"A complete system for preprocessing and word spotting of very old historical document images is presented. Document images are processed for extraction of salient information using a word spotting technique which does not need line and word segmentation and is language independent.A multi-class library of connected components of document text is created based on six features. The spotting is performed using Euclidean distance measure enhanced by rotation and dynamic time wrapping transforms. The method is applied to a dataset from Juma Al Majid Center (Dubai)with promising results. A promising performance of the word spotting technique is obtained using an automatic preprocessing stage. In this stage, using content-level classifiers, accurate stroke pixels are extracted in a robust way. The preprocessed document images are also more legible to the end user and are less costly to archive and transfer.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277605,,Robustness;Euclidean distance;Wrapping;Image segmentation;Image restoration;Text analysis;Image analysis;Image recognition;Laboratories;Multimedia communication,document image processing;feature extraction;history;image classification;pattern clustering,historical document image;word spotting technique;feature extraction;Euclidean distance measure;dynamic time wrapping transform;Juma Al Majid Center;content-level classifier;clustering technique,,22,,7,,2-Oct-09,,,IEEE,IEEE Conferences
An approach for printed document labeling,打印文檔標籤的方法,C. Adak,"Dept. of Computer Science and Engineering, University of Kalyani, West Bengal-741235, India","2014 First International Conference on Automation, Control, Energy and Systems (ACES)",1-May-14,2014,,,1,4,"A document image contains texts and non-texts, it may be printed, handwritten, or hybrid of both. In this paper we deal with printed document where textual region is of printed characters, and non-texts are mainly photo images. Here we propose a model which performs labeling of different components of a printed document image, i.e. identification of heading, subheading, caption, article and photo. Our method consists of a preprocessing stage where fuzzy c-means clustering is used to segment the document image into printed (object) region and background. Then Hough transformation is used to find white-line dividers of object region and grid structure examination is used to extract the non-text portion. After that, we use horizontal histogram to find text lines and then we label different components. Our method gives promising results on printed document of different scripts.",,978-1-4799-3894-0,10.1109/ACES.2014.6808032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808032,Document Image Analysis;Document Labeling;Fuzzy C-Means Clustering;Hough Transform;Optical Character Recognition,Labeling;Optical character recognition software;Text analysis;Image analysis;Image segmentation;Transforms;Histograms,document image processing;fuzzy set theory;Hough transforms;pattern clustering;text analysis,printed document labeling;document image;textual region;printed characters;printed document image;preprocessing stage;fuzzy c-means clustering;Hough transformation;white-line dividers;object region;grid structure examination;nontext portion;horizontal histogram,,,,8,,1-May-14,,,IEEE,IEEE Conferences
An Efficient Hybrid Hierarchical Document Clustering Method,一種高效的混合層次文檔聚類方法,Y. Zhu; B. C. M. Fung; D. Mu; Y. Li,"Northwestern Polytech. Univ., Xi'an; Concordia Univ., Montreal, QC; Northwestern Polytech. Univ., Xi'an; Northwestern Polytech. Univ., Xi'an",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,2,,395,399,"Document clustering is a technique for grouping document objects together such that documents within a cluster have high similarity while documents in different clusters have low similarity. Hierarchical document clustering organizes the clusters into a hierarchy such that a parent cluster is a general topic of its child clusters. In this paper, we propose a novel hierarchical document clustering method that is a hybrid version of partitioning and agglomerative clustering approaches. The proposed method inherits the merit of efficiency from the partitioning approach and the hierarchical structure from agglomerative approach. Experiments on real-life datasets suggest that our method is effective and efficient.",,978-0-7695-3305-6,10.1109/FSKD.2008.159,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666146,data mining;clustering algorithm;document clustering;hybrid method,Clustering methods;Information retrieval;Clustering algorithms;Partitioning algorithms;Itemsets;Search engines;Fuzzy systems;Keyword search;Scalability;Performance analysis,document handling,hybrid hierarchical document clustering;document objects grouping;agglomerative clustering,,1,,9,,5-Nov-08,,,IEEE,IEEE Conferences
Accessing Accurate Documents by Mining Auxiliary Document Information,通過挖掘輔助文檔信息來訪問準確的文檔,P. J. Joby; J. Korra,"Dept. of Comput. Sci. & Eng., Christ Univ., Bangalore, India; Dept. of Comput. Sci. & Eng., Christ Univ., Bangalore, India",2015 Second International Conference on Advances in Computing and Communication Engineering,26-Oct-15,2015,,,634,638,"Earlier techniques of text mining included algorithms like k-means, Na簿ve Bayes, SVM which classify and cluster the text document for mining relevant information about the documents. The need for improving the mining techniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside the text documents to improve the mining. This auxiliary information can be a description to the content. This information can be either useful or completely useless for mining. The user should assess the worth of the auxiliary information before considering this technique for text mining. In this paper, a combination of classical clustering algorithms is used to mine the datasets. The algorithm runs in two stages which carry out mining at different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.",,978-1-4799-1734-1,10.1109/ICACCE.2015.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306760,Auxiliary Information;Document clustering;Text mining,Clustering algorithms;Classification algorithms;Text mining;Algorithm design and analysis;Indexes;Computer science,data mining;pattern classification;pattern clustering;text analysis,text mining;auxiliary document information;document clustering;document classification,,,,11,,26-Oct-15,,,IEEE,IEEE Conferences
Document clustering using sequential pattern (SP): Maximal frequent sequences (MFS) as SP representation,使用順序模式（SP）的文檔聚類：最大頻繁序列（MFS）作為SP表示,D. Rahmawati; G. A. P. Saptawati; Y. Widyani,"School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia",2015 International Conference on Data and Software Engineering (ICoDSE),21-Mar-16,2015,,,98,102,"This research proposes an idea to apply Feature Based Clustering (FBC) in document clustering. A huge number of existing documents will be easier to be used if they are clustered into several topics. FBC uses K-Means algorithm to cluster sequential data of features. Features of text document can be presented as sequence of word. In order to be processed as sequential data, features must be extracted from collection of unstructured text documents. Therefore, we need preprocessing tasks to deliver appropriate form of document features. There are two types of sequential pattern using simple form: Frequent Word Sequence (FWS) and Maximal Frequent Sequence (MFS). Both types are appropriate for text data. The difference is in applying the maximum principle in MFS. Therefore, MFS amount from a text document would be less than the amount of its FWS. In this research, we choose maximal frequent sequences (MFS) as feature representation. We proposes framework to conduct FBC using MFS as features. The framework is tested to cluster dataset that is subset of the Twenty News Group Text Data. The result shows that the accuracy of clustering result is affected by the parameter's value, dataset, and the number of target cluster.",,978-1-4673-8430-8,10.1109/ICODSE.2015.7436979,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436979,document clustering;feature-based clustering;sequential pattern;maximal frequent sequences,Clustering algorithms;Text mining;Software engineering;Clustering methods;Information retrieval;Feature extraction,data structures;feature extraction;maximum principle;pattern clustering;text analysis,document clustering;maximal frequent sequence;MFS;SP representation;feature-based clustering;FBC;K-means algorithm;cluster sequential data;unstructured text document feature extraction;frequent word sequence;FWS;feature representation;Twenty News group text data,,2,,13,,21-Mar-16,,,IEEE,IEEE Conferences
Merging clustering and classification results for whole book recognition,合併聚類和分類結果以實現全書識別,M. R. Soheili; M. R. Yousefi; E. Kabir; D. Stricker,"Electrical and Computer Engineering Department, Kharazmi University, Karaj, Iran; DFKI, Kaiserslautern, Germany; School of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; DFKI, Kaiserslautern, Germany",2017 10th Iranian Conference on Machine Vision and Image Processing (MVIP),23-Apr-18,2017,,,134,138,"Historical printed books OCR is one of the challenging tasks in the area of document image analysis. Low quality of print and paper and unfamiliar font faces are the most known problems. However, redundancy of word and sub-word occurrences in the document can be used to improve the recognition results. In this paper, we propose a highly accurate recognition system for printed old books. We use the combination of sub-word clustering and a LSTM neural network as a character recognizer to reduce the error rate. Due to the lack of information about the font faces, we manually label some part of the books. We show that the recognition error rate can be reduced noticeably by combining the results of the LSTM recognizer and sub-word clustering and rejecting the cases where the two methods differ in their prediction.",2166-6784,978-1-5386-4405-8,10.1109/IranianMVIP.2017.8342338,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342338,Document Image Analysis;Whole Book Recognition;Sub-word Image Clustering;LSTM Neural Network;Merging Clustering and Classification Results;Farsi Sub-word Recognition;Persian,Optical character recognition software;Neural networks;Image recognition;Text analysis;Error analysis;Clustering methods,document image processing;image classification;optical character recognition;pattern clustering;recurrent neural nets,unfamiliar font faces;recognition results;printed old books;LSTM neural network;character recognizer;recognition error rate;LSTM recognizer;classification results;book recognition;historical printed books OCR;document image analysis;low print quality;recognition system;subword clustering;whole book recognition;clustering results;subword occurrences,,,,21,,23-Apr-18,,,IEEE,IEEE Conferences
Clustering Algorithm on Block Division of Documents,文檔分塊的聚類算法,G. Liu; M. Luo,"Sch. of Electron. & Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Electron. & Eng., Beijing Univ. of Posts & Telecommun., Beijing, China",2010 6th International Conference on Wireless Communications Networking and Mobile Computing (WiCOM),14-Oct-10,2010,,,1,4,"In the traditional K-means algorithm, the selection of cluster number and the initial cluster center brings huge affection on the quality of clustering. To reduce the dependence on the initial center and to locate the types of new data rapidly, an algorithm applicable for text data is proposed. In this algorithm, document density is considered as parameter. Documents are divided into blocks first. After that every divided block is clustered separately. Experiment shows that this algorithm not only makes higher quality for clustering, but also does well in the new increasing data.",2161-9654,978-1-4244-3708-5,10.1109/WICOM.2010.5600166,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600166,,Clustering algorithms;Algorithm design and analysis;Partitioning algorithms;Internet;Computational modeling;Fluctuations;Vocabulary,document handling;pattern clustering,clustering algorithm;document block division;K-means algorithm;clustering quality;document density,,,,4,,14-Oct-10,,,IEEE,IEEE Conferences
Flexible document organization by mixing fuzzy and possibilistic clustering algorithms,通過混合模糊和可能的聚類算法來靈活地組織文檔,N. V. Carvalho; S. O. Rezende; H. A. Camargo; T. M. Nogueira,"Department of Computer Science, Federal University of Bahia- Brazil; Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Department of Computer Science, Federal University of S瓊o Carlos - Brazil; Department of Computer Science, Federal University of Bahia- Brazil",2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),10-Nov-16,2016,,,790,797,"A powerful and flexible organization of documents can be obtained by mixing fuzzy and possibilistic clustering. In such organization, documents can belong to more than one cluster simultaneously with different compatibility degrees. Clusters represent topics, which are identified by one or more descriptors extracted by a proposed method. In this manuscript, we investigated whether or not the descriptors extracted after applying possibilistic fuzzy clustering improve the flexible organization of documents. Experiments were carried out on real-world document collections and we evaluated the ability of descriptors to capture the essential information in every dataset. Results have shown the effectiveness of extracting possibilistic fuzzy cluster descriptors, improving the flexible organization of documents.",,978-1-5090-0626-7,10.1109/FUZZ-IEEE.2016.7737768,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737768,fuzzy clustering;possibilistic clustering;flexible organization;documents;text mining,Clustering algorithms;Organizations;Phase change materials;Prototypes;Mathematical model;Computer science;Feature extraction,document handling;fuzzy set theory;pattern clustering;possibility theory,flexible document organization;mixing fuzzy clustering;possibilistic fuzzy clustering;document collections,,1,,33,,10-Nov-16,,,IEEE,IEEE Conferences
Hierarchical Clustering Algorithms for Large Datasets,大數據集的分層聚類算法,Y. Stekh; A. Kernytskyy; M. Lobur,"Lviv Polytechnic National University, S. Bandery Str., 12, Lviv, 79013, UKRAINE; Lviv Polytechnic National University, S. Bandery Str., 12, Lviv, 79013, UKRAINE; Lviv Polytechnic National University, S. Bandery Str., 12, Lviv, 79013, UKRAINE","2006 International Conference - Modern Problems of Radio Engineering, Telecommunications, and Computer Science",17-Dec-07,2006,,,388,390,This paper focuses on document clustering algorithms that build hierarchical solutions. In this paper is evaluate the performance of different criterion functions for the problem of clustering documents.,,966-553-507-2,10.1109/TCSET.2006.4404560,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4404560,Clustering algorithm;Criterion function;Document datasets,Clustering algorithms;Frequency measurement;Length measurement;Organizing;Visualization;Euclidean distance;Navigation,document handling;pattern clustering,hierarchical clustering algorithms;document datasets;document clustering algorithms,,1,,13,,17-Dec-07,,,IEEE,IEEE Conferences
Dirichlet Process Mixture Model for Document Clustering with Feature Partition,具有特徵分區的文檔聚類Dirichlet過程混合模型,R. Huang; G. Yu; Z. Wang; J. Zhang; L. Shi,"Guizhou University, China and The Hong Kong Polytechnic University, Hong Kong; The University of North Carolina at Chapel Hill, Chapel Hill; Nankai University, Tianjin; Sun Yat-sen University, Guangzhou; Tianjin University, Tianjin",IEEE Transactions on Knowledge and Data Engineering,28-Jun-13,2013,25,8,1748,1759,"Finding the appropriate number of clusters to which documents should be partitioned is crucial in document clustering. In this paper, we propose a novel approach, namely DPMFP, to discover the latent cluster structure based on the DPM model without requiring the number of clusters as input. Document features are automatically partitioned into two groups, in particular, discriminative words and nondiscriminative words, and contribute differently to document clustering. A variational inference algorithm is investigated to infer the document collection structure as well as the partition of document words at the same time. Our experiments indicate that our proposed approach performs well on the synthetic data set as well as real data sets. The comparison between our approach and state-of-the-art document clustering approaches shows that our approach is robust and effective for document clustering.",1558-2191,,10.1109/TKDE.2012.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152106,Database management;database applications-text mining;pattern recognition;clustering document clustering;Dirichlet process mixture model;feature partition,Clustering algorithms;Mathematical model;Inference algorithms;Data models;Approximation algorithms;Approximation methods;Equations,document handling;pattern clustering,Dirichlet process mixture model;document clustering;feature partition;DPMFP;DPM model;nondiscriminative words;variational inference algorithm;document collection structure;synthetic data set;document words partition,,18,,25,,14-Feb-12,,,IEEE,IEEE Journals
A Hybrid Geometric Approach for Measuring Similarity Level Among Documents and Document Clustering,測量文檔之間相似度和文檔聚類的混合幾何方法,A. Heidarian; M. J. Dinneen,"Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand; Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand",2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService),23-May-16,2016,,,142,151,"The increasing numbers of textual documents from diverse sources such as different websites (e.g. social networks, news, magazines, blogs and medical recommendation websites), publications and articles and medical prescriptions leads to massive amounts of daily complex data. This phenomenon has caused many researchers to focus on analysing the content and measuring the similarities among the documents and texts to cluster them. One popular method to measure the similarity between documents is to represent the documents as vectors and measure the similarity among them based on the angle or Euclidean distance between each pair. By only considering these two criteria for similarity measurement, we may miss important underlying similarities in this area. We propose a new method, TS-SS, to measure the similarity level among documents, in such a way that one hopes to better understand which documents are more (or less) similar. This similarity level can be used as a handy measure for clustering and recommendation systems for documents. It also can be used to show top n similar documents to a particular document or a search query. Our study gives insights on the drawbacks of geometrical and non-geometrical similarity measures and provides a novel method to combine the other geometric criteria into a method to measure the similarity level among documents from new prospective. We apply Euclidean distance, Cosine similarity and our new method on four labelled datasets. Finally we report how these three geometrical similarity measures perform in terms of similarity level and clustering purity using four evaluation techniques. The evaluations' results show that our new model outperforms the other measures.",,978-1-5090-2251-9,10.1109/BigDataService.2016.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474366,Document Clustering;Similarity Level;Document Similarity;Geometric Similarity;VSM,Euclidean distance;Semantics;Computational modeling;Current measurement;Frequency measurement;Extraterrestrial measurements;Correlation,geometry;pattern clustering;query processing;recommender systems;text analysis,hybrid geometric approach;similarity level measurement;document clustering;textual documents;content analysis;Euclidean distance;TS-SS;recommendation systems;search query;geometrical similarity measures;nongeometrical similarity measures;geometric criteria;cosine similarity;similarity level;clustering purity,,7,,33,,23-May-16,,,IEEE,IEEE Conferences
A Methodology for Clustering XML Documents Based on Labeled Tree,基於標籤樹的XML文檔聚類方法,L. Liu; Y. Zheng; B. Ding; H. Liu,"Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China; Software Coll., Hebei Normal Univ., Shijiazhuang, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,1,,397,401,"The amount of XML documents is increasing rapidly. In order to analyze the information represented in XML documents efficiently, researches on XML document clustering are actively in progress. The key issue is how to devise the similarity measure between XML documents to be used for clustering. Since XML documents have hierarchical structure, it is not appropriate to cluster them by using a general document similarity measure. In this paper, we propose the novel similarity calculation measure by reducing Nesting and repeating in the whole XML document. Then propose an improved Edge-set comparison algorithm to calculate two XML documents' similarity. Our experiments show that the proposed method improves accuracy on the clustering, compared to the previous works.",,978-0-7695-3735-1,10.1109/FSKD.2009.181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358550,XML;Clustering;Data mining;Structural similarity;Semi-structured data,XML;Information analysis;Clustering algorithms;Fuzzy systems;Computer science;Educational institutions;Measurement standards;Knowledge management;Management information systems;Information retrieval,pattern clustering;XML,XML documents;document clustering;document similarity measure;nesting reduction measure;repeating reduction method;edge-set comparison algorithm;hypermedia markup language;labeled tree,,1,,15,,28-Dec-09,,,IEEE,IEEE Conferences
Boosting Projection Neural Features for Semantic Similar Clustered Documents in Cloud,提升雲中語義相似聚類文檔的投影神經功能,B. Vinothini; N. Gnanambigai; P. Dinadayalan,"Bharathiar University,Coimbatore,India; Indira Gandhi Arts and Science College,Puducherry,India; Kanchi mamunivar Centre for Postgraduate Studies,Puducherry,India",2019 International Conference on Smart Systems and Inventive Technology (ICSSIT),10-Feb-20,2019,,,1163,1169,"Cloud computing has emerged as real world technology over the Internet. Due to the development of big data with high dimensionality, data storage possibility over cloud has created large scope in recent times. Document clustering is the fundamental topic that turned into an indispensable component in many areas like cloud computing. Document clustering partitions the document into significant classes or groups for retrieving the relevant document. Many researchers used the factorization methods and ontologies for internal and external knowledge based document clustering. However, existing methods failed to provide the semantic feature construction and leads to the information loss while covering all the ideas in documents. In order to address these problems, different document clustering techniques in cloud has been reviewed in this paper. In addition to that Document clustering by Entropy-based Boosting with Projection Neural Feature (EB-PNF) method is presented. The proposed method involves two stages. They are, similar document identification based on semantic similarity score, feature extraction which includes the extraction of both single and multi-label features based on the precision, recall and computational complexity to prove that EB-PNF method produces high-quality clusters comparable to the state-of-the-art methods.",,978-1-7281-2119-2,10.1109/ICSSIT46314.2019.8987838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987838,Document clustering;Cloud Computing;Entropy;Boosting;Projection Neural Feature;dimensionality;semantic feature;ontologies;factorization,,Big Data;cloud computing;computational complexity;document handling;entropy;feature extraction;information retrieval;ontologies (artificial intelligence);pattern clustering;storage management,semantic similar clustered documents;cloud computing;Big Data;data storage;Document clustering partitions;factorization methods;ontologies;internal knowledge based document clustering;external knowledge based document clustering;semantic feature construction;Projection Neural Feature method;feature extraction;computational complexity;EB-PNF method;projection neural feature boosting;entropy-based boosting with projection neural feature method,,,,14,,10-Feb-20,,,IEEE,IEEE Conferences
Ontologies improve text document clustering,本體改善文本文檔聚類,A. Hotho; S. Staab; G. Stumme,"Inst. fur Angewandte Inf. und Formale Beschreibungsverfahren, Karlsruhe Univ., Germany; Inst. fur Angewandte Inf. und Formale Beschreibungsverfahren, Karlsruhe Univ., Germany; Inst. fur Angewandte Inf. und Formale Beschreibungsverfahren, Karlsruhe Univ., Germany",Third IEEE International Conference on Data Mining,19-Dec-03,2003,,,541,544,"Text document clustering plays an important role in providing intuitive navigation and browsing mechanisms by organizing large sets of documents into a small number of meaningful clusters. The bag of words representation used for these clustering methods is often unsatisfactory as it ignores relationships between important terms that do not cooccur literally. In order to deal with the problem, we integrate core ontologies as background knowledge into the process of clustering text documents. Our experimental evaluations compare clustering techniques based on pre-categorizations of texts from Reuters newsfeeds and on a smaller domain of an eLearning course about Java. In the experiments, improvements of results by background knowledge compared to a baseline without background knowledge can be shown in many interesting combinations.",,0-7695-1978-4,10.1109/ICDM.2003.1250972,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250972,,Ontologies;Java;Electronic learning;Clustering algorithms;Navigation;Organizing;Clustering methods;Knowledge management;Web sites;Information retrieval,document handling;distance learning;pattern clustering;data mining,text document clustering;ontology;Reuters newsfeeds;information navigation;information browsing;Java elearning course;data mining,,138,,8,,19-Dec-03,,,IEEE,IEEE Conferences
A Novel Graph Based Clustering Approach to Document Topic Modeling,一種新穎的基於圖的聚類文檔主題建模方法,P. Chanda; A. K. Das,"Department of Computer Science and Technology, Indian Institute of Engineering Science and Technology, Shibpur, Howrah, India; Department of Computer Science and Technology, Indian Institute of Engineering Science and Technology, Shibpur, Howrah, India","2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",18-Oct-18,2018,,,1,7,"Clustering is the task of assigning a set of objects into groups so that the objects within the same cluster are more similar to each other than to those in other clusters based on some similarity measures. Clustering of documents is an important task in text mining based on their research topics. In this field, cluster analysis is the task of grouping a set of documents in such a way that the documents in the same cluster have similar topic and documents of different clusters have different topics. The proposed method introduces a novel graph based clustering method which uses the importance factor of a document based on a better mathematical approach than well known classical methods. Document with the maximum importance factor in a cluster is considered as the centroid of the cluster. Publicly available synthetic dataset is used to evaluate the performance of the proposed algorithm and the method is compared with some traditional graph based methods to demonstrate its accuracy.",,978-1-5386-4430-0,10.1109/ICCCNT.2018.8494134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8494134,Text mining;Document clustering;Graph based clustering;Importance factor;Newsgroup20 dataset,Clustering algorithms;Task analysis;Mathematical model;Partitioning algorithms;Computational modeling;Text mining;Feature extraction,data mining;graph theory;pattern clustering;text analysis,document topic modeling;similarity measures;cluster analysis;clustering approach;graph based clustering method;object clustering;document clustering;text mining,,,,12,,18-Oct-18,,,IEEE,IEEE Conferences
An Approach for Document Pre-processing and K Means Algorithm Implementation,一種文檔預處理方法及K均值算法的實現,S. Gowtham; M. Goswami; K. Balachandran; B. S. Purkayastha,"Fac. of Eng., Christ Univ., Bangalore, India; Fac. of Eng., Christ Univ., Bangalore, India; Fac. of Eng., Christ Univ., Bangalore, India; Assam Central Univ., Silchar, India",2014 Fourth International Conference on Advances in Computing and Communications,29-Sep-14,2014,,,162,166,"The web mining is a cutting edge technology, which includes information gathering and classification of information over web. This paper puts forth the concepts of document pre-processing, which is achieved by extraction of keywords from the documents fetched from the web, processing it and generating a term-document matrix, TF-IDF and the different approaches of TF-IDF (term frequency Inverse document frequency) for each respective document. The last step is the clustering of these results through K Means algorithm, by comparing the performance of each approach used. The algorithm is realized on an X64 architecture and coded on Java and Matlab platform. The results are tabulated.",,978-1-4799-4363-0,10.1109/ICACC.2014.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906015,Stop words;stemming;term-document matrix;frequency;tf-idf;augmented;logarithmic;K Means clustering,Clustering algorithms;Information retrieval;Algorithm design and analysis;Classification algorithms;Java;MATLAB;Data mining,classification;data mining;document handling;Internet;Java;pattern clustering,Matlab platform;Java;X64 architecture;term frequency inverse document frequency;TF-IDF;term-document matrix;World Wide Web;information classification;information gathering;cutting edge technology;Web mining;k means algorithm implementation;document preprocessing,,2,,9,,29-Sep-14,,,IEEE,IEEE Conferences
Large-scale multi-dimensional document clustering on GPU clusters,GPU集群上的大規模多維文檔集群,Y. Zhang; F. Mueller; X. Cui; T. Potok,"Dept. of Computer Science, North Carolina State University, Raleigh, NC 27695-7534; Dept. of Computer Science, North Carolina State University, Raleigh, NC 27695-7534; Oak Ridge National Laboratory, Computational Sciences and Engineering Division, Oak Ridge, TN 37831; Oak Ridge National Laboratory, Computational Sciences and Engineering Division, Oak Ridge, TN 37831",2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS),24-May-10,2010,,,1,10,"Document clustering plays an important role in data mining systems. Recently, a flocking-based document clustering algorithm has been proposed to solve the problem through simulation resembling the flocking behavior of birds in nature. This method is superior to other clustering algorithms, including k-means, in the sense that the outcome is not sensitive to the initial state. One limitation of this approach is that the algorithmic complexity is inherently quadratic in the number of documents. As a result, execution time becomes a bottleneck with large number of documents. In this paper, we assess the benefits of exploiting the computational power of Beowulf-like clusters equipped with contemporary Graphics Processing Units (GPUs) as a means to significantly reduce the runtime of flocking-based document clustering. Our framework scales up to over one million documents processed simultaneously in a sixteen-node moderate GPU cluster. Results are also compared to a four-node cluster with higher-end GPUs. On these clusters, we observe 30X-50X speedups, which demonstrate the potential of GPU clusters to efficiently solve massive data mining problems. Such speedups combined with the scalability potential and accelerator-based parallelization are unique in the domain of document-based data mining, to the best of our knowledge.",1530-2075,978-1-4244-6443-2,10.1109/IPDPS.2010.5470429,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5470429,,Large-scale systems;Clustering algorithms;Computational modeling;Data mining;Acceleration;Birds;Information analysis;Computer science;Laboratories;Data engineering,computational complexity;computer graphic equipment;data mining;document handling;parallel processing;pattern clustering,large-scale multidimensional document clustering;GPU clusters;data mining systems;flocking-based document clustering algorithm;k-means;algorithmic complexity;computational power;Beowulf-like clusters;contemporary graphics processing units;accelerator-based parallelization;document-based data mining,,16,,19,,24-May-10,,,IEEE,IEEE Conferences
Scalable Overlapping Co-clustering of Word-Document Data,Word文檔數據的可擴展重疊共簇,F. O. D. Fran癟a,"Center of Math., Comput. & Cognition (CMCC), Fed. Univ. of ABC (UFABC), Santo Andre, Brazil",2012 11th International Conference on Machine Learning and Applications,10-Jan-13,2012,1,,464,467,"Text clustering is used on a variety of applications such as content-based recommendation, categorization, summarization, information retrieval and automatic topic extraction. Since most pair of documents usually shares just a small percentage of words, the dataset representation tends to become very sparse, thus the need of using a similarity metric capable of a partial matching of a set of features. The technique known as Co-Clustering is capable of finding several clusters inside a dataset with each cluster composed of just a subset of the object and feature sets. In word-document data this can be useful to identify the clusters of documents pertaining to the same topic, even though they share just a small fraction of words. In this paper a scalable co-clustering algorithm is proposed using the Locality-sensitive hashing technique in order to find co-clusters of documents. The proposed algorithm will be tested against other co-clustering and traditional algorithms in well known datasets. The results show that this algorithm is capable of finding clusters more accurately than other approaches while maintaining a linear complexity.",,978-1-4673-4651-1,10.1109/ICMLA.2012.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406666,co-clustering;text clustering;hashing,Clustering algorithms;Complexity theory;Text mining;Feature extraction;Machine learning;Mutual information;Accuracy,data structures;pattern clustering;text analysis,scalable overlapping coclustering;word-document data clustering;text clustering;dataset representation;locality-sensitive hashing technique,,3,,11,,10-Jan-13,,,IEEE,IEEE Conferences
Adaptive document image thresholding using foreground and background clustering,使用前景和背景聚類的自適應文檔圖像閾值化,A. E. Savakis,"Eastman Kodak Co., Rochester, NY, USA",Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269),6-Aug-02,1998,,,785,789 vol.3,"Two algorithms for document image thresholding are presented, that are suitable for scanning document images at high-speed. They are designed to operate on a portion of the image while scanning the document, thus, they fit a pipeline architecture and lend themselves to real-time implementation. The first algorithm is based on adaptive thresholding and uses local edge information to switch between global thresholding and adaptive local thresholding determined from the statistics of a local image window. The second thresholding algorithm is based on tracking the foreground and background levels using clustering based on a variant of the K-means algorithm. The two approaches may be used independently or may be combined for improved performance. Results are presented illustrating the algorithms' performance for document and pictorial images.",,0-8186-8821-1,10.1109/ICIP.1998.999064,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999064,,Clustering algorithms;Pipelines;Switches;Statistics;Image processing;Pixel;Image converters;Graphics;Costs;Histograms,adaptive signal processing;document image processing;edge detection;pattern clustering;pipeline processing;image scanners;statistical analysis,adaptive document image thresholding;foreground clustering;background clustering;foreground levels;document image scanning;high-speed scanning;pipeline architecture;real-time implementation;local edge information;global thresholding;local image window statistics;thresholding algorithm;background levels;K-means algorithm;pictorial images;performance,,24,,12,,6-Aug-02,,,IEEE,IEEE Conferences
Optimizing K-means text document clustering using latent semantic indexing and pillar algorithm,使用潛在語義索引和支柱算法優化K-means文本文檔聚類,S. Adinugroho; Y. A. Sari; M. A. Fauzi; P. P. Adikara,"Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Malang, Indonesia; Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Malang, Indonesia; Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Malang, Indonesia; Computer Vision Research Group, Faculty of Computer Science, Brawijaya University, Malang, Indonesia",2017 5th International Symposium on Computational and Business Intelligence (ISCBI),2-Oct-17,2017,,,81,85,"Document clustering is an important tool to help managing the vast amount of digital text document. This paper introduces a new approach to cluster text document. First, text is preprocessed and indexed using inverted index. Then the index is trimmed using TF-DF thresholding. After that, Term Document Matrix is built based on TF-IDF. Next step uses Latent Semantic Indexing to extract important feature from Term Document Matrix. The following process is selecting seeds via Pillar algorithm. Based on determined seeds, K-Means clustering is performed. Experiment result proves that this approach outperforms standard K-Means document clustering.",,978-1-5386-1772-4,10.1109/ISCBI.2017.8053549,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053549,document clustering;latent semantic indexing;k-means;seeds selection,Clustering algorithms;Indexing;Time division multiplexing;Semantics;Feature extraction;Large scale integration;Matrix decomposition,document handling;indexing;matrix algebra;pattern clustering;text analysis,text document clustering;latent semantic indexing;pillar algorithm;digital text document;TF-DF thresholding;Term Document Matrix;k-means document clustering,,1,,27,,2-Oct-17,,,IEEE,IEEE Conferences
Semantic document retrieval system using fuzzy clustering and reformulated query,基於模糊聚類和重構查詢的語義文檔檢索系統,D. Murali; A. Damodaram,"CSE, CMR CET, Hyderabad, T.S, India; CSE, JNT University, Hyderabad, T.S, India",2015 International Conference on Advances in Computer Engineering and Applications,23-Jul-15,2015,,,746,753,"In this paper, we develop an algorithm for document retrieval system through clustering process and query basis. Initially, the pre-processing is applied on whole documents to remove the unnecessary words and phrases of every document. Then the clustering process in applied to make the partition of the documents through the proposed semantic similarity measure used in the possibilistic fuzzy c means (PFCM) clustering algorithm. For each cluster, the index constructed, which contains common important keywords of the documents of cluster. Once the user enter the keyword as the input to the system, it will process the keywords with the WORDNET ontology to obtain the neighbourhood keywords and related synset keywords. From the set of keywords obtained from the WORDNET is refined and the refined keywords are matched with the index keywords of the clusters to calculate the matching score. Finally, the documents inside the cluster are released at first as the resultant related documents for the query keyword, which clusters have the maximum matching score values. The experimentation process is carried out with the help of different set of documents to achieve the results, the performance analysis of the proposed approach is estimated by precision, and we proved our proposed approach is outperformed in terms of precision.",,978-1-4673-6911-4,10.1109/ICACEA.2015.7164788,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164788,Document clustering;WORDNET;Ontology;Semantic similarity measure;possibilistic fuzzy c means,Semantics;Ontologies;Clustering algorithms;Indexing;Computers;Information retrieval,document handling;fuzzy set theory;ontologies (artificial intelligence);pattern clustering;query processing,semantic document retrieval system;fuzzy clustering;reformulated query;clustering process;query basis;documents partition;possibilistic fuzzy c means;performance analysis;query keyword;matching score;neighbourhood keywords;WORDNET ontology;PFCM clustering algorithm,,,,27,,23-Jul-15,,,IEEE,IEEE Conferences
Clustering research using dynamic modeling based on granular computing,基於粒度計算的動態建模聚類研究,Qun Liu; WenBiao Jin; SiYuan Wu; YingHua Zhou,"Dept. of Comput. & Sci., ChongQing Univ. of Posts & Telecommun., China; Dept. of Comput. & Sci., ChongQing Univ. of Posts & Telecommun., China; Dept. of Comput. & Sci., ChongQing Univ. of Posts & Telecommun., China; Dept. of Comput. & Sci., ChongQing Univ. of Posts & Telecommun., China",2005 IEEE International Conference on Granular Computing,5-Dec-05,2005,2,,539,543 Vol. 2,"Clustering techniques is a discovery process in data mining, especially used in characterizing customer groups based on purchasing patterns, categorizing Web documents, and so on. Many of the traditional clustering algorithms falter when the dimensionality of the feature space becomes high, relativing to the size of the document space, So it is important to precondition for modeling. Secondly, we are usually disappointed to their clustering speed. when having very large complex data sets, and another defect is that they always fit some static model, so if the user doesn't select appropriate static-model parameters, these algorithms can break down. In this paper, we introduce a new clustering algorithm to improve the speed of clustering, this clustering technique, which is based on granular computing and hypergraph partition, and it is capable of automatically discovering document similarities or associations, and this approach considers the internal characteristics of the clusters themselves, thus it doesn't depend on a static model. Finally, we conduct several experiments on real Web data searched by ordinary search engine and received the satisfied results.",,0-7803-9017-2,10.1109/GRC.2005.1547350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547350,Association rule discovery;Clustering research;Dynamic model;Frequent item sets;Granular computing;Hyper-graph partition algorithm;Web documents,Clustering algorithms;Telecommunication computing;Data mining;Partitioning algorithms;Web sites;Search engines;Contracts;Data analysis;Decision trees;Set theory,pattern clustering;data mining;document handling,dynamic modeling;granular computing;clustering technique;data mining;customer group;feature space;static-model parameter;hypergraph partition;document similarity discovery,,2,,,,5-Dec-05,,,IEEE,IEEE Conferences
Simultaneous Layout Style and Logical Entity Recognition in a Heterogeneous Collection of Documents,異構文檔集中的同時佈局樣式和邏輯實體識別,S. Chen; S. Mao; G. Thoma,U.S. National Library of Medicine; U.S. National Library of Medicine; U.S. National Library of Medicine,Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),12-Nov-07,2007,1,,118,122,"Logical entity recognition in heterogeneous collections of document page images remains a challenging problem since the performance of traditional supervised methods degrades dramatically in case of many distinct layout styles. In this paper we present an unsupervised method where layout style information is explicitly used in both training and recognition phases. We represent the layout style, local features, and logical labels of physical regions of a document compactly by an ordered labeled X-Y tree. Style dissimilarity of two document pages is represented by the distance between their respective trees. During the training phase, document pages with true logical labels in training set are classified into distinct layout styles by unsupervised clustering. During the recognition phase, the layout style and logical entities of an input document are recognized simultaneously by matching the input tree to the trees in closest- matched layout style cluster of training set. Experimental results show that our algorithm is robust with both balanced and unbalanced style cluster sizes, zone over-segmentation, zone length variation, and variation in tree representations of the same layout style.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4378687,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378687,,Tree graphs;Image recognition;Clustering algorithms;Robustness;Text analysis;Labeling;Costs;Libraries;Biomedical imaging;Degradation,document image processing;formal logic;pattern clustering,logical entity recognition;document page images;layout styles;unsupervised clustering;training set,,2,,13,,12-Nov-07,,,IEEE,IEEE Conferences
Improved document skew detection based on text line connected-component clustering,基於文本行連接組件聚類的改進文檔偏斜檢測,N. Liolios; N. Fakotakis; G. Kokkinakis,"Electr. & Comput. Eng. Dept., Patras Univ., Greece; NA; NA",Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205),7-Aug-02,2001,1,,1098,1101 vol.1,"The classical method of document skew detection, based on nearest-neighbor clustering, is revisited. A heuristic is proposed which attempts to group all the connected components that belong to the same line of text, into one cluster. The larger clusters are known to result in better skew angle estimation. The skew detection accuracy of this improved connected-components method is several orders of magnitude better, when compared to the classical approach, with no change in the order of complexity.",,0-7803-6725-1,10.1109/ICIP.2001.959241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959241,,Optical character recognition software;Histograms;Computational efficiency;Nearest neighbor searches;Wire;Laboratories;Character recognition;Clustering methods;Data mining;Clustering algorithms,optical character recognition;document image processing;pattern clustering;parameter estimation;computational complexity,document skew detection;text line;connected-component clustering;nearest-neighbor clustering;skew angle estimation;optical character recognition,,11,,14,,7-Aug-02,,,IEEE,IEEE Conferences
Hierarchical Document Clustering Using Fuzzy Association Rule Mining,基於模糊關聯規則挖掘的層次文檔聚類,C. Chen; F. S. C. Tseng; T. Liang,"Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu; NA; NA",2008 3rd International Conference on Innovative Computing Information and Control,22-Aug-08,2008,,,326,326,"In this paper, we will present an effective Fuzzy Frequent Itemset-Based Hierarchical Clustering (F2IHC) approach, which uses fuzzy frequent itemsets discovered by fuzzy association rule mining to improve the clustering accuracy of FIHC (Frequent Itemset-Based Hierarchical Clustering) method. Our approach can alleviate the deficiencies of most of the traditional document clustering methods in dealing with the problems of high dimensionality, large data size, and meaningful cluster labels. We have conducted experiments to evaluate our approach on Reuters 21578 dataset. The experimental results show that our approach not only absolutely retains the merits of FIHC, but also improves the document clustering accuracy quality as compared with the FIHC method.",,978-0-7695-3161-8,10.1109/ICICIC.2008.305,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4603515,,Data mining;Clustering algorithms;Association rules;Itemsets;Accuracy;Pediatrics;Databases,data mining;document handling;fuzzy set theory;pattern clustering,hierarchical document clustering;fuzzy association rule mining;fuzzy frequent itemset-based hierarchical clustering;FIHC method,,3,,11,,22-Aug-08,,,IEEE,IEEE Conferences
Document vector compression and its application in document clustering,文檔向量壓縮及其在文檔聚類中的應用,T. W. Fox,"Intelligent Engines, Calgary Univ., Alta., Canada","Canadian Conference on Electrical and Computer Engineering, 2005.",3-Jan-06,2005,,,2029,2032,"Document clustering organizes documents into groups such that each group contains documents with similar content. The majority of document clustering algorithms require a vector representation for each document. Each vector has well over 10,000 elements. Consequently, the memory required during clustering can be extremely high when clustering hundreds of thousands of documents. This paper introduces document vector compression, which is based on the discrete cosine transform (DCT). Document vector compression reduces the run-time memory requirements by as much as 60%. Document vector compression does not degrade the final cluster quality (total F-measure) as does other document vector reduction techniques",0840-7789,0-7803-8885-2,10.1109/CCECE.2005.1557384,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1557384,,Frequency;Discrete cosine transforms;Degradation;Clustering algorithms;Runtime;Engines;Data mining;Information retrieval;Compaction;Arithmetic,data compression;discrete cosine transforms;document image processing;image coding;image representation,document vector compression;document clustering algorithms;vector representation;discrete cosine transform;DCT;run-time memory requirements,,1,,8,,3-Jan-06,,,IEEE,IEEE Conferences
Document clustering using hierarchical SOMART neural network,使用分層SOMART神經網絡進行文檔聚類,M. F. Hussin; M. Kamel,"Dept. of Comput. Sci. & Autom. Control, Alexandria Univ., Egypt; NA","Proceedings of the International Joint Conference on Neural Networks, 2003.",26-Aug-03,2003,3,,2238,2242 vol.3,"Availability of large full-text document collections in electronic form has created a need for tools and techniques that assist users in organizing these collections. Document clustering is one of the popular methods used for this purpose. In this paper, we propose the neural network based document clustering method by using a hierarchically organized network built up from independent Self-Organizing Map (SOM) and Adaptive Resonance Theory (ART) neural networks. We present clustering results using the REUTERS corpus and show an improvement in clustering performance using both entropy and F-measure as evaluation measures.",1098-7576,0-7803-7898-9,10.1109/IJCNN.2003.1223758,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223758,,Neural networks;Subspace constraints;Artificial neural networks;Organizing;Clustering methods;Computer science;Automatic control;Entropy;Information retrieval;Search engines,pattern clustering;document handling;ART neural nets;self-organising feature maps,document clustering;hierarchical SOMART neural network;ART neural networks;adaptive resonance theory;SOM;self-organizing map;electronic text document collection;REUTERS test corpus,,7,,23,,26-Aug-03,,,IEEE,IEEE Conferences
Improving Arabic document clustering using K-means algorithm and Particle Swarm Optimization,使用K-means算法和粒子群優化算法改善阿拉伯文文檔聚類,A. S. Daoud; A. Sallam; M. E. Wheed,"Sinai University, North Sinai, Egypt; Faculty of Computers and Informatics, Suez Canal University, Ismailia, Egypt; Faculty of Computers and Informatics, Suez Canal University, Ismailia, Egypt",2017 Intelligent Systems Conference (IntelliSys),26-Mar-18,2017,,,879,885,"Document clustering plays a vital role in text mining fields such as information retrieval, sentiment analysis, and text organizing. Document clustering aims to automatically divide a collection of documents based on some aspects of similarity into groups that are meaningful, useful or both. This paper aims to improve the clustering task for the Arabic documents. Recent studies show that partitioning clustering algorithms are more suitable for clustering process. However, k-means is the most common algorithm that is being used for clustering process because of its simplicity and speed. It can only generate an arbitrary solution because the results depend on the initial centers for the desired clusters ?the seeds?? In this paper, a new modified k-means algorithm called PSO K-means, supported by Particle Swarm Optimization (PSO) is applied to enhance the Arabic document clustering process. Then, an intensive comparative study between the proposed model and the standard k-means algorithm is applied. Also, the stemming algorithms those are being used in Arabic language processing were assessed. Through the experiments, an evaluation for the new algorithm is done with three different Arabic data sets. The results demonstrate that the proposed model can produce more accurate results compared to the standard k-means algorithm for Arabic language documents. On the other hand, Arabic light stemmer is more suitable for the stemming step.",,978-1-5090-6435-9,10.1109/IntelliSys.2017.8324233,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324233,Arabic document clustering;NLP;swarm intelligence;PSO;k-means;features selection;features extraction,Clustering algorithms;Particle swarm optimization;Partitioning algorithms;Standards;Feature extraction;Classification algorithms;Iron,data mining;document handling;natural language processing;particle swarm optimisation;pattern clustering;text analysis,Arabic language documents;Arabic light stemmer;Particle Swarm Optimization;text mining fields;text organizing;clustering task;common algorithm;Arabic document clustering process;stemming algorithms;Arabic language processing;clustering algorithm partitioning;PSO;standard k-means algorithm;Arabic data sets,,,,37,,26-Mar-18,,,IEEE,IEEE Conferences
Concept-Enhanced Multi-view Clustering of Document Data,概念增強的文檔數據多視圖聚類,B. Diallo; J. Hu; T. Li; G. Khan; C. Ji,"Southwest Jiaotong University,Department of Computer Science and Technology,Chengdu,China; Southwest Jiaotong University,Department of Computer Science and Technology,Chengdu,China; Southwest Jiaotong University,Department of Computer Science and Technology,Chengdu,China; Southwest Jiaotong University,Department of Computer Science and Technology,Chengdu,China; Georgia State University,Department of Computer Science,Atlanta,USA",2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),18-Aug-20,2019,,,1258,1264,"Many works implemented multi-view clustering algorithms in document clustering. One challenging problem in document clustering is the similarity metric. Existing multi-view document clustering methods widely used two measurements: the Cosine similarity and the Euclidean Distance (ED). The first did not consider the magnitude between the two vectors. The second cannot compute the dissimilarity of two vectors that share the same ED. In this paper, we proposed a multi-view document clustering scheme to overcome these drawbacks by calculating the heterogeneity between documents with the same ED while taking into consideration their magnitudes. The experimental results show that the proposed similarity function can measure the similarity between documents more accurately than the existing metrics, and the proposed document clustering scheme goes beyond the limit of several state-of-the-art algorithms.",,978-1-7281-2348-6,10.1109/ISKE47853.2019.9170436,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9170436,Document clustering;Similarity measurement;Multi-view clustering,,document handling;pattern clustering,multiview document;document clustering;similarity function;cosine similarity;similarity metric;document data;concept-enhanced multiview clustering,,,,30,,18-Aug-20,,,IEEE,IEEE Conferences
Performance of Unsupervised Learning Algorithms for Online Document Clustering,在線文檔聚類的無監督學習算法的性能,D. S. Sisodia; A. Verma,"Department of Computer Science & Engineering, National Institute of Technology Raipur, Raipur, India; Department of Computer Science & Engineering, National Institute of Technology Raipur, Raipur, India",2018 International Conference on Inventive Research in Computing Applications (ICIRCA),3-Jan-19,2018,,,920,925,"The huge amounts of documents are available over the Internet. The effective automated process for indexing and searching of the online documents is essential for better user experience. The unsupervised learning techniques are useful for categorizing the available documents into correlated clusters for easier access. In this paper, partition based and hierarchical clustering techniques are discussed for clustering of the 20NewsGroups dataset documents. The experiments are performed using two partitions based on clustering such as K-Means and K-Medoids algorithms, and two hierarchical clustering, such as Single link analysis and complete link analysis methods. The different similarity measures such as Euclidean distance, Jaccard Coefficient, Cosine similarity, Pearson Correlation and ChiSquared distance is used for matching the similarity between documents. The clustering performance is evaluated using internal measures including DB Index, Silhouette Index, and C-Index; and external measures including F-Measure, Jaccard Index, and Rand Index. The results suggested that the partition based clustering algorithms perform better than the hierarchical clustering algorithms.",,978-1-5386-2456-2,10.1109/ICIRCA.2018.8597378,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8597378,Clustering;20NewsGroups data set;K-Means;K-Medoids;Single Link Analysis;Complete Link Analysis;similarity measures,Clustering algorithms;Partitioning algorithms;Indexes;Correlation;Euclidean distance;Machine learning algorithms;Clustering methods,document handling;pattern clustering;unsupervised learning,unsupervised learning algorithms;online document clustering;effective automated process;indexing;online documents;user experience;unsupervised learning techniques;20NewsGroups dataset documents;K-Medoids algorithms;Single link analysis;complete link analysis methods;Cosine similarity;clustering performance;internal measures;DB Index;Silhouette Index;C-Index;Jaccard Index;Rand Index;hierarchical clustering algorithms,,,,29,,3-Jan-19,,,IEEE,IEEE Conferences
Concept Detection and Cluster Analysis from Newsfeed-Singular Value Decomposition Based Approach,基於新聞源-奇異值分解的概念檢測和聚類分析,R. C. Chikkamath; B. S. Babu,"Comput. Sci. & Eng., Siddaganga Inst. of Technol., Tumkur, India; Comput. Sci. & Eng., Siddaganga Inst. of Technol., Tumkur, India",2016 2nd International Conference on Computational Intelligence and Networks (CINE),1-Sep-16,2016,,,130,135,"Concept detection plays an important role if there is a huge amount of data available. We know that cluster analysis, topic detection, opinion mining have got a major role in the product marketing, online shopping, E-commerce. In this paper, we have conducted the topic detection and clustering experiments on the News samples which were sourced from online newspapers. Our aim is to find out the topics which also available in the text documents as a group of words and apply a clustering technique using the Singular value decomposition method. Then opinions are extracted from the comments, collected on a particular subject of interest like the comments for Smartphone. Finally, the clustering technique is applied on these sentiments to figure out the opinions of the people towards different features of the Smartphone. The results obtained here are competitive with the technology available.",2375-5822,978-1-5090-0451-5,10.1109/CINE.2016.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556817,Singular value decomposition;Topic detection;Clusters;Sentiments;Term document matrix;Latent semantic Indexing;Independent Component analysis;Machine learning,Semantics;Clustering algorithms;Data mining;Matrix decomposition;Singular value decomposition;Feature extraction;Indexing,document handling;pattern clustering;sentiment analysis;singular value decomposition,concept detection;cluster analysis;newsfeed;topic detection;online newspapers;text documents;singular value decomposition method;opinion extraction;clustering technique;sentiment analysis;smartphone,,,,16,,1-Sep-16,,,IEEE,IEEE Conferences
Sentence clustering in text document using fuzzy clustering algorithm,基於模糊聚類算法的文本文檔句子聚類,S. Sruthi; L. Shalini,"Dept of CSE, MCET, TVM; Dept of CSE, MCET, TVM","2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)",22-Dec-14,2014,,,1473,1476,"The paper proposes FRECCA algorithm to find out the clusters of sentences having the inter relation among them. The system uses the fuzzy clustering algorithm that compares the sentences and find out the similarity value and forms a cluster. The fuzzy clustering algorithm checks the possibility of the sentence to which cluster it belongs to. In this paper a Genetic algorithm approach is also suggested that checks the highest similarity value between the sentences and grouped to form a cluster in a more sophisticated manner, there by forming n number of clusters from n sentences.",,978-1-4799-4190-2,10.1109/ICCICCT.2014.6993192,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993192,fuzzy clustering algorithm;FRECCA algorithm;Genetic algorithm,Clustering algorithms;Genetic algorithms;Accuracy;Dictionaries;Instruments;Redundancy;Semantics,fuzzy set theory;genetic algorithms;pattern clustering;text analysis,sentence clustering;text document;fuzzy clustering algorithm;FRECCA algorithm;sentence clusters;genetic algorithm approach,,1,,15,,22-Dec-14,,,IEEE,IEEE Conferences
Speeding-up Chinese character recognition in an automatic document reading system,在自動文件閱讀系統中加速漢字識別,Yi-Hong Tseng; Chi-Chang Kuo; Hsi-Jian Lee,"Dept. of Comput. & Inf. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; NA; NA",Proceedings of the Fourth International Conference on Document Analysis and Recognition,6-Aug-02,1997,2,,629,632 vol.2,"We present two techniques for speeding up character recognition. Our character recognition system, including the candidate cluster selection and detail matching modules, is implemented using two statistical features: crossing counts and contour direction counts. In the training stage, we divide characters into different clusters. To keep a very high recognition rate, the candidate cluster selection module selects the top 60 clusters with minimal distances from among 300 predefined clusters. To further speed up the recognition speed, we use a modified branch and bound algorithm in the detail matching module. In the automatic document reading system, characters and punctuation marks are first extracted from printed document images and sorted according to their positions and the document orientation. The system then recognizes all printed Chinese characters between pairs of punctuation marks. The results are then spoken aloud by a speech synthesis system.",,0-8186-7898-4,10.1109/ICDAR.1997.620581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620581,,Character recognition;Clustering algorithms;Strips;Optical character recognition software;Speech synthesis;Computer science;Statistical analysis;Costs;Hazards;Clustering methods,optical character recognition;tree searching;natural languages;speech synthesis;document image processing;statistical analysis,Chinese character recognition speed up;automatic document reading system;character recognition system;candidate cluster selection;detail matching modules;statistical features;crossing counts;contour direction counts;training stage;predefined clusters;modified branch and bound algorithm;punctuation marks;printed document images;document orientation;speech synthesis system,,3,,10,,6-Aug-02,,,IEEE,IEEE Conferences
A New Framework for Recognition of Heavily Degraded Characters in Historical Typewritten Documents Based on Semi-Supervised Clustering,基於半監督聚類的歷史打字文檔中嚴重降級字符識別新框架,S. Pletschacher; J. Hu; A. Antonacopoulos,"Pattern Recognition & Image Anal. (PRImA) Res. Lab., Univ. of Salford, Manchester, UK; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA; Pattern Recognition & Image Anal. (PRImA) Res. Lab., Univ. of Salford, Manchester, UK",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,506,510,"This paper presents a new semi-supervised clustering framework to the recognition of heavily degraded characters in historical typewritten documents, where off-the-shelf OCR typically fails. The constraints are generated using typographical (collection-independent) domain knowledge and are used to guide both sample (glyph set) partitioning and metric learning. Experimental results using simple features provide encouraging evidence that this approach can lead to significantly improved clustering results compared to simple K-means clustering, as well as to clustering using a state-of-the art OCR engine.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277612,Degraded character recognition;semi-supervised clustering;analysis of historical documents;typewritten documents,Character recognition;Degradation;Optical character recognition software;Pattern recognition;Image recognition;Text analysis;Engines;Image analysis;Pattern analysis;Image texture analysis,document image processing;history;learning (artificial intelligence);optical character recognition;pattern clustering,heavily degraded character recognition framework;historical typewritten document;semisupervised clustering framework;off-the-shelf OCR;typographical domain knowledge;sample partitioning;metric learning formulation;K-means clustering,,4,,11,,2-Oct-09,,,IEEE,IEEE Conferences
A Clustering Approach to Improving Pseudo-Relevance Feedback: Improving Retrieval Effetiveness by Removing Noisy Documents,一種改進偽相關反饋的聚類方法：通過刪除嘈雜的文檔來提高檢索效果,C. Li; J. Wang,"Coll. of Comput. Sci., Inner Mongolia Univ., Huhhot, China; Coll. of Comput. Sci., Inner Mongolia Univ., Huhhot, China",2012 Fourth International Symposium on Information Science and Engineering,11-Apr-13,2012,,,35,38,"Pseudo relevance feedback is an effective technique for improving retrieval results, which assumes a small number of top-ranked documents in the initial retrieval results are relevant and selects from these documents related terms to the query to improve the query representation through query expansion. However, these documents are often a mixture of relevant and irrelevant documents. The relevance feedback is quite effective and performs significantly better than pseudo-relevance feedback, which needs the user explicitly provides information on relevant documents to a query. This paper makes a case for the use of query-specific density clustering in IR on the grounds of improved retrieval effectiveness in a fully automatic manner and without relevance information provided by human and the experimental results show that significant improvements can be obtained on several collections when our new model FWN (Feedback Without Noise) is used.",2160-1291,978-1-4673-5680-0,10.1109/ISISE.2012.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495293,Information Retrieval;Language Model;Query Expansion;Density Clustering;relevance feedback,,document handling;pattern clustering;query processing;relevance feedback,clustering approach;pseudorelevance feedback;retrieval effectiveness;noisy document removal;query-specific density clustering;feedback without noise model;FWN model,,,,14,,11-Apr-13,,,IEEE,IEEE Conferences
Document Clustering – A Feasible Demonstration with K-means Algorithm,文檔聚類–使用K-means算法的可行演示,W. Arif; N. A. Mahoto,"Mehran University of Engineering and Technology, Jamshoro, Pakistan; Mehran University of Engineering and Technology, Jamshoro, Pakistan","2019 2nd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)",25-Mar-19,2019,,,1,6,"The manual structural organization of documents is expensive in terms of time and efforts. Traversing large number of documents to interpret manually is also challenging issue. Therefore, sophisticated means are needed to cope with this challenge. Clustering is one of the automated solutions. It is a major tool in many applications of business and data sciences. Document clustering sorts out records into various gatherings called as groups, where the documents in each group share some regular properties as indicated in closeness or similarity measure. Robust document clustering assumes an essential role in helping its users to successfully explore, condense, and sort out the data. This paper aims at clustering textual documents using TF-IDF (Term Frequency - Inverse Document Frequency) scheme. This research proposed methods for the selection of initial centroids in k-means clustering algorithm, which reduces efforts to great extent by minimizing the number of iterations usually one and efficiently ensures the accuracy of obtained clusters. Besides, the proposed methods expressed potentially promising results for small document sets.",,978-1-5386-9509-8,10.1109/ICOMET.2019.8673480,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673480,Document clustering;Textual dataset;TF-IDF scheme;Document organization,Clustering algorithms;Mathematical model;Machine learning algorithms;Feature extraction;Organizations;Complexity theory;Sparse matrices,pattern clustering;text analysis,Term Frequency - Inverse Document Frequency;document sets;textual document clustering;TF-IDF;k-means clustering algorithm,,,,17,,25-Mar-19,,,IEEE,IEEE Conferences
Chinese Keyword Spotting Using Knowledge-Based Clustering,基於知識聚類的中文關鍵詞發現,Y. Xia; K. Wang; M. Li,"Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,789,793,"Content-based document image retrieval is a new and promising research area. Without OCR, document indexing directly based on image content is more general and convenient. However content-based Chinese document retrieval is difficult for the complexity of Chinese character structure and large class numbers. Few papers cover this issue, and this paper will focus on it. This paper presents a novel algorithm of knowledge-based clustering and gives a mechanism of serial batch clustering for large data set. Knowledge derives from an artificial document image collection. Chinese characters with high frequency are edited and synthesized to images automatically. Cluster IDs are adopted to index the characters. A Dream of Red Mansions, a famous classical Chinese literature work including near one million characters, is used to evaluate the performance of Chinese keyword spotting. Experimental results confirm the effectiveness of knowledge-based clustering and its application on Chinese keyword spotting.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065419,Content-based Chinese keyword spotting;Knowledge-based clustering;Serial batch clustering;Document image synthesis,Clustering algorithms;Feature extraction;Knowledge based systems;Indexing;Image retrieval;Optical character recognition software;Complexity theory,content-based retrieval;document image processing;image retrieval;knowledge based systems;natural language processing;pattern clustering,Chinese keyword spotting;knowledge based clustering;content based document image retrieval;OCR;document indexing;Chinese character structure complexity;serial batch clustering;Dream of Red Mansions;Chinese literature work,,,,18,,3-Nov-11,,,IEEE,IEEE Conferences
Keyword extraction for document clustering using submodular optimization,使用亞模優化的文檔聚類關鍵詞提取,X. Zhang; K. Mueller; S. Yoo,"Stony Brook University, Brookhaven National Laboratory; Stony Brook University, Brookhaven National Laboratory; Stony Brook University, Brookhaven National Laboratory",2017 New York Scientific Data Summit (NYSDS),26-Oct-17,2017,,,1,2,"With the rapid growth of information services, enormous amount of text corpus cannot simply be read and understand. Therefore, text clustering and visualization present a direct way to observe the documents as well as understand the topic by corresponding keywords. However, even a short paragraph contains a variety of words, which makes the keyword or topic extraction difficult to achieve. Therefore, we propose an algorithm to extract keywords efficiently and effectively, which makes use of the latent semantic indexing and submodular optimization. The visual layout allows users to simultaneously visualize (1) the overview of the whole dataset, (2) the detailed information in the specific scope of the collection of documents, and (3) the relationships of documents with their keywords.",,978-1-5386-3161-4,10.1109/NYSDS.2017.8085056,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085056,Visualization;Document Clustering;Submodular;Keyword Extraction,Data visualization;Semantics;Singular value decomposition;Matrix decomposition;Optimization;Sparse matrices;Visual analytics,indexing;information retrieval;pattern clustering;text analysis,documents;keyword extraction;document clustering;submodular optimization;information services;text corpus;text clustering;visualization;short paragraph;latent semantic indexing;visual layout,,,,4,,26-Oct-17,,,IEEE,IEEE Conferences
Network-Based Document Clustering Using External Ranking Loss for Network Embedding,使用外部排名損失進行網絡嵌入的基於網絡的文檔聚類,Y. C. Yoon; H. K. Gee; H. Lim,"Telecommunications and Media Research Laboratory, Electronics and Technology Research Institute (ETRI), Daejeon, South Korea; Telecommunications and Media Research Laboratory, Electronics and Technology Research Institute (ETRI), Daejeon, South Korea; Department of Computer Science and Engineering, Korea University, Seoul, South Korea",IEEE Access,31-Oct-19,2019,7,,155412,155423,"Network-based document clustering involves forming clusters of documents based on their significance and relationship strength. This approach can be used with various types of metadata that express the significance of the documents and the relationships among them. In this study, we defined a probabilistic network graph for fine-grained document clustering and developed a probabilistic generative model and calculation method. Furthermore, a novel neural-network-based network embedding learning method was devised that considers the significance of a document based on its rankings with external measures, such as the download counts of relevant files, and reflects the relationship strength between the documents. By considering the significance of a document, reputative documents of clusters can be centralized and shown as representative documents for tasks such as data analysis and data representation. During evaluation tests, the proposed ranking-based network-embedding method performs significantly better on various algorithms, such as the k-means algorithm and common word/phrase-based clustering methods, than the existing network embedding approaches.",2169-3536,,10.1109/ACCESS.2019.2948662,Institute for Information and communications Technology Promotion; Korean Government (Digital Content In-House Research and Development; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878093,Clustering algorithms;artificial neural networks,Probabilistic logic;Clustering algorithms;Task analysis;Metadata;Semantics;Clustering methods;Classification algorithms,data analysis;document handling;embedded systems;graph theory;neural nets;pattern clustering,network-based document clustering;external ranking loss;relationship strength;probabilistic network graph;fine-grained document clustering;probabilistic generative model;calculation method;novel neural-network-based network;reputative documents;representative documents;ranking-based network-embedding method,,,,35,CCBY,21-Oct-19,,,IEEE,IEEE Journals
Document Summarization by Agglomerative nested clustering approach,聚集嵌套聚類方法的文檔匯總,A. Sharaff; H. Shrawgi; P. Arora; A. Verma,"Computer Science & Engineering, National Institute Of Technology, Raipur, India; Computer Science & Engineering, National Institute Of Technology, Raipur, India; Computer Science & Engineering, National Institute Of Technology, Raipur, India; Computer Science & Engineering, National Institute Of Technology, Raipur, India","2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology (ICAECCT)",8-Jun-17,2016,,,187,191,"Digital documents are ubiquitous in this age and every person faces an inundation of data today. In an era where being expeditious in work is becoming a necessity, providing people with the gist of verbose documents is an essential task. Thus in this paper, we document the process of Text Summarization in which a concise summary of a larger text is generated. We propose an Extractive Model of Document Summarization based on Agglomerative clustering which ranks sentences and forms a summary of the highly ranked sentences.",,978-1-5090-3662-2,10.1109/ICAECCT.2016.7942580,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942580,Extractive Text Summarization;TF-ISF;Hierarchical Clustering;AGNES;Cosine Similarity,Feature extraction;Computer science;Computers;Coherence;Silicon;Research and development;Semantics,document handling;pattern clustering;ubiquitous computing,document summarization;agglomerative nested clustering;digital documents;ubiquitous computing,,,,20,,8-Jun-17,,,IEEE,IEEE Conferences
Algorithm of documents clustering based on minimum spanning tree,基於最小生成樹的文檔聚類算法,Xiao-Shen Zheng; Pi-Lian He; Mei Tian; Fu-Yong Yuan,"Dept. of Comput. Sci. & Technol., Tianjin Univ., China; Dept. of Comput. Sci. & Technol., Tianjin Univ., China; NA; NA",Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),19-Feb-04,2003,1,,199,203 Vol.1,This paper puts forward a method of document clustering based on minimum spanning tree (MST) in vector space model (VSM). This algorithm adopts classical VSM and combines with the method of MST in graph theory. The quality and performance of document clustering are higher than other traditional clustering methods.,,0-7803-7865-2,10.1109/ICMLC.2003.1264470,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264470,,Clustering algorithms;Clustering methods;Databases;Space technology;Tree graphs;Computer science;Electronic mail;Graph theory;Partitioning algorithms;Helium,pattern clustering;document handling;information services;trees (mathematics),documents clustering;minimum spanning tree;vector space model;graph theory,,,,7,,19-Feb-04,,,IEEE,IEEE Conferences
A novel method for text and non-text segmentation in document images,用於文檔圖像中文本和非文本分割的新方法,S. Deivalakshmi; P. Palanisamy; G. Vishwanathan,"Electronics and Communication Engineering Department, National Institute of Technology, Tiruchirappalli, TN 620015 India; Electronics and Communication Engineering Department, National Institute of Technology, Tiruchirappalli, TN 620015 India; Electronics and Communication Engineering Department, National Institute of Technology, Tiruchirappalli, TN 620015 India",2013 International Conference on Communication and Signal Processing,15-Aug-13,2013,,,255,259,"Segmentation of the contents of document images into text and non-text regions is an essential pre-processing step for applications such as document analysis and classification, as well as OCR. This paper presents a novel technique to segment the document image into text and non-text regions using a combination of Wavelet-based Gray Level Co-Occurrence Matrix (GLCM) features and K-means clustering. A comparison between the performances of different wavelets in document image segmentation is also performed and tabulated. The technique was tested on a number of scanned article images from the MediaTeam Document Database and results show a marked improvement over the already existing method based on GLCM features.",,978-1-4673-4866-9,10.1109/iccsp.2013.6577054,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577054,Discrete Wavelet Transform;Document Image Segmentation;GLCM Features;K-means Clustering Algorithm,Image segmentation;Feature extraction;Classification algorithms;Clustering algorithms;Discrete wavelet transforms,document image processing;feature extraction;image segmentation;optical character recognition;text detection;wavelet transforms,nontext segmentation;document images;document analysis;document classification;OCR;wavelet based gray level cooccurrence matrix feature;K-means clustering;MediaTeam document database,,3,,16,,15-Aug-13,,,IEEE,IEEE Conferences
A clustering technique using single pass clustering algorithm for search engine,使用單遍聚類算法的搜索引擎聚類技術,Z. Indra; N. Zamin; J. Jaafar,"Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Tronoh, 31750 Bandar Seri Iskandar, Perak; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Tronoh, 31750 Bandar Seri Iskandar, Perak; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Tronoh, 31750 Bandar Seri Iskandar, Perak",2014 4th World Congress on Information and Communication Technologies (WICT 2014),2-Apr-15,2014,,,182,187,"Internet users rely heavily on search engine to explore and find useful information buried in the websites. Up to now, the search results returned by the search engines are still far from satisfaction due to a long list of search results which in practice contains a mix of relevant and irrelevant information. The manual process of filtering the irrelevant information is daunting and time consuming. Clustering is one of the popular solutions for this cumbersome task. However, our literature studies revealed that research on document clustering for Asian languages are relatively limited as compared to English. Whilst the application of document clustering technique in search engines is commonly less available. In this research, a clustering technique for search engine using Single Pass Clustering (SPC) Algorithm is proposed. The technique is experimented on a set of Indonesian news documents to support the limited research of document clustering for Indonesian language. An experiment done on 200 Indonesian news documents has produced a number of satisfactory labelled clusters and the application of the algorithm is shown on a simulated search engine.",,978-1-4799-8115-1,10.1109/WICT.2014.7077325,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077325,Clustering document;Single Pass Clustering;Term Frequency ??Inverse Document Frequency (TF-IDF);Cosine Similarity;Search Engine,Search engines;Clustering algorithms;Classification algorithms;Internet;Web pages;Algorithm design and analysis;Media,Internet;natural languages;pattern clustering;search engines;Web sites,single pass clustering algorithm;search engine;Internet users;Web sites;information filtering process;document clustering technique;Asian languages;English;SPC algorithm;Indonesian news documents;simulated search engine,,2,,35,,2-Apr-15,,,IEEE,IEEE Conferences
An Effective Search Results Semantic Optimization Clustering Method for XML Fragments,一種有效的XML片段搜索結果語義優化聚類方法,Z. Minjuan,"Sch. of Inf. Technol., Jiangxi Univ. of Finance & Econ., Nanchang, China",2013 International Conference on Computer Sciences and Applications,19-Jun-14,2013,,,479,482,"With the emergence of more and more XML documents, the clustering of XML documents has become an active research area. However, it is more significance of XML element clustering than whole document due to the more focused and specific amount of information, especially to some application domain. Therefore, in this paper, we study the xml element clustering, in which the latent semantic indexing model is used to obtain the semantic relationship between terms firstly, and then a evaluation function for k-mediod is presented to automatic generate the optimal cluster number. In addition, information gain, for evaluating clustering quality is introduced. Experiment results show that the proposed semantic clustering optimization method outperforms the traditional method (No_optimization) in information gain and produces better clustering quality.",,978-0-7695-5125-8,10.1109/CSA.2013.117,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6835644,XML Fragments;Search Results Clustering;Evaluation Function;Optimal Cluster Number,XML;Semantics;Clustering algorithms;Indexing;Optimization methods;Mathematical model,document handling;indexing;information retrieval;pattern clustering;XML,search results;semantic optimization clustering method;XML fragments;XML documents;document clustering;latent semantic indexing model;k-mediod;evaluation function;clustering quality,,,,13,,19-Jun-14,,,IEEE,IEEE Conferences
A Chinese document layout analysis method based on minimal spanning tree clustering,基於最小生成樹聚類的中文文檔佈局分析方法,Xue-Dong Tian; Chong Zhang,"Fac. of Math. & Comput. Sci., Hebei Univ., China; Fac. of Math. & Comput. Sci., Hebei Univ., China",Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),19-Feb-04,2003,5,,3183,3187 Vol.5,"For adapting to some special characteristics of Chinese documents, a method based on minimal spanning tree clustering is presented. This method is a bottom-up approach. First apply run-length smoothing algorithm on the document in horizontal direction, and then in vertical direction. After that, minimal spanning tree clustering is applied. We can infer from experiments that the problem of Chinese document layout analysis can be resolved in a better way.",,0-7803-7865-2,10.1109/ICMLC.2003.1260127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260127,,Text analysis;Smoothing methods;Clustering algorithms;Graphics;Optical character recognition software;Noise generators;Mathematics;Computer science;Character recognition;Layout,document image processing;text analysis;pattern clustering;image texture;image segmentation;image classification,Chinese document layout analysis;minimal spanning tree clustering;run length smoothing algorithm;top-down method;bottom-up approach;document image processing,,2,,9,,19-Feb-04,,,IEEE,IEEE Conferences
Document Clustering via Matrix Representation,通過矩陣表示法進行文檔聚類,X. Wang; J. Tang; H. Liu,"Arizona State Univ., Tempe, AZ, USA; Arizona State Univ., Tempe, AZ, USA; Arizona State Univ., Tempe, AZ, USA",2011 IEEE 11th International Conference on Data Mining,23-Jan-12,2011,,,804,813,"Vector Space Model (VSM) is widely used to represent documents and web pages. It is simple and easy to deal computationally, but it also oversimplifies a document into a vector, susceptible to noise, and cannot explicitly represent underlying topics of a document. A matrix representation of document is proposed in this paper: rows represent distinct terms and columns represent cohesive segments. The matrix model views a document as a set of segments, and each segment is a probability distribution over a limited number of latent topics which can be mapped to clustering structures. The latent topic extraction based on the matrix representation of documents is formulated as a constraint optimization problem in which each matrix (i.e., a document) Ai is factorized into a common base determined by non-negative matrices L and RT, and a non-negative weight matrix Mi such that the sum of reconstruction error on all documents is minimized. Empirical evaluation demonstrates that it is feasible to use the matrix model for document clustering: (1) compared with vector representation, using matrix representation improves clustering quality consistently, and the proposed approach achieves a relative accuracy improvement up to 66% on the studied datasets, and (2) the proposed method outperforms baseline methods such as k-means and NMF, and complements the state-of-the-art methods like LDA and PLSI. Furthermore, the proposed matrix model allows more refined information retrieval at a segment level instead of at a document level, which enables the return of more relevant documents in information retrieval tasks.",2374-8486,978-1-4577-2075-8,10.1109/ICDM.2011.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137285,Document Clustering;Document Representation;Matrix Representation;Non-Negative Matrix Approximation,Vectors;Bismuth;Approximation methods;Matrix decomposition;Clustering algorithms;Probability distribution;Data mining,constraint handling;document handling;information retrieval;matrix algebra;optimisation;pattern clustering;probability;Web sites,document clustering;matrix representation;vector space model;Web pages;matrix model;cohesive segments;probability distribution;clustering structures;constraint optimization problem;k-means;NMF;LDA;PLSI;information retrieval tasks,,1,,,,23-Jan-12,,,IEEE,IEEE Conferences
Structural- based clustering technique of XML documents,XML文檔的基於結構的聚類技術,A. Mary Posonia; V. L. Jyothi,"Department of computer science, Sathyabama University of India, India; Department of computer science, Jeppiaar Engineering college, India","2013 International Conference on Circuits, Power and Computing Technologies (ICCPCT)",13-Jun-13,2013,,,1239,1242,Keyword search is an efficient technique for information retrieval and querying XML documents by the casual end users without understanding the complex syntax and semantics of the structured and semi structured data. This paper proposes a method of grouping structurally similar XML documents. A schematic description of a XML documents designed as a rooted ordered trees. Efficient clustering algorithm based on structural and semantic measure to estimate the similar XML document trees. The structural grouping of XML documents to furnish inherent clustering and better performance.,,978-1-4673-4922-2,10.1109/ICCPCT.2013.6528941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528941,Document Clustering;Information retrieval;XML database,XML;Algorithm design and analysis;Databases,pattern clustering;query processing;tree data structures;XML,structural-based clustering technique;keyword search;information retrieval;XML document querying;casual end users;structured data syntax;structured data semantics;semistructured data syntax;semistructured data semantics;structurally similar XML document grouping;schematic description;rooted ordered trees;cIustering algorithm;semantic measure,,,,20,,13-Jun-13,,,IEEE,IEEE Conferences
Cluster Correction on Polysemy and Synonymy,多義和同義詞的聚類校正,Z. Qin; H. Lian; T. He; B. Luo,"Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Stake Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China",2017 14th Web Information Systems and Applications Conference (WISA),9-Apr-18,2017,,,136,138,"Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering. At the same time, there are still many challenges, for example the accuracy of clustering needs to be improved. In this regard, the process of cluster correction becomes the object of analysis. In this paper, we focus on the polysemy and synonymy issue in clustering process. Polysemy represents the ambiguity of an individual word or phrase that can be used (in different contexts) to express two or more different meanings. However, synonymy is the semantic relation that holds between two or more words that can (in a given context) express the same meaning. These two conditions will affect our results of clustering. In order that, we use bag of words model to distinguish contexts of the same words and word2vec to re-cluster word with the similar meaning. Cosine similarity is also use to measure of similarity between two nonzero vectors in these two model.",,978-1-5386-4806-3,10.1109/WISA.2017.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8332601,cluster correction;polysemy;synonymy;cosine similarity;bag of words;word2vec,Clustering algorithms;Semantics;Context modeling;Training;Software;Euclidean distance;Data mining,document handling;information retrieval;pattern clustering;text analysis,cluster correction;polysemy;cluster analysis;textual documents;automatic document organization;topic extraction;fast information retrieval;synonymy issue;words model;word2vec;re-cluster word;text clustering,,,,5,,9-Apr-18,,,IEEE,IEEE Conferences
A comprehensive comparison study of document clustering for a biomedical digital library MEDLINE,生物醫學數字圖書館MEDLINE文檔聚類的全面比較研究,X. Hu; I. Yoo,"Drexel University, Philadelphia, PA; Drexel University, Philadelphia, PA",Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06),5-Mar-07,2006,,,220,229,"Document clustering has been used for better document retrieval, document browsing, and text mining in digital library. In this paper, we perform a comprehensive comparison study of various document clustering approaches such as three hierarchical methods (single-link, complete-link, and complete link), Bisecting K-means, K-means, and suffix tree clustering in terms of the efficiency, the effectiveness, and the scalability. In addition, we apply a domain ontology to document clustering to investigate if the ontology such as MeSH improves clustering qualify for MEDLINE articles. Because an ontology is a formal, explicit specification of a shared conceptualization for a domain of interest, the use of ontologies is a natural way to solve traditional information retrieval problems such as synonym/hypernym/ hyponym problems. We conducted fairly extensive experiments based on different evaluation metrics such as misclassification index, F-measure, cluster purity, and entropy on very large article sets from MEDLINE, the largest biomedical digital library in biomedicine",,1-59593-354-9,10.1145/1141753.1141802,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119128,comparison study;document clustering;ontology,Software libraries;Clustering algorithms;Partitioning algorithms;Information retrieval;Ontologies;Iterative algorithms;Educational institutions;Information science;Text mining;Biomedical measurements,bibliographic systems;data mining;document handling;information retrieval;medical information systems;ontologies (artificial intelligence);pattern clustering;text analysis,document clustering;biomedical digital library MEDLINE;document retrieval;document browsing;text mining;hierarchical methods;bisecting K-means;K-means;suffix tree clustering;domain ontology;formal explicit specification;shared conceptualization;biomedicine,,10,,27,,5-Mar-07,,,IEEE,IEEE Conferences
Clustering Description Extraction Based on Statistical Machine Learning,基於統計機器學習的聚類描述提取,C. Zhang; H. Xu,"Dept. of Inf. Manage., Nanjing Univ. of Sci. & Technol., Nanjing; Inst. of Sci. & Tech. Inf. of China, Beijing",2008 Second International Symposium on Intelligent Information Technology Application,6-Jan-09,2008,2,,22,26,"Clustering description problem is one of key issues of the traditional document clustering algorithm. The traditional document algorithm can cluster the objects, but it can not give concept description for the clustered results. Document clustering description is a problem of labeling the clustered results of document collection clustering. It can help users determine whether one of the clusters is relevant to users' information requirement. Therefore, labeling a clustered set of documents is an important and challenging work in document clustering applications. To resolve the problem of the weak readability of the traditional document clustering results, a method of automatic labeling documents clusters based on machine learning is put forward. Experimental results show that the method based on SVM will provide users with more concise and comprehensive document clustering results. It also reflects the linear trend of clustering description problem.",,978-0-7695-3497-8,10.1109/IITA.2008.114,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4739719,Clustering Description;Document Clustering;Statistical Machine Learning,Machine learning;Clustering algorithms;Labeling;Support vector machines;Data mining;Frequency;Learning systems;Information technology;Information management;Machine learning algorithms,document handling;learning (artificial intelligence);pattern clustering;statistical analysis,document clustering description extraction;statistical machine learning;clustered document labeling;document collection clustering,,2,,15,,6-Jan-09,,,IEEE,IEEE Conferences
Algorithms for clustering XML documents: A review,集群XML文檔的算法：回顧,S. Gulati; G. Munjal,"Department of Computer Science & Engineering, ITM University Gurgaon, India; Department of Computer Science & Engineering, ITM University Gurgaon, India",2015 International Conference on Advances in Computer Engineering and Applications,23-Jul-15,2015,,,654,658,"This paper provides a brief survey of various algorithms that are widely being used for the clustering of XML (Extensible Markup Language) documents. The scalable integration techniques and algorithms, like XClust algorithm, S-GRACE algorithm, XProj algorithm, XCleaner algorithm and many more, are being developed for the growing number of data sources of XML documents. These techniques have been used for reduction in many problems of clustering but still we can find the problem of clustering complexity which is being discussed here and the technique to overcome that is being thought to be taken up as the future work.",,978-1-4673-6911-4,10.1109/ICACEA.2015.7164772,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164772,,Clustering algorithms;XML;Algorithm design and analysis;Complexity theory;Partitioning algorithms;Computers;Rocks,document handling;pattern clustering;XML,XML documents clustering;extensible markup language;scalable integration techniques;XClust algorithm;S-GRACE algorithm;XProj algorithm;XCleaner algorithm;clustering complexity,,1,,12,,23-Jul-15,,,IEEE,IEEE Conferences
Adaptive Centroid-Based Clustering Algorithm for Text Document Data,基於自適應質心的文本文檔數據聚類算法,X. Li; J. Ouyang; X. Zhou; B. Fu,"CCST, Jilin Univ., Changchun, China; CCST, Jilin Univ., Changchun, China; CCST, Jilin Univ., Changchun, China; LNNU, Liaoning Normal Univ., Dalian, China","2014 Sixth International Symposium on Parallel Architectures, Algorithms and Programming",7-Oct-14,2014,,,63,68,"Document clustering is a significantly popular research, which aims to partition a corpus into many subgroups of homogeneous documents. Traditional clustering approaches catholically lack of considerations of word weights with clusters. To address this problem, we propose an Adaptive Centroid-based Clustering (ACC) algorithm. As a successful supervised centroid-based classifier, Class-Feature-Centroid (CFC) algorithm takes relationships among words into account. ACC attempts to employ this discriminative CFC vector to drive the clustering procedure. Since clustering is unsupervised, ACC begins with hundreds of small clusters for acceptable CFC vectors, and then iteratively regroups clusters of documents until convergence. As ACC is self-organized, it can determine the number of clusters adaptively. The experimental results validate that ACC achieves competitive performance with the state-of-art clustering approaches.",2168-3042,978-1-4799-3845-2,10.1109/PAAP.2014.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916438,document clustering;Class-Feature-Centroid;adaptively,Clustering algorithms;Vectors;Entropy;Partitioning algorithms;Measurement;Frequency modulation;Algorithm design and analysis,document handling;pattern classification;pattern clustering;vectors,CFC vector;CFC algorithm;class-feature-centroid algorithm;supervised centroid-based classifier;homogeneous documents;corpus partition;document clustering;text document data;ACC algorithm;adaptive centroid-based clustering algorithm,,1,,12,,7-Oct-14,,,IEEE,IEEE Conferences
Distributed Document Clustering Using Word-clusters,使用Word-clusters進行分佈式文檔聚類,D. Deb; R. A. Angryk,"Department of Computer Science, Montana State University, Bozeman, MT 59717, USA; Department of Computer Science, Montana State University, Bozeman, MT 59717, USA",2007 IEEE Symposium on Computational Intelligence and Data Mining,4-Jun-07,2007,,,376,383,"Document clustering has become an increasingly important task in analyzing huge numbers of documents distributed among various sites. The challenging aspect is to analyze this enormous number of extremely high dimensional distributed documents and to organize them in such a way that results in better search and knowledge extraction without introducing much extra cost and complexity. This paper presents a distributed document clustering approach called distributed information bottleneck (DIB). DIB adopts a two stage agglomerative information bottleneck (aIB) algorithm to generate local clusters. At the first stage, the high-dimensional document vector is significantly reduced by finding word-clusters. These word-clusters are then used to obtain document-clusters in the second stage. DIB then extracts compact but informative local models from these document-clusters and transfers them to a central site. At the global site, the local models, that are likely to describe the same document set, are first combined. The resultant local models are then clustered by using the aIB algorithm to produce a hierarchical organization of all distributed documents. Our experimental results demonstrate the robustness, efficiency and effectiveness of DIB approach to cluster distributed documents.",,1-4244-0705-2,10.1109/CIDM.2007.368899,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221323,,Costs;Clustering algorithms;Data mining;Distributed computing;Computational intelligence;Computer science;USA Councils;Robustness;IEEE online publications;Software libraries,distributed processing;document handling;pattern clustering,distributed document clustering;word-clusters;high dimensional distributed documents;knowledge extraction;distributed information bottleneck;agglomerative information bottleneck;high-dimensional document vector;local models,,3,,19,,4-Jun-07,,,IEEE,IEEE Conferences
Inducing Word Senses for Cross-lingual Document Clustering,為跨語言文檔聚類引入詞義,G. Tang; Y. Xia; E. Cambria; P. Jin,"Dept. of Comput. Sci., Tsinghua Univ., Beijing, China; Dept. of Comput. Sci., Tsinghua Univ., Beijing, China; Temasek Labs., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput. Sci., Leshan Normal Univ., Leshan, China",2013 Ninth International Conference on Computational Intelligence and Security,24-Feb-14,2013,,,409,414,"Cross-lingual document clustering is the task of automatically organizing a large collection of cross-lingual documents into a few groups according to their content or topic. It is well known that language barrier and translation ambiguity are two challenging issues for cross-lingual document representation. To address such issues, we propose to represent cross-lingual documents through statistical word senses, which are learned from a parallel corpus by means of a novel cross-lingual word sense induction model. Furthermore, a sense clustering method is adopted to discover semantic relation of word senses, which are used to represent cross-lingual documents through a sense-based vector space model. Evaluation on a benchmarking dataset shows that the proposed model outperforms two state-of-the-art models in cross-lingual document clustering.",,978-1-4799-2549-0,10.1109/CIS.2013.93,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746429,Word sense;cross-lingual document representation;cross-lingual document clustering,Context;Clustering algorithms;Vectors;Dictionaries;Educational institutions;Semantics;Adaptation models,document handling;pattern clustering,cross-lingual document clustering;statistical word senses;cross-lingual word sense induction model;sense clustering method;sense-based vector space model,,,,26,,24-Feb-14,,,IEEE,IEEE Conferences
An Improved K_Means Algorithm for Document Clustering Based on Knowledge Graphs,基於知識圖的改進的K_Means文檔聚類算法,X. Wang; Y. Li; M. Wang; Z. Yang; H. Dong,"School of Software Xiamen University, Xiamen, China; School of Software Xiamen University, Xiamen, China; School of Software Xiamen University, Xiamen, China; School of Software Xiamen University, Xiamen, China; School of Software Xiamen University, Xiamen, China","2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",3-Feb-19,2018,,,1,5,"K _means algorithm is one of the typical clustering algorithms in text mining tasks. K_means algorithm is widely used in many areas because of its easy to implement and ability to handle large datasets with better scalability. However, the random selection of initial cluster centroid in traditional K_means algorithm for text clustering easily leads to local optimization and instability of clustering results. Therefore, in order to overcome this shortcoming, this paper propose an improved K_means algorithm for document clustering which based on following two points: (i)we used concept distance to optimize the choice of the initial cluster centroid, which can avoid the drawbacks caused by random selection; (ii)we adopted knowledge graphs to improve traditional k_means text clustering algorithm by optimizing the calculation of text similarity. Theoretical analysis and experimental results show that the improved algorithm could optimize the accuracy of text clustering effectively.",,978-1-5386-7604-2,10.1109/CISP-BMEI.2018.8633187,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633187,Text clustering;knowledge graphs;K_means,Clustering algorithms;Semantics;Signal processing algorithms;Classification algorithms;Software;Software algorithms;Data mining,data mining;graph theory;optimisation;pattern clustering;text analysis,text mining;text clustering;K_means;initial cluster centroid;random selection;knowledge graphs;document clustering;text similarity,,2,,12,,3-Feb-19,,,IEEE,IEEE Conferences
Efficient Phrase-Based Document Similarity for Clustering,基於短語的高效基於短語的文檔相似度,H. Chim; X. Deng,City University of Hong Kong; City University of Hong Kong,IEEE Transactions on Knowledge and Data Engineering,22-Aug-08,2008,20,9,1217,1229,"In this paper, we propose a phrase-based document similarity to compute the pair-wise similarities of documents based on the suffix tree document (STD) model. By mapping each node in the suffix tree of STD model into a unique feature term in the vector space document (VSD) model, the phrase-based document similarity naturally inherits the term tf-idf weighting scheme in computing the document similarity with phrases. We apply the phrase-based document similarity to the group-average Hierarchical Agglomerative Clustering (HAC) algorithm and develop a new document clustering approach. Our evaluation experiments indicate that, the new clustering approach is very effective on clustering the documents of two standard document benchmark corpora OHSUMED and RCV1. The quality of the clustering results significantly surpass the results of traditional single-word \textit{tf-idf} similarity measure in the same HAC algorithm, especially in large document data sets. Furthermore, by studying the property of STD model, we conclude that the feature vector of phrase terms in the STD model can be considered as an expanded feature vector of the traditional single-word terms in the VSD model. This conclusion sufficiently explains why the phrase-based document similarity works much better than the single-word tf-idf similarity measure.",1558-2191,,10.1109/TKDE.2008.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459328,Clustering;Linguistic processing;Trees;Clustering;Linguistic processing;Trees,Clustering algorithms;Frequency;Tree graphs;Data visualization;Navigation;Organizing;Clustering methods;Data models;Natural languages;Euclidean distance,pattern clustering;text analysis;trees (mathematics),phrase-based document similarity;document clustering;pair-wise document similarity;suffix tree document model;vector space document model;group-average hierarchical agglomerative clustering algorithm;feature vector,,79,,29,,3-Mar-08,,,IEEE,IEEE Journals
Refining Spherical K-Means for Clustering Documents,細化聚類文檔的球形K均值,Jiming Peng; Jiaping Zhu,"Advanced Optimization Lab, Department of Computing and Software, McMaster University, Hamilton, Ontario L8S 4K1, Canada. email: pengj@mcmaster.ca; NA",The 2006 IEEE International Joint Conference on Neural Network Proceedings,30-Oct-06,2006,,,4146,4150,"Spherical k-means is a popular algorithm for document clustering. However, it may still yield poor performance in some circumstances. In this paper, we consider a discrete optimization model for spherical k-means. By using the convexity of objective function and specific structure of constraint set, we first reformulate the discrete problem as an equivalent convex maximization problem with linear constraints. Then we characterize the local optimality of relaxed problem. Based on the characteristics, we refine the spherical k-means algorithm by alternatively performing spherical k-means and switching data points between clusters. This strategy guarantees that the refined algorithm can always attain a local optimal solution.",2161-4407,0-7803-9490-9,10.1109/IJCNN.2006.246962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716671,,Clustering algorithms;Text mining;Frequency;Euclidean distance;Text categorization;Information retrieval;Data mining;Standards development;Clustering methods;Mathematics,document handling;pattern clustering,spherical k-means refining;document clustering;discrete optimization model;convex maximization problem;linear constraints,,,,11,,30-Oct-06,,,IEEE,IEEE Conferences
Utilizing Different Link Types to Enhance Document Clustering Based on Markov Random Field Model With Relaxation Labeling,基於帶鬆弛標記的馬爾可夫隨機域模型，利用不同的鏈接類型增強文檔聚類,X. Zhang; X. Hu; T. Hu; E. K. Park; X. Zhou,"College of Information Science and Technology, Drexel University, Philadelphia, PA, USA; College of Information Science and Technology, Drexel University, Philadelphia , PA, USA; Department of Computer Science, Central China Normal University, Wuhan, China; Department of Computer Science, California State University?Chico, Chico , CA, USA; College of Information Science and Technology, Drexel University, Philadelphia , PA, USA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",15-Aug-12,2012,42,5,1167,1182,"With the fast growing number of works utilizing link information in enhancing unsupervised document clustering, it is becoming necessary to make a comparative evaluation of the impacts of different link types on document clustering. Various types of links between text documents, including explicit links such as citation links and hyperlinks, implicit links such as coauthorship and cocitation links, and similarity links such as content similarity links, convey topic similarity or topic transferring patterns, which is very useful for document clustering. In this paper, we adopt a clustering algorithm based on Markov random field and relaxation labeling, which employs both content and linkage information, to evaluate the effectiveness of the aforementioned types of links for document clustering on ten data sets. The experimental results show that linkage information is quite effective in improving content-based document clustering. Furthermore, a series of important findings regarding the impacts of different link types on document clustering is discovered through our experiments.",1558-2426,,10.1109/TSMCA.2012.2187183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182733,Link-based document clustering;Markov random field (MRF);relaxation labeling (RL),Clustering algorithms;Markov random fields;Labeling;Probabilistic logic,citation analysis;Markov processes;pattern clustering;random processes;text analysis,link type;document clustering enhancement;Markov random field model;relaxation labeling;link information;unsupervised document clustering;text documents;hyperlinks;implicit links;coauthorship links;cocitation links;content similarity links;topic similarity;topic transferring pattern;clustering algorithm;content information;linkage information;content-based document clustering,,3,,34,,12-Apr-12,,,IEEE,IEEE Journals
A framework for hierarchical clustering based indexing in search engines,用於搜索引擎中基於層次聚類的索引的框架,P. Gupta; A. K. Sharma,"Department of Computer Engineering, Y.M.C.A. University of Science and Technology, Faridabad, India; Department of Computer Engineering, Y.M.C.A. University of Science and Technology, Faridabad, India","2010 First International Conference On Parallel, Distributed and Grid Computing (PDGC 2010)",6-Jan-11,2010,,,372,377,"Granting efficient and fast accesses to the index is a key issue for performances of Web Search Engines. In order to enhance memory utilization and favor fast query resolution, WSEs use Inverted File (IF) indexes that consist of an array of the posting lists where each posting list is associated with a term and contains the term as well as the identifiers of the documents containing the term. Since the document identifiers are stored in sorted order, they can be stored as the difference between the successive documents so as to reduce the size of the index. This paper describes a clustering algorithm that aims at partitioning the set of documents into ordered clusters so that the documents within the same cluster are similar and are being assigned the closer document identifiers. Thus the average value of the differences between the successive documents will be minimized and hence storage space would be saved. The paper further presents the extension of this clustering algorithm to be applied for the hierarchical clustering in which similar clusters are clubbed to form a mega cluster and similar mega clusters are then combined to form super cluster. Thus the paper describes the different levels of clustering which optimizes the search process by directing the search to a specific path from higher levels of clustering to the lower levels i.e. from super clusters to mega clusters, then to clusters and finally to the individual documents so that the user gets the best possible matching results in minimum possible time.",,978-1-4244-7674-9,10.1109/PDGC.2010.5679966,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679966,Inverted files;Index compression;Document Identifiers Assignment;Hierarchical Clustering,Grid computing;Conferences,document handling;indexing;pattern clustering;search engines;storage management,hierarchical clustering;indexing;Web search engines;memory utilization;query resolution;inverted file;document identifiers,,1,,11,,6-Jan-11,,,IEEE,IEEE Conferences
Map reduce programming model: Construction of inverted index for automated document clustering,地圖歸約編程模型：用於自動文檔聚類的倒排索引的構建,K. Santhiya; V. Bhuvaneswari,"Dept. of Computer Applications, BU, Coimbatore, India; Dept. of Computer Applications, BU, Coimbatore, India",2016 IEEE International Conference on Advances in Computer Applications (ICACA),30-Mar-17,2016,,,308,312,"Inverted index is an important data structure used in Information Retrieval operation, which enable all retrieval engines to easily facilitate full-text search. In this paper, Map Reduce algorithm is used for the construction of inverted index, so as to enable it to work in a parallelized manner and also make the data structure to support large scale document corpora. Here, we have considered crime articles related to women and children drawn from various English newspapers all over India. The paper aims to cluster the news articles according to a specific type of crime committed in a parallelized manner. We proposed a Hadoop based framework integrated with R environment that preprocess the corpus and stores the news articles and process it with the Map Reduce algorithm which identifies the type of crime like sexual harassment, physical abuse, emotional abuse, rape, murder and cluster it accordingly. We observe that the proposed method outperforms the other conventional methods and is more suited for batch processing applications.",,978-1-5090-3770-4,10.1109/ICACA.2016.7887971,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887971,Map Reduce;RHadoop;Inverted Index;Cluster;Crime partition;Information Retrieval,Indexes;Data models;Big Data;Information retrieval;Data structures;Computer applications;Computational modeling,data structures;document handling;information retrieval;parallel programming;pattern clustering,MapReduce programming model;inverted index;automated document clustering;data structure;information retrieval operation;full-text search;large-scale document corpora;woman-related crime articles;child-related crime articles;English newspapers;India;Hadoop based framework;R environment;sexual harassment;physical abuse;emotional abuse;rape;murder,,,,12,,30-Mar-17,,,IEEE,IEEE Conferences
Multi-view clustering of clinical documents based on conditions and medical responses of patients,基於患者的狀況和醫療反應的臨床文檔的多視圖聚類,J. Sabthami; K. Thirumoorthy; K. Muneeswaran,"Department of Computer Science and Engineering, Mepco Schlenk Engineering College, Sivakasi, India; Department of Computer Science and Engineering, Mepco Schlenk Engineering College, Sivakasi, India; Department of Computer Science and Engineering, Mepco Schlenk Engineering College, Sivakasi, India",2016 10th International Conference on Intelligent Systems and Control (ISCO),3-Nov-16,2016,,,1,5,"Clinical documents from electronic patient record system has rich data which addresses the details about patient's disease, injuries, medication response. Patient's data in the clinical documents are clustered into groups based on conditions (symptoms names) and medical response (medication/drug names). The documents with patients who are affected with disease, not affected, and are in hypothetical condition information are clustered using single-view algorithms like Non-negative matrix factorization (NMF) and K-Means algorithm based on cosine similarity and multi-view algorithms like (Multi-View NMF). Multi-view clustering is used to cluster the documents by finding the relationship between different views. The comparison among the algorithms are made to identify the best clustering method to give priority for the patients who are in hypothetical condition.",,978-1-4673-7807-9,10.1109/ISCO.2016.7726951,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726951,Clinical documents;Symptom;Medication;Single-view;Non-negative Matrix Factorization;K-Means,Feature extraction;Diseases;Drugs;History;Clustering algorithms;Biomedical imaging;Matrix decomposition,matrix decomposition;medical information systems;pattern clustering,multiview clustering;clinical documents;medical responses;electronic patient record system;patient disease;medication response;medical response;condition information;nonnegative matrix factorization;NMF;k-means algorithm;cosine similarity;multiview algorithms;hypothetical condition,,2,,18,,3-Nov-16,,,IEEE,IEEE Conferences
A combined approach of formal concept analysis and text mining for concept based document clustering,基於概念的文檔聚類的形式化概念分析和文本挖掘的組合方法,N. N. Myat; Khin Haymar Saw Hla,NA; NA,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),17-Oct-05,2005,,,330,333,"Nowadays, the demand of conceptual document clustering is becoming increase to manage various types of vast amount of information published on the World Wide Web. In this paper, we use formal concept analysis (FCA) method for clustering documents according to their formal contexts. Concept hierarchy of documents is built using the formal concepts of the documents in the document corpus. We use tf.idf (term frequency /spl times/ inverse document frequency) term weighting model to reduce less useful concepts from these formal concepts and the association and correlation mining techniques to analyze the relationship of terms in the document corpus.",,0-7695-2415-X,10.1109/WI.2005.1,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517867,conceptual document clustering;formal concept analysis (FCA);frequent termsets;association;correlation,Text mining;Clustering algorithms;Information retrieval;Itemsets;Web sites;Frequency;Search engines;Publishing;Organizing;Partitioning algorithms,text analysis;data mining;Internet,formal concept analysis;text mining;concept based document clustering;World Wide Web;term frequency;inverse document frequency;correlation mining,,4,,9,,17-Oct-05,,,IEEE,IEEE Conferences
A precise skew estimation algorithm for document images using KNN clustering and fourier transform,使用KNN聚類和傅里葉變換的文檔圖像精確偏斜估計算法,J. Fabrizio,"LRDE-EPITA, 14-16, rue Voltaire, 94276 Le Kremlin-Bic癡tre CEDEX France",2014 IEEE International Conference on Image Processing (ICIP),29-Jan-15,2014,,,2585,2588,"In this article, we propose a simple and precise skew estimation algorithm for binarized document images. The estimation is performed in the frequency domain. To get a precise result, the Fourier transform is not applied to the document itself but the document is preprocessed: all regions of the document are clustered using a KNN and contours of grouped regions are smoothed using the convex hull to form more regular shapes, with better orientation. No assumption has been made concerning the nature or the content of the document. This method has been shown to be very accurate and was ranked first at the DISEC'13 contest, during the ICDAR competitions.",2381-8549,978-1-4799-5751-4,10.1109/ICIP.2014.7025523,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025523,Skew estimation;KNN;Fourier transform,Estimation;Fourier transforms;Frequency-domain analysis;Robustness;Clustering algorithms;Text analysis,document image processing;estimation theory;Fourier transforms;frequency-domain analysis;pattern clustering,skew estimation algorithm;KNN clustering;Fourier transform;binarized document imaging;frequency domain estimation algorithm;convex hull;DISEC'13 contest;ICDAR competition,,6,,13,,29-Jan-15,,,IEEE,IEEE Conferences
A New Approach for Clustering Variable Length Documents,變長文檔聚類的新方法,N. Kumar; K. Srinathan,"IIIT-Hyderabad, INDIA, niraj_kumar@research.iiit.ac.in; IIIT-Hyderabad, INDIA, srinathan@iiit.ac.in",2009 IEEE International Advance Computing Conference,31-Mar-09,2009,,,982,987,"This paper proposes a method to cluster documents of variable length. The main idea is to apply (a) automatic identification of 1, 2, and 3 grams (To reduce the dependency on huge background vocabulary support or learning or complex probabilistic approach), (b) order them by some measure of relevance, which is developed with the help of Tf-Idf and Term-Weighting approach, and finally (c) use them (instead of bag of words based approach) to create vector space model and apply some known clustering methods i. e. Bisecting K-means, K-means, hierarchical method (single link) and Graph based method. Our experimental results with publicly available text dataset (Cogprints and NewsGroup20) show remarkable improvements in the performance of these clustering algorithms with this new approach.",,978-1-4244-2927-1,10.1109/IADCC.2009.4809148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809148,Document clustering;hierarchical methods;K-means;Bisecting K-means;Clustering algorithms;Vector Space Modelor,Clustering algorithms;Partitioning algorithms;Vocabulary;Extraterrestrial measurements;Clustering methods;Classification tree analysis,document handling;learning (artificial intelligence);pattern clustering;vocabulary,variable length documents clustering;automatic identification;background vocabulary support;learning;complex probabilistic approach;term-weighting approach;K-means clustering,,1,,16,,31-Mar-09,,,IEEE,IEEE Conferences
CQIG: An Improved Web Search Results Clustering Algorithm,CQIG：一種改進的Web搜索結果聚類算法,Y. Ren; D. Fan,"Sch. of Comput. & Inf. Technol., Liaoning Normal Univ., Dalian, China; Sch. of Comput. & Inf. Technol., Liaoning Normal Univ., Dalian, China",2010 Seventh Web Information Systems and Applications Conference,23-Sep-10,2010,,,75,78,"Massive linear search results returned from traditional search engines bring much inconvenience to users when extract the information they need. Search result clustering is of critical need for grouping similar topics of documents. The existing algorithm has drawbacks in clustering labels screening, cluster quality assessment, overlapping clusters controlling. The improved clustering algorithm-CQIG, which based on LINGO, improved the cluster and cluster label scoring function, increased the cluster merging process and improved the processing effect of Chinese. Finally, a recommended platform for Web search results clustering is established based on carrot framework to prove the accuracy, distinction and readability of CQIG.",,978-1-4244-8440-9,10.1109/WISA.2010.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5581368,search results clustering;singular value decomposition;clustering quality assessment;cluster label Introduction,Clustering algorithms;Web search;Merging;Matrix decomposition;Open systems;Algorithm design and analysis;Singular value decomposition,document handling;Internet;natural language processing;pattern clustering;search engines,CQIG;Web search results clustering algorithm;linear search results;search engines;clustering labels screening;cluster quality assessment;overlapping clusters controlling;LINGO;cluster label scoring function;cluster merging process,,,,12,,23-Sep-10,,,IEEE,IEEE Conferences
Sparse Poisson Latent Block Model for Document Clustering (Extended Abstract),用於文檔聚類的稀疏Poisson潛在塊模型（擴展摘要）,M. Ailem; F. Role; M. Nadif,NA; NA; NA,2018 IEEE 34th International Conference on Data Engineering (ICDE),25-Oct-18,2018,,,1743,1744,"We present a novel generative mixture model for co-clustering text data. This model, the Sparse Poisson Latent Block Model (SPLBM), is based on the Poisson distribution, which arises naturally for contingency tables, such as document-term matrices. The advantages of SPLBM are two-fold. First, it is a rigorous statistical model which is also very parsimonious. Second, it has been designed from the ground up to deal with data sparsity problems. Extensive experiments on various real-world datasets of different size and structure provide strong evidence for the effectiveness of the proposed approach.",2375-026X,978-1-5386-5520-7,10.1109/ICDE.2018.00229,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8509450,Co clustering;Clustering;Poisson distribution;Mixture Model;Latent Block Model;Text Mining;Data Sparsity,Sparse matrices;Clustering algorithms;Mutual information;Sports;Data models;Partitioning algorithms;Convergence,matrix algebra;pattern clustering;Poisson distribution;statistical analysis;text analysis,sparse Poisson latent block model;text data co-clustering;generative mixture model;rigorous statistical model;document-term matrices;Poisson distribution;SPLBM;document clustering,,,,8,,25-Oct-18,,,IEEE,IEEE Conferences
Web Document Clustering Research Based on Granular Computing,基於粒度計算的Web文檔聚類研究,Z. Shangzhi; Z. Xiaolong; Z. Buqun; B. Hualong,"Dept. of Comput. Sci. & Technol., Chaohu Univ., Chaohu, China; Dept. of Comput. Sci. & Technol., Chaohu Univ., Chaohu, China; Dept. of Comput. Sci. & Technol., Chaohu Univ., Chaohu, China; Dept. of Comput. Sci. & Technol., Chaohu Univ., Chaohu, China",2009 Second International Symposium on Electronic Commerce and Security,23-Oct-09,2009,2,,446,450,"In this paper, a method of Web document clustering based on granular computing (WDCGrc) is presented. The method computes the weight value of the words in documents by adopting the TF-IDF principle. Meanwhile, combinative ways defining documents threshold and average weight value are adopted to reduce dimensions and extract the keywords in each document. The paper establishes the transformation between the keywords in documents and the binary granules, and adopts the algorithm of association rules based on granular computing to obtain frequent item sets between documents. Bring in the set theory thought, numbers of the same word between documents as the document similarity and the clustering result is obtained. The experiment shows that the method is practical and feasible, with good quality of clustering.",,978-0-7695-3643-9,10.1109/ISECS.2009.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209712,Granularcomputing;Clustering;Association rules;Web documents,Chaos;Data mining;Web pages;Internet;Computer science;Association rules;Information processing;Electronic commerce;Computer security;Clustering algorithms,data mining;document handling;Internet;pattern clustering;set theory,Web document clustering;granular computing;WDCGrc;TF-IDF principle;document word;average weight value;dimension reduction;document keyword;association rule;set theory;document threshold;binary granule,,,,6,,23-Oct-09,,,IEEE,IEEE Conferences
A Novel Approach for Email Clustering Based on Semantics,基於語義的電子郵件聚類新方法,B. He; Z. Li; N. Yang,"Sch. of Inf., Renmin Univ. of China, Beijing, China; Sch. of Inf., Renmin Univ. of China, Beijing, China; Sch. of Inf., Renmin Univ. of China, Beijing, China",2014 11th Web Information System and Application Conference,12-Mar-15,2014,,,269,272,"An increasing interest has been recently devoted to clustering short documents. Short documents don't contain enough text to compute similarities accurately by implementing the most widely used technique called Vector Space Model (VSM). Adding semantics to short documents clustering is one efficient way to solve this problem. However, real life collections are often composed of very short or long documents. For example, the length of email messages for each email user follows a power-law distribution. Long emails and short emails both appear in email corpus. Therefore, both state-of-the-art short documents and long document clustering approaches can't get a high cluster quality or high efficiency in short and long documents clustering. In order to solve this problem, we propose a novel approach for email clustering based on semantics. Empirical validation shows that our method can obtain high cluster quality and high efficiency in real world email datasets.",,978-1-4799-5727-9,10.1109/WISA.2014.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058025,email clustering;conditional similarity;directed graph transformation;semantics vector,Electronic mail;Vectors;Semantics;Clustering algorithms;Data mining;Clustering methods;Algorithm design and analysis,document handling;electronic mail;natural language processing;pattern clustering;statistical distributions;vectors,email clustering;semantics;vector space model;document clustering;power-law distribution,,3,,15,,12-Mar-15,,,IEEE,IEEE Conferences
Clustering and classification of document structure-a machine learning approach,文檔結構的聚類和分類-一種機器學習方法,A. Dengel; F. Dubiel,"German Res. Center for Artificial Intelligence, Kaiserslautern, Germany; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,587,591 vol.2,"We describe a system which is capable of learning the presentation of document logical structures, exemplarily shown for business letters. Presenting a set of instances to the system, it clusters them into structural concepts and induces a concept hierarchy. This concept hierarchy is taken as a source for classifying future input. The paper introduces the different learning steps, describes how the resulting concept hierarchy is applied for logical labeling and reports on the results.",,0-8186-7128-9,10.1109/ICDAR.1995.601965,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601965,,Machine learning;Labeling;Decision trees;Artificial intelligence;Fuzzy logic;Information retrieval;Costs;Logic testing;Classification tree analysis;Text analysis,document handling;learning by example;classification;knowledge based systems;technical presentation;business data processing,document structure clustering;document structure classification;machine learning approach;document logical structure presentation;business letters;learning by example;concept hierarchy;logical labeling,,28,,13,,6-Aug-02,,,IEEE,IEEE Conferences
DocNet: A document embedding approach based on neural networks,DocNet：基於神經網絡的文檔嵌入方法,Z. Mo; J. Ma,"School of Artificial Intelligence and Data Science, Hebei University of Technology, Tianjin, China; School of Artificial Intelligence and Data Science, Hebei University of Technology, Tianjin, China",2018 24th International Conference on Automation and Computing (ICAC),1-Jul-19,2018,,,1,5,"Embedding texts into vector spaces is a common and fundamental preprocessing. Despite there are several approaches to put documents into vectors, reducing the dimension and improving ability of expression can still be a problem when facing large scale data and sophisticated demand. Distributed dense vector have been shown to be powerful in capturing token level semantics. In this paper, we propose a new method to embed entire documents into vector space using a deep neural network which described as DocNet in this paper. With DocNet, we trained end-to-end learning the vector space and by that we take all the information including semantics into account. Once this space has been produced, tasks such as classification and clustering can be simply done using standard techniques. Our method introduces triplet loss to train. The benefit is vector space can be learned directly so we can control the final dimension of embedding vectors. To demonstrate performance of our method, we built a clustering system compared with several baseline methods. Experiments prove that our approach achieves state-of-art document clustering performance. Furthermore, it proves that complicated clustering or classification demands can be satisfied by our method.",,978-1-86220-341-9,10.23919/IConAC.2018.8749095,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8749095,DocNet;text clustering;document embedding;LSTM;keras,Neural networks;Computer architecture;Semantics;Task analysis;Microprocessors;Text categorization,document handling;embedded systems;learning (artificial intelligence);neural nets;pattern classification;pattern clustering;text analysis;vectors,DocNet;document embedding approach;neural networks;vector space;distributed dense vector;embed entire documents;deep neural network;embedding vectors;text embedding;trained end-to-end learning;clustering system,,,,20,,1-Jul-19,,,IEEE,IEEE Conferences
A Novel Document Clustering Model Based on Latent Semantic Analysis,基於潛在語義分析的新型文檔聚類模型,W. Song; S. C. Park,"Chonbuk Nat. Univ. Korea, Jeonju; Chonbuk Nat. Univ. Korea, Jeonju","Third International Conference on Semantics, Knowledge and Grid (SKG 2007)",22-Jan-08,2007,,,539,542,"In this paper we propose a document representation model based on latent semantic analysis (LSA) for text clustering. Most classic clustering systems represent document with a set of indices, which have been known as vector space model (VSM). In such a model, documents are encoded as vectors in N-dimensional space, where N is the number of unique terms. However, this method causes that the scalability will be poor and the cost of computational time will be high. Latent semantic analysis is a promising approach which attempts to construct a latent semantic structure in textual data and finds relevant documents such that they may not even share any common words, moreover, it reduces the large term-by-document matrix to a smaller one and provides a robust space for clustering. Two clustering algorithms, K-means and genetic algorithm (GA), are constructed in LSA space to demonstrate the effectiveness and validity of our text representation model. We use SSTRESS criteria to analyze the dissimilarity between the original corpus matrix and the approximate objective matrix with different ranks corresponding to the performance of the two clustering algorithms. The superiority of GA and K-means applied in LSA model over conventional GA and K-means in VSM is demonstrated by providing good text clustering results.",,0-7695-3007-9,10.1109/SKG.2007.154,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438614,,Clustering algorithms;Partitioning algorithms;Computational efficiency;Genetic algorithms;Information analysis;Scalability;Knowledge engineering;Robustness;Performance analysis;Algorithm design and analysis,data structures;genetic algorithms;pattern clustering;statistical analysis;text analysis,document clustering model;latent semantic analysis;document representation model;text clustering;vector space model;term-by-document matrix;K-means clustering;genetic algorithm;text representation model;SSTRESS criteria,,9,,7,,22-Jan-08,,,IEEE,IEEE Conferences
Density-based adaptive spatial clustering algorithm for identifying local high-density areas in georeferenced documents,基於密度的自適應空間聚類算法，用於識別地理參考文件中的局部高密度區域,T. Sakai; K. Tamura; H. Kitakami,"Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",4-Dec-14,2014,,,513,518,"An emerging topic in social media is the increase in the number of geo-annotated documents, which include not only posted time but also posted location. Social media users have been transmitting information about things they witnessed themselves in their daily life through such geo-annotated (georeferenced) documents. Georeferenced documents are usually related to not only personal topics but also local topics and events. Therefore, identifying high-density areas associated with local ?attractive??topics in georeferenced documents is one of the most important challenges in many application domains. In this study, we propose a novel density-based spatial clustering algorithm called the (庰,?)- density-based adaptive spatial clustering algorithm for identifying high-density areas in which geo-related local topics in georeferenced documents are located. The (庰,?)-density-based adaptive spatial clustering algorithm can identify local high-density areas by using adaptive spatial clustering criteria.",1062-922X,978-1-4799-3840-7,10.1109/SMC.2014.6973959,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973959,density-based cluster;spatial cluster;DBSCAN;social media;local topic extraction;adaptive spatial clustering,Clustering algorithms;Media;Twitter;Algorithm design and analysis;Kernel;Data mining;Spatiotemporal phenomena,document handling;pattern clustering;social networking (online),density-based adaptive spatial clustering algorithm;local high density areas;georeferenced documents;geo-annotated documents;social media users;personal topics;local attractive topics;high-density areas;geo-related local topics;adaptive spatial clustering criteria,,6,,15,,4-Dec-14,,,IEEE,IEEE Conferences
A feature selection method for document clustering based on part-of-speech and word co-occurrence,基於本體的，特定領域的財務文檔聚類方法,Z. Liu; W. Yu; Y. Deng; Y. Wang; Z. Bian,"International School of Software, Wuhan University, China; International School of Software, Wuhan University, China; International School of Software, Wuhan University, China; International School of Software, Wuhan University, China; International School of Software, Wuhan University, China",2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery,9-Sep-10,2010,5,,2331,2334,"Feature selection is a process which chooses a subset from the original feature set according to some rules. The selected feature retains original physical meaning and provides a better understanding for the data and learning process. However, few modern feature selection approaches take the advantage of features' context information. Based on this analysis, we propose a novel feature selection method based on part-of-speech and word co-occurrence. According the components of Chinese document text, we utilize the words' part-of-speech attributes to filter lots of meaningless terms. Then we define and use co-occurrence words by their part-of-speech to select features. In the evaluating process, we use the text corpus from Sogou Lab to do some experiments and use Entropy and Precision as criteria to give an objective evaluation of document clustering performance. The results show that our method can select better features and get a more pleasant clustering performance.",,978-1-4244-5934-6,10.1109/FSKD.2010.5569827,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569827,feature selection;document clustering;part-ofspeech;word co-occurrence,Entropy;Feature extraction;Speech;Context;Machine learning;Software;Educational institutions,feature extraction;pattern clustering;speech synthesis;text analysis;unsupervised learning;word processing,feature selection method;document clustering;part of speech;word co-occurrence;learning process;context information;Chinese document;text corpus;Sogou lab,,7,,15,,9-Sep-10,,,IEEE,IEEE Conferences
Study on Sub Topic Clustering of Multi-Documents Based on Semi-Supervised Learning,使用GIS可視化和EM聚類方法進行文檔聚類,X. Xu,NA,2010 2nd International Workshop on Database Technology and Applications,6-Dec-10,2010,,,1,3,"Sub-topic detecting is an important step in the abstracting of multi-documents.This paper describes a new method for sub-topic detecting based on semi-supervised learning:it firstly gets the primal sets of topics by hierarchy clustering,and labels the sentences which have high scores in the topics,then use the method of constrained-kMeans to decide the number of topics(k),and finally get the topic sets by k-Means clustering.The experiment result indicates that its value is stable.",2167-194X,978-1-4244-6977-2,10.1109/DBTA.2010.5659111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659111,,Semantics;Clustering algorithms;Dictionaries;Web pages;Clustering methods;Monitoring,document handling;learning (artificial intelligence);pattern clustering,subtopic clustering;semisupervised learning;subtopic detection;multidocument handling;hierarchy clustering;k-means clustering,,,,8,,6-Dec-10,,,IEEE,IEEE Conferences
Feature Space Transformations in Document Clustering,基於語義本體的文檔組織者，對學習文檔進行聚類,K. Csorba; I. Vajk,"Department of Automation and Applied Informatics, Budapest University of Technology and Economics, Goldmann Gy. t矇r 3, Budapest, Hungary, H-1111, Email: kristof@aut.bme.hu; Dept. of Autom. & Appl. Informatics, Budapest Univ. of Technol. & Econ.",2006 International Conference on Intelligent Engineering Systems,11-Sep-06,2006,,,175,179,"Document clustering is a part of information retrieval, where documents written in natural language are being assigned to different groups based on some criteria. In the current case, documents with similar topics are collected. As there are many methods and additional noise filtering techniques to do this, this paper focuses on the composition of such transformations and on the comparison of the configurations build from a subset of these transformations as tiles of the whole procedure. 5 tile methods (term filtering, frequency quantizing, principal component analysis (PCA), term clustering and document clustering of course) are used. These are compared based on the maximal achieved F-measure and time consumption to find the best composition",1543-9259,0-7803-9708-8,10.1109/INES.2006.1689364,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1689364,,Principal component analysis;Tiles;Filtering;Frequency;Space technology;Automation;Informatics;Information retrieval;Natural languages;Text analysis,document handling;information retrieval;pattern clustering;principal component analysis,feature space transformation;document clustering;information retrieval;document retrieval;natural language;document collection;noise filtering technique;term filtering;frequency quantization;principal component analysis;PCA;term clustering,,,,10,,11-Sep-06,,,IEEE,IEEE Conferences
Improving Efficiency of Similarity of Document Network Using Bisect K-Means,基於NMF和SVDD的文檔聚類算法,P. Kadam; G. S. Mate,"Department of Computer Engineering, Rajarshi Shahu College of Engineering, Pune Savitribai Phule, Pune University, Pune, Maharashtra, 411041, India; Department of Information Technology, Rajarshi Shahu College of Engineering, Pune Savitribai Phule, Pune University, Pune, Maharashtra, 411041, India","2017 International Conference on Computing, Communication, Control and Automation (ICCUBEA)",13-Sep-18,2017,,,1,6,"In today's internet world, most of the document related to various fields are stored in digital or electronic format. It allows easy access to users and also save the space. So the main challenge is to retrieve the interested document from huge set of document dataset. For this problem, document clustering becomes popular solution in the area of data mining. In this approach, all similar kind of document are stored collectively. This paper focus on the documents of clinical cases. The proposed system makes cluster of similar kind of clinical cases documents and improves the search efficiency. For document clustering two kinds of techniques are used named as K-means and topic detection. Initially, Document network is formed by linking various kinds of clinical reports. On this network, Bisect K-means clustering algorithm is applied after cluster formation, system identify the unique topic for each cluster. The performance of the system is tested on Vaccine Adverse Event Reporting System (VAERS) dataset and evaluation results prove that Bisect K-means is more accurately divide the number of clinical reports in appropriate cluster, than K-means clustering algorithm. The performance of the system is rely on three terms: precision, recall, and F-measure.",,978-1-5386-4008-1,10.1109/ICCUBEA.2017.8463865,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463865,Bisect k-means;document network;document clustering;topic detection;clinical report,Clustering algorithms;Medical diagnostic imaging;Training;Patents;Testing;Partitioning algorithms;Classification algorithms,data mining;document handling;medical information systems,digital format;electronic format;document dataset;document clustering;clinical cases documents;topic detection;clinical reports;cluster formation;document network similarity;bisect k-means clustering algorithm;vaccine adverse event reporting system;VAERS dataset;precision measure;recall measure;F-measure,,,,12,,13-Sep-18,,,IEEE,IEEE Conferences
Graph-Cut Based Iterative Constrained Clustering,用於文檔聚類的基於GPU的Harmony K-means算法,M. Okabe; S. Yamada,"Toyohashi Univ. of Technol., Toyohashi, Japan; Nat. Inst. of Infomatics, Tokyo, Japan",2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology,10-Oct-11,2011,3,,126,129,"This paper proposes a constrained clustering method that is based on a graph-cut problem formalized by SDP (Semi-Definite Programming). Our SDP approach has the advantage of convenient constraint utilization compared with conventional spectral clustering methods. The algorithm starts from a single cluster of a complete dataset and repeatedly selects the largest cluster, which it then divides into two clusters by swapping rows and columns of a relational label matrix obtained by solving the maximum graph-cut problem. This swapping procedure is effective because we can create clusters without any computationally heavy matrix decomposition process to obtain a cluster label for each data. The results of experiments using a Web document dataset demonstrated that our method outperformed other conventional and the state of the art clustering methods in many cases. Hence we consider our clustering provides a promising basic method to interactive Web clustering.",,978-1-4577-1373-6,10.1109/WI-IAT.2011.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040822,constrained clustering;semidefinite programming;graph cut,Kernel;Clustering methods;Programming;Clustering algorithms;Matrix decomposition;Measurement;Machine learning,document handling;graph theory;Internet;iterative methods;learning (artificial intelligence);matrix algebra;pattern clustering,iterative constrained clustering;semidefinite programming;constraint utilization;conventional spectral clustering methods;relational label matrix;maximum graph-cut problem;swapping procedure;Web document dataset;interactive Web clustering;semisupervised learning approach,,,,8,,10-Oct-11,,,IEEE,IEEE Conferences
Sparse Subspace Representation for Spectral Document Clustering,基於加權BERT模型的文本文檔聚類方法,B. Saha; D. Phung; D. S. Pham; S. Venkatesh,NA; NA; NA; NA,2012 IEEE 12th International Conference on Data Mining,17-Jan-13,2012,,,1092,1097,"We present a novel method for document clustering using sparse representation of documents in conjunction with spectral clustering. An ??-norm optimization formulation is posed to learn the sparse representation of each document, allowing us to characterize the affinity between documents by considering the overall information instead of traditional pair wise similarities. This document affinity is encoded through a graph on which spectral clustering is performed. The decomposition into multiple subspaces allows documents to be part of a sub-group that shares a smaller set of similar vocabulary, thus allowing for cleaner clusters. Extensive experimental evaluations on two real-world datasets from Reuters-21578 and 20Newsgroup corpora show that our proposed method consistently outperforms state-of-the-art algorithms. Significantly, the performance improvement over other methods is prominent for this datasets.",2374-8486,978-1-4673-4649-8,10.1109/ICDM.2012.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413803,Sparse representation;Document clustering,Matrix decomposition;Clustering algorithms;Sparse matrices;Indexing;Symmetric matrices;Eigenvalues and eigenfunctions;Laplace equations,data structures;document handling;graph theory;optimisation;pattern clustering,sparse subspace representation;spectral document clustering;spectral clustering;L1-norm optimization formulation;document affinity;pairwise similarity;graph;subspace decomposition;Reuters-21578 dataset;20Newsgroup dataset,,1,,22,,17-Jan-13,,,IEEE,IEEE Conferences
Flexible document organization: Comparing fuzzy and possibilistic approaches,文本文檔聚類：問題與挑戰,T. M. Nogueira; S. O. Rezende; H. A. Camargo,"Department of Computer Science, Federal University of Bahia- Brazil; Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Department of Computer Science, Federal University of S瓊o Carlos - Brazil",2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),30-Nov-15,2015,,,1,8,System flexibility means the ability of a system to manage imprecise and/or uncertain information. A lot of commercially available Information Retrieval Systems (IRS) address this issue at the level of query formulation. Another way to make the flexibility of an IRS possible is by means of the flexible organization of documents. Such organization can be carried out using clustering algorithms by which documents can be automatically organized in multiple clusters simultaneously. Fuzzy and possibilistic clustering algorithms are examples of methods by which documents can belong to more than one cluster simultaneously with different membership degrees. The interpretation of these membership degrees can be used to quantify the compatibility of a document with a particular topic. The topics are represented by clusters and the clusters are identified by one or more descriptors extracted by a proposed method. We aim to investigate if the performance of each clustering algorithm can affect the extraction of meaningful overlapping cluster descriptors. Experiments were carried using well-known collections of documents and the predictive power of the descriptors extracted from both fuzzy and possibilistic document clustering was evaluated. The results prove that descriptors extracted after both fuzzy and possibilistic clustering are effective and can improve the flexible organization of documents.,,978-1-4673-7428-6,10.1109/FUZZ-IEEE.2015.7338064,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338064,fuzzy clustering;possibilistic clustering;flexible organization;documents;information retrieval,Clustering algorithms;Organizations;Prototypes;Phase change materials;Feature extraction;Mathematical model;Information retrieval,document handling;fuzzy set theory;pattern clustering;possibility theory;query processing,flexible-document organization;fuzzy approach;possibilistic approach;imprecise-uncertain information management;Information Retrieval Systems;query formulation;IRS;fuzzy clustering algorithms;possibilistic clustering algorithms;membership degrees;document compatibility quantification;overlapping cluster descriptors,,3,,37,,30-Nov-15,,,IEEE,IEEE Conferences
Collecting Handwritten Nom Character Patterns from Historical Document Pages,相似度指標對文檔聚類的影響,T. V. Phan; B. Zhu; M. Nakagawa,"Dept. of Comput. & Inf. Sci., Tokyo Univ. of Agric. & Technol., Tokyo, Japan; Dept. of Comput. & Inf. Sci., Tokyo Univ. of Agric. & Technol., Tokyo, Japan; Dept. of Comput. & Inf. Sci., Tokyo Univ. of Agric. & Technol., Tokyo, Japan",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,344,348,"In this paper, we present methods of segmenting Nom historical documents and clustering character patterns to build a Nom character pattern database. Nom is an ideographic script to represent Vietnamese, used from the 10th century to 20th century. However, this heritage is nearly lost. In order to preserve the wisdom and knowledge expressed in Nom, recognition and digitalization are indispensable. Because there is no OCR for Nom yet, we have to start from collecting patterns. We have employed a projection profile based method for segmenting hundreds of pages into individual characters. Then, we have implemented a combination of Chinese OCR-based clustering and K-means clustering to group characters into categories. The experiment shows that the proposed system can help collecting the characters patterns effectively. Moreover, it has revealed that there are many character classes lost or uncategorized so far.",,978-0-7695-4661-2,10.1109/DAS.2012.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195391,segmentation;clustering;Chu Nom;Han Nom;historical document;pattern collection;offline character database;document image analysis;Vietnamese ancient text,Character recognition;Image segmentation;Accuracy;Optical character recognition software;Noise;Databases;Libraries,document image processing;handwritten character recognition;history;optical character recognition;pattern clustering,handwritten Nom character patterns collection;historical document pages;Nom historical documents segmentation;character patterns clustering;Nom character pattern database;ideographic script;Vietnamese;Chinese OCR-based clustering;K-means clustering,,2,,10,,7-May-12,,,IEEE,IEEE Conferences
Review on mining and investigation of criminal records from digital devices,一種新的分層文檔聚類方法,M. A. Javed; S. Jaiswal,"Dept. of Computer Science & Engg, G. H. Raisoni College of Engineering, Nagpur (M.S), India; Dept. of Computer Science & Engg, G. H. Raisoni College of Engineering, Nagpur (M.S), India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)",13-Aug-15,2015,,,1,3,"In computer forensics analysis, hundreds and thousands of files are usually examined. Lots of the data present in those files consists of unstructured text, and the analysis of those texts difficult to be performed by computer examiners. For that there is a need of automated methods of analysis. In particular, algorithms for clustering documents can help the discovery of new and useful knowledge from the document under analysis. For that document clustering can be used for forensic analysis digital devices in police investigations. Document clustering has great potential to be useful for computer inspection. The clusters of either relevant or irrelevant documents can facilitate computer examiners to efficiently focus on the most relevant documents instead of inspecting all of them. This paper focuses on the method of document clustering for investigation of criminal records from digital devices.",,978-1-4799-6818-3,10.1109/ICIIECS.2015.7193172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193172,Document Clustering;text mining;forensic analysis;digital devices,Computers;Forensics;Clustering algorithms;Algorithm design and analysis;Technological innovation;Entropy;Conferences,data mining;digital forensics;document handling;law administration;pattern clustering,criminal records investigation;criminal records mining;computer forensics analysis;computer examiners;document clustering;forensic analysis digital devices;irrelevant documents;relevant documents,,,,7,,13-Aug-15,,,IEEE,IEEE Conferences
Link based K-Means clustering algorithm for information retrieval,一種通過語義後綴樹和自組織特徵映射方法聚合Web文檔的新方法,M. Sathya; J. Jayanthi; N. Basker,"Department of Computer Science and Engineering Sona College of Technology, Salem, TN, India; Department of Computer Science and Engineering Sona College of Technology, Salem, TN, India; Department of Computer Science and Engineering Sona College of Technology, Salem, TN, India",2011 International Conference on Recent Trends in Information Technology (ICRTIT),4-Aug-11,2011,,,1111,1115,"In the rapid development of internet technologies, search engines play a vital role in information retrieval. To provide efficient search engine to the user, Link Based Search Engine for information retrieval using K-Means clustering algorithm has been developed. The traditional search engines provide users with a set of non-classified web pages to their request based on its ranking mechanism. In order to satisfy the needs of the user, an improvement to the search engine called Intelligent Cluster Search Engine (ICSE) has been proposed. The improvement of information retrieval process can be divided into two parts such as: comparison of co-occurrence terms and clustering of documents. In this information retrieval, the relevancy of documents is obtained based on the number of occurrences of each co-occurrent term (in links and out links) in a particular web page. The clustering of these relevant documents can be done based on the threshold values assigning to cluster and then the web pages are grouped into that cluster. When the web pages are clustered, a boost up factor is given to a web page based on the relevancy of content from title and summary. The documents can be classified into most relevant, relevant and irrelevant clusters. K-Means clustering algorithm is used to cluster the relevant web pages in order to increase the relevance rate of search results and reduce the computational time of the user.",,978-1-4577-0590-8,10.1109/ICRTIT.2011.5972402,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972402,Clustering;Information retrieval;Information extraction;K-Means algorithm,Clustering algorithms;Web pages;Search engines;Algorithm design and analysis;Data mining;Classification algorithms,document handling;information retrieval;Internet;pattern clustering;search engines,link based k-means clustering algorithm;information retrieval;Internet technologies;link based search engine;intelligent cluster search engine;document clustering;Web pages,,3,,15,,4-Aug-11,,,IEEE,IEEE Conferences
Document Clustering for Event Identification and Trend Analysis in Market News,數據挖掘文檔聚類的混合策略,L. Dey; A. Mahajan; S. M. Haque,"Innovation Labs., Tata Consultancy Services Ltd., Delhi; Innovation Labs., Tata Consultancy Services Ltd., Delhi; Innovation Labs., Tata Consultancy Services Ltd., Delhi",2009 Seventh International Conference on Advances in Pattern Recognition,13-Feb-09,2009,,,103,106,In this paper we have proposed a stock market analysis system that analyzes financial news items to identify and characterize major events that impact the market. The events have been identified using Latent Dirichlet Allocation(LDA) based topic extraction mechanism. The topic-document data is then clustered using kernel k means algorithm. The clusters are analyzed jointly with the SENSEX raw data to extract major events and their effects. The system has been implemented on capital market news about the Indian share market of the past three years.,,978-1-4244-3335-3,10.1109/ICAPR.2009.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782752,Document Clustering;Financial News;Stock Market;Event Analysis;Trend Analysis;Topic Identification,Stock markets;Data mining;Pattern analysis;Text mining;Economic forecasting;Information analysis;Kernel;Data analysis;Time series analysis;Portfolios,data analysis;data mining;financial data processing;pattern clustering;stock markets;text analysis,financial market news analysis;document text clustering;event identification system;trend analysis;stock market analysis system;latent dirichlet allocation;topic extraction mechanism;topic-document data;kernel k means algorithm;text mining system;financial data analysis;event extraction system,,6,,9,,13-Feb-09,,,IEEE,IEEE Conferences
Leveraging probabilistic segmentation to document clustering,用於文檔分類的雙重模糊可能性聚類,A. Banerjee,"College of Engineering and Management, Kolaghat, India",2015 Eighth International Conference on Contemporary Computing (IC3),7-Dec-15,2015,,,82,87,"In this paper a novel approach to document clustering has been introduced by defining a representative-based document similarity model that performs probabilistic segmentation of documents into chunks. The frequently occuring chunks that are considered as representatives of the document set, may represent phrases or stem of true words. The representative based document similarity model, containing a term-document matrix with respect to the representatives, is a compact representation of the vector space model that improves quality of document clustering over traditional methods.",,978-1-4673-7948-9,10.1109/IC3.2015.7346657,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346657,document clustering;boundary entropy;forward-backward algorithm,Entropy;Clustering algorithms;Probabilistic logic;Approximation algorithms;Algorithm design and analysis;Frequency conversion;Computational modeling,document handling;matrix algebra;pattern clustering;probability,probabilistic segmentation;document clustering;representative-based document similarity model;document set;term-document matrix;vector space model,,,,23,,7-Dec-15,,,IEEE,IEEE Conferences
Dynamic semantic textual document clustering using frequent terms and named entity,文檔聚類中相似性度量的比較分析,W. M. S. Yafooz; S. Z. Z. Abidin; N. Omar; R. A. Halim,"Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia",2013 IEEE 3rd International Conference on System Engineering and Technology,31-Oct-13,2013,,,336,340,"Data is mostly stored in digital format rather than hard copy because the former is safer, more secure, smaller in size, and faster to retrieve than the latter. With the increasing number of electronic documents to be organized for users to obtain knowledge and integrate information, document clustering has been applied by grouping textual documents based on their similarities. Many attempts have been made to perform textual document clustering with highly accurate results (i.e., close to nature classes) and high processing performance. However, such proposed techniques work in batch (or static) mode in which performance tend to be sacrificed with the use of all the terms in the document, at times resulting in overlapping or scalability issues. Few studies that focus on dynamic clustering also reported on performance issues. This paper contributes in the investigation of textual document clustering approaches and highlights the importance of using dynamic clustering in mining frequent terms with included named entity. This method is used to achieve high efficiency and high-quality data clustering. The method is also beneficial to be used in textual document clustering algorithms for many text domain applications.",,978-1-4799-1030-4,10.1109/ICSEngT.2013.6650195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650195,document clustering;frequent term;named entity;dynamic textual clustering,Clustering algorithms;Semantics;Data mining;Partitioning algorithms;Conferences;Systems engineering and theory;Algorithm design and analysis,data mining;pattern clustering;text analysis,dynamic semantic textual document clustering;named entity;data storage;electronic documents;textual document grouping;document similarities;frequent term mining;text domain applications,,6,,34,,31-Oct-13,,,IEEE,IEEE Conferences
A fast chinese web-document clustering method under Pareto?s Principle,具有Web文檔群集的瀏覽器,Zhang Tianlei; Chen Guishen; Che Hao,"Department of Computer Science and Technology, Tsinghua university Beijing, China, 100086; Institute of Beijing Electronic System Engineering, China, 100141; Artificial Intelligence Institute, Beijing City University, China, 10008",2008 IEEE International Conference on Granular Computing,31-Oct-08,2008,,,801,804,"Nowadays most search engine like Google, Baidu, demonstrate their query results by the value of item, listing them in several pages. As we are now in an age of information explosion, the number of pages will be huge and users have to glance over several before they get what they want. If we cluster the results, this problem will be solved. There are several clustering methods, but not quite accurate and efficient, especially when the result sets are consist of millions of items. this article describe an fast method under Paretopsilas Principle.",,978-1-4244-2512-9,10.1109/GRC.2008.4664707,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664707,,Clustering methods;Clustering algorithms;Computers;Search engines;Information retrieval;Accuracy;Explosions,document handling;pattern clustering;search engines,Chinese Web-document clustering method;Pareto principle;search engine;Google;Baidu;information explosion,,,,5,,31-Oct-08,,,IEEE,IEEE Conferences
WordNet and Semantic similarity based approach for document clustering,基於信息檢索量表的文檔聚類知識獲取評估,S. S. Desai; J. A. Laxminarayana,"Dept. of Computer Engineering, Goa College of Engineering, Farmagudi, Ponda, India; Dept. of Computer Engineering, Goa College of Engineering, Farmagudi, Ponda, India",2016 International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS),12-Dec-16,2016,,,312,317,"With the ceaseless flourishing of the internet, the number of text documents in electronic form is increasing exorbitantly. Thus document clustering which organizes such large collections of documents into meaningful clusters has become an important technique. Traditional clustering methods cluster documents based on statistical features. Thus, documents clustered together using traditional clustering methods are not conceptually similar to one another as semantic relationships between documents are ignored. In this paper, a model for document clustering that groups documents with similar concepts together is introduced. Proposed model initially identifies all the coreferences in each of the documents in the collection. Polysemy and synonymy problems are tackled by capturing an appropriate sense of the word based on its context using the WordNet and the Semantic similarity. The proposed clustering model is implemented for the classic4 dataset and the results show an improvement in the efficiency.",,978-1-5090-1022-6,10.1109/CSITSS.2016.7779377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779377,coreference resolution;clustering;polysemy;semantic;synonymy;WordNet,Semantics;Atmospheric measurements;Particle measurements;Tagging,Internet;pattern clustering;statistical analysis;text analysis;word processing,WordNet;semantic similarity based approach;Internet;electronic text documents;document clustering;statistical feature based documents;synonymy problems;polysemy problems;classic4 dataset,,3,,13,,12-Dec-16,,,IEEE,IEEE Conferences
Text Document Preprocessing and Dimension Reduction Techniques for Text Document Clustering,搜索結果文檔的聚類算法比較,A. I. Kadhim; Y. Cheah; N. H. Ahamed,"Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia",2014 4th International Conference on Artificial Intelligence with Applications in Engineering and Technology,10-Dec-15,2014,,,69,73,"Text mining defines generally the process of extracting interesting features (non-trivial) and knowledge from unstructured text documents. Text mining is an interdisciplinary field which depends on information retrieval, data mining, machine learning, parameter statistics and computational linguistics. Standard text mining and retrieval information techniques of text document usually rely on similar categories. An alternative method of retrieving information is clustering documents to preprocess text. The preprocessing steps have a huge effect on the success to extract knowledge. This study implements TF-IDF and singular value decomposition (SVD) dimensionality reduction techniques. The proposed system presents an effective preprocessing and dimensionality reduction techniques which help the document clustering by using k-means algorithm. Finally, the experimental results show that the proposed method enhances the performance of English text document clustering. Simulation results on BBC news and BBC sport datasets show the superiority of the proposed algorithm.",,978-1-4799-7910-3,10.1109/ICAIET.2014.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351815,Text mining; retrieval information; clustering; singular value decomposition; dimension reduction; clustering; k-means,Clustering algorithms;Text mining;Algorithm design and analysis;Data models;Singular value decomposition;Indexing,data mining;data reduction;information retrieval;knowledge acquisition;learning (artificial intelligence);natural language processing;pattern clustering;singular value decomposition;text analysis,text document preprocessing;dimension reduction techniques;text mining;feature extraction process;unstructured text document knowledge extraction;information retrieval;data mining;machine learning;parameter statistics;computational linguistics;information retrieval techniques;TF-IDF;singular value decomposition dimensionality reduction techniques;SVD dimensionality reduction techniques;k-means algorithm;English text document clustering;BBC sport datasets;BBC news datasets,,12,,8,,10-Dec-15,,,IEEE,IEEE Conferences
System for fuzzy document clustering and fast fuzzy classification,通過生成反饋會話和聚類偽文檔來重組Web搜索結果,M. Roj?ek,"Department of Informatics, Faculty of Education, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1A 034 01 Ru鱉omberok, Slovakia",2014 IEEE 15th International Symposium on Computational Intelligence and Informatics (CINTI),2-Feb-15,2014,,,39,42,"The paper introduces uncontrolled fuzzy document clustering and fast fuzzy classification. This system is based on KMART neural network that realizes clustering, and original Fuzzy classification algorithm on the base of Fuzzy ART network that realizes classification. Both algorithms share their weights. Uncontrolled system has two separate flows: by first one we influence structure of categories (plasticity) and second one classifies without possibility to influence defined structure (stability). The paper shows legitimacy of such an approach with regard on quality and speed of classification.",,978-1-4799-5338-7,10.1109/CINTI.2014.7028711,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7028711,,Classification algorithms;Subspace constraints;Clustering algorithms;Training;Neural networks;Adaptive systems;Testing,ART neural nets;document handling;fuzzy set theory;pattern classification;pattern clustering,uncontrolled fuzzy document clustering;fast fuzzy classification;KMART neural network;fuzzy classification algorithm;fuzzy ART network;uncontrolled system;classification quality;classification speed,,1,,15,,2-Feb-15,,,IEEE,IEEE Conferences
Using topic keyword clusters for automatic document clustering,在最小化用於文檔聚類的術語文檔矩陣表示方面的高性能,Hsi-Cheng Chang; Chiun-Chieh Hsu,"Dept. of Electron. Eng., Hwa Hsia Inst. of Technol., Taipei, Taiwan; NA",Third International Conference on Information Technology and Applications (ICITA'05),1-Aug-05,2005,1,,419,424 vol.1,"Data clustering is a technique for grouping similar data items together for convenient understanding. Conventional data clustering methods, including agglomerative hierarchical clustering and partitional clustering algorithms frequently perform unsatisfactorily for large text article collections, as well as the computation complexity of the conventional data clustering methods increase very quick with the number of data items. This paper presents a system for automatic document clustering by identifying topic keyword clusters of the text corpus. The proposed system adopts a multi-stage process. First, an aggressive data cleaning approach is employed to reduce the noise in the free text and further identify the topic keywords within the documents. All extracted keywords are then grouped into topic keyword clusters using the k-nearest neighbor graph approach and the keyword clustering function. Finally, all documents in the corpus are clustered based on the topic keyword clusters. The proposed method was assessed against conventional data clustering methods on a Web news collection, indicating that the proposed method is an efficient and effective clustering approach.",,0-7695-2316-1,10.1109/ICITA.2005.303,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488841,,Clustering methods;Clustering algorithms;Merging;Extraterrestrial measurements;Cities and towns;Information management;Partitioning algorithms;Cleaning;Noise reduction;Data mining,document handling;computational complexity;graph theory,topic keyword clusters;automatic document clustering;data clustering;agglomerative hierarchical clustering;partitional clustering;computation complexity;multistage process;k-nearest neighbor graph;keyword clustering function;Web news collection,,8,,19,,1-Aug-05,,,IEEE,IEEE Conferences
Research on mixture language model-based document clustering,基於組合策略的文檔聚類描述,J. Wen; Z. Li,"Computer School, National University of Defence Technology, Changsha, 410073 China; School of Computer Science & Engineering, Beihang University, 100083, China",2008 IEEE International Conference on Granular Computing,31-Oct-08,2008,,,649,652,"Language modeling with semantic smoothing is proposed as an effective way to improve the quality of document clustering. However, the existing semantic smoothing model is not effective for partitional clustering because it can not assign fit weight to ldquogeneralrdquo word in a collection. In this paper, inspired by mixture probability model, we put forward a mixture language model for document clustering. The new model can alleviate the effect of ldquogeneralrdquo word, simultaneously, it can integrate the context information and solve the polysemy problems in a document. Based the new model, an EM algorithm for partitional clustering is present. The experimental results show our algorithms are more effective than the previous methods to improve the cluster quality.",,978-1-4244-2512-9,10.1109/GRC.2008.4664755,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664755,,Smoothing methods;Integrated circuit modeling;Clustering algorithms;Partitioning algorithms;Context modeling;Clustering methods;Entropy,document handling;natural language processing,mixture language model-based document clustering;language modeling;semantic smoothing model;mixture probability model;context information;polysemy problems;partitional clustering;cluster quality,,,,9,,31-Oct-08,,,IEEE,IEEE Conferences
"Document Clustering Using K-Means, Heuristic K-Means and Fuzzy C-Means",文本文檔聚類：聚類分析在文本文檔中的應用,V. K. Singh; N. Tiwari; S. Garg,"Dept. of Comput. Sci., South Asian Univ., New Delhi, India; Dept. of Comput. Sci., Banaras Hindu Univ., Varanasi, India; Dept. of Comput. Sci., Banaras Hindu Univ., Varanasi, India",2011 International Conference on Computational Intelligence and Communication Networks,29-Dec-11,2011,,,297,301,"Document clustering refers to unsupervised classification (categorization) of documents into groups (clusters) in such a way that the documents in a cluster are similar, whereas documents in different clusters are dissimilar. The documents may be web pages, blog posts, news articles, or other text files. This paper presents our experimental work on applying K-means, heuristic K-means and fuzzy C-means algorithms for clustering text documents. We have experimented with different representations (tf, tf.idf & Boolean) and different feature selection schemes (with or without stop word removal & with or without stemming). We ran our implementations on some standard datasets and computed various performance measures for these algorithms. The results indicate that tf.idf representation, and use of stemming obtains better clustering. Moreover, fuzzy clustering produces better results than both K-means and heuristic K-means on almost all datasets, and is a more stable method.",,978-0-7695-4587-5,10.1109/CICN.2011.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6112875,Document Clustering;K-means;Heuristic K-means;Fuzzy C-means;Cluster Evaluation,Clustering algorithms;Vectors;Heuristic algorithms;Classification algorithms;Partitioning algorithms;Algorithm design and analysis;Frequency measurement,fuzzy set theory;pattern classification;pattern clustering;text analysis;Web sites,document clustering;heuristic k-means;fuzzy c-means;unsupervised document classification;Web pages;blog posts;news articles;text files;feature selection schemes;tf.idf representation;fuzzy clustering,,22,,17,,29-Dec-11,,,IEEE,IEEE Conferences
Shredded Document Reconstruction Based on Intelligent Algorithms,使用一袋符號表示對文檔圖像進行聚類,Y. Liu; H. Qiu; J. Lu; Y. Fang,"Bus. Sch., China Univ. of Political Sci. & Law, Beijing, China; Bus. Sch., China Univ. of Political Sci. & Law, Beijing, China; Bus. Sch., China Univ. of Political Sci. & Law, Beijing, China; Acad. of Math. & Syst. Sci., Beijing, China",2014 International Conference on Computational Science and Computational Intelligence,29-May-14,2014,1,,108,113,"The technique of shredded documentation reconstruction is widely used in different areas such as military affairs and archaeology. This process, however, can hardly be finished merely by human efforts. Therefore, it is important to develop an effective method basing on intelligent algorithms. Designed for effectively solving this problem, this paper attempts to develop an automatic reconstructing method by discomposing the process into three parts, which are tackled with by different algorithms respectively. Firstly, the document pieces, which are in the first column of the original document, are picked out by the blank-area-searching algorithm. Then follows the searching of the adjacent document piece by the help of rightward-eduction algorithm and hence all rows in the original document are reconstructed. Considering the special cases of 'endless loop' as well as 'false searching' at this stage, the corresponding methods of fixed-distance-based clustering analysis and pattern recognition are given. Lastly, by revising the eduction algorithms into upward and downward directions, all rows can be putted back into the original document in the right order. Following these three steps, the shredded original document can be reconstructed automatically.",,978-1-4799-3010-4,10.1109/CSCI.2014.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822092,Document Reconstruction;Intelligent Algorithm;Pattern Recognition,Pattern recognition;Vectors;Clustering algorithms;Educational institutions;Algorithm design and analysis;Reliability;Business,character recognition;document image processing;image reconstruction;pattern clustering;search problems,intelligent algorithms;shredded documentation reconstruction technique;military affairs;archaeology;automatic reconstructing method;blank-area-searching algorithm;adjacent document piece searching;rightward-eduction algorithm;fixed-distance-based clustering analysis;pattern recognition,,5,,15,,29-May-14,,,IEEE,IEEE Conferences
Identifying main topics in density-based spatial clusters using network-based representative document extraction,Web文檔挖掘的動態模糊聚類算法,T. Sakai; K. Tamura; H. Kitakami,"Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan",2015 IEEE 8th International Workshop on Computational Intelligence and Applications (IWCIA),9-Apr-16,2015,,,77,82,"Geo-tagged documents on social media are usually related to local topics and events. Extracting areas of interest associated with local ?attractive??topics from geo-tagged documents is one of the most important challenges in many application domains. In this paper, we propose a novel method for extracting the areas of interest from geo-tagged documents. There are two main steps in the proposed method. First, the (庰, ?)-density-based adaptive spatial clustering algorithm extracts areas where local topics are attracting attention as spatial clusters. Second, representative geo-tagged documents are detected to identify the main topic in each spatial cluster. The (庰, ?)-density-based adaptive spatial clustering algorithm changes the threshold for seamlessly extracting spatial clusters regardless of the local densities of the posted geo-tagged documents. Moreover, the proposed method utilizes the network-based important sentence extraction method in order to extract representative geo-tagged documents from each spatial cluster. The experimental results show that the proposed method can extract the areas of interest as spatial clusters and representative documents as main topics.",1883-3977,978-1-4799-9886-9,10.1109/IWCIA.2015.7449466,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449466,Density-based spatial clustering;PageRank algorithm;Node-based betweenness centrality;Geo-tagged tweet;Local topic extraction,Clustering algorithms;Twitter;Algorithm design and analysis;Data mining;Media;Videos;Earthquakes,document handling;pattern clustering;social networking (online),"main topics identification;density-based spatial clusters;network-based representative document extraction;geo-tagged documents;social media;local attractive topics;(庰, ?)-density-based adaptive spatial clustering algorithm;area extraction;local topics;spatial cluster extraction;network-based important sentence extraction method;representative documents",,,,12,,9-Apr-16,,,IEEE,IEEE Conferences
Distributed Document Clustering for Search Engine,基於MapReduce的文本文檔模糊聚類方法。,Chang Liu; Song-Nian Yu; Qiang Guo,"School of Computer Engineering and Science, Shanghai University, China; School of Computer Engineering and Science, Shanghai University, China; School of Computer Engineering and Science, Shanghai University, China",2009 International Conference on Wavelet Analysis and Pattern Recognition,18-Aug-09,2009,,,454,459,"Considering that data searched from the search engine is not comprehensive, and the inconsistencies between desired results and received results are inevitable, a more effective search tool called Distributed Document Clustering for Search Engine (DDCSE) is proposed in this paper. In the DDCSE, the utilizing of distributed clustering and several search engines is used to categorize the results, in order to feedback a set of better refined results. Experiments show that a significant improvement is achieved via the distribution document clustering, so as to refine the results and reduce the time used to filter out irrelevant data for the search engines.",2158-5709,978-1-4244-3728-3,10.1109/ICWAPR.2009.5207461,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5207461,Distribution clustering;Search engine;Peer to peer,Search engines;Data mining;Distributed decision making;Internet;Wavelet analysis;Pattern analysis;Pattern recognition;Distributed computing;Clustering algorithms;Data engineering,data mining;distributed processing;document handling;search engines,distributed document clustering;search engine;data search,,,,9,,18-Aug-09,,,IEEE,IEEE Conferences
An adaptive distributed approach of a self organizing map model for document clustering using ring topology,使用重力集成聚類的文檔聚類,M. Ajeissh; S. Harikumar,"Department of Computer Science and Engineering, Amrita School of Engineering, Amritapuri, Amrita Vishwa Vidyapeetham, Amrita University, India; Department of Computer Science and Engineering, Amrita School of Engineering, Amritapuri, Amrita Vishwa Vidyapeetham, Amrita University, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",3-Nov-16,2016,,,776,781,"Document clustering aims at grouping the documents that are coherent internally with substantial difference amongst different groups. Due to huge availability of documents, the clustering face scalability and accuracy issues. Moreover, there is a dearth for a tool that performs clustering of such voluminous data efficiently. Conventional models focus either on fully centralized or fully distributed approach for document clustering. Hence, this paper proposes a novel approach to perform document clustering by modifying the conventional Self Organizing Map (SOM). The contribution of this work is threefold. The first is a distributed approach to pre-process the documents; the second being an adaptive bottom-up approach towards document clustering and the third being a neighbourhood model suitable for Ring Topology for document clustering. Experimentation on real datasets and comparison with traditional SOM show the efficacy of the proposed approach.",,978-1-5090-2029-4,10.1109/ICACCI.2016.7732140,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732140,Self Organizing Maps;Document clustering;Radius function;Ring Topology;MapReduce;Distributed approach,Topology;Neurons;Feature extraction;Self-organizing feature maps;Informatics;Clustering algorithms;Brain modeling,distributed processing;document handling;pattern clustering;self-organising feature maps,adaptive distributed approach;selforganizing map model;document clustering;ring topology;scalability issues;accuracy issues;voluminous data clsutering;fully centralized approach;fully distributed approach;SOM;document preprocessing;adaptive bottom-up approach;neighbourhood model,,2,,7,,3-Nov-16,,,IEEE,IEEE Conferences
Clustering of Text Streams via Facility Location and Spherical K-means,從彩色文檔中提取文本-3維和4維聚類方法,A. Jain; I. Sharma,"Dept. of Computer Science & Engg., R. N. Modi Engineering College, Kota, India; Dept. of Computer Science & Engg., R. N. Modi Engineering College, Kota, India","2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)",30-Sep-18,2018,,,1209,1213,"Spherical k-means is a fast and effective method for clustering text documents in their directional representation over a unit hypersphere. Current needs of text clustering are related to clustering of streams. Due to memory restrictions, fast and effective methods are required that incur less space complexity. Few research works exist that have adapted spherical k-means to streaming text data, but recorded performance is not satisfactory for novelty detection imbalanced cluster structure. This paper presents streaming spherical k-means with associated facility location costs. Arriving documents are detected as new topic or join an existing depending on these costs.",,978-1-5386-0965-1,10.1109/ICECA.2018.8474757,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474757,text clustering;document clustering;spherical km-means;text stream;online clustering;facility location,Linear programming;Clustering algorithms;Conferences;Aerospace electronics;Clustering methods;Runtime;Computer science,computational complexity;facility location;pattern clustering;text analysis,spherical k-means;space complexity;text documents clustering;facility location costs;memory restriction,,,,14,,30-Sep-18,,,IEEE,IEEE Conferences
Using Topic and Subjectivity Analysis for Overlapped Co-clustering Documents,邁向基於零件的文檔圖像解碼,J. Huang,"Dept. of Comput. Sci. & Inf. Manage., Soochow Univ., Taipei, Taiwan",2017 IEEE Third International Conference on Multimedia Big Data (BigMM),3-Jul-17,2017,,,105,108,"The purpose of text mining is to extract meaningful information from documents for various applications. One such application is document clustering. Document clustering refers to the clustering of similar documents into a segment. However, the numerous sentiments, emotions, and concepts involved in documents complicate the task of document clustering. In this paper, we combine the results of topic analysis and sentiment analysis to perform the co-clustering of documents. In contrast to previous papers, the key characteristic of the proposed method is soft clustering, and it considers topics and subjectivity (including emotions and sentiments) simultaneously. The empirical results indicate that the proposed method can effectively cluster documents based on the MANOVA test. In addition, the proposed method also provides a flexible way to co-cluster documents based on topics and/or subjectivity.",,978-1-5090-6549-3,10.1109/BigMM.2017.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966727,text mining;clustering analysis;topic analysis;sentiment analysis;co-clustering,Text mining;Clustering methods;Sentiment analysis;Clustering algorithms;Algorithm design and analysis;Semantics,data mining;pattern clustering;sentiment analysis,topic analysis;subjectivity analysis;text mining;information extraction;document clustering;document sentiments;sentiment analysis;soft clustering;MANOVA test,,,,37,,3-Jul-17,,,IEEE,IEEE Conferences
Event-network clustering using similarity,增量多文檔摘要：一種基於增量聚類的方法,J. Shan; Z. Liu; Fu Jian-feng,"School of Computer Engineering and Science, Shanghai University, China; School of Computer Engineering and Science, Shanghai University, China; School of Computer Engineering and Science, Shanghai University, China",2010 Sixth International Conference on Natural Computation,23-Sep-10,2010,8,,3970,3973,Events in natural language document exhibit a small world structure. The paper proposes a novel method for semantic clustering events in a document clustering results show document semantic segmentation and provide meaningful interpretation and organization for the document.,2157-9563,978-1-4244-5961-2,10.1109/ICNC.2010.5584793,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584793,small world;event network;event similarity;cluster,Semantics;Clustering algorithms;Information retrieval;Humans;Event detection;Syntactics;Computers,document handling;natural language processing;pattern clustering,event-network clustering;natural language document clustering;semantic clustering;document semantic segmentation;event similarity,,,,22,,23-Sep-10,,,IEEE,IEEE Conferences
A novel algorithm for restructuring web search results by clustering pseudo documents,帶有模糊聚類的大規模文檔分類,S. B. Girdhar; R. B. Wagh,"Department of Computer Engineering, North Maharashtra University, SES's R. C. Patel Institute of Technology, Shirpur, Shirpur, India; Department of Computer Engineering, North Maharashtra University, SES's R. C. Patel Institute of Technology, Shirpur, Shirpur, India",2015 International Conference on Information Processing (ICIP),13-Jun-16,2015,,,795,800,"The search results produced by the search engine are in scattered form. The user may enter the same query for different information needs. So, it is difficult for user to get accurate information related to the submitted query. In this system feedback session is used to show the click sequences of user for a particular keyword. These click sequences are mapped into TF-IDF vector which is used to generate pseudo document. A Pseudo document represents the most important words for the searched keywords. User search goals are generated by using K-means clustering algorithm from pseudo document and the restructuring of web search results is done on the basis of user search goal. But, sometimes the URLs are not considered in clustering because of less TF-IDF values. This results in omission of some URLs from restructured result. So we propose new algorithm to restructure web search results. By using this algorithm the restructuring is done properly and all the URLS are present in the restructuring results. Finally, the metric VAP, risk and CAP are calculating the performance of restructuring. After conducting experiments on the system, we observed that the results were improved.",,978-1-4673-7758-4,10.1109/INFOP.2015.7489490,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489490,user queries;search engine results;classified average precision;K-means clustering;TF-IDF vector,Uniform resource locators;Web search;Clustering algorithms;Measurement;Search engines;Computers;Navigation,pattern clustering;search engines,pseudo document clustering;search engine;feedback session;TF-IDF vector;K-means clustering algorithm;Web search restructuring;VAP;CAP,,,,10,,13-Jun-16,,,IEEE,IEEE Conferences
Click based inferring of user search goals using pseudo document,EDSC：高維數據的有效文檔子空間聚類技術,H. P. Bhambure; M. Mokashi,"Dept. of Computer Engineering, KJCOEMR, Pune, Pune, India; Dept. of Computer Engineering, KJCOEMR, Pune, Pune, India","2015 International Conference on Computer, Communication and Control (IC4)",11-Jan-16,2015,,,1,7,"The user enters any query to find desired information. To discover number of user search goals and representing each goal with some keyword, we first infer user search goals for a query by clustering feedback sessions. For that, we use a concept of pseudo document, which is the revised version of feedback session. Then the user search goals are determined by clustering the pseudo documents and it also represents the cluster with some keyword. Evaluation is done by using the Classified Average Precision (CAP), it is used to evaluate the performance of the restructured web search results. The existing system is using the k means clustering algorithm but in proposed system, we are using bisecting k means clustering algorithm which is increasing the efficiency of result. After the segmented result formation, the result in the every segment is reorganized as per number of clicks of URLs. The link which is clicked more number of times will appear at first location in the segment. This reduces the time requirement for searching.",,978-1-4799-8164-9,10.1109/IC4.2015.7375700,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375700,Bisecting K Means;Classified Average Precision (CAP);Feedback session;Pseudo-document;Segmented Result;User Goals,Data mining;Clustering algorithms;Computers;Sun;Search engines;Internet;Bipartite graph,data mining;document handling;Internet;pattern clustering;performance evaluation;search problems,click based inferring;user search goal;pseudo document;feedback session;classified average precision;CAP;performance evaluation;Web search result;k means clustering algorithm;URL,,,,10,,11-Jan-16,,,IEEE,IEEE Conferences
Clustering search engine at Petra Christian University Library using Suffix Tree Clustering,Web文檔聚類的元啟發式方法概述,A. Handojo; A. Wibowo; J. L. Santo,"Informatics Engineering Department, Faculty of Industrial Technology, Petra Christian University; Informatics Engineering Department, Faculty of Industrial Technology, Petra Christian University; Informatics Engineering Department, Faculty of Industrial Technology, Petra Christian University",2011 International Conference on Uncertainty Reasoning and Knowledge Engineering,1-Sep-11,2011,1,,263,266,"At Petra Christian University Library, book searching engine system is using a common keyword matching as search queries. So, the user must enter the correct keyword and this case often takes a long time for users to find book that needed. Therefore, required an application that's capable display the search results that classified into a group/cluster. Which is each cluster will contain the documents that have the same classification base on keyword that been input by user in order to assist users in perform book searching. In this study, have been build an application to classify the document search results into a group of documents that have the same classification/cluster. Input to the cluster obtained from regular search results then the classification/clustering done by using Suffix Tree Clustering (STC) uses the document phrase. Based on the testing, the resulting cluster has been able to cluster documents that correlate or have the same classification with the keyword that input by user. The average time that needed to process for each document is 0.1139 seconds.",,978-1-4244-9984-7,10.1109/URKE.2011.6007813,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007813,Suffix Tree Clustering;Suffix Tree;Clustering;Searching;Library,Microcontrollers;Cleaning;Libraries;Mice;Dairy products;Embedded systems;Search engines,document handling;educational institutions;libraries;pattern classification;pattern clustering;query processing;search engines;trees (mathematics),clustering search engine;Petra Christian university library;suffix tree clustering;book searching engine system;search queries;document search results classification,,2,,5,,1-Sep-11,,,IEEE,IEEE Conferences
Structure-oriented clustering of XML documents: A transactional approach,特徵點聚類方法對錶單文檔進行細分,G. Costa; R. Ortale,"ICAR-CNR, Via P. Bucci 41C, 87036 Rende (CS), Italy; ICAR-CNR, Via P. Bucci 41C, 87036 Rende (CS), Italy",2012 6th IEEE International Conference Intelligent Systems,22-Oct-12,2012,,,188,193,"Clustering XML documents by structure has been, generally, accomplished by looking at the occurrence of one pre-established type of structural component in the structures of the XML documents. It is likely that focusing only on one type of structural component may produce clusters with a certain extent of inner structural inhomogeneity, because of uncaught differences in the structures of the XML documents or for an inappropriate choice of structural component. To overcome these limitations, a new parameter-free approach to clustering XML document is proposed, that allows to consider simultaneously multiple types of structural components to isolate structurally-homogeneous clusters of XML documents. The idea behind the approach is to represent each XML document as a transaction of boolean feature, enlightening of suitable selection of its structural components. A parameter-free clustering scheme is, then, used to isolate structural homogeneous clusters. A comparative evaluation over both real and synthetic XML data provides evidence of effectiveness and efficacy of the devised approach.",1941-1294,978-1-4673-2278-2,10.1109/IS.2012.6335134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335134,Data Mining;XML clustering;XML transactional representation,XML;Vegetation;Clustering algorithms;Partitioning algorithms;Electronic mail;Focusing;Nonhomogeneous media,Boolean algebra;data mining;data structures;document handling;pattern clustering;XML,structure-oriented clustering;XML document clustering;inner structural inhomogeneity;parameter-free approach;structurally-homogeneous clusters;XML document representation;boolean feature transactional approach;structural component selection;parameter-free clustering scheme;synthetic XML data;real XML data,,6,,20,,22-Oct-12,,,IEEE,IEEE Conferences
The research on Chinese document clustering based on WEKA,使用案例語法結構的Web文檔聚類技術,P. Han; D. Wang; Q. Zhao,"Department of Information Management, Nanjing University, Nanjing 210093, China; Department of Information Management, Nanjing University, Nanjing 210093, China; Dean's Office, Nanjing Forest Police College, Nanjing 210046, China",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1953,1957,"This paper gives an experiment on Chinese document clustering based on WEKA. WEKA is an excellent open-source of data mining tool in abroad, but it is rarely used at home. We conducted the Chinese document clustering by K-means algorithm through adjusting the parameters in WEKA. Recall, precision and F-measure method are used to evaluate the experiment. We hope to provide a reference for researchers in this field.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016955,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016955,WEKA;Document clustering;Document feature;Document representation,Clustering algorithms;Software algorithms;Partitioning algorithms;Feature extraction;Computational modeling;Machine learning;Principal component analysis,data mining;document handling;Java;learning (artificial intelligence);pattern clustering,Chinese document clustering;WEKA;data mining tool;K-means algorithm;F-measure method,,3,,9,,12-Sep-11,,,IEEE,IEEE Conferences
A Document Clustering Technique Based on Term Clustering and Association Rules,使用基於本體的概念權重的生物醫學文檔聚類,Y. Cheng; T. Li; S. Zhu,"Comput. Sci. & Eng. Dept., North China Inst. of Aerosp. Eng., Langfang, China; Comput. Sci. & Eng. Dept., North China Inst. of Aerosp. Eng., Langfang, China; Comput. Software Technol. Dept., Peking Univ. Founder Technol. Coll., Langfang, China",2010 2nd International Workshop on Database Technology and Applications,6-Dec-10,2010,,,1,3,"With development of internet and database technology, web mining has got more and more attentions from information science domain. This paper proposes a document clustering technique based on term clustering and association rules. In this technique, extract words from document collection firstly, then construct term clustering according to AMI(Average Mutual Information) between terms, document VSM(Vector Space Model) is represented by term clustering, and use association rules to mine document clustering. Experiment results show that performance and clustering quality of this technique are improved than those of traditional methods in the clustering process.",2167-194X,978-1-4244-6977-2,10.1109/DBTA.2010.5659049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659049,,Association rules;Clustering algorithms;Semantics;Web mining;Aerospace engineering;Variable speed drives;Mutual information,data mining;pattern clustering;vectors;word processing,association rule;document clustering technique;term clustering;word extraction;average mutual information;vector space model;Web mining,,,,12,,6-Dec-10,,,IEEE,IEEE Conferences
A Fused Multi-feature Based Co-training Approach for Document Clustering,使用共識和分類促進基於歧視信息的文檔聚類,Y. Wang; W. Wang; W. Dai; P. Jiao; W. Yu,"Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China",2016 3rd International Conference on Information Science and Control Engineering (ICISCE),3-Nov-16,2016,,,38,43,"Document clustering is a popular topic in data mining and information retrieval. Most models and methods for this problem are based on computing the similarity between pair documents modeled in a space of all terms, or a new feature space obtained by applying a topic modeling technique for a given corpus. In this paper, we regard these two ideas as clustering on term feature and on semantic feature, and have an assumption that they can contribute to each other in clustering. Also, we propose a co-training approach for spectral clustering taking two features into account. Experiments on four real-world datasets show the feasibility and efficacy of our proposed approach compared with a number of the baseline methods.",,978-1-5090-2535-0,10.1109/ICISCE.2016.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726116,multi-feature;co-training;document clustering;spectral clustering,Semantics;Clustering algorithms;Large scale integration;Kernel;Matrix decomposition;Feature extraction;Classification algorithms,data mining;document handling;information retrieval;pattern clustering;sensor fusion,fused multifeature based cotraining;document clustering;data mining;information retrieval;topic modeling;spectral clustering,,,,31,,3-Nov-16,,,IEEE,IEEE Conferences
Web search result refinement by document clustering,基於概念的文檔相似度聚類模型,Ming Hei Tsui; B. Lim; Daming Shi,"School of Computer Engineering, Nanyang Technological University, 639798 Singapore; School of Computer Engineering, Nanyang Technological University, 639798 Singapore; School of Computer Engineering, Nanyang Technological University, 639798 Singapore","2007 IEEE International Conference on Systems, Man and Cybernetics",2-Jan-08,2007,,,3081,3086,"A simple search keyword usually returns million of search results. The result count may appear impressive, at the same time it confuse the users. User usually will not wish to browse through million of entries. This paper proposed a query refinement method by iterative clustering of information from the Web page content. QRSE system is developed to demonstrate the above mentioned concept.",1062-922X,978-1-4244-0990-7,10.1109/ICSMC.2007.4413950,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413950,Web Search;Query Expansion;Query Refinement;User Feedback,Web search;Search engines;Web pages;Clustering algorithms;Support vector machines;Support vector machine classification;Data mining;Static VAr compensators;Feedback;Electronic mail,document handling;Internet;iterative methods;pattern clustering;query processing;search engines,Web search result refinement;document clustering;search keyword;query refinement search engine;iterative clustering;Web page content,,,,15,,2-Jan-08,,,IEEE,IEEE Conferences
Entropy-based clustering for improving document re-ranking,平衡的詞簇，可解釋的文檔表示,C. Teng; Y. He; D. Ji; Cheng zhou; Yixuan Geng; Shu Chen,"Computer School, Wuhan University, China; Computer School, Wuhan University, China; Computer School, Wuhan University, China; School of Mathematics and Statistics, Wuhan University, China; International School of Software, Wuhan University, China; International School of Software, Wuhan University, China",2009 IEEE International Conference on Intelligent Computing and Intelligent Systems,28-Dec-09,2009,3,,662,666,"Document re-ranking locates between initial retrieval and query expansion in information retrieval system. In this paper, we propose entropy-based clustering approach for document re-ranking. The value of within-cluster entropy determines whether two classes should be merged, and the value of between-cluster entropy determines how many clusters are reasonable. What to do next is finding a suitable cluster from clustering result to construct pseudo labeled document, and conduct document re-ranking as our previous method. We focus clustering strategy for documents after initial retrieval. Experiment with NTCIR-5 data show that the approach can improve the performance of initial retrieval, and it is helpful for improving the quality of document re-ranking.",,978-1-4244-4754-1,10.1109/ICICISYS.2009.5358089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358089,component;Information Retrieval;Document re-ranking;Clustering;within-cluster entropy;between-cluster entropy,Information retrieval;Entropy;Helium;Mathematics;Statistics;Text analysis;Large-scale systems;Thesauri;Vocabulary;Concrete,entropy;information retrieval;pattern clustering,document re-ranking;entropy-based clustering approach;within-cluster entropy;between-cluster entropy;initial document retrieval;query expansion;information retrieval system,,,,16,,28-Dec-09,,,IEEE,IEEE Conferences
A survey of document clustering using semantic approach,基於文檔報價關係的聚類算法,N. Y. Saiyad; H. B. Prajapati; V. K. Dabhi,"Department of Information Technology, Dharamsinh Desai University, Nadiad, India; Department of Information Technology, Dharamsinh Desai University, Nadiad, India; Department of Information Technology, Dharamsinh Desai University, Nadiad, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)",24-Nov-16,2016,,,2555,2562,"Document clustering is the application of cluster analysis to textual documents. It is commonly used technique in data mining, information retrieval, knowledge discovery from data, pattern recognition, etc. In traditional document clustering, a document is considered as a bag of words; where semantic meaning of word is not taken into consideration. However, to achieve accurate document clustering, feature such as meanings of the words is important. Document clustering can be done using semantic approach because it takes semantic relationship among words into account. This paper highlights the problems in traditional approach as well as semantic approach. This paper identifies four major areas under semantic clustering and presents a survey of 23 papers that are studied, covering major significant work. Moreover, this paper also provides a survey of tools specifically used for text processing, and clustering algorithms, that help in applying and evaluating document clustering. The presented survey is used in preparing the proposed work in the same direction. This proposed work uses the sense of a word for text clustering system. Lexical chains will be used as features that are to be developed using the identity/synonymy relation from WordNet ontology as background knowledge. Later, clustering will be done using the lexical chains.",,978-1-4673-9939-5,10.1109/ICEEOT.2016.7755154,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755154,clustering;semantic clustering;ontology;WordNet;Latent Semantic Indexing;lexical chains,Semantics;Clustering algorithms;Ontologies;Large scale integration;Matrix decomposition;Text analysis;Information retrieval,feature selection;ontologies (artificial intelligence);pattern clustering;text analysis,WordNet ontology;synonymy relation;identity relation;lexical chains;text clustering system;clustering algorithms;text processing;semantic clustering;textual documents;cluster analysis;document clustering,,6,,34,,24-Nov-16,,,IEEE,IEEE Conferences
Comparison of deep learning based concept representations for biomedical document clustering,基於多類光譜聚類的Web文檔聚類,S. Shah; X. Luo,"Purdue School of Engineering Technology, IUPUI, 799 W. Michigan Street, Indianapolis, IN 46202, USA; Purdue School of Engineering Technology, IUPUI, 799 W. Michigan Street, Indianapolis, IN 46202, USA",2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),9-Apr-18,2018,,,349,352,"In this research, document representations based on distributed representations of the concepts along with new weighting schemes for the documents are explored. The baseline weighting scheme is the traditional Term Frequency-Inverse Document Frequency (TF-IDF) of the concepts, whereas, the other two newly proposed ones consider both local content using the TF-IDF and associations between concepts. The distributed representations of the concepts are measured using a deep learning algorithm. The evaluation of the proposed document representations is based on the k-means clustering results. The results show that document representation based on TF-IDF in combination with the term based distributed representations for concepts outperforms the other two based on the returned evaluation metrics - F1-measure (80.21%) and Purity (77.1%).",,978-1-5386-2405-0,10.1109/BHI.2018.8333440,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333440,,Biomedical measurement;Diabetes;Semantics;Cancer;Colon;Machine learning,document handling;learning (artificial intelligence);medical information systems;pattern clustering,concept representations;biomedical document clustering;document representation;distributed representations;baseline weighting scheme;TF-IDF;deep learning algorithm;Term Frequency-Inverse Document Frequency,,,,15,,9-Apr-18,,,IEEE,IEEE Conferences
A mathematic morphology approach for radial lens correction of document image,基於K-Medoids聚類算法的P2P文檔共享系統用戶興趣建模,Miao Ligang; Chang Jun,"Department of Electronics Information, Northeastern University at Qinhuangdao, China; Information Center, Inner Mongolia Meteorological Administration, Huhehaote, China",2010 International Conference On Computer Design and Applications,5-Aug-10,2010,1,,V1-465,V1-468,"Document image captured by hand-held camera suffers from lens distortion with different degrees, and a mathematic morphology based lens correction algorithm is proposed with the text line of document images. First, it uses Niblack adaptive thresholding algorithm to segment the document images, and clusters connected components into text lines with morphology closing algorithm. Then, it uses second order polynomial to fit the central line of text line, and construct the object function of lens distortion. It warps the curved text line to the straight text line, thus, to solve the distortion parameters of document image. Experiments show this method can effectively correct lens distortion of document image with various degrees.",,978-1-4244-7164-5,10.1109/ICCDA.2010.5540729,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540729,document image;radial distortion;radial correction;directional smoothing closing algorithm,Mathematics;Morphology;Lenses;Nonlinear distortion;Clustering algorithms;Digital cameras;Image segmentation;Polynomials;Calibration;Handheld computers,document image processing;image segmentation;mathematical morphology;pattern clustering;polynomials,mathematic morphology approach;radial lens correction;hand held camera;Niblack adaptive thresholding algorithm;lens distortion;document image segmentation;clusters connected components,,1,,9,,5-Aug-10,,,IEEE,IEEE Conferences
Fast document clustering based on weighted comparative advantage,基於單應性的後置過濾器改進徽標分類和匹配以進行文檔分類,J. Ji; T. Y. T. Chan; Q. Zhao,"Intelligent System Lab, The University of Aizu, Aizuwakamatsu, Fukushima, Japan; School of Computing, The University of Akureyri, Iceland; Intelligent System Lab, The University of Aizu, Aizuwakamatsu, Fukushima, Japan","2009 IEEE International Conference on Systems, Man and Cybernetics",4-Dec-09,2009,,,541,546,"Document clustering is the process of partitioning a set of unlabeled documents into clusters such that documents within each cluster share some common concepts. To help with this analysis, concepts are conveniently represented using some key terms. For clustering algorithm, the most costly CPU time has to do with the classification phase. Using words as features, text data are represented in a very high dimensional vector space. We have studied a comparative advantage based algorithm for clustering sparse data in this space, it used one 聶ruler聶 instead of k centers to identify the comparative advantage of each cluster and define the cluster label for each document. However, this algorithm only considered the relative strength between clusters, the relationship between terms was ignored. In this paper, we proposed a weighted comparative advantage based clustering algorithm. The experimental results based on SMART system databases show that the new algorithm is better than simple comparative advantage algorithm, without any extra computation time. Compare with k-means, not only can it get comparable results but it can also significantly accelerate the clustering procedure.",1062-922X,978-1-4244-2793-2,10.1109/ICSMC.2009.5346877,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346877,Document clustering;dimension reduction;key term extraction;sparsity;k-means;weighted comparative advantage,Clustering algorithms;Frequency;Intelligent systems;Partitioning algorithms;Databases;Cybernetics;USA Councils;Acceleration;Data mining;Virtual manufacturing,document handling;pattern clustering,fast document clustering;CPU;text data representation;very high dimensional vector space;sparse data sparse clustering;SMART system databases;weighted comparative advantage based clustering algorithm,,6,,12,,4-Dec-09,,,IEEE,IEEE Conferences
Optimal Display Adaptation of Iconic Document Visualizations via BFOS-Style Tree Pruning,Web文檔聚類算法綜述,K. Berkner; M. J. Gormish,"Ricoh Innovations, Inc., 2882 Sand Hill Rd, Suite 115, Menlo Park, CA 95025, USA; Ricoh Innovations, Inc., 2882 Sand Hill Rd, Suite 115, Menlo Park, CA 95025, USA",2006 International Conference on Image Processing,20-Feb-07,2006,,,3189,3192,"This paper introduces a new visual representation of a document or group of documents, a dynamic document icon, or Dydocon. Its representation is symbolic like an icon, but changes depending on document content. A Dydocon can be used for multiple documents and thus is useful for presentation of clustered search results. Because a large number of clusters may be returned by a search it is desirable to optimally select clusters given available display space. We cluster documents into a visual taxonomy described by a tree of Dydocons, define tree functionals for icon size and distortion of a cluster, and perform display-size adaptation through optimization using a BFOS-style algorithm.",2381-8549,1-4244-0480-0,10.1109/ICIP.2006.313065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4107248,Document image processing;tree searching;rate distortion theory;visual languages,Displays;Visualization;Taxonomy;Clustering algorithms;Technological innovation;Document image processing;Decision trees;Rate distortion theory;Image databases;Information retrieval,data visualisation;document image processing;image representation;trees (mathematics),visual representation;dynamic document icon;Dydocon;taxonomy;optimization;BFOS-style tree pruning;display adaptation,,,,7,,20-Feb-07,,,IEEE,IEEE Conferences
Density-Based Spatiotemporal Clustering Algorithm for Extracting Bursty Areas from Georeferenced Documents,使用元素和文檔配置文件進行信息聚類,K. Tamura; T. Ichimura,"Grad. Sch. of Inf. Sci., Hiroshima City Univ., Hiroshima, Japan; Fac. of Manage. & Inf. Syst., Prefectural Univ. of Hiroshima, Hiroshima, Japan","2013 IEEE International Conference on Systems, Man, and Cybernetics",27-Jan-14,2013,,,2079,2084,"Nowadays, with the increasing attention being paid to social media, a huge number of georeferenced documents, which include location information, are posted on social media sites. People transmit and collect information over the Internet through these georeferenced documents. Georeferenced documents are usually related to not only personal topics but also local topics and events. Therefore, extracting bursty areas associated with local topics and events from georeferenced documents is one of the most important challenges in different application domains. In this paper, a novel spatiotemporal clustering algorithm, called the (炵,?)-density-based spatiotemporal clustering algorithm, for extracting bursty areas from georeferenced documents is proposed. The proposed clustering algorithm can recognize not only temporally-separated but also spatially-separated clusters. To evaluate our proposed clustering algorithm, geo-tagged tweets posted on the Twitter site are used. The experimental results show that the (炵,?)-density-based spatiotemporal clustering algorithm can extract bursty areas as (炵,?)-density-based spatiotemporal clusters associated with local topics and events.",1062-922X,978-1-4799-0652-9,10.1109/SMC.2013.356,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722109,Spatiotemporal clustering algorithm;Density-based clustering;Georeferenced document;Spatiotemporal data stream;Topic and event detection,Clustering algorithms;Spatiotemporal phenomena;Snow;Twitter;Rain;Media;Internet,document handling;Internet;pattern clustering,"炵,?-density-based spatiotemporal clustering algorithm;georeferenced documents;social media;location information;Internet;bursty area extraction",,17,,19,,27-Jan-14,,,IEEE,IEEE Conferences
Fuzzy C-Means Text Clustering with Supervised Feature Selection,使用文檔到矢量模型對文檔進行聚類以減少維數,W. Wang; C. Wang; X. Cui; A. Wang,"Key Lab. of Complex Syst. & Intell. Sci., Chinese Acad. of Sci., Beijing; Key Lab. of Complex Syst. & Intell. Sci., Chinese Acad. of Sci., Beijing; Key Lab. of Complex Syst. & Intell. Sci., Chinese Acad. of Sci., Beijing; Key Lab. of Complex Syst. & Intell. Sci., Chinese Acad. of Sci., Beijing",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,1,,57,61,"The traditional text clustering algorithm often uses the unsupervised feature selection method to select the feature. In this paper we propose a new text clustering algorithm SFFCM which use the supervised feature selection method to select the feature. The SFFCM is based on the EM algorithm. In the E-step, to calculate the expectation, we use the supervised feature selection algorithm to calculate the relevancy score for each term. In the M step we use the FCM algorithm to obtain the cluster results based on the selected terms. Our experimental results on standard document clustering benchmark corpuses: OHSUMED, 20-Newsgroups and Reuters-21578 show that the SFFCM text clustering algorithm can generate better clustering results than other control clustering methods and the supervised feature selection can improve the performance of the text clustering algorithm. We also propose a supervised feature selection measure CRF-CHI measure which is based on the chi2 statistic and the category relative frequency. The experimental results also confirm that the CRF-CHI is an effective supervised feature selection measure.",,978-0-7695-3305-6,10.1109/FSKD.2008.161,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665939,Feature Selection;Clustering;Fuzzy C means,Clustering algorithms;Statistics;Fuzzy systems;Frequency measurement;Laboratories;Intelligent systems;Automation;Clustering methods;Text categorization;Iterative methods,document image processing;expectation-maximisation algorithm;fuzzy systems;pattern clustering;text analysis,fuzzy c-means text clustering;supervised feature selection;EM algorithm;standard document clustering benchmark;OHSUMED;20-Newsgroups;Reuters-21578,,1,,9,,5-Nov-08,,,IEEE,IEEE Conferences
Kernel-based consensus clustering for ontology-embedded document repository of power substations,個人數據空間中的文檔聚類,L. Yan; W. Tang; Q. H. Wu; J. S. Smith,"Department of Electrical Engineering and Electronic, The University of Liverpool, Liverpool, L69, 3GJ, U.K.; School of Electric Power Engineering, South China University of Technology, Guangzhou 510641, China; School of Electric Power Engineering, South China University of Technology, Guangzhou 510641, China; Department of Electrical Engineering and Electronic, The University of Liverpool, Liverpool, L69, 3GJ, U.K.",CSEE Journal of Power and Energy Systems,13-Jul-17,2017,3,2,212,221,"This paper presents a novel consensus clustering (CC) approach for a document repository concerning power substations (PSD) and contributes to the intangible asset management of power systems. A domain ontology model, i.e., substation ontology (SONT), is applied to modify the traditional vector space model (VSM) for document representation, which is concerned with the semantic relationship between terms. A new document representation is generated using a term mutual information matrix with the aid of SONT. In addition, compared with two other novel CC algorithms, i.e., non-negative matrix factorisation-based CC (NNMF-CC) and information theory-based CC (INT-CC), weighted partition via kernel-based CC algorithm (WPK-CC) is utilised to solve the CC issue for PSD. Meanwhile, genetic algorithms (GA) were applied to WPK-CC for PSD, as there are limitations in the original WPK-CC for document clustering. Subsequently, selected mechanisms in each GA's procedure are compared and improved, resulting in comprehensive parameter settings for the PSD CC. Four simulation studies have been designed, in which the results are evaluated by purity validation method and show that the SONT-based document representation and improved WPK-CC, via modified GA, significantly improve the performance of the PSD CC.",2096-0042,,10.17775/CSEEJPES.2017.0026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976169,power system asset management;documentclustering;consensus clustering;ontology;kernel method;GA,Clustering algorithms;Algorithm design and analysis;Substations;Ontologies;Linear programming;Genetic algorithms,document handling;genetic algorithms;matrix decomposition;ontologies (artificial intelligence);pattern clustering;power engineering computing;substations,domain ontology;substation ontology;SONT;document representation;nonnegative matrix factorisation;information theory;genetic algorithms;document clustering;document repository;power substations;consensus clustering approach;intangible asset management;power systems;vector space model;mutual information matrix;Kernel-based Consensus Clustering;Ontology-embedded Document Repository,,,,,,13-Jul-17,,,CSEE,CSEE Journals
On-line legal aid: Markov chain model for efficient retrieval of legal documents,使用Dempster Shafer方法對財務文件進行司法鑑定,R. Ghosh-Roy; I. O. Habiballah; T. J. Stonham; M. R. Irving,"Dept. of Electr. Eng. & Electron., Brunel Univ., Uxbridge, UK; NA; NA; NA",IEE Colloquium on Document Image Processing and Multimedia Environments,6-Aug-02,1995,,,1月15日,7月15日,"It is widely accepted that, with large databases, the key to good performance is effective data-clustering. In any large document database clustering is essential for efficient search, browse and therefore retrieval. Cluster analysis allows the identification of groups, or clusters, of similar objects in multi-dimensional space. Conventional document retrieval systems involve the matching of a query against individual documents, whereas a clustered search compares a query with clusters of documents, thereby achieving efficient retrieval. In most document databases periodic updating of clusters is required due to the dynamic nature of a database. Experimental evidence, however shows that clustered searches are substantially less effective than conventional searches of corresponding non-clustered documents. We investigate the present clustering criteria and its drawbacks. We propose a new approach to clustering and justify the reasons why this new approach should be tested and (if proved beneficial) adopted.",,,10.1049/ic:19951196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=498887,,Data processing;Database systems;Markov processes;Document handling,law administration;query processing;very large databases;Markov processes;document handling,Markov chain model;efficient legal document retrieval;on-line legal aid;data clustering;large document database;searching;browsing;cluster analysis;similar object cluster identification;multi-dimensional space;document clusters;query;periodic cluster updating;clustered searches,,,,,,6-Aug-02,,,IET,IET Conferences
Fast historic document retrieval by extracting document image summary,基於後綴樹的搜索結果聚類算法,Chwan-Yi Shiah; Yun-Sheng Yen,"Department of Applied Informatics, Fo Guang University, YiLan, Taiwan 26247; Department of Applied Informatics, Fo Guang University, YiLan, Taiwan 26247",2011 International Conference on Multimedia Technology,25-Aug-11,2011,,,3062,3065,"Historic documents such as Chinese calligraphy and old newspapers usually were handwritten or printed in poor quality so that an automatic optical character recognition procedure for scanned document images is difficult to apply. Thus efficient pattern matching techniques are required in order to do content-based information retrieval based on user's queries. In this paper, a fast pattern clustering and image matching procedure is proposed to do image/pattern search in a historic document image based on user's query images. The image summary extracted from the document image is constructed so that a set of distinct image clusters are formed. A couple of distance measures that calculate distance between image patterns are also proposed to evaluate their cluster similarities. By precise pattern matching and hierarchical image clustering, our experimental results show that an online query image can produce accurate and faster results than traditional approaches for a broad range of historic document images.",,978-1-61284-774-0,10.1109/ICMT.2011.6003077,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003077,historic document image;pattern matching;image clustering;information retrieval,Pattern matching;Shape;Information retrieval;Histograms;Clustering algorithms;Image segmentation;Complexity theory,content-based retrieval;feature extraction;image matching;optical character recognition;pattern clustering;query processing,fast historic document retrieval;document image summary extraction;Chinese calligraphy;old newspapers;automatic optical character recognition procedure;scanned document images;pattern matching techniques;content-based information retrieval;pattern clustering;image matching procedure;image-pattern search;user query images;distinct image clusters;hierarchical image clustering,,,,14,,25-Aug-11,,,IEEE,IEEE Conferences
Clustering of Words from Czech Written Documents Using GHSOM,基於集群核心和LSPX的XML文檔聚類算法,R. Mou?ek; P. Mautner,"Dept. of Comput. Sci. & Eng., Univ. of West Bohemia, Pilsen, Czech Republic; Dept. of Comput. Sci. & Eng., Univ. of West Bohemia, Pilsen, Czech Republic",2010 Second WRI Global Congress on Intelligent Systems,4-Feb-11,2010,3,,150,153,"The paper focuses on possibilities of improving categorization of words from input collection of Czech written documents into syntactic categories using the Growing Hierarchical Self-Organizing Map (GHSOM). Clustering on two document collections is presented, results of GHSOM and WEBSOM are compared. The modified GHSOM is introduced.",2155-6091,978-1-4244-9247-3,10.1109/GCIS.2010.191,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709344,Clustering;Czech written documents;GHSOM;WEBSOM;syntactic categories,Syntactics;Context;Semantics;Training;Quantization;Self organizing feature maps;Computer architecture,pattern clustering;self-organising feature maps;word processing,words clustering;Czech written documents;GHSOM;syntactic categories;growing hierarchical self-organizing map,,,,8,,4-Feb-11,,,IEEE,IEEE Conferences
A Novel Method for Hierarchical Clustering of Search Results,CFTDISM：使用改進的相似性度量來聚類財務文本文檔,G. Zhang; Y. Liu; S. Tan; X. Cheng,"Inst. of Comput. Technol., Beijing; Inst. of Comput. Technol., Beijing; Inst. of Comput. Technol., Beijing; Inst. of Comput. Technol., Beijing",2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops,14-Jan-08,2007,,,181,184,"Search result clustering can help users quickly browse through the documents returned by search engine. Traditional clustering techniques are inadequate since they don't generate clusters with highly readable names. Label-based clustering is quite promising, which usually takes n-gram (usually bi-gram) as label candidates. However, meaningless n-grams are not removed from the candidates. In this paper, DF, user log and query context are introduced as label ranking features. An integrated model is used to combine these three features for cluster label ranking. Further more, a novel graph based clustering algorithm (GBCA) for hierarchical clustering is proposed. Experiments indicate that the cluster label extraction makes a great improvement (about 8%) over the baseline in precision, and GBCA outperforms STC and Snaket in F-measure.",,0-7695-3028-1,10.1109/WI-IATW.2007.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427567,search result clusteringsearch engineclustering,Clustering algorithms;Partitioning algorithms;Clustering methods;Intelligent agent;Search engines;Animals;Conferences;Computers;Scattering;Fractionation,document handling;graph theory;pattern clustering;query processing;search engines,search result clustering;document browsing;search engine;label-based clustering;document frequency;user log;query context;label ranking features;graph based clustering algorithm;hierarchical clustering;cluster label extraction,,2,,16,,14-Jan-08,,,IEEE,IEEE Conferences
A Coupled Mean Shift-Anisotropic Diffusion Approach for Document Image Segmentation and Restoration,多源文檔聚類的分層Dirichlet多項式分配模型,F. Drira; F. LeBourgeois; H. Emptoz,"LIRIS - INSA Lyon, 20 avenue Albert Einstein, 69621 Villeurbanne Cedex, France; LIRIS - INSA Lyon, 20 avenue Albert Einstein, 69621 Villeurbanne Cedex, France; LIRIS - INSA Lyon, 20 avenue Albert Einstein, 69621 Villeurbanne Cedex, France",Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),5-Nov-07,2007,2,,814,818,"Mean shift, a powerful color clustering approach successfully applied to image segmentation, has two main properties that are relevant for use in document image segmentation. These properties include: the autonomous definition of both color clusters' centers and numbers and the good tolerance to noisy data sets. Hence, mean shift could robustly process degraded background document images and improve their legibility. Nevertheless, this paper proves that coupling this approach and anisotropic diffusion within a joint iterative framework has more interesting results. For instance, this framework generates segmented images with more reduced artefacts on edges and background than those obtained after applying each method alone. This improvement is explained by the mutual interaction of global and local information, respectively introduced by the mean shift and anisotropic diffusion, and by the nature of this latter, smoothing while preserving continuities across edges. Some experiments, done on real ancient document images, illustrate these ideas and indicate that our proposed framework provides an efficient tool for document image segmentation and restoration.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4377028,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4377028,,Image segmentation;Image restoration;Anisotropic magnetoresistance;Smoothing methods;Degradation;Robustness;Image generation;Colored noise;Iterative methods;Pixel,document image processing;image colour analysis;image restoration;image segmentation;iterative methods;pattern clustering,coupled mean shift-anisotropic diffusion approach;document image segmentation;document image restoration;color clustering approach;joint iterative framework,,4,,5,,5-Nov-07,,,IEEE,IEEE Conferences
Table Based Single Pass Algorithm for Clustering Electronic Documents in 20NewsGroups,使用空間信息通過線定向聚類對彩色文檔進行分割,T. Jo; G. Jo,"Sch. of Comput. & Inf. Eng., Inha Univ., Incheon; Sch. of Comput. & Inf. Eng., Inha Univ., Incheon",2008 IEEE International Workshop on Semantic Computing and Applications,22-Jul-08,2008,,,66,71,"This research proposes a modified version of single pass algorithm which is specialized for text clustering. Encoding documents into numerical vectors for using the traditional version of single pass algorithm causes the two main problems: huge dimensionality and sparse distribution. Therefore, in order to address the two problems, this research modifies the single pass algorithm into its version where documents are encoded into not numerical vectors but alternative forms. In the proposed version, documents are mapped into tables and a similarity of two documents is computed by comparing their tables with each other. The goal of this research is to improve the performance of single pass algorithm for text clustering by modifying it into the specialized version.",,978-0-7695-3317-9,10.1109/IWSCA.2008.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4573152,,,document handling;information resources;pattern clustering;text analysis,table based single pass algorithm;electronic document clustering;document encoding;single pass algorithm;text clustering;newsgroups,,,,7,,22-Jul-08,,,IEEE,IEEE Conferences
NAVIDOMASS: Structural-based Approaches Towards Handling Historical Documents,使用短語的基於SOM的文檔聚類,S. Jouili; M. Coustaty; S. Tabbone; J. Ogier,"LORIA, INRIA, Vandoeuvre-l癡s-Nancy, France; L3i Lab., La Rochelle, France; LORIA, INRIA, Vandoeuvre-l癡s-Nancy, France; L3i Lab., La Rochelle, France",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,946,949,"In the context of the NAVIDOMASS project, the problematic of this paper concerns the clustering of historical document images. We propose a structural-based framework to handle the ancient ornamental letters data-sets. The contribution, firstly, consists of examining the structural (i.e. graph) representation of the ornamental letters, secondly, the graph matching problem is applied to the resulted graph-based representations. In addition, a comparison between the structural (graphs) and statistical (generic Fourier descriptor) techniques is drawn.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.237,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5595831,Historical document analysis;Graph clustering;Structural pattern recognition,Shape;Image segmentation;Pattern recognition;Approximation methods;Matrices;Noise;Context,document image processing;Fourier analysis;graph theory;pattern clustering;statistical analysis,NAVIDOMASS;structural based approaches;historical documents handling;historical document images clustering;ornamental letters representation;graph matching problem;statistical techniques;generic Fourier descriptor,,9,,14,,7-Oct-10,,,IEEE,IEEE Conferences
Sequential and Unsupervised Document Authorial Clustering Based on Hidden Markov Model,使用概念空間和余弦相似度度量的文檔聚類,K. Aldebei; H. Farhood; W. Jia; P. Nanda; X. He,"Global Big Data Technol. Centre, Univ. of Technol. Sydney, Sydney, NSW, Australia; Global Big Data Technol. Centre, Univ. of Technol. Sydney, Sydney, NSW, Australia; Global Big Data Technol. Centre, Univ. of Technol. Sydney, Sydney, NSW, Australia; Global Big Data Technol. Centre, Univ. of Technol. Sydney, Sydney, NSW, Australia; Global Big Data Technol. Centre, Univ. of Technol. Sydney, Sydney, NSW, Australia",2017 IEEE Trustcom/BigDataSE/ICESS,11-Sep-17,2017,,,379,385,"Document clustering groups documents of certain similar characteristics in one cluster. Document clustering has shown advantages on organization, retrieval, navigation and summarization of a huge amount of text documents on Internet. This paper presents a novel, unsupervised approach for clustering single-author documents into groups based on authorship. The key novelty is that we propose to extract contextual correlations to depict the writing style hidden among sentences of each document for clustering the documents. For this purpose, we build an Hidden Markov Model (HMM) for representing the relations of sequential sentences, and a two-level, unsupervised framework is constructed. Our proposed approach is evaluated on four benchmark datasets, widely used for document authorship analysis. A scientific paper is also used to demonstrate the performance of the approach on clustering short segments of a text into authorial components. Experimental results show that the proposed approach outperforms the state-of-the-art approaches.",2324-9013,978-1-5090-4906-6,10.1109/Trustcom/BigDataSE/ICESS.2017.261,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029464,Intrinsic Plagiarism Detection;Document Segmentation;Forensic Analysis;Intelligence Issues,Hidden Markov models;Correlation;Writing;Estimation;Clustering algorithms;Decoding;TV,hidden Markov models;information retrieval;pattern clustering;text analysis;unsupervised learning,sequential document authorial clustering;unsupervised document authorial clustering;hidden Markov model;text documents;Internet;single-author document clustering;document authorship analysis;contextual correlation extraction;writing style;HMM;sequential sentence relations;benchmark datasets,,,,21,,11-Sep-17,,,IEEE,IEEE Conferences
Document clustering using locality preserving indexing,使用自動關鍵詞提取的Web文檔聚類,D. Cai; X. He; J. Han,"Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA; NA",IEEE Transactions on Knowledge and Data Engineering,31-Oct-05,2005,17,12,1624,1637,"We propose a novel document clustering method which aims to cluster the documents into different semantic classes. The document space is generally of high dimensionality and clustering in such a high dimensional space is often infeasible due to the curse of dimensionality. By using locality preserving indexing (LPI), the documents can be projected into a lower-dimensional semantic space in which the documents related to the same semantics are close to each other. Different from previous document clustering methods based on latent semantic indexing (LSI) or nonnegative matrix factorization (NMF), our method tries to discover both the geometric and discriminating structures of the document space. Theoretical analysis of our method shows that LPI is an unsupervised approximation of the supervised linear discriminant analysis (LDA) method, which gives the intuitive motivation of our method. Extensive experimental evaluations are performed on the Reuters-21578 and TDT2 data sets.",1558-2191,,10.1109/TKDE.2005.198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1524963,Index Terms- Document clustering;locality preserving indexing;dimensionality reduction;semantics.,Indexing;Large scale integration;Clustering methods;Clustering algorithms;Linear discriminant analysis;Performance evaluation;Stochastic processes;Geometry;Laplace equations,document handling;pattern clustering;database indexing;data mining,document clustering method;locality preserving indexing;lower-dimensional semantic space;supervised linear discriminant analysis;Reuters-21578 data set;TDT2 data set;geometric structures;discriminating structures,,366,,30,,31-Oct-05,,,IEEE,IEEE Journals
Semisupervised Fuzzy Clustering With Partition Information of Subsets,用於群集媒體文檔的聲明性工具的自動創建技術,J. Mei,"College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",IEEE Transactions on Fuzzy Systems,30-Aug-19,2019,27,9,1726,1737,"Pairwise constraint is a type of side information that is widely considered in existing semisupervised clustering approaches. In this paper, we explore a new form of supervision for clustering. We consider the partition results of a number of subsets as additional information to assist clustering. Compared to the pairwise constraint, which only involves the ?must-link??or ?cannot-link??relationship of two objects, the partition of a subset of objects provides information about the group structure of more objects and hence can possibly serve as a more effective form of supervision for clustering. In this paper, we instantiate the idea of clustering with subset partitions under the fuzzy clustering framework for document categorization. The proposed fuzzy clustering approach is formulated to learn from the partition of subsets and has the ability to handle high-dimensional document data. Specifically, the partition results of subsets are collectively transformed into pairwise relationships, based on which a penalty term is constructed and incorporated into a cosine-distance-based fuzzy c-means approach. The experimental results on benchmark data sets demonstrate the effectiveness of the proposed approach for a semisupervised document clustering.",1941-0034,,10.1109/TFUZZ.2018.2889010,National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584118,Clustering algorithm;document categorization;fuzzy clustering;semisupervised learning,Linear programming;Clustering algorithms;Integrated circuits;Partitioning algorithms;Euclidean distance;Semisupervised learning;Benchmark testing,document handling;fuzzy set theory;learning (artificial intelligence);pattern clustering,pairwise relationships;semisupervised document clustering;partition information;pairwise constraint;semisupervised clustering approaches;subset partitions;fuzzy clustering framework;document categorization;semisupervised fuzzy clustering,,1,,28,Traditional,20-Dec-18,,,IEEE,IEEE Journals
Clustering of web search results using Suffix tree algorithm and avoidance of repetition of same images in search results using L-Point Comparison algorithm,基於主題短語的文檔聚類新相似度度量,M. Suneetha; S. Sameen Fatima; Shaik Mohd. Zaheer Pervez,"Department of Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College, Vijayawada, Andhra Pradesh, India; Dept. of Computer Science and Engg., University College of Engineering, Osmania University, Hyderabad, Andhra Pradesh, India; 4/4B.Tech., Department of Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College, Vijayawada, Andhra Pradesh, India",2011 International Conference on Emerging Trends in Electrical and Computer Technology,2-May-11,2011,,,1041,1046,"It is a common experience to the web users with the existing search engines like Google, Yahoo, MSN, Ask, e.t.c., that the information related to the entered query returns a long ranked list of results (snippets). It becomes cumbersome to the user to go through each title, snippet and even sometimes link of the search results until relevant results are found to the query. Clustering of search results is a special technique in data mining using which the retrieved results are organized into meaningful groups enlightening the user work. This paper deals with the generalized Suffix tree based clustering approach. The most repeated phrase in the document tags is considered as cluster name. Thus in short, web search results that are fetched from the prevailing web search engines grouped under phrases that contain one or more search keywords. This paper aims at organizing web search results into clusters facilitating quick browsing options to the browser providing an excellent interface to results precisely. Suffix tree clustering produces comparatively more accurate and informative grouped results. A basic problem during image searching in any search engine is Image Repetition. This can be avoided by using the L-Point Comparison algorithm, a specially worked out technique in field of Information Retrieval systems, is also discussed with a practical example.",,978-1-4244-7926-9,10.1109/ICETECT.2011.5760272,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5760272,Coherent clustering;Cleaning of Document;Suffix Tree Based Clustering (STBC);L-point image Comparison (LPC);Shared phrase,Pixel;Clustering algorithms;Web search;Search engines;Data mining;Engines;Shape,content-based retrieval;data mining;image retrieval;Internet;pattern clustering;search engines;tree data structures;trees (mathematics),Web search result clustering;suffix tree algorithm;L-point comparison algorithm;search engines;Google;Yahoo;MSN;Ask;query return;data mining;generalized suffix tree based clustering approach;document tags;cluster name;quick browsing option;image repetition avoidance;information retrieval system;image searching,,,,6,,2-May-11,,,IEEE,IEEE Conferences
Clustering XML Documents Based on the Weight of Frequent Structures,使用文檔索引圖的實時聚類方法,J. H. Hwang; M. S. Gu,"Namseoul Univ., Chonan; NA",2007 International Conference on Convergence Information Technology (ICCIT 2007),7-Jan-08,2007,,,845,849,"The previous clustering methods of XML document group XML documents with similar structures, measuring structural similarity and distance between XML documents. In this paper, however, we propose a novel clustering method for XML documents using the weight of frequent structures in XML documents, considering that an XML document as a transaction and the extracted structures from XML documents as items of a transaction. Our experiment results show the high speed and cluster cohesion of our clustering method.",,0-7695-3038-9,10.1109/ICCIT.2007.101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420365,,XML;Clustering methods;Clustering algorithms;Books;Information technology;Computer science;Databases;Bioinformatics;Laboratories;Internet,document handling;XML,XML documents;clustering methods;structural similarity,,2,,13,,7-Jan-08,,,IEEE,IEEE Conferences
An improved K-means algorithm using modified cosine distance measure for document clustering using Mahout with Hadoop,基於本體的文檔聚類-一種高效的混合方法,L. Sahu; B. R. Mohan,"Dept. Of Information Technology, National Institute of Technology Karnataka, Surathkal, Karnataka, India; Dept. Of Information Technology National Institute of Technology Karnataka Surathkal, Karnataka, India",2014 9th International Conference on Industrial and Information Systems (ICIIS),12-Feb-15,2014,,,1,5,"In this paper, we have proposed a novel K-means algorithm with modified Cosine Distance Measure for clustering of large datasets like Wikipedia latest articles and Reuters dataset. We are customizing Cosine Distance Measure for computing similarity between objects for improving cluster quality. Our method will calculate the similarity between objects by Cosine Distance Measure and then try to bring distance more closer by squaring the distance if it is between 0 to 0.5 else increase it. It will result in minimum Intra-cluster and maximizes Inter-cluster distance value. We are measuring cluster quality in term of Inter and Intra-cluster distances, good Feature weighting such as TF-IDF, Cluster Size and Top terms of the clusters. We have compared K-means algorithm by Cosine and modified Cosine Distance measure by setting performance metric such as Inter-cluster and Intra-cluster distances, Cluster size, Execution time etc. Our experimental result shows in minimizing Intra-cluster by 0.016% and maximizing Inter-cluster distance by 0.012%, reducing the cluster size by 1.5% and reducing sequence file size by 4%, that will result in good cluster quality.",2164-7011,978-1-4799-6500-7,10.1109/ICIINFS.2014.7036661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7036661,Document Clustering;K-means;Hadoop;Mahout,Clustering algorithms;Size measurement;Vectors;Time measurement;Algorithm design and analysis;Internet;Encyclopedias,document handling;parallel processing;pattern clustering,k-means algorithm;modified cosine distance measure;document clustering;Mahout;Hadoop;Wikipedia;Reuters dataset;object similarity analysis;cluster quality improvement;intercluster distance value maximization;minimum intracluster distance value;cluster quality measurement;feature weighting;TF-IDF;performance metric;execution time;cluster size reduction;sequence file size reduction,,6,,9,,12-Feb-15,,,IEEE,IEEE Conferences
A clustering retrieval system of Chinese information,使用改進的相似性度量進行基因-miRNA相互作用文檔的新型聚類分析,Xin-Guang Sha; Yuan-Chao Liu; Ming Liu; Xiao-Long Wang,"Intelligent Technology and Natural Language Processing Lab, Harbin Institute of Technology, No. 92, West Dazhi Street, NanGang, 150001, China; Intelligent Technology and Natural Language Processing Lab, Harbin Institute of Technology, No. 92, West Dazhi Street, NanGang, 150001, China; Intelligent Technology and Natural Language Processing Lab, Harbin Institute of Technology, No. 92, West Dazhi Street, NanGang, 150001, China; Intelligent Technology and Natural Language Processing Lab, Harbin Institute of Technology, No. 92, West Dazhi Street, NanGang, 150001, China",2008 International Conference on Natural Language Processing and Knowledge Engineering,2-May-09,2008,,,1,6,"With tremendous and ever-growing amounts of electronic documents from World Wide Web and digital libraries, it becomes more and more difficult to get information that people really want. In order to predigest search process, people use clustering method to browse through search results. However traditional Chinese information clustering techniques are inadequate since they don't generate clusters with highly readable themes. This paper reformats the clustering problem as a salient phrase ranking problem. Given a query and its related ranked list of documents (typically a list of titles and snippets) returned from a certain Web search engine, this method first extracts and ranks salient phrases as candidate cluster theme, based on regression model of SVR (support vector regression) learned from human labeled training data. The documents are assigned to relevant salient phrases to form candidate clusters, and the final clusters are generated by merging these candidate clusters. This paper also searches for a reasonable format to display the final themes of clusters, in order to help users to find the interesting documents easily. Experiment results verified our method feasible and effective.",,978-1-4244-4515-8,10.1109/NLPKE.2008.4906815,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906815,Salient phrase;document clustering;performance of clustering theme,Information retrieval;Web sites;Software libraries;Clustering methods;Web search;Search engines;Data mining;Humans;Training data;Merging,document handling;Internet;merging;natural language processing;pattern clustering;query processing;regression analysis;search engines;support vector machines,clustering retrieval system;electronic documents;World Wide Web;digital libraries;Chinese information clustering techniques;salient phrase ranking problem;document querying;Web search engine;support vector regression;candidate clusters merging,,,,12,,2-May-09,,,IEEE,IEEE Conferences
Self-Tuned Descriptive Document Clustering Using a Predictive Network,利用Wikipedia知識進行文檔的概念層次聚類,A. J. Brockmeier; T. Mu; S. Ananiadou; J. Y. Goulermas,"School of Electrical Engineering, Electronics, & Computer Science, University of Liverpool, Liverpool, United Kingdom; School of Computer Science, University of Manchester, Manchester, United Kingdom; School of Computer Science, University of Manchester, Manchester, United Kingdom; School of Electrical Engineering, Electronics, & Computer Science, University of Liverpool, Liverpool, United Kingdom",IEEE Transactions on Knowledge and Data Engineering,11-Sep-18,2018,30,10,1929,1942,"Descriptive clustering consists of automatically organizing data instances into clusters and generating a descriptive summary for each cluster. The description should inform a user about the contents of each cluster without further examination of the specific instances, enabling a user to rapidly scan for relevant clusters. Selection of descriptions often relies on heuristic criteria. We model descriptive clustering as an auto-encoder network that predicts features from cluster assignments and predicts cluster assignments from a subset of features. The subset of features used for predicting a cluster serves as its description. For text documents, the occurrence or count of words, phrases, or other attributes provides a sparse feature representation with interpretable feature labels. In the proposed network, cluster predictions are made using logistic regression models, and feature predictions rely on logistic or multinomial regression models. Optimizing these models leads to a completely self-tuned descriptive clustering approach that automatically selects the number of clusters and the number of features for each cluster. We applied the methodology to a variety of short text documents and showed that the selected clustering, as evidenced by the selected feature subsets, are associated with a meaningful topical organization.",1558-2191,,10.1109/TKDE.2017.2781721,Medical Research Council Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301532,Descriptive clustering;feature selection;logistic regression;model selection;sparse models,Predictive models;Logistics;Feature extraction;Motion pictures;Clustering algorithms;Prediction algorithms;Analytical models,document handling;feature extraction;pattern clustering;regression analysis;text analysis,descriptive document;predictive network;data instances;descriptive summary;model descriptive clustering;auto-encoder network;sparse feature representation;interpretable feature labels;cluster predictions;logistic regression models;feature predictions;multinomial regression models;completely self-tuned descriptive clustering approach;short text documents;selected clustering;selected feature subsets,,,,83,CCBY,23-Feb-18,,,IEEE,IEEE Journals
Improved CURE algorithm and application of clustering for large-scale data,分層分佈的對等文檔聚類和聚類匯總,Shao Xiufeng; Cheng Wei,"Department of Soft and Information Management, BeiJing City University, China; Artificial Intelligence Institute, BeiJing City University, China",2011 IEEE International Symposium on IT in Medicine and Education,16-Jan-12,2011,1,,305,308,"Aiming at the classification problem of large-scale document information, a large-scale data clustering algorithm based on improved CURE algorithm is proposed. By clustering the data partition and the initial class of after partition, data tracking, the large-scale data hierarchical clustering and sample classification is achieved, that better solved the balance of clustering quality and clustering effectiveness. Taking the actual document processing of Large-scale network data, the experiment results show that the algorithm is efficient.",,978-1-61284-704-7,10.1109/ITiME.2011.6130839,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130839,Data mining;CURE algorithm;Clustering for large-scale data,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Databases;Data mining;Classification algorithms;Educational institutions,document handling;pattern classification;pattern clustering,CURE algorithm;large scale data clustering;classification problem;large scale document information;data tracking;clustering quality;clustering effectiveness,,2,,11,,16-Jan-12,,,IEEE,IEEE Conferences
Document Clustering Using Differential Evolution,基於模糊的Web文檔聚類算法,A. Abraham; S. Das; A. Konar,"IITA Professorship Program, School of Computer Science and Engineering, Chung Ang University (CAU), Seoul; NA; NA",2006 IEEE International Conference on Evolutionary Computation,11-Sep-06,2006,,,1784,1791,"This paper investigates a novel approach for partitional clustering of a large collection of text documents by using an improved version of the classical differential algorithm (DE). Fast and accurate clustering of documents plays an important role in the field of text mining and automatic information retrieval systems. The k-means has served as the most widely used partitional clustering algorithm for text documents. However, in most cases it provides only locally optimal solutions. In this work, the clustering problem has been formulated as an optimization task and is solved using a modified DE algorithm. To reduce the computational time, a hybrid k-means with DE method has also been proposed. The new algorithms were tested on a number of document datasets. Comparison with k-means, a state of the art PSO and one recently proposed real coded GA based text clustering methods reflects the superiority of the proposed techniques in terms of speed and quality of clustering.",1941-0026,0-7803-9487-9,10.1109/CEC.2006.1688523,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688523,,Clustering algorithms;Partitioning algorithms;Information retrieval;Tree graphs;Genetic algorithms;Particle swarm optimization;Computer science;Text mining;Testing;Clustering methods,data mining;document handling;evolutionary computation;information retrieval;pattern clustering,document clustering;differential evolution;partitional clustering;text documents;text mining;automatic information retrieval systems;hybrid k-means;document datasets;text clustering methods,,27,,28,,11-Sep-06,,,IEEE,IEEE Conferences
XCLSC: Structure and content-based clustering of XML documents,聚類模型概述及其在文檔聚類中的應用,K. Bessine; A. Nehar; H. Cherroun; A. Moussaoui,"Laboratoire d'Informatique et Math矇matiques Universit矇 Amar T矇lidji Laghouat, Alg矇rie; D矇partement d'Informatique et Math矇matiques Universit矇 Ziane Achour Djelfa, Alg矇rie; Laboratoire d'Informatique et Math矇matiques Universit矇 Amar T矇lidji Laghouat, Alg矇rie; Laboratoire d'Informatique et Math矇matiques Universit矇 Ferhat Abbas S矇tif, Alg矇rie",2015 12th International Symposium on Programming and Systems (ISPS),10-Sep-15,2015,,,1,7,"This paper proposes a novel Clustering approach for XML documents that combines both their content and structure information using tree structural-content summaries in order to reduce the size of the document. This reduction has twofold purpose. First, it reduces the size of the XML tree by eliminating redundant nodes. Second, it gathers similaire content. The clustering is performed according to a similarity measure that takes into account the structure and the content between levels. Several experiments are performed to explore the effectiveness of using tree structural summaries and constrained content in the clustering process. Empirical analysis reveals that the designed clustering approach using content within structure and tree structural summaries gives a better solution for XML clustering while improving runtime. It is very suitable when we deal with big data sets.",,978-1-4799-7699-7,10.1109/ISPS.2015.7244989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244989,XML documents;Clustering;Structural Summary;Level Structure-Content,XML;Clustering algorithms;Entropy;Data structures;Electronic mail;Big data;Information services,document handling;pattern clustering;tree data structures;trees (mathematics);XML,XCLSC;content-based clustering;XML documents;document content;structure information;tree structural-content summaries;document size reduction;XML tree;redundant node elimination;similarity measure,,1,,16,,10-Sep-15,,,IEEE,IEEE Conferences
"Revisiting K-Means and Topic Modeling, a Comparison Study to Cluster Arabic Documents",混合二等分K均值聚類算法,M. Alhawarat; M. Hegazi,"Department of Computer Science, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia; Department of Computer Science, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia",IEEE Access,19-Aug-18,2018,6,,42740,42749,"Clustering Arabic text documents is of high importance for many natural language technologies. This paper uses a combined method to cluster Arabic text documents. Mainly, we use generative models and clustering techniques. The study uses latent Dirichlet allocation and k-means clustering algorithm and applies them to a news data set used in previous similar studies. The aim of this paper is twofold: it first shows that normalizing the weights in the vector space, for the document-term matrix of the text documents, dramatically improves the quality of clusters and hence the accuracy of clustering when using k-means algorithm. The results are compared to a recent study on clustering Arabic text documents. Second, it shows that the combined method is superior in terms of clustering quality for Arabic text documents according to external measures, such as purity, F-measure, entropy, accuracy, and other measures. It is shown in this paper that the purity of the combined method is 0.933 compared to 0.82 for k-means algorithm, and these figures are higher in comparison to a recent similar study. This is also confirmed by the other used validation measures. The correctness of the combined method is then confirmed using different Arabic data sets.",2169-3536,,10.1109/ACCESS.2018.2852648,Prince Sattam bin Abdulaziz University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402221,Clustering text documents;K-means;Arabic language;topic modeling;latent Dirichlet allocation (LDA),Clustering algorithms;Task analysis;Partitioning algorithms;Data mining;Natural language processing;Resource management,learning (artificial intelligence);natural language processing;pattern clustering;text analysis,generative models;clustering techniques;document-term matrix;Arabic text document clustering;topic modeling;K-means clustering algorithm;natural language technologies;latent Dirichlet allocation;clustering quality,,7,,61,,3-Jul-18,,,IEEE,IEEE Journals
Co-Clustering WSDL Documents to Bootstrap Service Discovery,基於無標度網絡理論的生物醫學文檔富集本體豐富圖表示,T. Liang; L. Chen; H. Ying; J. Wu,"Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China",2014 IEEE 7th International Conference on Service-Oriented Computing and Applications,6-Dec-14,2014,,,215,222,"With the increasing popularity of web service, it is indispensable to efficiently locate the desired service. Utilizing WSDL documents to cluster web services into functionally similar service groups is becoming mainstream in recent years. However, most existing algorithms cluster WSDL documents solely and ignore the distribution of words rather than cluster them simultaneously. Different from the traditional clustering algorithms that are on one-way clustering, this paper proposes a novel approach named WCCluster to simultaneously cluster WSDL documents and the words extracted from them to improve the accuracy of clustering. WCCluster poses co-clustering as a bipartite graph partitioning problem, and uses a spectral graph algorithm in which proper singular vectors are utilized as a real relaxation to the NP-complete graph partitioning problem. To evaluate the proposed approach, we design comprehensive experiments based on a real-world data set, and the results demonstrate the effectiveness of WCCluster.",2163-2871,978-1-4799-6833-6,10.1109/SOCA.2014.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6978612,Web service;WSDL documents clustering;bipartite graph partitioning;co-clustering,Web services;Clustering algorithms;Vectors;Feature extraction;Bipartite graph;Search engines;Partitioning algorithms,computational complexity;document handling;graph theory;pattern clustering;Web services,WSDL documents coclustering;bootstrap service discovery;Web service;one-way clustering;WCCluster;bipartite graph partitioning problem;NP-complete graph partitioning problem;singular vectors,,6,,21,,6-Dec-14,,,IEEE,IEEE Conferences
Initialized and guided EM-clustering of sparse binary data with application to text based documents,基於關鍵字聚類的無監督文檔聚類,A. Kaban; M. Girolami,"Dept. of Comput. & Inf. Syst., Paisley Univ., UK; NA",Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,6-Aug-02,2000,2,,744,747 vol.2,"We investigate an alternative way of combining classification and clustering techniques for sparse binary data in order to reduce the amount of training samples required. Initializing EM from the available labels also reduces the algorithms' known dependency on the initialization, which is more evident in the case of sparse data. In addition, the two-valued Poisson class-model is proposed in this paper as a sparse variant of the usual binomial assumption. Our method can be seen as a fusion between generalized logistic regression and parametric mixture modeling. Comparative simulation results on subsets of the 20 Newsgroups' binary coded text corpora and binary handwritten digits data demonstrate the potential usefulness of the suggested method.",1051-4651,0-7695-0750-6,10.1109/ICPR.2000.906182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906182,,Clustering algorithms;Humans;Computational intelligence;Information systems;Labeling;Text categorization;Sufficient conditions;Frequency;Sparse matrices;Noise generators,pattern classification;pattern clustering;statistical analysis;optimisation;document image processing,initialized guided EM-clustering;sparse binary data;text based documents;expectation maximisation;two-valued Poisson class-model;generalized logistic regression;parametric mixture modeling;Newsgroup binary coded text corpora;binary handwritten digits data,,6,,9,,6-Aug-02,,,IEEE,IEEE Conferences
A Comparative Study on English-Chinese Bilingual News Document Clustering,一種高效的文檔簇集成譜方法,P. Han; G. Lu; D. Wang; Y. Liu,"Sch. of Inf. Manage., Nanjing Univ. Nanjing, Nanjing, China; Sch. of Inf. Manage., Nanjing Univ. Nanjing, Nanjing, China; Sch. of Inf. Manage., Nanjing Univ. Nanjing, Nanjing, China; Inst. of Command Autom., PLA Univ. of Technol. & Sci., Nanjing, China","2012 8th International Conference on Wireless Communications, Networking and Mobile Computing",14-Mar-13,2012,,,1,4,"Bilingual or multilingual document clustering is a valuable research. Based on monolingual algorithm, the paper makes a comparative study on monolingual-based clustering and bilingual-based clustering by using the corpus of English-Chinese bilingual news text. The experimental results show that mixed language-based method can make a better and more stable performance.",2161-9654,978-1-61284-683-5,10.1109/WiCOM.2012.6478271,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478271,,Clustering algorithms;Partitioning algorithms;Encoding;Educational institutions;Organizations;Information processing;Clustering methods,language translation;pattern clustering;text analysis,English-Chinese bilingual news document clustering;multilingual document clustering;monolingual-based clustering;mixed language-based method;bilingual news text corpus,,,,11,,14-Mar-13,,,IEEE,IEEE Conferences
Scalable construction of topic directory with nonparametric closed termset mining,基於擴展ACO的文檔集群,Hwanjo Yu; D. Searsmith; Xiaolei Li; Jiawei Han,"Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA",Fourth IEEE International Conference on Data Mining (ICDM'04),4-Apr-05,2004,,,563,566,"A topic directory, e.g., Yahoo directory, provides a view of a document set at different levels of abstraction and is ideal for the interactive exploration and visualization of the document set. We present a method that dynamically generates a topic directory from a document set using a frequent closed termset mining algorithm. Our method shows experimental results of equal quality to recent document clustering methods and has additional benefits such as automatic generation of topic labels and determination of a clustering parameter.",,0-7695-2142-8,10.1109/ICDM.2004.10056,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410361,topic directory;document clustering;hierarchical clustering,Clustering algorithms;Clustering methods;Itemsets;Data mining;Taxonomy;Permission;Computer science;Visualization;Tree graphs;Organizing,data mining;document handling;pattern clustering,topic directory;document clustering;hierarchical clustering;nonparametric closed termset mining;Yahoo directory;automatic generation,,4,,8,,4-Apr-05,,,IEEE,IEEE Conferences
Dynamic clustering analysis of documents based on cluster centroids,基於聚類質心的文檔動態聚類分析,Xiao-Shen Zheng; Pi-Lian He; Fu-Yong Yuan; Zhong Wang; Guang-Yuan Wu,"Dept. of Comput. Sci. & Technol., Tianjin Univ., China; Dept. of Comput. Sci. & Technol., Tianjin Univ., China; NA; NA; NA",Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),19-Feb-04,2003,1,,194,198 Vol.1,"This paper puts forward a method of documents dynamic clustering based on cluster centroids. First, the documents are modeled as elements in vector space and clustered based on some cluster centroids. Then the clustering results are dynamically adjusted based on the principle of group modification. Finally, the experiment results of documents clustering are contrasted, which show dynamic clustering can improve the performance of documents clustering to a certain extent.",,0-7803-7865-2,10.1109/ICMLC.2003.1264469,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264469,,Text analysis;Frequency;Databases;Vectors;Space technology;Computer science;Machine learning;Helium;Wide area networks;Books,pattern clustering;document handling;statistical analysis;data mining,dynamic clustering analysis;cluster centroids;vector space;group modification principle,,1,,5,,19-Feb-04,,,IEEE,IEEE Conferences
Clustering using Cuckoo search levy flight,使用Cuckoo搜索徵稅航班進行聚類,A. Palaiah; A. H. Prabhu; R. Agrawal; S. Natarajan,"Department of Information Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Information Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Information Science and Engineering, PES Institute of Technology, Bangalore, India; Department of Information Science and Engineering, PES Institute of Technology, Bangalore, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",3-Nov-16,2016,,,567,572,"Clustering of Web document has become a vital task, due to the tremendous amount of information that is available on web today. The task of finding suitable information with less time has become a big challenge in information retrieval. So, it's very much necessary to adopt a method that can be used organize the information well. This is possible only when good document groups are formed, which in turn can be achieved when effective and optimized cluster heads are identified. Our concern is to apply an algorithm for web document clustering. The algorithm proposed in this paper is, Cuckoo Search based on Levy Flight. Efficient cluster heads can be located using proposed Cuckoo Search algorithm. And Levy Flight helps us to speed up the local search which also ensures that it covers output domain efficiently. This algorithm is simple, efficient and it is easy to implement. A relative study of the proposed Cuckoo Search based on Levy Flight and K-means algorithm is carried out. The obtained result shows that good performance can be achieved when Cuckoo Search based on Levy Flight algorithm is used for clustering of web documents.",,978-1-5090-2029-4,10.1109/ICACCI.2016.7732106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732106,Clustering;Information retrieval;Cuckoo Search;Levy Flight;K-means,Clustering algorithms;Algorithm design and analysis;Optimization;Informatics;Web search;Classification algorithms,document handling;information retrieval;Internet;optimisation;pattern clustering;search problems,Web document clustering;information retrieval;optimized cluster heads;Cuckoo Search algorithm;local search;K-means algorithm;Levy Flight algorithm,,2,,20,,3-Nov-16,,,IEEE,IEEE Conferences
Finding main topics in blogosphere using document clustering based on topic model,使用基於主題模型的文檔聚類在Blogosphere中查找主要主題,W. Xuan; B. Liu; C. Sun; D. Zhang; X. Wang,"Intelligent Technology & Natural Language Processing Lab, School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; Intelligent Technology & Natural Language Processing Lab, School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; Intelligent Technology & Natural Language Processing Lab, School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; Intelligent Technology & Natural Language Processing Lab, School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; Intelligent Technology & Natural Language Processing Lab, School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1902,1908,"Along with the rapid growth of user generated content in blogosphere, it becomes more and more difficult for users to get the information they want. An effective way of organizing the information in blogosphere is becoming increasingly important under this circumstance. In this paper, a triple layer method is presented. The first layer is for document representation, which is based on topic model latent dirichlet allocation. The second one is for document clustering, which is on the basis of Markov cluster algorithm. The last layer is for topic words generation. Using this method, information in blogosphere, such as blog posts, can be organized in terms of topic. A topic is expressed by a list of topic words. Empirical study on real-world blog site CSDN shows that the proposed method is effective. Besides, this method can also provide a convenient way for users to access and explore information in blogosphere.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016947,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016947,Weblogs;Clustering description;Document representation;Dimension reduction;Topic model,Blogs;Clustering algorithms;Indexes;Machine learning;Cybernetics;Algorithm design and analysis;Internet,document handling;Internet;Markov processes;pattern clustering;Web sites,document clustering;topic model;blogosphere information;triple layer method;document representation;latent dirichlet allocation;Markov cluster algorithm,,3,,17,,12-Sep-11,,,IEEE,IEEE Conferences
A K-medoids based clustering scheme with an application to document clustering,基於K-medoids的聚類方案及其在文檔聚類中的應用,A. Onan,"Department of Software Engineering, Manisa Celal Bayar University, Manisa, Turkey",2017 International Conference on Computer Science and Engineering (UBMK),2-Nov-17,2017,,,354,359,"Clustering is an important unsupervised data analysis technique, which divides data objects into clusters based on similarity. Clustering has been studied and applied in many different fields, including pattern recognition, data mining, decision science and statistics. Clustering algorithms can be mainly classified as hierarchical and partitional clustering approaches. Partitioning around medoids (PAM) is a partitional clustering algorithms, which is less sensitive to outliers, but greatly affected by the poor initialization of medoids. In this paper, we augment the randomized seeding technique to overcome problem of poor initialization of medoids in PAM algorithm. The proposed approach (PAM++) is compared with other partitional clustering algorithms, such as K-means and K-means++ on text document clustering benchmarks and evaluated in terms of F-measure. The results for experiments indicate that the randomized seeding can improve the performance of PAM algorithm on text document clustering.",,978-1-5386-0930-9,10.1109/UBMK.2017.8093409,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093409,clustering;text mining;PAM;randomized seeding,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Classification algorithms;Clustering methods;Couplings;Benchmark testing,data analysis;data mining;pattern clustering;text analysis,PAM algorithm;text document clustering;K-medoids;data objects;decision science;hierarchical clustering approaches;partitional clustering algorithms;randomized seeding technique;PAM++;unsupervised data analysis technique;randomized seeding,,4,,22,,2-Nov-17,,,IEEE,IEEE Conferences
Application of the SpecHybrid Algorithm to text document clustering problem,SpecHybrid算法在文本文檔聚類中的應用,Z. Uykan; M. C. Ganiz,"Electronics and Communications Engineering Dept, Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey; Computer Engineering Dept., Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey",2011 International Symposium on Innovations in Intelligent Systems and Applications,11-Jul-11,2011,,,118,122,"In this paper, we present a relaxed version of the SpecHybrid Algorithm originally proposed for wireless cellular systems, and apply it to text document clustering problem. We conduct several experiments on two different datasets; a widely used benchmark dataset in English, and a Turkish textual dataset commonly used in text classification. Our results show that the proposed algorithm gives superior performance in text document clustering as compared to the standard k-means algorithm for any number of clusters while giving a comparable or better performance as compared to the standard EM algorithm for relatively large number of clusters depending on the similarity matrices used.",,978-1-61284-922-5,10.1109/INISTA.2011.5946085,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946085,textual data mining;document clustering;Turkish document clustering;max cut;spectral clustering,Clustering algorithms;Data mining;Entropy;Classification algorithms;Partitioning algorithms;Euclidean distance,expectation-maximisation algorithm;pattern classification;pattern clustering;text analysis,SpecHybrid algorithm;text document clustering problem;Turkish textual dataset;text classification;standard k-means algorithm;standard EM algorithm;similarity matrices,,3,,18,,11-Jul-11,,,IEEE,IEEE Conferences
A new document clustering algorithm based on association rule,基於關聯規則的文檔聚類新算法,Jiang-Chun Song; Jun-Yi Shen; Qing-Bao Song,"Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China",Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),24-Jan-05,2004,2,,1310,1313 vol.2,"Owing to the widely application in the fields of information retrieval, document analysis and information extraction, document cluster analysis has been concerned broadly, and gotten a great deal of research issues simultaneously. It is one of essential theme in document mining. Based on vector space model (VSM), This work presents a new document cluster algorithm based on association rule, and discusses the efficiency, the scalability and time complexity of the algorithm.",,0-7803-8403-2,10.1109/ICMLC.2004.1382395,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382395,,Clustering algorithms;Association rules;Data mining;Information retrieval;Information analysis;Machine learning;Databases;Machine learning algorithms;Computer science;Application software,information retrieval;data mining;document handling,document clustering algorithm;association rule;information retrieval;document analysis;information extraction;document mining;vector space model,,1,,7,,24-Jan-05,,,IEEE,IEEE Conferences
Document image dataset indexing and compression using connected components clustering,使用連接的組件聚類對文檔圖像數據集進行索引和壓縮,H. Chatbri; K. Kameyama,"Department of Computer Science, Graduate School of Systems and Information Engineering, University of Tsukuba, Japan; Faculty of Engineering, Information and Systems, University of Tsukuba, Japan",2015 14th IAPR International Conference on Machine Vision Applications (MVA),13-Jul-15,2015,,,267,270,"We present a method for document image dataset indexing and compression by clustering of connected components. Our method extracts connected components from each dataset image and performs component clustering to make a hash table that is a compressed indexing of the dataset. Clustering is based on component similarity which is estimated by comparing shape features extracted from the components. Then, the hash table is saved in a text file, and the text file is further compressed using any available compression methodology. Component encoding in the hash table is storage efficient and done using components' contour points and a reduced number of interior points that are sufficient for component reconstruction. We evaluate our method's performances in indexing and compression using four document image datasets. Experimental results show that indexing significantly improves efficiency when used in document image retrieval. In addition, comparative evaluation with two compression standards, namely the ZIP and XZ formats, show competitive performances. Our compression rates are below 20% and the compression errors are very low being at the order of 10-6% per image.",,978-4-9011-2214-6,10.1109/MVA.2015.7153182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153182,,Image coding;Indexing;Image reconstruction;Feature extraction;Encoding;Clustering algorithms;Redundancy,data compression;document image processing;feature extraction;image coding;image reconstruction,document image dataset indexing;document image dataset compression;connected components clustering;hash table;compressed indexing;component similarity;feature extraction;text file;compression methodology;component encoding;component contour points;component reconstruction;document image retrieval;ZIP formats;XZ formats;compression errors,,2,,12,,13-Jul-15,,,IEEE,IEEE Conferences
Semantic Document Clustering Using a Similarity Graph,使用相似度圖的語義文檔聚類,L. Stanchev,"Comput. Sci. Dept., California Polytech. State Univ., San Luis Obispo, CA, USA",2016 IEEE Tenth International Conference on Semantic Computing (ICSC),24-Mar-16,2016,,,1,8,"Document clustering addresses the problem of identifying groups of similar documents without human supervision. Unlike most existing solutions that perform document clustering based on keywords matching, we propose an algorithm that considers the meaning of the terms in the documents. For example, a document that contains the words ""dog"" and ""cat"" multiple times may be placed in the same category as a document that contains the word ""pet"" even if the two documents share only noise words in common. Our semantic clustering algorithm is based on a similarity graph that stores the degree of semantic relationship between terms (extracted from WordNet), where a term can be a word or a phrase. We experimentally validate our algorithm on the Reuters-21578 benchmark, which contains 11,362 newswire stories that are grouped in 82 categories using human judgment. We apply the k-means clustering algorithm to group the documents using a similarity metric that is based on keywords matching and one that uses the similarity graph. We show that the second approach produces higher precision and recall, which means that this approach matches closer the results of the human study.",,978-1-5090-0662-5,10.1109/ICSC.2016.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439298,semantic document clustering;WordNet,Semantics;Clustering algorithms;Measurement;Natural languages;Probabilistic logic;Benchmark testing;Footwear,document handling;graph theory;pattern clustering;pattern matching,semantic document clustering;similarity graph;keywords matching;noise words;semantic clustering,,6,,41,,24-Mar-16,,,IEEE,IEEE Conferences
A Self-Adaptive Method for Extraction of Document-Specific Alphabets,提取文檔專用字母的自適應方法,S. Pletschacher,"Pattern Recognition & Image Anal. (PRImA) Res. Lab., Univ. of Salford, Salford, UK",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,656,660,"Recognition and encoding of digitized historical documents is still a challenging and difficult task. A major problem is the occurrence of unknown glyphs and symbols which might not even exist in modern alphabets. Current pre-trained OCR-methods hardly deliver usable results for such documents. This paper describes an alternative approach and framework for handling printed historical documents without restrictions on the contained alphabets or fonts. The basic idea is to derive all information required for encoding directly from the document itself. This is achieved by extracting a document-specific prototype alphabet of locatable glyphs. Core of the system is a customized clustering method which adapts automatically to new documents by ascertaining appropriate threshold parameters based on the special characteristics of glyphs. This way, the system is able to run without manual interventions and can be integrated into automated mass digitization workflows.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.253,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277564,Clustering;OCR;Encoding;Mass Digitization,Prototypes;Optical character recognition software;Pattern recognition;Data mining;Image analysis;Image recognition;Document handling;Encoding;Dictionaries;Character recognition,document image processing;encoding;feature extraction;history;optical character recognition;pattern clustering,self-adaptive method;document-specific prototype alphabet extraction;digitized historical document recognition;digitized historical document encoding;pre-trained OCR-method;printed historical document handling;ancient glyph;customized clustering method;threshold parameter;automated mass digitization workflow;feature extraction,,1,,7,,2-Oct-09,,,IEEE,IEEE Conferences
Towards Compromising Structural and Bag of Words Approaches for Clustering Heterogeneous XML Documents,致力於破壞異構XML文檔的結構化和單詞袋方法,N. Zerida; J. Yao,"GREYC Lab., Univ. of Caen - Ensicaen, Caen; GREYC Lab., Univ. of Caen - Ensicaen, Caen",2008 The Second International Conference on Advanced Engineering Computing and Applications in Sciences,10-Oct-08,2008,,,69,72,"The presence of a large quantity of unlabeled documents on the web increases, and organizing related heterogeneous XML documents by using their structural and conceptual properties into clusters become a great need. In this paper, we consider the pre-processing step as a key step to improve clustering quality, we propose a new pre-processing method which is based on combining Hapax words and path-based descriptors. A constrained agglomerative clustering method is used, and a comparison between different document representations is performed. The effectiveness of the method is evaluated on the INEX corpus, and clustering quality is measured by using micro and macro average purity measures.",,978-0-7695-3369-8,10.1109/ADVCOMP.2008.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4640995,,XML;USA Councils;Data models;Computational modeling;Data structures;Data mining;Clustering methods,pattern clustering;text analysis;XML,structural approach;bag of words approach;heterogeneous XML documents;documents clustering;structural properties;conceptual properties;clustering quality;Hapax words;path based descriptors;constrained agglomerative clustering;micro average purity measure;macro average purity measure,,,,9,,10-Oct-08,,,IEEE,IEEE Conferences
A Model-Based Ruling Line Detection Algorithm for Noisy Handwritten Documents,嘈雜手寫文檔的基於模型的裁線檢測算法,J. Chen; D. Lopresti,"Dept. of Comput. Sci. & Eng., Lehigh Univ., Bethlehem, PA, USA; Dept. of Comput. Sci. & Eng., Lehigh Univ., Bethlehem, PA, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,404,408,"Ruling lines are commonly used to help people write neatly on paper. In document image analysis, however, they create challenges for handwriting recognition and writer identification. In this paper, we model ruling line detection as a multi-line linear regression problem and then derive a globally optimal solution giving the Least Square Error. We demonstrate the efficacy of the technique on both synthetic and real datasets. A comparative study shows that our algorithm outperforms a previously published method on the public Germana dataset.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065344,handwritten documents;ruling line detection,Hidden Markov models;Detection algorithms;Measurement;Linear regression;Image segmentation;Transforms;Clustering algorithms,document image processing;edge detection;handwriting recognition;least squares approximations;regression analysis,model-based ruling line detection algorithm;noisy handwritten documents;document image analysis;handwriting recognition;writer identification;multiline linear regression problem;least square error;public Germana dataset,,5,,9,,3-Nov-11,,,IEEE,IEEE Conferences
Web document clustering approach using wordnet lexical categories and fuzzy clustering,使用Wordnet詞彙類別和模糊聚類的Web文檔聚類方法,T. F. Gharib; M. M. Fouad; M. M. Aref,"Faculty of Computer and Information Science, Ain Shams University, Egypt; Computer Science Department, Akhbar El-Yom Academy, Egypt; Faculty of Computer and Information Science, Ain Shams University, Egypt",2008 11th International Conference on Computer and Information Technology,21-Mar-09,2008,,,48,55,"Web mining is defined as applying data mining techniques to the content, structure, and usage of Web resources. The three areas of Web mining are commonly distinguished: content mining, structure mining, and usage mining. In all these areas, a wide range of general data mining techniques, in particular association rule discovery, clustering, classification, and sequence mining, are employed and developed further to reflect the specific structures of Web resources and the specific questions posed in Web mining. In this paper, we introduced a Web document clustering approach that uses WordNet lexical categories and fuzzy c-means algorithm to improve the performance of clustering problem for Web document. Experiments show that fuzzy c-means algorithm achieves great performance optimization with comparison with the recent algorithms for document clustering.",,978-1-4244-2135-0,10.1109/ICCITECHN.2008.4803109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4803109,,Data mining;Web mining;Clustering algorithms;Text mining;Clustering methods;Artificial intelligence;Information science;Computer science;Association rules;Optimization,content management;data mining;document handling;fuzzy set theory;Internet;pattern clustering,Web document clustering;WordNet lexical category;fuzzy clustering;Web mining;data mining;Web resource;content mining;structure mining;usage mining;association rule discovery;sequence mining;fuzzy c-means algorithm,,1,,28,,21-Mar-09,,,IEEE,IEEE Conferences
Fuzzy cluster descriptors improve flexible organization of documents,模糊聚類描述符可改善文檔的靈活組織,T. M. Nogueira; S. O. Rezende; H. A. Camargo,"Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Department of Computer Science, Federal University of S瓊o Carlos - Brazil",2012 12th International Conference on Intelligent Systems Design and Applications (ISDA),24-Jan-13,2012,,,616,621,"System flexibility means the ability of a system to manage imprecise and/or uncertain information. There are two ways to address the Information Retrieval Systems (IRS) flexibility: through methods that improve the query formulation and through methods that improve the document organization. Since the query formulation has obtained more attention in retrieval process, we aim to investigate the flexibility in document organization. When a document organization is carried out using fuzzy clustering, the documents can belong to more than one cluster simultaneously with different membership degrees, allowing the management of imprecise and/or uncertain information in the collection organization. Clusters represent topics and are identified by one or more descriptors. In this work we use an unsupervised method to extract cluster descriptors for a specific database and investigate whether the quality of the fuzzy cluster descriptors improves the flexible organization of documents.",2164-7151,978-1-4673-5119-5,10.1109/ISDA.2012.6416608,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416608,fuzzy clustering;flexible organization;documents;information retrieval,Organizations;Keyboards;Information retrieval;Clustering algorithms;Electronic publishing;Consumer electronics;Humans,document handling;fuzzy set theory;pattern clustering;query formulation;unsupervised learning,fuzzy cluster descriptors;information retrieval systems flexibiltiy;IRS flexibility;document organization;fuzzy clustering;membership degrees;unsupervised method;flexible organization;system flexibility;query formulation,,3,,15,,24-Jan-13,,,IEEE,IEEE Conferences
Multi-Document summarization based on improved features and clustering,基於改進功能和聚類的多文檔摘要,Y. Xiong; H. Liu; L. Li,"Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China",Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010),30-Sep-10,2010,,,1,5,"Multi-Document summarization is an emerging technique for understanding the main purpose of many documents about the same topic. This paper proposes a new feature selection method to improve the summarization result. When calculating similarity, we use a modified TFIDF formula which achieves a better result. We adopt two ways for exactly extracting keywords. Experimental results demonstrate that our improved method performs better than the traditional one.",,978-1-4244-6899-7,10.1109/NLPKE.2010.5587834,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587834,Multi-document summarization;feature selection;cluster;sentence selection,Telecommunications;Context,document handling;information retrieval;pattern clustering,multidocument summarization;TFIDF formula;keyword extraction;feature selection method;sentence selection,,2,,14,,30-Sep-10,,,IEEE,IEEE Conferences
Learning Domain-Specific Feature Descriptors for Document Images,學習文檔圖像的特定領域特徵描述符,K. Ramakrishnan; E. Bart,"Dept. of Comput. Sci. & Eng., Univ. of Minnesota, Minneapolis, MN, USA; Intell. Syst. Lab., Palo Alto Res. Center, Palo Alto, CA, USA",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,415,418,"Many machine learning algorithms rely on feature descriptors to access information about image appearance. Using an appropriate descriptor is therefore crucial for the algorithm to succeed. Although domain- and task-specific feature descriptors may result in excellent performance, they currently have to be hand-crafted, a difficult and time-consuming process. In contrast, general-purpose descriptors (such as SIFT) are easy to apply and have proved successful for a variety of tasks, including classification, segmentation, and clustering. Unfortunately, most general-purpose feature descriptors are targeted at natural images and may perform poorly in document analysis tasks. In this paper, we propose a method for automatically learning feature descriptors tuned to a given image domain. The method works by first extracting the independent components of the images, and then building a descriptor by pooling these components over multiple overlapping regions. We test the proposed method on several document analysis tasks and several datasets, and show that it outperforms existing general-purpose feature descriptors.",,978-0-7695-4661-2,10.1109/DAS.2012.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195405,Feature descriptors;feature learning;classification,Dictionaries;Image edge detection;Feature extraction;Text analysis;Visualization;Optical character recognition software;Detectors,data analysis;document image processing;feature extraction;image classification;image segmentation;learning (artificial intelligence);pattern clustering,domain-specific feature descriptor;document image;machine learning algorithm;image appearance;task-specific feature descriptor;general-purpose descriptor;SIFT descriptor;scale invariant feature transform;classification task;segmentation task;clustering task;document analysis task,,,,22,,7-May-12,,,IEEE,IEEE Conferences
Combination of Latent Dirichlet Allocation (LDA) and Term Frequency-Inverse Cluster Frequency (TFxICF) in Indonesian text clustering with labeling,帶有標籤的印度尼西亞文本聚類中潛在狄利克雷分配（LDA）和詞頻逆聚類頻率（TFxICF）的組合,L. H. Suadaa; A. Purwarianti,"School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia",2016 4th International Conference on Information and Communication Technology (ICoICT),22-Sep-16,2016,,,1,6,"Due to the limited labeled data, clustering is a solution for classifying documents that do not have prior knowledge. The combination of Latent Dirichlet Allocation (LDA) in grouping documents by topic and Term Frequency-Inverse Cluster Frequency (TFxICF) in the labeling was proposed to resolve the problem of classification using clustering completed with a description of the cluster results. Indonesian text preprocessing has been done by the extraction of abbreviations and acronyms, tokenization, stemming and stopwords elimination. Experiments were conducted using Indonesian digital library documents, 113 documents from digital library of STIS and 60 documents from digital library of ITB, to examine the effects of text preprocessing, to compare the cluster results of LDA with other clustering algorithms and to compare the use of word and phrase tokens in the clustering and labeling. The cluster quality was measured by using precision, recall, and F-measure and the label quality was determined by similarity with the keywords that most frequently appear in the clusters. Based on the experimental results, preprocessing techniques can improve the cluster quality. LDA algorithm produces documents cluster by topic with cluster quality better than K-Means and Lingo. Word based LDA generates cluster with better quality than phrase based LDA. Moreover, the labeling by using word based TFxICF is more descriptive than phrase based TFxICF. Therefore, the use of word based LDA for clustering and phrase based TFxICF for labeling was proposed.",,978-1-4673-9879-4,10.1109/ICoICT.2016.7571885,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571885,text clustering;cluster labeling;latent dirichlet allocation;tfxidf,Terminology;Labeling;Information systems;Libraries;Clustering algorithms;Resource management;Radio spectrum management,pattern clustering;text analysis,latent dirichlet allocation;term frequency-inverse cluster frequency;Indonesian text clustering;labeling;limited labeled data;grouping documents;Indonesian text preprocessing;acronyms;tokenization;stemming;stopwords elimination;Indonesian digital library documents;STIS;ITB;clustering algorithms;cluster quality;F-measure;label quality;LDA algorithm;k-means;Lingo;word based LDA;phrase based LDA;word based TFxICF,,1,,18,,22-Sep-16,,,IEEE,IEEE Conferences
A Bayesian neural network model for dynamic web document clustering,動態Web文檔聚類的貝葉斯神經網絡模型。,Jun-Hui Her; Sung-Hae Jun; Jun-Heyog Choi; Jung-Hyun Lee,"Dept. of Comput. Sci. & Eng., Inha Univ., Inchon, South Korea; NA; NA; NA",Proceedings of IEEE. IEEE Region 10 Conference. TENCON 99. 'Multimedia Technology for Asia-Pacific Information Infrastructure' (Cat. No.99CH37030),6-Aug-02,1999,2,,1415,1418 vol.2,"There has been lots of research to improve the precision of IR system. These research have been studied on the document ranking, user profiles, relevance feedback and the information processing that includes document classification, clustering, routing and filtering. This paper proposes and incarnates method of neural approach about the information processing which makes users can search documents effectively and of the document clustering. In this paper the system calculates entropy between the query, the profile and the each of the web documents each other; and clusters documents using the calculated entropy as the value of the clustering variable through SOM. As the Bayesian Neural Network model has high classification accuracy with a rapid learning speed and clustering, it is possible that dynamic document clustering as it was combined with Bayesian probability model used in real-time document classification. We used KTSET which is a test collection to evaluate Korean IR system for the experiment.",,0-7803-5739-6,10.1109/TENCON.1999.818696,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818696,,Bayesian methods;Neural networks;Clustering algorithms;Entropy;Information processing;Clustering methods;Unsupervised learning;Computer science;Statistics;Software engineering,Bayes methods;relevance feedback;self-organising feature maps,Bayesian neural network model;dynamic web document clustering;document ranking;neural approach;classification accuracy;dynamic document clustering;real-time document classification,,,,10,,6-Aug-02,,,IEEE,IEEE Conferences
An Improved K-means Algorithm for Document Clustering,一種改進的K-means文檔聚類算法,G. Wu; H. Lin; E. Fu; L. Wang,"Sch. of Comput. Sci. & Technol., Hang Zhou Dian Zi Univ., Hangzhou, China; Sch. of Comput. Sci. & Technol., Hang Zhou Dian Zi Univ., Hangzhou, China; Sch. of Comput. Sci. & Technol., Hang Zhou Dian Zi Univ., Hangzhou, China; Sch. of Comput. Sci. & Technol., Hang Zhou Dian Zi Univ., Hangzhou, China",2015 International Conference on Computer Science and Mechanical Automation (CSMA),7-Jan-16,2015,,,65,69,"K-Means algorithm has a major shortcoming of high dimensional and sparse data. So the traditional measurement of the distance can't deal with the data effectively. Motivated by this, this paper proposed a K-Means algorithm based on Sim Hash. After preprocessing of the text, Sim Hash is used to calculate the feature vectors extracted, and then the fingerprint of each text is obtained. Sim Hash not only reduces the dimension of the text, but also directly calculates the Hamming distance between the fingerprints as the vector distance. According to the Hamming distance, it can judge which clustering the data is belongs to. Experimental result shows that the algorithm guarantees the quality of the clustering, and greatly reduces the speed of K-means clustering algorithm.",,978-1-4673-9166-5,10.1109/CSMA.2015.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371624,K-Means;SimHash;Text clustering,Clustering algorithms;Algorithm design and analysis;Fingerprint recognition;Hamming distance;Classification algorithms;Computer science;Feature extraction,pattern clustering;text analysis;vectors,improved k-means clustering algorithm;document clustering;SimHash;text preprocessing;feature vectors;Hamming distance,,6,,10,,7-Jan-16,,,IEEE,IEEE Conferences
Document Clustering in Correlation Similarity Measure Space,相關相似性度量空間中的文檔聚類,T. Zhang; Y. Y. Tang; B. Fang; Y. Xiang,"Chongqing University, Chongqing; Chongqing University, Chongqing; Chongqing University, Chongqing; Deakin University, Geelong",IEEE Transactions on Knowledge and Data Engineering,20-Apr-12,2012,24,6,1002,1013,"This paper presents a new spectral clustering method called correlation preserving indexing (CPI), which is performed in the correlation similarity measure space. In this framework, the documents are projected into a low-dimensional semantic space in which the correlations between the documents in the local patches are maximized while the correlations between the documents outside these patches are minimized simultaneously. Since the intrinsic geometrical structure of the document space is often embedded in the similarities between the documents, correlation as a similarity measure is more suitable for detecting the intrinsic geometrical structure of the document space than euclidean distance. Consequently, the proposed CPI method can effectively discover the intrinsic structures embedded in high-dimensional document space. The effectiveness of the new method is demonstrated by extensive experiments conducted on various data sets and by comparison with existing document clustering methods.",1558-2191,,10.1109/TKDE.2011.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710934,Document clustering;correlation measure;correlation latent semantic indexing;dimensionality reduction.,Correlation;Semantics;Euclidean distance;Clustering algorithms;Nearest neighbor searches;Indexing,correlation methods;document handling;learning (artificial intelligence);pattern clustering,document clustering;correlation preserving indexing;correlation similarity measure space;intrinsic geometrical structure;document space;euclidean distance;intrinsic structures,,19,,31,,10-Feb-11,,,IEEE,IEEE Journals
Selection of Cluster Hierarchy Depth in Hierarchical Clustering Using K-Means Algorithm,K-Means算法在層次聚類中選擇聚類層次深度,S. Lee; W. Lee; S. Chung; D. An; I. Bok; H. Ryu,"Chonbuk Nat. Univ., Chonju; Chonbuk Nat. Univ., Chonju; Chonbuk Nat. Univ., Chonju; Chonbuk Nat. Univ., Chonju; NA; NA",2007 International Symposium on Information Technology Convergence (ISITC 2007),26-Dec-07,2007,,,27,31,"Many papers have shown that the hierarchical clustering method takes good-performance, but is limited because of its quadratic time complexity. In contrast, with a large number of variables, K-means has a time complexity that is linear in the number of documents, but is thought to produce inferior clusters. Think of the factor of simplify, high-quality and high-efficiency, we combine the two approaches providing a new system named CONDOR system with hierarchical structure based on document clustering using K-means algorithm. Evaluated the performance on different hierarchy depth and initial uncertain centroid number based on variational relative document amount correspond to given queries. Comparing with regular method that the initial centroids have been established in advance, our method performance has been improved a lot.",,0-7695-3045-1,10.1109/ISITC.2007.5,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410600,,Clustering algorithms;Clustering methods;Partitioning algorithms;Information technology;Merging;Metasearch;Natural languages;Speech;Information analysis;Information retrieval,computational complexity;document handling;pattern clustering,cluster hierarchy depth;hierarchical clustering;K-means algorithm;time complexity;CONDOR system;document clustering,,,,12,,26-Dec-07,,,IEEE,IEEE Conferences
A clustering approach for XML linked documents,XML鏈接文檔的聚類方法,B. Catania; A. Maddalena,"Univ. of Genova, Italy; Univ. of Genova, Italy",Proceedings. 13th International Workshop on Database and Expert Systems Applications,10-Dec-02,2002,,,121,125,"Clustering algorithms for hypertext documents consider not only the document content but also the links existing between them. All the similarity functions proposed in the literature assume that just one type of link exists between documents, with a unique semantic meaning. With the rapid diffusion of XML documents, a specific language, called XLink, has been proposed to specify inside XML documents different types of links. Each type of link forces a different degree of similarity between the documents on which it is defined, thus we claim it must influence in a different way the computation of distance values. In this paper, after presenting a graph-based formalization of the hypertexts we consider, we introduce a distance function, based on both the number and the type of the links connecting documents. Some preliminary experimental results on clustering algorithms based on the proposed function conclude the paper.",1529-4188,0-7695-1668-8,10.1109/DEXA.2002.1045887,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1045887,,XML;Clustering algorithms;Joining processes;Scattering;Unsupervised learning;Merging;Visualization;Conferences;Databases;Expert systems,hypermedia markup languages;pattern clustering;graph theory;information resources,clustering approach;XML linked documents;hypertext documents;XLink;document similarity;distance values;WWW;graph-based formalization;hypertexts;distance function,,2,,7,,10-Dec-02,,,IEEE,IEEE Conferences
An improved K-means algorithm combined with Particle Swarm Optimization approach for efficient web document clustering,一種改進的K-means算法結合粒子群優化方法進行有效的Web文檔聚類,P. Jaganathan; S. Jaiganesh,"Department of Computer Applications, PSNA College of Engineering and Technology, Dindigul, India; Department of Computer Applications, PSNA College Engineering and Technology, Dindigul, India","2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)",2-Jun-14,2013,,,772,776,"Searching and discovering the relevant information on the web have always been challenging task. It is very hard to wade through the large number of returned documents in a response to a user query. This leads to the need to organize a large set of documents into categories through clustering. There is a need of efficient clustering algorithms for organizing documents. Clustering on large dataset can be effectively done using partitional clustering algorithms. The K-means algorithm is the appropriate partitional clustering approach for handling large dataset because of its efficiency with respect to execution time. But this algorithm is highly susceptible to the selection of initial positions of cluster centers. This paper introduces a new hybrid method using Particle Swarm Optimization (PSO) combined with an improved K-means algorithm for document clustering. We have tested K-means, PSO, our proposed PSOK, KPSO and KPSOK algorithms on various text document collections. The document range varies from 204 to 878 in the dataset and the terms ranges from 5804 to 7454. There is clear evidence from our results that the proposed method achieves better clustering than other methods taken for study.",,978-1-4673-6126-2,10.1109/ICGCE.2013.6823538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823538,PSO;Cluster Centroid;Vector Space Model;Euclidian distance;cosine correlation,Clustering algorithms;Vectors;Algorithm design and analysis;Particle swarm optimization;Partitioning algorithms;Mathematical model;Equations,document handling;Internet;particle swarm optimisation;pattern clustering,text document collections;KPSOK algorithms;PSO;partitional clustering approach;Web document clustering;particle swarm optimization approach;improved K-means algorithm,,12,,23,,2-Jun-14,,,IEEE,IEEE Conferences
Efficient incremental phrase-based document clustering,高效的基於短語的增量文檔聚類,A. M. Bakr; N. A. Yousri; M. A. Ismail,"Computer and Systems Engineering, University of Alexandria, Egypt; Computer and Systems Engineering, University of Alexandria, Egypt; Computer and Systems Engineering, University of Alexandria, Egypt",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,517,520,"Document clustering has become inevitable for applications that aim to extract information from huge corpuses. Such applications face two main challenges; one is the efficient representation of the documents, along with using an efficient similarity measure, and the second is dealing with the dynamic nature of the corpus. In this paper, an efficient document clustering model is introduced for incrementally storing and updating clusters of a dataset. A new phrase-based similarity method is developed along with the model to calculate the similarity between documents and clusters. Experimental results show that the new clustering model can achieve more accurate results than the traditional algorithms.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460185,,Clustering algorithms;Indexes;Vectors;Computational modeling;Accuracy;Equations,information retrieval;pattern clustering;text analysis,incremental phrase-based document clustering;information extraction;document representation;similarity measure;corpus;dataset clustering;phrase-based similarity method,,,,7,,14-Feb-13,,,IEEE,IEEE Conferences
Semantic Document Clustering Using Information from WordNet and DBPedia,使用WordNet和DBPedia的信息進行語義文檔聚類,L. Stanchev,"Comput. Sci. Dept., California Polytech. State Univ., San Luis, CA, USA",2018 IEEE 12th International Conference on Semantic Computing (ICSC),12-Apr-18,2018,,,100,107,"Semantic document clustering is a type of unsupervised learning in which documents are grouped together based on their meaning. Unlike traditional approaches that cluster documents based on common keywords, this technique can group documents that share no words in common as long as they are on the same subject. We compute the similarity between two documents as a function of the semantic similarity between the words and phrases in the documents. We model information from WordNet and DBPedia as a probabilistic graph that can be used to compute the similarity between two terms. We experimentally validate our algorithm on the Reuters-21578 benchmark, which contains 11,362 newswire stories that are grouped in 82 categories using human judgment. We apply the k-means clustering algorithm to group the documents using a similarity metric that is based on keyword matching and one that uses the probabilistic graph. We show that the second approach produces higher precision and recall, which corresponds to better alignment with the classification that was done by human experts.",,978-1-5386-4408-9,10.1109/ICSC.2018.00023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334446,semantic document clustering;WordNet;Wikipedia,Semantics;Probabilistic logic;Clustering algorithms;Internet;Cats;Speech;Encyclopedias,document handling;graph theory;pattern classification;pattern clustering;pattern matching;probability;unsupervised learning,semantic document clustering;semantic similarity;WordNet;DBPedia;unsupervised learning;k-means clustering algorithm;keyword matching;probabilistic graph,,,,42,,12-Apr-18,,,IEEE,IEEE Conferences
Spectral approach to find number of clusters of short-text documents,頻譜方法來查找短文本文檔簇的數量,A. Goyal; M. K. Jadon; A. K. Pujari,"The LNM Institute of Information Technology, Jaipur, India; The LNM Institute of Information Technology, Jaipur, India; School of Computer and Inf Sciences, Univ of Hyderabad, India","2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)",20-Mar-14,2013,,,1,4,We propose a technique of determining the number of clusters of a corpus of short-text documents. A spectral algorithm suitable for short-texts is used to generate an ensemble. A Markov chain induced by the co-association matrix is studied to observe nearly uncoupling phenomenon over iterations. A large spectral gap and number of eigenvectors close to 1 indicate the number of clusters. We demonstrate by experimenting on several datasets.,,978-1-4799-1588-0,10.1109/NCVPRIPG.2013.6776152,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776152,number of clusters;short-texts;term-weighting;uncoupling;spectral method,Eigenvalues and eigenfunctions;Clustering algorithms;Feature extraction;Markov processes;Data mining;Visualization;Electronic mail,eigenvalues and eigenfunctions;learning (artificial intelligence);Markov processes;matrix algebra;pattern clustering;text analysis,spectral approach;short-text documents;cluster number determination;ensemble learning;Markov chain;coassociation matrix;spectral gap;eigenvectors,,1,,17,,20-Mar-14,,,IEEE,IEEE Conferences
Hybrid neural network model for web document clustering,Web文檔聚類的混合神經網絡模型,M. Hemalatha; D. Sathya srinivas,"Dept of Computer Science, Karpagam University, Coimbatore- 641021, India; Dept of Comput. Sci., Karpagam Univ., Coimbatore, India",2009 Second International Conference on the Applications of Digital Information and Web Technologies,2-Oct-09,2009,,,531,538,"The popularity of the Internet has caused a massive increase in the amount of Web pages. The information explosion has led to a growing challenge for information retrieval systems. Document clustering becomes an important process for helping the information retrieval systems organize this vast amount of data. It is believed that grouping similar documents together into clusters will help the users find relevant information quicker, and will allow them to focus their search in the appropriate direction. Feature selection is an important task in data analysis. It is useful to limit redundancy of features, promote comprehensibility, and find clusters (or structures) hidden in high dimensional data. This paper addresses the problems of document mining related with Web page clustering and classification using the principle component analysis for feature vector selection. Singular value decomposition is used to find the similarity measure and multilayer neural network used to improve the performance of the clustering algorithm. We illustrate and discuss the system performance by experimental evaluation results.",,978-1-4244-4456-4,10.1109/ICADIWT.2009.5273918,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273918,Singular Value Decomposition;Principle component Analysis;Web document Clustering;Multilayer Neural Network,Neural networks;Web pages;Information retrieval;Multi-layer neural network;Internet;Explosions;Data analysis;Singular value decomposition;Clustering algorithms;System performance,data analysis;data mining;document handling;feature extraction;information retrieval;information retrieval systems;Internet;learning (artificial intelligence);multilayer perceptrons;pattern classification;pattern clustering;principal component analysis;singular value decomposition,Web document clustering;Internet;information retrieval system;data analysis;information search;document mining;Web page classification;principle component analysis;feature vector selection;singular value decomposition;similarity measure;multilayer neural network;machine learning,,2,,15,,2-Oct-09,,,IEEE,IEEE Conferences
A feature selection algorithm for document clustering based on word co-occurrence frequency,基於詞共現頻率的文檔聚類特徵選擇算法,Yuan-Chao Liu; Xiao-Long Wang; Bing-Quan Liu,"Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., China",Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),24-Jan-05,2004,5,,2963,2968 vol.5,"Constructing feature space by only selecting more informative words can speed up document clustering algorithm greatly, and the cluster quality is not affected. In this paper, firstly, the impact of feature selection on document clustering is discussed, then, a new solution for feature selection was brought forward which is based on word co-occurrence frequency. According to cluster hypothesis, the documents from the same class are more similar to each other when they are represented in vector space model (VSM), so many of the words from these documents are always in company with each other. We find these words by word co-occurrence, and then construct reduced feature space for clustering. Experiments show that the selected features are more salient. Clustering documents in the new reduced feature space, run time is shortened greatly, whereas the cluster quality is almost unchanged, thus make clustering algorithm more suitable for practical use.",,0-7803-8403-2,10.1109/ICMLC.2004.1378540,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378540,,Clustering algorithms;Frequency;Partitioning algorithms;Space technology;Computer science;Explosives;Internet;Navigation;Search engines;Unsupervised learning,pattern clustering;text analysis;feature extraction;vectors,feature selection algorithm;document clustering algorithm;word cooccurrence frequency;feature space construction;cluster hypothesis;vector space model,,1,,15,,24-Jan-05,,,IEEE,IEEE Conferences
Multi-Document Summarization Using Clustering Algorithm,使用聚類算法的多文檔摘要,X. Ma; G. Yu; L. Ma,"Sch. of Mech. Eng., Tianjin Univ., Tianjin; NA; NA",2009 International Workshop on Intelligent Systems and Applications,12-Jun-09,2009,,,1,4,"This paper treat the query sentence as common sentence segmented from multi-documents set for multi-document summarization, and mixed it into sentences set, then this paper create efficient cluster algorithm to cluster all the sentences, The clusters which contain the query sentence will be merged to one cluster, and modified MMR will be used to extract summary sentences. Experimental result shows that the proposed cluster algorithm is efficient.",,978-1-4244-3893-8,10.1109/IWISA.2009.5072965,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072965,,Clustering algorithms;Data mining;Flowcharts;Iterative algorithms;Mechanical engineering;Petroleum;Educational institutions;Computer science;Filters;Kernel,document handling;pattern clustering,multidocument summarization;clustering algorithm;query sentence,,3,,8,,12-Jun-09,,,IEEE,IEEE Conferences
Document cluster detection on latent projections,潛在投影的文檔聚類檢測,D. Alvarez-Medina; H. Hidalgo-Silva,"Universidad Polit矇cnica de Baja California, Mexicali, 21376 M矇xico; CICESE-Ciencias de la Computaci籀n, Km. 107 Carr. Tijuana-Eda Ensenada, 22800 M矇xico",2009 Fourth International Conference on Digital Information Management,18-Dec-09,2009,,,1,7,"Probabilistic text data modeling is usually considered with Bernoulli or multinomial event models. The main problem of text mining is the large amount of zero account in the matrix representation. Recently a document visualization technique incorporating the Zero Inflated Poisson model in the Generative Topographic Mapping algorithm has been proposed. This probabilistic model can be applied as a text document visualization tool. In this work, an algorithm for automatically extracting the clusters in the visualization results is presented. The combination of visualization-cluster extraction algorithms allows to obtain and evaluate document collections. Several results are presented for 20-Newsgroups and Reuters data.",,978-1-4244-4253-9,10.1109/ICDIM.2009.5356765,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5356765,,Data visualization;Clustering algorithms;Data mining;Text mining,data models;data visualisation;pattern clustering;probability;stochastic processes;text analysis,document cluster detection;latent projection;probabilistic text data modeling;matrix representation;zero inflated Poisson model;generative topographic mapping algorithm;text document visualization;cluster extraction;document collection,,,,16,,18-Dec-09,,,IEEE,IEEE Conferences
Text document clustering and the space of concept on text document automatically generated,文本文檔聚類和文本文檔概念空間自動生成,Fu Weipeng; Wu Bin; He Qing; Shi Zhongzhi,"Graduate Sch., Univ. of Sci. & Technol. of China, Beijing, China; NA; NA; NA",2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479),6-Aug-02,2001,3,,107,112 vol.3,"A method of automatically generating text document space is presented. Firstly, the paper considers text document clustering through the use of SOM and then fuzzy clustering to automatically generate and sum up the concept space for text document management. The result can be used to cluster Chinese documents and generates an index of the cluster automatically. SOM is an unsupervised learning neural network method that produces a mapping from text space into concept space. Some experiments and test results show that the concept space does well in arranging the classification of the text and is convenient for information retrieval.",,0-7803-7010-4,10.1109/ICII.2001.983044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983044,,Space technology;Humans;Neural networks;Clustering algorithms;Neurons;Educational institutions;Laboratories;Information processing;Computers;Testing,text analysis;data mining;self-organising feature maps;indexing;unsupervised learning;classification;information retrieval,data mining;knowledge discovery;text document clustering;self-organizing maps;text document space;SOM;fuzzy clustering;text document management;Chinese document;concept space;indexing;unsupervised learning;neural network;experiments;information retrieval;classification,,,,9,,6-Aug-02,,,IEEE,IEEE Conferences
Learning Free Document Image Binarization Based on Fast Fuzzy C-Means Clustering,基於快速模糊C均值聚類的免費文檔圖像二值化學習,T. Mondal; M. Coustaty; P. Gomez-Kr瓣mer; J. Ogier,"L3i, La-Rochelle University, France; L3i, La-Rochelle University, France; L3i, La-Rochelle University, France; L3i, La-Rochelle University, France",2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,1384,1389,"In this paper, a novel local threshold binarization method using fast Fuzzy C-Means clustering is proposed. Historical document images with non-uniform background, stains, faded ink are first processed by removing the background using inpainting based method. Then using Fuzzy C-Means clustering is used to cluster out the pixels into three main clusters : sure text pixels, sure background pixels and confused pixels which may or may not be labeled as text. Based on the structural symmetry of pixels (SSP), these confused pixels are then classified into text or background pixels. The SSP is defined as those pixels around strokes whose gradient magnitudes are big enough and whose directions are opposite. As the gradient map is our basis for computing the SSP, we further propose to estimate the background surface first and to extract potential SSP in the compensated image so as to deal with degradations of document images such as uneven illumination, low contrast and stain. To prove the effectiveness of our method, tests on eight public document image datasets are preformed and the experimental results show that our method outperforms other local threshold binarization approaches on both F-measure and PSNR.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00223,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978039,Binarization;Fuzzy C-Means;Background Removal;Stroke Width Estimation,Image edge detection;Ink;Degradation;Estimation;Clustering algorithms;Two dimensional displays;Text analysis,document image processing;image restoration;image segmentation;pattern clustering;text analysis,free document image binarization;fast Fuzzy C-Means clustering;local threshold binarization method;historical document images;nonuniform background;inpainting based method;text pixels;background pixels;confused pixels;SSP;background surface;compensated image;public document image datasets,,,,12,,3-Feb-20,,,IEEE,IEEE Conferences
Clustering sentences to discover events from multiple news articles using Buckshot and Fractionation,使用Buckshot和Fractionation對句子進行聚類以發現多個新聞中的事件,D. SaravanaPriya; M. Karthikeyan,"Department of IT, P.A College of Engineering and Technology Coimbatore, Tamilnadu; ECE, Tamilnadu Engineering College, Coimbatore, Tamilnadu",2014 IEEE International Conference on Computational Intelligence and Computing Research,7-Sep-15,2014,,,1,5,"Sentence Clustering is performed based on the key terms in sentences within a document or group of documents. A sentence may come under different topics in a single document with different word of similar meaning which will not be clustered correctly by using hierarchical clustering methods. Hierarchical clustering methods are robust. They are not very efficient as its time complexity is O (n2). To overcome this problem, K-means type algorithms are used, but it handles only few documents. A proposed algorithm uses both hierarchical and partitional clustering method alternatively. It increases the accuracy and reduces the time complexity for multiple news articles. It is applied to group the text spans from multiple news articles that refer to the same event.",,978-1-4799-3975-6,10.1109/ICCIC.2014.7238566,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238566,Hierarchical relational clustering;Sentence clustering;semantically similar sentence,Clustering algorithms;Data mining;Algorithm design and analysis;Time complexity;Partitioning algorithms;Fractionation;Clustering methods,computational complexity;document handling;pattern clustering,sentence clustering;hierarchical clustering methods;time complexity;k-means type algorithm,,,,20,,7-Sep-15,,,IEEE,IEEE Conferences
Binarization-Free Text Line Segmentation for Historical Documents Based on Interest Point Clustering,基於興趣點聚類的歷史文檔無二值化文本行分割,A. Garz; A. Fischer; R. Sablatnig; H. Bunke,"Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Inst. of Comput. Sci. & Appl. Math., Bern, Switzerland; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Inst. of Comput. Sci. & Appl. Math., Bern, Switzerland",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,95,99,"Segmenting page images into text lines is a crucial pre-processing step for automated reading of historical documents. Challenging issues in this open research field are given \eg by paper or parchment background noise, ink bleed-through, artifacts due to aging, stains, and touching text lines. In this paper, we present a novel binarization-free line segmentation method that is robust to noise and copes with overlapping and touching text lines. First, interest points representing parts of characters are extracted from gray-scale images. Next, word clusters are identified in high-density regions and touching components such as ascenders and descenders are separated using seam carving. Finally, text lines are generated by concatenating neighboring word clusters, where neighborhood is defined by the prevailing orientation of the words in the document. An experimental evaluation on the Latin manuscript images of the Saint Gall database shows promising results for real-world applications in terms of both accuracy and efficiency.",,978-0-7695-4661-2,10.1109/DAS.2012.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195342,historical documents;manuscripts;ancient documents;handwritten;text line segmentation;binarization-free,Image segmentation;Layout;Noise;Databases;Robustness;Green products;Merging,document image processing;image segmentation;pattern clustering;text analysis,binarization free text line segmentation;historical documents;interest point clustering;automated reading;open research field;parchment background noise;touching text lines;overlapping text lines;gray-scale images,,23,,22,,7-May-12,,,IEEE,IEEE Conferences
Two Texture Segmentation of Document Image Using Wavelet Packet Analysis,基於小波包分析的文檔圖像兩種紋理分割,G. Lee; W. O. Odoyo; J. Lee; I. Chung; B. Cho,"Department of Computer Engineering, University of Chosun Gwang-ju, Korea. E-mail: goldylee@empal.com; Department of Computer Engineering, University of Chosun Gwang-ju, Korea; Department of Computer Engineering, University of Chosun Gwang-ju, Korea. E-mail: goldylee@empal.com; Department of Computer Engineering, University of Chosun Gwang-ju, Korea; Department of Computer Engineering, University of Chosun Gwang-ju, Korea",The 9th International Conference on Advanced Communication Technology,7-May-07,2007,1,,395,398,"In this paper, we present a text segmentation method using wavelet packet analysis and k-means clustering algorithm. This approach assumes that the text and non-text regions are considered as two different texture regions. The text segmentation is achieved by using wavelet packet analysis as a feature analysis. The wavelet packet analysis is a method of wavelet decomposition that offers a richer range of possibilities for document image. From these multiscale features, we compute the local energy and intensify the features before adapting the k-means clustering algorithm based on the unsupervised learning rule. The results show that our text segmentation method is effective for document images scanned from newspapers and journals.",1738-9445,978-89-5519-131-8,10.1109/ICACT.2007.358379,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4195158,wavelet packet analysis;document image segmentation;k-means clustering algorithm;energy estimation,Image segmentation;Wavelet analysis;Wavelet packets;Image analysis;Image texture analysis;Clustering algorithms;Algorithm design and analysis;Text analysis;Signal analysis;Unsupervised learning,document image processing;image segmentation;image texture;wavelet transforms,two texture document image segmentation;wavelet packet analysis;text segmentation method;k-means clustering algorithm;feature analysis;wavelet decomposition;unsupervised learning rule,,5,,8,,7-May-07,,,IEEE,IEEE Conferences
Summarization of Odia Text Document Using Cosine Similarity and Clustering,使用餘弦相似度和聚類總結Odia文本文檔,S. Pattnaik; A. K. Nayak,"Siksha 'O' Anusandhan University, India; Siksha 'O' Anusandhan University, India",2019 International Conference on Applied Machine Learning (ICAML),10-Feb-20,2019,,,143,146,"Automatic text summarization a subfield of Natural Language Processing (NLP) aims at producing precise and non redundant text aided by machine learning techniques. Using varied methods, especially machine learning techniques has enhanced its performance from different perspectives. The proposed work efficiently utilizes hierarchical clustering by using cosine similarity measure for segregating sentences. The model adopts an extractive method for summarizing Odia text document. It focuses on minimizing redundancy and achieves it through cosine similarity matrix. Though the methods employed are primitive for European languages like English, Odia language which is computationally in a passive state and has complex morphological structure, is a novel work. The results obtained can modestly be considered satisfactory.",,978-1-7281-3908-1,10.1109/ICAML48257.2019.00035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989281,NLP;TF-IDF;text summarization;clustering;cosine similarity,Machine learning;Redundancy;Tools;Natural language processing;Clustering algorithms;Computational modeling;Conferences,learning (artificial intelligence);natural language processing;pattern clustering;text analysis,sentences segregation;Odia text document summarization;Odia language;European languages;cosine similarity matrix;extractive method;cosine similarity measure;hierarchical clustering;machine learning techniques;natural language processing;automatic text summarization,,,,21,,10-Feb-20,,,IEEE,IEEE Conferences
Ontology-Based Fuzzy Semantic Clustering,基於本體的模糊語義聚類,Y. Cheng,"Commun. Eng. Dept., Changsha Univ., Changsha",2008 Third International Conference on Convergence and Hybrid Information Technology,18-Nov-08,2008,2,,128,133,"Document clustering plays an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of documents into a small number of meaningful clusters. Most of the documents clustering methods were grounded in the bag of words representation to measure similarity, ignoring the semantic relationships between words that do not co-occur literally. A novel fuzzy semantic method that integrates ontology as background knowledge into the process of computing similarity between documents is proposed so as to improve the performance of documents clustering in terms of quality and efficiency. Ontology is represented as a graph-based model that reflects semantic relationship between concepts, with which a semantic similarity matrix of concepts that exploits semantic relation of the ontology is defined. Based on conceptual matrix a document can be represented to a semantic fuzzy set. Then similarity between documents is computed with fuzzy matching measure. The result of this process may make documents not similar with vector representation become similar. Maximal fuzzy spanning tree algorithm is used as a document-clustering algorithm. Finally the efficacy of our approach is demonstrated through relevant experiments.",,978-0-7695-3407-7,10.1109/ICCIT.2008.232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682226,Ontology;Fuzzy Semantic Clustering;Document Clustering,Ontologies;Fuzzy sets;Clustering algorithms;Large scale integration;Asia;Information technology;Navigation;Organizing;Clustering methods;Unsupervised learning,document handling;fuzzy set theory;ontologies (artificial intelligence),ontology-based fuzzy semantic clustering;intuitive navigation;browsing mechanism;words representation;graph-based model;semantic similarity matrix;conceptual matrix;semantic fuzzy set;document similarity;fuzzy matching measure;vector representation;maximal fuzzy spanning tree algorithm;document clustering algorithm,,4,,14,,18-Nov-08,,,IEEE,IEEE Conferences
Semantic based clustering of Web documents,Web文檔的基於語義的群集,T. Y. Lin; I-Jen Chiang,"Dept. of Comput. Sci., San Jose State Univ., CA, USA; NA",2005 IEEE International Conference on Granular Computing,5-Dec-05,2005,1,,189,192 Vol. 1,"A new methodology that structures the semantics of a collection of documents into the geometry of a simplicial complex is developed: a primitive concept is represented by a top dimension simplex, and a connected component represents a concept. Based on these structures, documents can be clustered into some meaningful classes. Experiments with three different data sets from web pages and medical literature have shown that the proposed unsupervised clustering approach performs significantly better than traditional clustering algorithms, such as k-means, AutoClass and hierarchical clustering (HAC). This abstract geometric model seems have captured the intrinsic semantics of the documents.",,0-7803-9017-2,10.1109/GRC.2005.1547264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1547264,clustering;document;polyhedron;semantics;web,Geometry;Humans;Topology;Web pages;Clustering algorithms;Solid modeling;Skeleton;Computer science;Biomedical informatics;Microcomputers,semantic Web;document handling;geometry;pattern clustering,Web document;semantic document collection;simplicial complex geometry;data set;Web page;unsupervised clustering;abstract geometric model,,1,,23,,5-Dec-05,,,IEEE,IEEE Conferences
Summarization of sentences using fuzzy and hierarchical clustering approach,使用模糊和層次聚類方法對句子進行匯總,K. G. Pradip; D. R. Patil,"Department of Computer Engineering, SES's R.C.P.I.T, Shirpur, Maharashtra, India; Department of Computer Engineering, SES's R.C.P.I.T., Shirpur, Maharashtra, India",2016 Symposium on Colossal Data Analysis and Networking (CDAN),19-Sep-16,2016,,,1,7,"The process of grouping or combining of data items known as clustering and groups formed are clusters. Sentence clustering mainly utilized in variety of applications such as grouping, categorization of documents, automatic summary generation, organization of the documents etc. Sentence clustering has importance in the text mining domain. Cluster sizes vary from one another. There are some problems in traditional clustering such as instability of clusters, complexity and sensitivity. In this paper we have implemented a Hierarchical and Fuzzy Relational Eigenvector Centrality-based Clustering Algorithm for the clustering of sentences to overcome the problems of traditional clustering methods. The experimental result shows that Hierarchical clustering will be useful algorithm for text documents and gives better results.",,978-1-5090-0669-4,10.1109/CDAN.2016.7570964,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570964,Sentence level clustering;Fuzzy relational clustering;Sentence Similarity Measure;Hierarchical clustering;Sentence Summarization,Clustering algorithms;Algorithm design and analysis;Data analysis;Text mining;Partitioning algorithms;Prediction algorithms,data mining;eigenvalues and eigenfunctions;pattern clustering;text analysis,data items;sentence clustering;text mining;cluster sizes;hierarchical clustering;fuzzy relational eigenvector centrality-based clustering;text documents,,,,,,19-Sep-16,,,IEEE,IEEE Conferences
Unsupervised document clustering using multi-resolution latent semantic density analysis,使用多分辨率潛在語義密度分析的無監督文檔聚類,J. R. Bellegarda,"Speech & Language Technologies, Apple Inc., Cupertino, California 95014, USA",2010 IEEE International Workshop on Machine Learning for Signal Processing,7-Oct-10,2010,,,361,366,"To find meaningful groupings in a given document collection, it is essential to learn the right granularity for the domain, uncover core themes and attendant outliers, and derive suitable labels with which to characterize each of the resulting clusters. The outcome is therefore affected both by the choice of representation and by the behavior of the clustering algorithm. This paper advocates a strategy which combines density-based clustering with latent semantic feature extraction. Documents are first mapped into a latent semantic vector space, and then clustered in that space on the basis of a multi-resolution density measure. Empirical evidence gathered on several document collections suggests that this procedure is effective in identifying semantically sound document clusters.",2378-928X,978-1-4244-7877-4,10.1109/MLSP.2010.5587982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587982,structured document collection;unsupervised clustering;latent semantic mapping;density measure;variable resolution,Semantics,document handling;feature extraction;pattern clustering,multiresolution latent semantic density analysis;document collection;density based clustering;latent semantic feature extraction;latent semantic vector space;multiresolution density measure;unsupervised document clustering,,1,,24,,7-Oct-10,,,IEEE,IEEE Conferences
Evaluation of Partition-Based Text Clustering Techniques to Categorize Indic Language Documents,基於分區的文本聚類技術對印度語言文檔進行分類的評估,D. A. Meedeniya; A. S. Perera,"Department of Computer Science and Engineering, Faculty of Engineering, University of Moratuwa, Sri Lanka, dulanim@uom.lk; Department of Computer Science and Engineering, Faculty of Engineering, University of Moratuwa, Sri Lanka, shehan@uom.lk",2009 IEEE International Advance Computing Conference,31-Mar-09,2009,,,1497,1500,"Wide availability of electronic data has led to the vast interest in text analysis, information retrieval and text categorization methods. To provide a better service, there is a need for non-English based document analysis and categorizing systems, as is currently available for English text documents. This study is mainly focused on categorizing Indic language documents. The main techniques examined in this study include data pre-processing and document clustering. The approach makes use of a transformation based on the text frequency and the inverse document frequency, which enhances the clustering performance. This approach is based on latent semantic analysis, k-means clustering and Gaussian mixture model clustering. A text corpus categorized by human readers is utilized to test the validity of the suggested approach. The technique introduced in this work enables the processing of text documents written in Sinhala, and empowers citizens and organizations to do their daily work eficiently.",,978-1-4244-2927-1,10.1109/IADCC.2009.4809239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809239,,Natural languages;Data engineering;Information retrieval;Humans;Text categorization;Computer science;Text analysis;Frequency;Testing;Availability,Gaussian processes;natural languages;pattern clustering;text analysis,partition-based text clustering technique;Indic language document categorization;information retrieval;text categorization;data pre-processing;document clustering;text frequency;inverse document frequency;latent semantic analysis;k-means clustering;Gaussian mixture model clustering,,5,,10,,31-Mar-09,,,IEEE,IEEE Conferences
Building naive bayes document classifier using word clusters based on bootstrap averaging,使用基於引導平均的單詞簇構建樸素貝葉斯文檔分類器,Y. Wang; Q. Zhang; L. Bai,"Institute of Information Engineering, Wuhan University of Technology, China, 430070; Institute of Information Engineering, Wuhan University of Technology, China, 430070; Henan University of Technology, Zhengzhou, China, 450052",2009 IEEE International Symposium on IT in Medicine & Education,15-Sep-09,2009,1,,202,207,"Aimed to solve the problem of low classification accuracy caused by poor distribution estimation by training naive Bayes document classifier on word clusters, we build a sequential word list based on mutual information between words and their semantic cluster labels, then construct a sample set of the same size with the word list through bootstrap sampling and use the average of the corresponding parameters estimated from the sample set as the last parameter to classify unknown documents. Experiment results on benchmark document data sets show that the proposed strategy gains higher classification accuracy comparing to naive Bayes documents classifier on word clusters or on words.",,978-1-4244-3928-7,10.1109/ITIME.2009.5236431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236431,,Sampling methods;Clustering algorithms;Mutual information;Probability distribution;Parameter estimation;Machine learning;Data mining;Sorting;Classification algorithms,Bayes methods;document handling,naive Bayes document classifier;word clusters;bootstrap averaging;distribution estimation;semantic cluster labels;bootstrap sampling,,,,10,,15-Sep-09,,,IEEE,IEEE Conferences
A method for web documents clustering based on dynamic concept,一種基於動態概念的Web文檔聚類方法,Y. Wang; H. Ke,"School of Computer, Wuhan University, China; Soil and Water Conservation Monitoring Centre, Department of Water Resources of Hubei Province, Wuhan, China",Proceedings of 2011 International Conference on Computer Science and Network Technology,12-Apr-12,2011,4,,2183,2187,"Conceptual clustering can be used to solve the lack of domain knowledge or incompleteness. According to the organizational structure and characteristics of web documents subject information, and based on the conceptual clustering techniques, the paper proposes and implements the dynamic concept clustering algorithm and merge algorithm for web documents clustering, and proves the superiority of these algorithms by analyzing the efficiency and clustering accuracy.",,978-1-4577-1587-7,10.1109/ICCSNT.2011.6182409,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182409,concept;clustering;dynamic concept;subject information,Electrostatic discharges,document handling;Internet,web documents clustering;dynamic concept;conceptual clustering,,,,8,,12-Apr-12,,,IEEE,IEEE Conferences
An efficient web document clustering algorithm for building dynamic similarity profile in Similarity-aware web caching,在相似性感知Web緩存中構建動態相似性概要文件的有效Web文檔聚類算法,J. Xiao,"School of Computer and Security Science, Edith Cowan University, 2 Bradford Street, Mount Lawley, WA 6050, Australia",2012 International Conference on Machine Learning and Cybernetics,24-Nov-12,2012,4,,1268,1273,"Discovering and establishing similarities among web documents have been one of the key research streams in web usage mining community in the recent years. The knowledge obtained from the exercise can be used for many applications such as optimizing web cache organization and improving the quality of web document pre-fetching. This paper presents an efficient matrix-based method to cluster web documents based on a predetermined similarity threshold. Our preliminary experiments have demonstrated that the new algorithm outperforms existing algorithms. The clustered web documents are then applied to a Similarity-aware web content management system, facilitating offline building of the similarity-ware web caches and online updating similarity profiles of the system.",2160-1348,978-1-4673-1487-9,10.1109/ICMLC.2012.6359547,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359547,Web document clustering;Similarity profile;Web caching,Abstracts;Clustering algorithms;Algorithm design and analysis,cache storage;data mining;document handling;Internet;matrix algebra;pattern clustering,Web document clustering algorithm;dynamic similarity profile;similarity-aware Web caching;similarity discovery;similarity establishment;Web usage mining community;Web cache organization optimization;Web document pre-fetching quality improvement;matrix-based method;predetermined similarity threshold;similarity-aware Web content management system;online updating similarity profiles,,,,7,,24-Nov-12,,,IEEE,IEEE Conferences
System for document clustering from mixed sources based on Fuzzy ART neural network,基於模糊ART神經網絡的混合源文檔聚類系統,M. Roj?ek; I. Mokri禳,"Department of Informatics, Faculty of Education, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1, 03401, Slovakia; Institute of Informatics, Slovak Academy of Sciences, D繳bravsk獺 cesta 9, 845 07 Bratislava 45, Slovakia",2013 International Conference on System Science and Engineering (ICSSE),30-Sep-13,2013,,,259,262,"The article presents a model for text document clustering based on Fuzzy ART neural network with two separate network segments. The first segment (Internet) enables clustering for the classification of documents into new categories, and the second segment (intranet) enables the modified Fuzzy ART algorithm to assign documents into existing categories. The article observe behavior of the model based on Fuzzy ART network, into which entering the different strategies in the different segments of synthetic text documents.",2325-0925,978-1-4799-0009-1,10.1109/ICSSE.2013.6614670,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614670,,Subspace constraints;Clustering algorithms;Neural networks;Vectors;Testing;Internet;Information retrieval,ART neural nets;fuzzy neural nets;Internet;intranets;pattern classification;pattern clustering;text analysis,fuzzy ART neural network;text document clustering;network segments;Internet;document classification;intranet;synthetic text documents;adaptive resonance theory,,1,,15,,30-Sep-13,,,IEEE,IEEE Conferences
A survey on semantic document clustering,語義文檔聚類研究,M. P. Naik; H. B. Prajapati; V. K. Dabhi,"Department of Information Technology, Dharmsinh Desai University, Nadiad, India; Department of Information Technology, Dharmsinh Desai University, Nadiad, India; Department of Information Technology, Dharmsinh Desai University, Nadiad, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",27-Aug-15,2015,,,1,10,"Clustering is the process of partitioning a set of data objects into subsets. It is commonly used technique in data mining, information retrieval, and knowledge discovery for finding hidden patterns or objects from a data of different category. Text clustering process deals with grouping of an unstructured collection of documents into semantically related groups. A document is considered as a bag of words in traditional document clustering methods; however, semantic meaning of word is not considered. Thus, more informative features like concept weight are important to achieve accurate document clustering and this can be achieved through semantic document clustering because it takes meaningful relationship into account. This paper highlights major challenges in traditional document clustering and semantic document clustering along with brief discussion. This paper identifies five major areas under semantic clustering and presents a survey of 17 papers that has studied, covering major significant works. Moreover, this paper also provides a survey of tools, ontology databases, and algorithms, which help in applying and evaluating document clustering. The presented survey is used in preparing the proposed work in the same direction. This proposed work uses the concept weight for text clustering system which is to be developed based on a Hierarchical Agglomerative Clustering, Bisecting k-means algorithm, and Self Organized Map Neural Network in accordance with the principles of WordNet ontology as a background knowledge.",,978-1-4799-6085-9,10.1109/ICECCT.2015.7226036,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226036,clustering;semantic clustering;ontology;HAC;bisecting k-means;SOM-NN;clustering algorithms;evaluation measures,Ontologies;Semantics;Databases;Frequency measurement;Context;Training;Biomedical imaging,ontologies (artificial intelligence);pattern clustering;self-organising feature maps;text analysis,semantic document clustering;data objects partitioning;text clustering process;unstructured documents collection;bag of words;concept weight;ontology databases;text clustering system;hierarchical agglomerative clustering;bisecting k-means algorithm;self organized map neural network;WordNet ontology;background knowledge,,13,,42,,27-Aug-15,,,IEEE,IEEE Conferences
XML document clustering based on common tag names anywhere in the structure,基於結構中任何位置的通用標籤名稱的XML文檔聚類,M. Alishahi; M. Ravakhah; B. Shakeriaski; M. Naghibzade,Islamic Azad University Mashhad Branch; Islamic Azad University Mashhad Branch; Islamic Azad University Ramsar Branch; Ferdowsi university of Mashhad Computer Department,2009 14th International CSI Computer Conference,8-Dec-09,2009,,,588,595,"One of the most effective ways to extract knowledge from large information resources is applying data mining methods. Since the amount of information on the Internet is exploding, using XML documents is common as they have many advantages. Knowledge extraction from XML documents is a way to provide more utilizable results. XCLS is one of the most efficient algorithms for XML documents clustering. In this paper we represent a new algorithm for clustering XML documents. This algorithm is an improvement over XCLS algorithm which tries to obviate its problems. We implemented both algorithms and evaluated their clustering quality and running time on the same data sets. In both cases, it is shown that the performance of the new algorithm is better.",,978-1-4244-4261-4,10.1109/CSICC.2009.5349643,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349643,data mining;clustering;XML documents;level structure;level similarity,XML;Clustering algorithms;Data mining;Tree data structures;Neural networks;Information resources;Internet;Web sites;Association rules;Search engines,data mining;document handling;pattern clustering;XML,XML document clustering;knowledge extraction;information resources;data mining methods;XCLS algorithm;clustering quality,,,,11,,8-Dec-09,,,IEEE,IEEE Conferences
Unsupervised Block Covering Analysis for Text-Line Segmentation of Arabic Ancient Handwritten Document Images,阿拉伯古代手寫文檔圖像的文本行分割的無監督塊覆蓋分析,W. Boussellaa; A. Zahour; H. Elabed; A. Benabdelhafid; A. M. Alimi,"Res. Group on Intell. Machines, Univ. of Sfax, Sfax, Tunisia; Inst. for Commun. Technol., Braunschweig Tech. Univ., Braunschweig, Germany; Group of Electron. Document Managment, Univ. of Le Havre, Le Havre, France; Inst. for Commun. Technol., Braunschweig Tech. Univ., Braunschweig, Germany; Res. Group on Intell. Machines, Univ. of Sfax, Sfax, Tunisia",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,1929,1932,"This paper presents a new method for automatic text-line extraction from Arabic historical handwritten documents presenting an overlapping and multi-touching characters problems. Our approach is based on block covering analysis using unsupervised technique. This algorithm performs firstly a statistical block analysis which computes the optimal number of document decomposition into vertical strips. Then, our algorithm achieves a fuzzy base line detection using fuzzy C-means algorithm. Finally, blocks are assigned to its corresponding lines. Experiment results show that the proposed method achieves high accuracy about 95% for detecting text lines in Arabic historical handwritten document images written with different scripts.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.475,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597247,Text-line segmentation;Fuzzy base line detection;Block covering Analysis;Fuzzy C-means;Arabic historical document,Strips;Algorithm design and analysis;Image segmentation;Accuracy;Pixel;Clustering algorithms;Partitioning algorithms,document image processing;handwriting recognition;image segmentation;natural languages;statistical analysis;text analysis,unsupervised block covering analysis;text-line segmentation;Arabic ancient handwritten document images;automatic text-line extraction;multitouching characters problems;statistical block analysis;fuzzy base line detection;fuzzy C-means algorithm,,9,,15,,7-Oct-10,,,IEEE,IEEE Conferences
A Graph-Structure-Based Method for Chinese Document Representation towards Clustering Application,一種基於圖結構的中文文檔表示方法在聚類中的應用,Q. Liu; J. Wu; Y. Wang,"Inst. of Syst. Eng., Dalian Univ. of Technol., Dalian; Inst. of Syst. Eng., Dalian Univ. of Technol., Dalian; Inst. of Syst. Eng., Dalian Univ. of Technol., Dalian","2008 4th International Conference on Wireless Communications, Networking and Mobile Computing",18-Nov-08,2008,,,1,4,"In this paper, we propose a graph-structure-based method to represent knowledge for Chinese document clustering. First, we introduce a new knowledge representation method called Graph Space Model (GSM) to convert each document to a graph structure, and then we adopt Maximum Common Subgraph (MCS) to compute the similarities between any two graph structures, which can be further used for document clustering. The results show that the GSM approach can outperform VSM method in representing capability of Chinese documents.",2161-9654,978-1-4244-2107-7,10.1109/WiCom.2008.1195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4679103,,GSM;Knowledge representation;Systems engineering and theory;Frequency measurement;Computer aided software engineering,document handling;graph theory;knowledge representation;natural language processing;pattern clustering,Chinese document clustering;graph structure;knowledge representation;graph space model;maximum common subgraph,,,,7,,18-Nov-08,,,IEEE,IEEE Conferences
Tamil News Clustering Using Word Embeddings,使用單詞嵌入的泰米爾語新聞聚類,M. S. Faathima Fayaza; S. Ranathunga,"University of Moratuwa,Department of Computer Science and Engineering,Katubedda,Sri Lanka,10400; University of Moratuwa,Department of Computer Science and Engineering,Katubedda,Sri Lanka,10400",2020 Moratuwa Engineering Research Conference (MERCon),3-Sep-20,2020,,,277,282,"News aggregators support the readers to view news from multiple news providers via a single point. At the moment, the only news aggregator that supports Tamil news is Google news, which has some noticeable shortages. In this study, Term Frequency-Inverse Document Frequency and word embedding (fastText) document representation techniques were experimented with one pass and affinity propagation clustering algorithms to news title, as well as title and body in order to implement a news aggregator for the Tamil language. For this study we collected data from nine different news providers. When fastText was applied with one pass algorithm to news title and body, it managed to beat other approaches to achieve an average pairwise F-score of 81% with respect to manual clustering. Also, we were able to create a Tamil fastText word embedding model using more than 21 million words.",,978-1-7281-9975-7,10.1109/MERCon50084.2020.9185282,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185282,document clustering;Tamil;word embedding;Term Frequency?Inverse Document Frequency;affinity propagation clustering;one pass algorithm,Clustering algorithms;Partitioning algorithms;Manuals;Uniform resource locators;Standards;Heuristic algorithms;YouTube,document handling;information retrieval;natural language processing;pattern clustering;text analysis,Tamil news clustering;word embeddings;news aggregator;multiple news providers;Google news;Term Frequency-Inverse Document Frequency;word embedding document representation techniques;news title;different news providers;Tamil fastText word embedding model,,,,31,,3-Sep-20,,,IEEE,IEEE Conferences
Ranking Through Clustering: An Integrated Approach to Multi-Document Summarization,通過聚類進行排名：一種集成的多文檔摘要方法,X. Cai; W. Li,"College of Information Engineering, Northwest A&F University, Shaanxi, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong","IEEE Transactions on Audio, Speech, and Language Processing",29-Mar-13,2013,21,7,1424,1433,"Multi-document summarization aims to create a condensed summary while retaining the main characteristics of the original set of documents. Under such background, sentence ranking has hitherto been the issue of most concern. Since documents often cover a number of topic themes with each theme represented by a cluster of highly related sentences, sentence clustering has been explored in the literature in order to provide more informative summaries. For each topic theme, the rank of terms conditional on this topic theme should be very distinct, and quite different from the rank of terms in other topic themes. Existing cluster-based summarization approaches apply clustering and ranking in isolation, which leads to incomplete, or sometimes rather biased, analytical results. A newly emerged framework uses sentence clustering results to improve or refine the sentence ranking results. Under this framework, we propose a novel approach that directly generates clusters integrated with ranking in this paper. The basic idea of the approach is that ranking distribution of sentences in each cluster should be quite different from each other, which may serve as features of clusters and new clustering measures of sentences can be calculated accordingly. Meanwhile, better clustering results can achieve better ranking results. As a result, ranking and clustering by mutually and simultaneously updating each other so that the performance of both can be improved. The effectiveness of the proposed approach is demonstrated by both the cluster quality analysis and the summarization evaluation conducted on the DUC 2004-2007 datasets.",1558-7924,,10.1109/TASL.2013.2253098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480794,Document summarization;sentence clustering;sentence ranking,Equations;Mathematical model;Clustering algorithms;Semantics;Hidden Markov models;Estimation;Vectors,document handling;pattern clustering;statistical analysis,multidocument summarization;sentence clustering;informative summaries;cluster-based summarization approach;sentence ranking distribution;cluster quality analysis;DUC 2004-2007 datasets,,16,,39,,15-Mar-13,,,IEEE,IEEE Journals
A Document Clustering Method Based on Hierarchical Algorithm with Model Clustering,基於層次聚類模型聚類的文檔聚類方法,H. Sun; Z. Liu; L. Kong,"Shantou Univ., Shantou; NA; NA",22nd International Conference on Advanced Information Networking and Applications - Workshops (aina workshops 2008),3-Apr-08,2008,,,1229,1233,"Document clustering is an important tool for text analysis and is used in many applications. This work develops a novel hierarchal algorithm for document clustering. We are particularly interested in studying and making use of cluster overlapping phenomenon to design cluster merging criteria. In our previous papers, the theoretical results on the overlap rate between clusters based on the Gaussian mixture model were reported. In this paper, we propose a new way to compute the overlap rate in order to improve time efficiency and ""the veracity"". The way is that we use a line passed through the two cluster's center instead of the ridge curve. Based on the hierarchical clustering method, we use the expectation-maximization (EM) algorithm in the Gaussian mixture model to count the parameters and make the two sub-clusters combined when their overlap is the largest. Experiments in both public data and document clustering data show that this approach can improve the efficiency of clustering and save computing time.",,978-0-7695-3096-3,10.1109/WAINA.2008.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483087,,Clustering methods;Clustering algorithms;Application software;Computer networks;Electronic mail;Mathematics;Frequency;Gaussian distribution;Partitioning algorithms;Mathematical model,expectation-maximisation algorithm;Gaussian processes;text analysis,document clustering method;model clustering;text analysis;Gaussian mixture model;ridge curve;hierarchical clustering method;expectation-maximization algorithm,,7,,15,,3-Apr-08,,,IEEE,IEEE Conferences
Document Clustering for Forensic Computing: An Approach for Improving Computer Inspection,取證計算的文檔聚類：一種改進計算機檢查的方法,L. F. da Cruz Nassif; E. R. Hruschka,"Brazilian Fed. Police Dept., Sao Paulo, Brazil; Univ. of Sao Paulo (USP) at Sao Carlos, Sao Carlos, Brazil",2011 10th International Conference on Machine Learning and Applications and Workshops,9-Feb-12,2011,1,,265,268,"In computer forensic analysis, hundreds of thousands of files are usually examined. Much of those files consist of unstructured text, whose analysis by computer examiners is difficult to be performed. In this context, automated methods of analysis are of great interest. In particular, algorithms for clustering documents can facilitate the discovery of new and useful knowledge from the documents under analysis. We present an approach that applies clustering algorithms to forensic analysis of computers seized in police investigations. We illustrate the proposed approach by carrying out experimentation with five clustering algorithms (K-means, K-medoids, Single Link, Complete Link, and Average Link) applied to five datasets obtained from computers seized in real-world investigations. In addition, two relative validity indexes were used to automatically estimate the number of clusters. Related studies in the literature are significantly more limited than our study. Our experiments show that the Average Link and Complete Link algorithms provide the best results for our application domain. If suitably initialized, partitional algorithms (K-means and K-medoids) can also yield to very good results. Finally, we also present and discuss practical results that can be useful for researchers and practitioners of forensic computing.",,978-1-4577-2134-2,10.1109/ICMLA.2011.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146981,Forensic computing;clustering;text mining,Clustering algorithms;Algorithm design and analysis;Forensics;Computers;Partitioning algorithms;TV;Computational efficiency,computer forensics;document handling;pattern clustering;police data processing,document clustering;forensic computing;computer inspection improvement;computer forensic analysis;K-means clustering algorithm;K-medoids clustering algorithm;single link clustering algorithm;complete link clustering algorithm;average link clustering algorithm;police investigations,,5,,15,,9-Feb-12,,,IEEE,IEEE Conferences
A semi-supervised document clustering algorithm based on EM,基於EM的半監督文檔聚類算法,L. Rigutini; M. Maggini,"Dipt. di Ingegneria dell'Informazione, Univ. di Siena, Italy; Dipt. di Ingegneria dell'Informazione, Univ. di Siena, Italy",The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),17-Oct-05,2005,,,200,206,"Document clustering is a very hard task in automatic text processing since it requires extracting regular patterns from a document collection without a priori knowledge on the category structure. This task can be difficult also for humans because many different but valid partitions may exist for the same collection. Moreover, the lack of information about categories makes it difficult to apply effective feature selection techniques to reduce the noise in the representation of texts. Despite these intrinsic difficulties, text clustering is an important task for Web search applications in which huge collections or quite long query result lists must be automatically organized. Semi-supervised clustering lies in between automatic categorization and auto-organization. It is assumed that the supervisor is not required to specify a set of classes, but only to provide a set of texts grouped by the criteria to be used, to organize the collection. In this paper, we present a novel algorithm for clustering text documents which exploits the EM algorithm together with a feature selection technique based on information gain. The experimental results show that only very few documents are needed to initialize the clusters and that the algorithm is able to properly extract the regularities hidden in a huge unlabeled collection.",,0-7695-2415-X,10.1109/WI.2005.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517843,Semi-supervised Document Clustering;EM;Information Gain,Clustering algorithms;Data mining;Humans;Text processing;Noise reduction;Web search;Ontologies;Noise measurement;Feedback;Text categorization,text analysis;feature extraction;pattern clustering,semisupervised document clustering;Web search;automatic categorization;auto-organization clustering;text document clustering;EM algorithm;feature selection,,7,,14,,17-Oct-05,,,IEEE,IEEE Conferences
Text Lines and Snippets Extraction for 19th Century Handwriting Documents Layout Analysis,19世紀手寫文檔佈局分析的文本行和摘錄,V. Malleron; V. Eglin; H. Emptoz; S. Dord-Crousl矇; P. R矇gnier,"INSA-Lyon, LIRIS, Univ. de Lyon, Lyon, France; INSA-Lyon, LIRIS, Univ. de Lyon, Lyon, France; INSA-Lyon, LIRIS, Univ. de Lyon, Lyon, France; INSA-Lyon, LIRIS, Univ. de Lyon, Lyon, France; INSA-Lyon, LIRIS, Univ. de Lyon, Lyon, France",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,1001,1005,"In this paper we propose a new approach to improve electronic editions of human science corpus, providing an efficient estimation of manuscripts pages structure. In any handwriting documents analysis process, the text line segmentation is an important stage. The presence of variable inter-line spaces, of inconstant base-line skews, overlapping and occlusions in unconstrained ancient 19th handwritten documents complexifies the text lines segmentation task. In this paper, we only use as prior knowledge of script the fact that text lines skews can be random and irregular.In that context, we model text line detection as an image segmentation problem by enhancing text line structure using Hough transform and a clustering of connected components so as to make text line boundaries appear. The proposed approach of snippets decomposition for page layout analysislies on a first step of content pages classification in five visual and genetic taxonomies, and a second step of text line extraction and snippets decomposition. Experiments show that the proposed method achieves high accuracy for detecting text lines in regular and semi-regular handwritten pages in the corpus of digitized Flaubert manuscripts (rdquoDossiers documentaires de Bouvard et Pecuchetrdquo, 1872-1880).",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.199,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277538,connected components classification;Hough transform;snippets decomposition,Text analysis,computer graphics;document image processing;handwriting recognition;history;Hough transforms;image classification;image enhancement;image segmentation;pattern clustering;text analysis,handwriting document layout analysis;snippet extraction;electronic edition;human science corpus;manuscript page structure;text line segmentation task;variable interline space;inconstant baseline skew;occlusion;text line detection;image enhancement;Hough transform;pattern clustering;snippet decomposition approach;content page classification;genetic taxonomy;digitized Flaubert manuscript,,8,,9,,2-Oct-09,,,IEEE,IEEE Conferences
Algorithms for Clustering Terms in Document Set Based on Fuzzy Neighborhoods,基於模糊鄰域的文檔集中術語聚類算法,S. Miyamoto; E. Kataoka,"Dept. of Risk Eng., Tsukuba Univ.; NA","The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.",20-Jun-05,2005,,,979,984,This paper describes similarity measures between two terms in a document set using the concept of a fuzzy neighborhood and algorithms for term clustering. Theoretical properties of neighborhood and similarity measures are studied. Agglomerative hierarchical as well as fuzzy/crisp c-means clustering algorithms are proposed. Examples of agglomerative and c-means clustering are given,1098-7584,0-7803-9159-4,10.1109/FUZZY.2005.1452527,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452527,,Clustering algorithms;Fuzzy sets;Electronic mail;Information retrieval;Engines;Web pages;Frequency measurement;Fuzzy systems,fuzzy set theory;pattern clustering;text analysis,document term clustering;fuzzy neighborhoods;similarity measures;agglomerative hierarchical clustering;fuzzy c-means clustering;crisp c-means clustering,,,,11,,20-Jun-05,,,IEEE,IEEE Conferences
An Improved KNN Text Categorization Method Based on Spanning Tree Documents Clustering,一種基於生成樹文檔聚類的改進的KNN文本分類方法,W. Zheng; G. Feng; Z. Nan,"Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China; NA; NA",2011 International Conference on Internet Technology and Applications,29-Aug-11,2011,,,1,5,"For the shortcoming that K-Nearest Neighbor(KNN) classification method is not efficient and it is difficult to determined the optimal parameter value K, a new KNN classification method based on spanning tree document clustering is presented. The basic idea is that using the clustering algorithm based on spanning tree to realize automatic clustering, each sub-tree generated retain a few core document nodes after a few nodes is cut and the core nodes retained have been merged into a new document. When the experiment of classification is carried out, the similarity of document test is computed with center document of sub-tree and the category of document test is the category of sub-tree that it has largest similarity. Experiments show that proposed method is better than KNN in stability of classification ,meanwhile it improve the classification speed and avoid the choice of value of the parameter k.",,978-1-4244-7255-0,10.1109/ITAP.2011.6006411,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006411,,Clustering algorithms;Text categorization;Boosting;Economics;Internet;Software;Government,pattern classification;pattern clustering;text analysis;trees (mathematics),spanning tree document clustering;KNN text categorization method;k-nearest neighbor classification method;automatic clustering;subtree;classification stability,,,,2,,29-Aug-11,,,IEEE,IEEE Conferences
Distributed document clustering analysis based on a hybrid method,基於混合方法的分佈式文檔聚類分析,J. E. Judith; J. Jayakumari,"Noorul Islam Centre for Higher Education, Kumaracoil, India.; Noorul Islam Centre for Higher Education, Kumaracoil, India.",China Communications,2-Mar-17,2017,14,2,131,142,Clustering is one of the recently challenging tasks since there is an ever-growing amount of data in scientific research and commercial applications. High quality and fast document clustering algorithms are in great demand to deal with large volume of data. The computational requirements for bringing such growing amount data to a central site for clustering are complex. The proposed algorithm uses optimal centroids for K-Means clustering based on Particle Swarm Optimization(PSO). PSO is used to take advantage of its global search ability to provide optimal centroids which aids in generating more compact clusters with improved accuracy. This proposed methodology utilizes Hadoop and MapReduce framework which provides distributed storage and analysis to support data intensive distributed applications. Experiments were performed on Reuter's and RCV1 document dataset which shows an improvement in accuracy with reduced execution time.,1673-5447,,10.1109/CC.2017.7868161,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868161,,Clustering algorithms;Algorithm design and analysis;Distributed databases;Text mining;Big data;Clustering methods,data handling;document handling;parallel processing;particle swarm optimisation;pattern clustering;search problems,distributed document clustering analysis;hybrid method;scientific research;commercial applications;computational requirements;optimal centroids;k-means clustering;particle swarm optimization;PSO;global search ability;Hadoop;MapReduce framework;data intensive distributed applications;RCV1 document dataset;Reuter document dataset,,3,,,,2-Mar-17,,,IEEE,IEEE Magazines
Fuzzy cluster descriptor extraction for flexible organization of documents,模糊聚類描述符提取可靈活組織文檔,T. M. Nogueira; S. O. Rezende; H. A. Camargo,"Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Institute of Mathematics and Computer Science, University of S瓊o Paulo - Brazil; Department of Computer Science, Federal University of S瓊o Carlos - Brazil",2011 11th International Conference on Hybrid Intelligent Systems (HIS),5-Jan-12,2011,,,528,533,"The text mining process and its set of techniques have been widely used in order to look for new knowledge in textual documents, which can be recovered by information retrieval systems. There is a variety of methods developed to automatically organize documents based on the knowledge extracted from their content. The management of imprecision and uncertainty is very important to improve these methods. Therefore, this work proposes a method to manage imprecision and uncertainty in document organization, by clustering the documents in fuzzy clusters and extracting cluster descriptors. The proposed method was evaluated and the obtained results showed that this is a promising approach to deal with the problem of imprecision and uncertainty when organizing textual documents.",,978-1-4577-2152-6,10.1109/HIS.2011.6122160,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122160,fuzzy clustering;text mining;document organization;information retrieval,Clustering algorithms;Organizations;Uncertainty;Semantics;Information retrieval;Keyboards;Hybrid intelligent systems,data mining;fuzzy set theory;information retrieval systems;pattern clustering;text analysis,fuzzy cluster descriptor extraction;flexible organization;text mining;textual document;information retrieval system;knowledge extraction;document organization;document clustering,,4,,21,,5-Jan-12,,,IEEE,IEEE Conferences
Document Classification with One-class Multiview Learning,一類多視圖學習的文檔分類,B. Chen; B. Li; Z. Pan; A. Feng,"Dept. of Comput., Yangzhou Univ. Yangzhou, Yangzhou, China; Dept. of Comput., Yangzhou Univ. Yangzhou, Yangzhou, China; NA; NA",2009 International Conference on Industrial and Information Systems,26-Jun-09,2009,,,289,292,"Recently, automatic document classification has attracted a lot of attentions due to the large quantity of web documents. Amongst, a special case is to distinguish whether a document belongs to a target class (directory) when only the documents of target class are given, which is a standard oneclass classification problem. Moreover, differed from other data, Web pages have intrinsic (text) and extrinsic(hyperlink) features. Thus they are very suitable for multiview learning. To tackle the task of one-class document classification, a multiview one-class classifier isproposed, it utilizes the one-cluster clustering based data description (OCCDD) as the base one-class classifier, then gets a one-class classifier in each view by setting a membership threshold, simultaneously, achieves the consensus of different views by a regularization term.Hereafter, different views boost each other, rather than ensemble the results independently or perform document recognition in single view case. We conduct the experiments on the standard WebKB dataset with OCCDD and the proposed multiview method. Experimental results show the good performance of the multiview method in terms of effectiveness and stability to parameter.",,978-0-7695-3618-7,10.1109/IIS.2009.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5116355,,Web pages;Object detection;Learning systems;Clustering algorithms;Labeling;Frequency;Sparse matrices;Aerospace industry;Computer industry;Information systems,document handling;Internet;pattern classification;pattern clustering,one-class multiview learning;automatic document classification;Web pages;one-cluster clustering based data description;document recognition,,,,12,,26-Jun-09,,,IEEE,IEEE Conferences
Retrieving Representative Structures from XML Documents Using Clustering Techniques,使用聚類技術從XML文檔中檢索代表結構,Y. Huang; P. Liou,"Dept. of Comput. Sci. & Inf. Eng., Nat. Yunlin Univ. of Sci. & Technol., Douliou, Taiwan; Inst. of Electron. & Inf. Eng., Nat. Yunlin Univ. of Sci. & Technol., Douliou, Taiwan",2011 European Intelligence and Security Informatics Conference,27-Oct-11,2011,,,332,339,"In the paper, we addressed the problem of finding the common structures in a collection of XML documents. Since an XML document can be represented as a tree structure, the problem how to cluster a collection of XML documents can be considered as how to cluster a collection of tree-structured documents. First, we used SOM (Self-Organizing Map) with the Jaccard coefficient to cluster XML documents. Then, an efficient sequential mining method called GST was applied to find maximum frequent sequences. Finally, we merged the maximum frequent sequences to produce the common structures in a cluster.",,978-1-4577-1464-1,10.1109/EISIC.2011.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061227,XML document;tree-structured;clustering;sequential pattern mining;common structure,XML;Merging;Clustering methods;Clustering algorithms;Electronic mail;Filtering;Data mining,information retrieval;pattern clustering;self-organising feature maps;tree data structures;XML,representative structure retrieval;XML documents;clustering techniques;tree structured documents;SOM;selforganizing map;Jaccard coefficient,,,,11,,27-Oct-11,,,IEEE,IEEE Conferences
PH-SSBM: Phrase Semantic Similarity Based Model for Document Clustering,PH-SSBM：基於短語語義相似度的文檔聚類模型,W. K. Gad; M. S. Kamel,"Dept. of Electr. & Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Dept. of Electr. & Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada",2009 Second International Symposium on Knowledge Acquisition and Modeling,28-Dec-09,2009,2,,197,200,"In this paper, a novel document representation model the phrases semantic similarity based model (PHSSBM), is proposed. This model combines phrases analysis as well as words analysis with the use of WordNet as background knowledge to explore better ways of documents representation for clustering. The PH-SSBM assigns semantic weights to both document words and phrases. The new weights reflect the semantic relatedness between documents terms and capture the semantic information in the documents. The PH-SSBM finds similarity between documents based on matching terms (phrases and words) and their semantic weights. Experimental results show that the phrases semantic similarity based model (PH-SSBM) in conjunction with WordNet has a promising performance improvement for text clustering.",,978-0-7695-3888-4,10.1109/KAM.2009.191,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362122,Clustering;Phrases-based analysis;semantic similarity,Ontologies;Knowledge acquisition;Performance evaluation;Clustering algorithms;Entropy;Fellows;Testing;Text mining;Frequency;Speech,text analysis;word processing,PH-SSBM;phrases semantic similarity based model;document clustering;document representation;WordNet;matching terms;text clustering,,2,,11,,28-Dec-09,,,IEEE,IEEE Conferences
Improving the Results and Performance of Clustering Bit-encoded XML Documents,改善對位編碼的XML文檔進行聚類的結果和性能,M. Kozielski,"Silesian University of Technology, Gliwice, Poland",Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06),15-Jan-07,2006,,,60,64,Clustering XML documents according to their structure is one of the techniques that may improve the effectiveness of XML documents storage and retrieval. One of existing approaches to this problem is to encode XML document structure as a string of bits and cluster such feature vectors. High dimensionality and sparseness of the feature vectors are the weaknesses of this method. The paper presents four methods reducing the dimensionality of the bit feature vectors. Two of these methods are novel. They are dedicated to XML documents and should be applied during the encoding process. The results showed good efficiency of these inner-encoding methods and their ability of improving clustering results in some cases. The methods presented in the paper are tested on two datasets of XML documents having different characteristics,2375-9259,0-7695-2792-2,10.1109/ICDMW.2006.97,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4063599,,XML;Encoding;Clustering algorithms;Data mining;Performance analysis;Algorithm design and analysis;Informatics;Testing;Database systems;Equations,pattern clustering;XML,XML document clustering;storage retrieval;inner-encoding;bit-encoded XML document,,2,,14,,15-Jan-07,,,IEEE,IEEE Conferences
Document clustering and cluster topic extraction in multilingual corpora,多語言語料庫中的文檔聚類和聚類主題提取,J. Silva; J. Mexia; A. Coelho; G. Lopes,"Univ. Nova de Lisboa, Lisbon, Portugal; NA; NA; NA",Proceedings 2001 IEEE International Conference on Data Mining,7-Aug-02,2001,,,513,520,"A statistics-based approach for clustering documents and for extracting cluster topics is described relevant (meaningful) expressions (REs) automatically extracted from corpora are used as clustering base features. These features are transformed and its number is strongly reduced in order to obtain a small set of document classification features. This is achieved on the basis of principal components analysis. Model-based clustering analysis finds the best number of clusters. Then, the most important REs are extracted from each cluster and taken as document cluster topics.",,0-7695-1119-8,10.1109/ICDM.2001.989559,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989559,,Size measurement;Data mining;Feature extraction;Instruction sets;Organizing;Agriculture;Probability;Dispersion,pattern clustering;document handling;data mining,document clustering;cluster topic extraction;multilingual corpora;statistics-based approach;relevant expressions;document classification features;principal components analysis;model-based clustering analysis,,3,,9,,7-Aug-02,,,IEEE,IEEE Conferences
Robust scheme to protect authentication code of message/image documents in cloud computing,在雲計算中保護消息/圖像文檔身份驗證代碼的可靠方案,Z. A. Abduljabbar; H. Jin; A. A. Yassin; Z. A. Hussien; M. A. Hussain; S. H. Abbdal; D. Zou,"Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; University of Basrah, Basrah, Iraq; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China","2016 International Conference on Computing, Networking and Communications (ICNC)",24-Mar-16,2016,,,1,5,"A number of image/message document authentication and integrity schemes have been conducted to recognize any modification in the exchange of documents between two entities (sender and receiver) within a cloud environment. Existing solutions are based on combining key-based hash function with traditional factors (steganography, smart-card, timestamp). However, none of the proposed schemes appear to be sufficiently designed as a secure scheme to prevent common forms of attack such as replay, forgery, stolen verifier, brute force, and insider attacks. In this paper, we propose a scheme to ensure message/image document integrity for each user's login by providing one-time biometric message/image authentication code called MACLESS, which is a summation of combining the key-based hash function (MAC-SHA-1) of a message/image document and the one-time bio-key. Thereafter, MACLESS is hidden in a cover image based steganography anonymity. The proposed scheme has several important security attributes, such as phase key agreement, users' one-time bio-key, and one-time authentication code is valid only for one user's login session. Finally, security analysis and experimental results demonstrate and prove the invulnerability and efficiency of the proposed scheme.",,978-1-4673-8579-4,10.1109/ICCNC.2016.7440585,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440585,Cloud Computing;MACLESS;One-Time Bio-key;One-Time Message/Image Document Authentication Code,Feature extraction;Discrete wavelet transforms;Biomedical imaging;Authentication;Cryptography;Receivers,biometrics (access control);cloud computing;document image processing;image coding;smart cards;steganography,robust scheme;message/image document authentication code protection;cloud computing;image/message document integrity scheme;cloud environment;key-based hash function;smart-card;timestamp;one-time biometric message/image authentication code;MACLESS;MAC-SHA-1;one-time bio-key;cover image based steganography anonymity;security analysis,,1,,16,,24-Mar-16,,,IEEE,IEEE Conferences
Document clustering using mixture model of von Mises-Fisher distributions on document manifold,使用文件流形上的von Mises-Fisher分佈的混合模型進行文件聚類,N. K. Anh; N. T. Tam; N. Van Linh,Hanoi University of Science and Technology; Hanoi University of Science and Technology; Hanoi University of Science and Technology,2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR),5-Mar-15,2013,,,140,145,"Document clustering has become an increasingly important technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering. The generative model for document clustering based on the von Mises-Fisher (vMF) distribution generally produces better clustering results than other generative models. However, in fact, it is more natural and reasonable to assume that the document space is a manifold and the probability distribution that generates the data is supported on a document manifold. In this paper, we propose a regularized probabilistic model based on manifold structure for data clustering, called Laplacian regularized vMF Mixture Model (LapvMFs), which explicitly considers the manifold structure. We have developed a generalized mean-field variational inference algorithm for the LapvMFs. Extensive experimental results on a large number of high dimensional text datasets demonstrate that our approach outperforms the three state-of-the-art clustering algorithms.",,978-1-4799-3400-3,10.1109/SOCPAR.2013.7054116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054116,Probabilistic graphical model;variational inference;clustering;graph laplacian;manifold,Manifolds;Data models;Clustering algorithms;Laplace equations;Vectors;Equations;Mathematical model,data mining;mixture models;pattern clustering;statistical distributions;text analysis,text mining;probability distribution;document manifold;LapvMF;Laplacian regularized vMF mixture model;von Mises-Fisher distribution;document clustering,,2,,10,,5-Mar-15,,,IEEE,IEEE Conferences
Textual Document Clustering in Traditional and Modern Approaches (Review),傳統和現代方法中的文本文檔聚類（綜述）,W. M. S. Yafooz; Z. A. Bakar; A. M. Mithun,"Faculty of Computer and Information Technology, Al-Madinah International University, Malaysia; Faculty of Computer and Information Technology, Al-Madinah International University, Malaysia; Faculty of Computer and Information Technology, Al-Madinah International University, Malaysia","2018 IEEE Conference on Systems, Process and Control (ICSPC)",2-May-19,2018,,,159,164,"An enormous quantity of textual documents is created from the advanced technological use concerning describing, intelligence, interconnection, and thousands of distinct authorizations and was expanding each moment of quotidian circumstances. The Clustering is an automated established process to organize the database on features. Outwardly implementing a clustering technique to textual data, a huge quantity of unstructured data is losing the capability of sharing knowledge. There are many tools and techniques proposed. This paper present and categorized the textual document clustering algorithms (approaches) into two types are classical and modern approaches. Both approaches are implemented to those textual data to obtain and consolidate knowledge from discharged to an extraordinary impression of a prepared document. The two important factors in clustering process are speed of clustering process and accuracy or purity of data clusters. This review paper can be benefits to many researchers who concern on textual document clustering, text mining and data scientist.",,978-1-5386-6327-1,10.1109/SPC.2018.8704130,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704130,Clustering;Textual Document;Frequent Term;Named Entity recognition,Clustering algorithms;Partitioning algorithms;Classification algorithms;Databases;Clustering methods;Conferences;Process control,data mining;pattern clustering;text analysis,automated established process;textual data;unstructured data;textual document clustering algorithms;data clusters;data scientist;text mining,,,,45,,2-May-19,,,IEEE,IEEE Conferences
Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization,使用多視圖非負矩陣分解基於藥物/症狀名稱的臨床文件聚類,Y. Ling; X. Pan; G. Li*; X. Hu,"College of Computing & Informatics, Drexel University, Philadelphia, PA, USA; School of Information Management, Nanjing University, Nanjing, China; School of Business, Hunan University, Changsha, eliguangrong@163.com; College of Computing & Informatics, Drexel University, Philadelphia, PA, USA",IEEE Transactions on NanoBioscience,5-Aug-15,2015,14,5,500,504,"Clinical documents are rich free-text data sources containing valuable medication and symptom information, which have a great potential to improve health care. In this paper, we build an integrating system for extracting medication names and symptom names from clinical notes. Then we apply nonnegative matrix factorization (NMF) and multi-view NMF to cluster clinical notes into meaningful clusters based on sample-feature matrices. Our experimental results show that multi-view NMF is a preferable method for clinical document clustering. Moreover, we find that using extracted medication/symptom names to cluster clinical documents outperforms just using words.",1558-2639,,10.1109/TNB.2015.2422612,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111340,Clinical document;clinical notes;document clustering;multi-view;nonnegative matrix factorization,Medical diagnostic imaging;Diseases;Accuracy;Hospitals;Informatics;Design automation;Data mining,document handling;matrix decomposition;medical information systems,medication names;symptom names;multiview nonnegative matrix factorization;sample-feature matrices;clinical document clustering,"Algorithms;Cluster Analysis;Data Mining;Diagnosis, Computer-Assisted;Drug Therapy;Humans;Medical Records;Models, Statistical;Natural Language Processing",11,,27,,21-May-15,,,IEEE,IEEE Journals
Secured distributed document clustering & keyphrase extraction algorithm in structured Peer to Peer networks,結構化對等網絡中的安全分佈式文檔聚類和關鍵詞提取算法,V. V. Nair; J. E. Judith; J. Jayakumari,"Department of Computer Science & Engineering, Noorul Islam Centre for Higher Education, Kumaracoil, TamilNadu, India; Department of Computer Science & Engineering, Noorul Islam Centre for Higher Education, Kumaracoil, TamilNadu, India; Department of Electronics & Engineering, Noorul Islam Centre for Higher Education, Kumaracoil, TamilNadu, India","2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies",22-Sep-11,2011,,,151,156,"A secured Hierarchically Distributed Peer-to-Peer (HDP2PC) architecture and Clustering algorithm is used to overcome the scalability problem in structured peer to peer networks. It is possible to incorporate any number of layers of nodes. The architecture is based on a multilayer overlay network of peer neighbourhoods. Supernodes, which act as representatives of neighbourhoods, are iteratively grouped to form higher level neighbourhoods. Within a certain level of the hierarchy, peers cooperate within their respective neighbourhoods to perform P2P clustering. A novel approach is proposed while indexing the documents in to various nodes arranged in hierarchy. A hashing mechanism is used to index the documents. A number of filters are applied as parameters thereby reducing the number of comparisons required to extract keyphrases. Distributed key phrase extraction algorithm is used to extract patterns by interpreting clusters stored in the neighbour workstations. The query can be applied for loosely structured format also. Speedup is provided by manipulating the neighbourhood size and height parameters. Privacy is also provided to data inside the peers. No data is shared between the peer nodes. Security can be enforced in the peers while clustering is performed.",,978-1-61284-653-8,10.1109/ICSCCN.2011.6024533,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024533,keyphrases;associativity;hierarchical P2P network;distributed document clustering;supernodes,Peer to peer computing;Clustering algorithms;Signal processing algorithms;Data mining;Computer architecture;Data models;Approximation algorithms,cryptography;data mining;document handling;file organisation;peer-to-peer computing,secured distributed document clustering;keyphrase extraction algorithm;structured peer to peer networks;secured hierarchically distributed peer-to-peer architecture;HDP2PC architecture;clustering algorithm;multilayer overlay network;supernodes;P2P clustering;document indexing;hashing mechanism;pattern extract;loosely structured format,,1,,13,,22-Sep-11,,,IEEE,IEEE Conferences
Correlation clustering based on genetic algorithm for documents clustering,基於遺傳算法的關聯聚類文檔聚類,Zhenya Zhang; Hongmei Cheng; Wanli Chen; Shuguang Zhang; Qiansheng Fang,"School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China; Management Engineering Department of AIAI, Hefei 230022, China; School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China; Department of Statistics & Finance, University of Science and Technology of China, China; School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China",2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence),23-Sep-08,2008,,,3193,3198,"Correlation clustering problem is a NP hard problem and technologies for the solving of correlation clustering problem can be used to cluster given data set with relation matrix for data in the given data set. In this paper, an approach based on genetic algorithm for correlation clustering problem, named as GeneticCC, is presented. To estimate the performance of a clustering division, data correlation based clustering precision is defined and features of clustering precision are discussed in this paper. Experimental results show that the performance of clustering division for UCI document data set constructed by GeneticCC is better than clustering performance of other clustering divisions constructed by SOM neural network with clustering precision as criterion.",1941-0026,978-1-4244-1822-0,10.1109/CEC.2008.4631230,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631230,,Correlation;Clustering algorithms;Genetic algorithms;Complexity theory;Catalogs;Artificial neural networks;Mathematical model,computational complexity;document handling;genetic algorithms;pattern clustering;self-organising feature maps;statistical analysis,correlation clustering;genetic algorithm;documents clustering;NP hard problem;GeneticCC;clustering division;data correlation;SOM neural network,,4,,19,,23-Sep-08,,,IEEE,IEEE Conferences
An analysis of document clustering algorithms,文檔聚類算法分析,V. Mary Amala Bai; D. Manimegalai,"Department of Information Technology Noorul Islam University Kumaracoil, India; Department of Information Technology National Engineering College Kovilpatti, India",2010 INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES,17-Dec-10,2010,,,402,406,"Document clustering organizes documents into groups such that each group contains documents with similar content. This paper presents the results of an experimental study of some common document clustering techniques. In particular, comparison of Euclidean K-means (K-Means), Spherical K-means(SK-Means) and unsupervised Principal Direction Divisive Partitioning (PDDP) algorithms is done. A comparative analysis of the algorithms is performed using the evaluation measures, Entropy and F-measure. The experiments were conducted on the standard dataset. Clustering algorithms such as K-means and SK-means are easy to implement but their answers strongly depend on their initialization. PDDP is comparatively difficult to implement since it is a hierarchical algorithm. On the other hand its performance does not depend on initial clusters. The results indicate that for certain initial clusters, the K-means and SK-means performed well than PDDP. When there are equal numbers of documents in all the classes, the clusters produced by the algorithms were very effective to that of when different classes had different number of documents. Also with no stop word removal the quality of PDDP degraded compared to K-Means.",,978-1-4244-7770-8,10.1109/ICCCCT.2010.5670585,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670585,Clustering;partitional;hierarchical;term document matrix;singular value decomposition,Entropy;Marine vehicles;Equations;Magnetic resonance imaging;Writing;MATLAB,document handling;pattern clustering;singular value decomposition,document clustering algorithm;euclidean K-mean algorithm;spherical K-mean algorithm;unsupervised principal direction divisive partitioning;singular value decomposition;term document matrix,,3,,16,,17-Dec-10,,,IEEE,IEEE Conferences
Combination of Structural and Factual Descriptors for Document Stream Segmentation,結構和事實描述符的組合，用於文檔流分割,R. Karpinski; A. Bela簿d,"LORIA, Univ. de Lorraine, Vandoeuvre-Les-Nancy, France; LORIA, Univ. de Lorraine, Vandoeuvre-Les-Nancy, France",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,221,226,"This paper extends a previous work being done by [4]. Having no information about the document separation in the flow, the system operates progressively by examining successive pairs of pages looking for continuity or rupture descriptors. Four document levels have been introduced to better extract those descriptors and reduce the ambiguity in their extraction: records, technical documents, fundamental documents and cases. At each level, structural and factual descriptors are first extracted and then compared between pairs of pages or documents. To reinforce the descriptor interest and focus the system on equivalent descriptors in the pairs, the descriptors are accompanied by their context. The extraction of the context is facilitated by the determination of the physical and logical structure in the pages. Contextual rules based on these descriptors are employed for the determination of either a continuity, a rupture or an uncertainty between the pairs. To overcome the problem of information emptiness in the current page, a logbook is used to gather the descriptors in all the previous pages of the record and a buffer allows to delay the comparison. These latter points were added to the previous work that widely reinforce the current system increasing its precision of more than 6%.",,978-1-5090-1792-8,10.1109/DAS.2016.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490121,Document stream segmentation;Structural and factual descriptors;Continuity;Rupture,Context;Clustering algorithms;Heuristic algorithms;Companies;Optical character recognition software;Uncertainty;Markov processes,database management systems;document handling;document image processing,structural descriptor;factual descriptor;document stream segmentation;document separation;document levels;record extraction;technical document extraction;fundamental document extraction;case extraction;structural descriptor extraction;factual descriptor extraction;physical structure;logical structure;contextual rules;information emptiness problem;logbook,,3,,13,,13-Jun-16,,,IEEE,IEEE Conferences
Clustering of XML documents based on structure and aggregated content,基於結構和聚合內容的XML文檔聚類,N. G. Rezk; A. Sarhan; A. Algergawy,"Electrical Engineering Dept., Faculty of Engineering, Kafr Elshiekh Univ., Egypt; Computer and Control Engineering Dept., Faculty of Engineering, Tanta Univ., Egypt; Institute of Computer Science, Friedrich Schiller University of Jena, Germany",2016 11th International Conference on Computer Engineering & Systems (ICCES),19-Jan-17,2016,,,93,102,"The main objective of the work is to improve the clustering efficiency and performance when we deal with very big datasets. This paper aims to improve the quality of XML data clustering by exploiting more features extracted from source schemas. In particular, it proposes clustering approach that gathers both content and structure of XML documents to determine similarity between them. The content and structure information are concluded using two different similarity methods that are then grouped via weight factor to compute the overall document similarity. The structural similarity of XML data are derived from edge summaries while content features similarity are derived from aggregate of set of similarity measures; Jaccard, Cosine measure and Jensen-Shannon divergence in one algorithm. However, we also experimented using Jaccard distance as content measure with edge summaries to prove that using an aggregation of content similarity measures can further improve the results. The experiments prove that clustering of XML documents based on structure only information produce worse solution in homogenous environment, while in heterogeneous environment clustering of XML document produce better result when the structure and the content are combined. Results have shown that performance and quality of the proposed approach is better in comparison of both XEdge and XCLSC approaches.",,978-1-5090-3267-9,10.1109/ICCES.2016.7821981,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821981,XML;aggregated content;structure similarity;content similarity;clustering,XML;Clustering algorithms;Feature extraction;Data mining;Atmospheric measurements;Particle measurements;Weight measurement,Big Data;document handling;feature extraction;pattern clustering;XML,Big Datasets;XML data clustering quality;feature extraction;content information;structure information;weight factor;document similarity;XML data structural similarity;edge summaries;content feature similarity;Jensen-Shannon divergence;Cosine measure;Jaccard distance;content measure;homogenous environment;XML document clustering;XEdge approach;XCLSC approach,,,,55,,19-Jan-17,,,IEEE,IEEE Conferences
A study on criteria for extracting key terms in document clustering,關於文檔聚類中關鍵術語提取標準的研究,Jie Ji; Qiangfu Zhao; Ryouhei Shindo; Yousuke Kunishi,"The University of Aizu, Aizuwakamatsu, Fukushima, Japan; The University of Aizu, Aizuwakamatsu, Fukushima, Japan; The University of Aizu, Aizuwakamatsu, Fukushima, Japan; Shin-Etsu Polymer Co., Ltd., Saitama, Japan","2008 IEEE International Conference on Systems, Man and Cybernetics",7-Apr-09,2008,,,3674,3679,"Document clustering is the process to partition a set of unlabelled documents into some clusters. To analyze the documents efficiently and effectively, it is expected that all documents in each cluster have some shared concept. The shared concept is most conveniently represented using some key terms. Many methods have been studied for selecting important key terms. However, most of them belong to the category of supervised learning. That is, the teacher signals must be provided in advance in order to measure the importance of the key terms. In this paper, we study un-supervised learning only. Specifically, we study three criteria for extracting important key terms through clustering. The first one is the mean squared error (MSE) function. It is well known that clusters obtained based on MSE are good in the sense that all documents in each cluster are similar. In addition to MSE, we introduce two new criteria. Both criteria encourage each cluster to use a different set of key terms. Experimental results with three databases show that MSE, although simple, is surprisingly good for generating representative key terms. One advantage of the proposed criteria is that they can generate more balanced clusters.",1062-922X,978-1-4244-2383-5,10.1109/ICSMC.2008.4811870,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811870,Document clustering;key term extraction;k-means;criterion function,Supervised learning;Databases;Flowcharts;Polymers;Unsupervised learning;Virtual manufacturing;Frequency,document handling;least mean squares methods;pattern clustering;unsupervised learning,document clustering;unlabelled documents;unsupervised learning;mean squared error function;key term extraction,,,,10,,7-Apr-09,,,IEEE,IEEE Conferences
Biomedical Ontology MeSH Improves Document Clustering Qualify on MEDLINE Articles: A Comparison Study,生物醫學本體論MeSH改善MEDLINE文章的文檔聚類資格：一項比較研究,Illhoi Yoo; Xiaohua Hu,"Drexel University, USA; Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA",19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06),5-Jul-06,2006,,,577,582,"Document clustering has been used for better document retrieval, document browsing, and text mining. In this paper, we investigate if biomedical ontology MeSH improves the clustering quality for MEDLINE articles. For this investigation, we perform a comprehensive comparison study of various document clustering approaches such as hierarchical clustering methods (single-link, complete-link, and complete link), bisecting K-means, K-means, and suffix tree clustering (STC) in terms of efficiency, effectiveness, and scalability. According to our experiment results, biomedical ontology MeSH significantly enhances clustering quality on biomedical documents. In addition, our results show that decent document clustering approaches, such as bisecting K-means, K-means and STC, gains some benefit from MeSH ontology while hierarchical algorithms showing the poorest clustering quality do not reap the benefit of MeSH ontology",1063-7125,978-0-7695-2517-4,10.1109/CBMS.2006.62,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647632,,Ontologies;Clustering algorithms;Partitioning algorithms;Iterative algorithms;Educational institutions;Information science;Information retrieval;Text mining;Clustering methods;Scalability,bibliographic systems;document handling;medical computing;ontologies (artificial intelligence);pattern clustering,biomedical ontology MeSH;document clustering;MEDLINE articles;hierarchical clustering methods;bisecting K-means;suffix tree clustering,,3,,26,,5-Jul-06,,,IEEE,IEEE Conferences
A model of document clustering using ant colony algorithm and validity index,基於蟻群算法和有效性指標的文檔聚類模型,Yan Yang; Mohamed Kamel; Fan Jin,"Sch. of Comput. & Commun. Eng., Southwest Jiaotong Univ., Chengdu, China; NA; NA","Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",27-Dec-05,2005,5,,2730,2735 vol. 5,"This paper discusses document clustering using ant colony algorithm and validity index. Clusterings are formed on the plane by ants walking, picking up or dropping down projected document vectors with different probability. The proposed model uses a clustering validity index not only to evaluate the performance of the algorithm, but also to find the optimal number of clusters and reduce outliers. Experiments on data from the Reuters-21578 collection show that the proposed model has better performance than that of LF algorithm and ART neural networks.",2161-4407,0-7803-9048-2,10.1109/IJCNN.2005.1556357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556357,,Clustering algorithms;Subspace constraints;Neural networks;Frequency;Text analysis;Indexing;Legged locomotion;Explosives;Web sites;Web search,document handling,document clustering;ant colony algorithm;clustering validity index;algorithm performance evaluation,,1,,16,,27-Dec-05,,,IEEE,IEEE Conferences
Clustering of Short Commercial Documents for the Web,Web的簡短商業文檔的聚類,M. Carullo; E. Binaghi; I. Gallo; N. Lamberti,"Universit? degli Studi dell'Insubria, Dipartimento di Informatica e Comunicazione, 21100 Varese, Italy; Universit? degli Studi dell'Insubria, Dipartimento di Informatica e Comunicazione, 21100 Varese, Italy; Universit? degli Studi dell'Insubria, Dipartimento di Informatica e Comunicazione, 21100 Varese, Italy; pixel srl, 20082 Binasco, Italy",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"Document clustering techniques have been applied in several areas, with the Web as one of the most recent and influent. Both general-purpose and text-oriented techniques exist and can be used to cluster a collection of documents in many ways. In this work we propose an online, single-pass document clustering model that can be combined with a variety of text-oriented similarity measures. An experimental evaluation of the proposed model was conducted in the e-commerce domain. Performances were measured using a clustering-oriented metric based on F-Measure and compared with those obtained by other well-known approaches.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761554,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761554,,Clustering algorithms;Performance evaluation;Encoding;Internet;Web search;Electronic commerce;Text analysis;Algorithm design and analysis;Clustering methods;Particle measurements,Internet;pattern clustering;text analysis,single-pass document clustering;text-oriented technique;general-purpose technique;Web search,,1,,18,,23-Jan-09,,,IEEE,IEEE Conferences
Clustering documents using tagging communities and semantic proximity,使用標記社區和語義相似性對文檔進行聚類,E. Cunha; ?. Figueira; ?. Mealha,"CRACS&INESC TEC, Porto, Portugal; CRACS&INESC TEC, Porto, Portugal; CETAC MEDIA, Porto, Portugal",2013 8th Iberian Conference on Information Systems and Technologies (CISTI),17-Oct-13,2013,,,1,6,"Euclidean distance and cosine similarity are frequently used measures to implement the k-means clustering algorithm. The cosine similarity is widely used because of it's independence from document length, allowing the identification of patterns, more specifically, two documents can be seen as identical if they share the same words but have different frequencies. However, during each clustering iteration new centroids are still computed following Euclidean distance. Based on a consideration of these two measures we propose the k-Communities clustering algorithm (k-C) which changes the computing of new centroids when using cosine similarity. It begins by selecting the seeds considering a network of tags where a community detection algorithm has been implemented. Each seed is the document which has the greater degree inside its community. The experimental results found through implementing external evaluation measures show that the k-C algorithm is more effective than both the k-means and k-means++. Besides, we implemented all the external evaluation measures, using both a manual and an automatic ?Ground Truth?? and the results show a great correlation which is a strong indicator that it is possible to perform tests with this kind of measures even if the dataset structure is unknown.",2166-0727,978-989-98434-0-0,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615753,clustering;effectiveness;k-means;k-Communities;communitie detection;tagging;cosine similarity,Clustering algorithms;Communities;Partitioning algorithms;Algorithm design and analysis;Euclidean distance;Indexes;Classification algorithms,document handling;pattern clustering,k-C algorithm;k-communities clustering algorithm;document length;k-means clustering algorithm;cosine similarity;Euclidean distance;semantic proximity;tagging communities;document clustering,,,,11,,17-Oct-13,,,IEEE,IEEE Conferences
Document Images Indexing with Relevance Feedback: An Application to Industrial Context,具有相關性反饋的文檔圖像索引：在工業環境中的應用,O. Augereau; N. Journet; J. -. Domenger,"Lab. Bordelais de Rech. en Inf. (LaBRI), Univ. de Bordeaux, Talence, France; Lab. Bordelais de Rech. en Inf. (LaBRI), Univ. de Bordeaux, Talence, France; Lab. Bordelais de Rech. en Inf. (LaBRI), Univ. de Bordeaux, Talence, France",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1190,1194,"This article presents a new method to index document images. This work is done in an industrial context where thousands of document images are daily digitized, these images have to be sorted in different classes like payroll, various bills, information letters. We propose a software method which aims to accelerate this task. Usually, the number of document classes is a priori unknown. In this paper, we propose an automatic estimation of this class number. According to this class number, we use a clustering algorithm in order to group document images. After this step, we propose an assisted classification tool based on content based image retrieval method (CBIR). For each cluster, a reference image is automatically selected then considering a similarity measure, the other images are sorted and shown to the user. By interacting with the process, the user can reject wrong images. The user feedback is automatically taken into account to enhance the similarity measure by weighting each feature. The first tests show that, on average, databases are indexed 3 times faster with our assisted classification method than with a standard manual classification process.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.240,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065498,document clustering;document retrieval;feature selection;relevance feedback;industrial application,Labeling;Accuracy;Companies;Indexing;Estimation;Humans,content-based retrieval;document image processing;image classification;image retrieval;pattern clustering;visual databases,document images indexing;relevance feedback;industrial context application;software method;automatic estimation;content based image retrieval method;CBIR;pattern clustering;pattern classification,,2,,13,,3-Nov-11,,,IEEE,IEEE Conferences
Clustering of web search results based on an Iterative Fuzzy C-means Algorithm and Bayesian Information Criterion,基於迭代模糊C-均值算法和貝葉斯信息準則的網頁搜索結果聚類,C. Cobos; M. Mendoza; M. Manic; E. Le籀n; E. Herrera-Viedma,"Computer Science Department, Universidad del Cauca, Popay獺n, Colombia; Computer Science Department, Universidad del Cauca, Popay獺n, Colombia; Department of Computer Science, University of Idaho at Idaho Falls, U.S.A.; Systems and Industrial Department, Universidad Nacional de Colombia, Bogot獺, Colombia; Department of Computer Science and Artificial Intelligence, University of Granada, Spain",2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS),26-Sep-13,2013,,,507,512,"The clustering of web search has become a very interesting research area among academic and scientific communities involved in information retrieval. Clustering of web search result systems, also called Web Clustering Engines, seek to increase the coverage of documents presented for the user to review, while reducing the time spent reviewing them. Several algorithms for web document clustering already exist, but results show there is room for more to be done. This paper introduces a new description-centric algorithm for clustering of web results called IFCWR. IFCWR initially selects a maximum estimated number of clusters using Forgy's strategy, then it iteratively merges clusters until results cannot be improved. Every merge operation implies the execution of Fuzzy C-Means for clustering results of web search and the calculus of Bayesian Information Criterion for automatically evaluating the best solution and number of clusters. IFCWR was compared against other established web document clustering algorithms, among them: Suffix Tree Clustering and Lingo. Comparison was executed on AMBIENT and MORESQUE datasets, using precision, recall, f-measure, SSLk and other metrics. Results show a considerable improvement in clustering quality and performance.",,978-1-4799-0348-1,10.1109/IFSA-NAFIPS.2013.6608452,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608452,web document clustering;fuzzy c-means;bayesian information criterion,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Web search;Bayes methods;Accuracy;Educational institutions,belief networks;document handling;Internet;iterative methods;pattern clustering,Web search results clustering;iterative fuzzy C-means algorithm;Bayesian information criterion;Web clustering engines;Web document clustering;description-centric algorithm;IFCWR;merge operation;suffix tree clustering;Lingo;MORESQUE dataset;AMBIENT dataset;precision metric;recall metric;f-measure metric;SSLk metric;clustering quality;clustering performance,,2,,28,,26-Sep-13,,,IEEE,IEEE Conferences
A New Approach and Compressive Survey on Restructuring User Search Results by Using Feedback Session,利用反饋會話重構用戶搜索結果的新方法與壓縮調查,S. S. Bhaskar; B. Tidke,"Dept. of Comput. Networking, Flora Inst. of Technol., Pune, India; Dept. of Comput. Networking, Flora Inst. of Technol., Pune, India",2015 International Conference on Computing Communication Control and Automation,16-Jul-15,2015,,,479,484,To improve web search engine relevance and user's satisfaction of results it is very credential to infer user search goal. User search goals can be improved by using user click-through for a single query. In this Technique a new approach is used to improve inferring user search goals. User search goals can be analyzed by using feedback session which is derived from click-thorough.by using feedback session user's search goal text is analyzed which helps to analyze search goal. In this approach firstly feedback session for a single query is generated from a user search log. By which pseudo-documents are crated this represents user's search goal text. A novel approach Pattern Matrix consists of documents and patterns as input is calculated. And by which clustering of documents is made. In this technique a new a Clustering method called as semantic clustering is used to cluster documents. And by using TF*IDF matrix user search goal text are analyzed. And lastly experiments are made to prove effectiveness of algorithm.,,978-1-4799-6892-3,10.1109/ICCUBEA.2015.217,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155893,feedback sessions;pseudo documents;Pattern Matrix;click-throughs;cosine similarity;clustering,Search engines;Clustering algorithms;Web search;Semantics;Uniform resource locators;Portable computers;Engines,document handling;Internet;matrix algebra;pattern clustering;query processing;search engines,user search restructuring;Web search engine relevance;user satisfaction;single query;user click-through;feedback session user search goal text;user search log;pattern matrix approach;document clustering method;semantic clustering;TF*IDF matrix user search goal text analysis,,1,,9,,16-Jul-15,,,IEEE,IEEE Conferences
Active document layout synthesis,活動文檔佈局綜合,X. Lin,"Hewlett-Packard Lab., Palo Alto, CA, USA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,86,90 Vol. 1,"Document layout analysis has been researched for many years. However, there is little work on the reverse of document layout analysis: document layout synthesis, whose goal is to generate logically correct and aesthetically appealing layout given the text/image contents and the flexible layout template. This paper introduces a new automatic document layout synthesis method, which can actively pursue the optimal text block height-width tradeoff simultaneously with the block position adjustment. The key idea is to use a cluster of linear functions to approximate the nonlinear height-width curve. Then two-pass simplex algorithm is used to solve the layout synthesis problem. This method has a wide range of applications, such as variable data printing (VDP), automatic table formatting, and automating document layout ground truth generation.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575515,,Text analysis;Image analysis;Graphics;Printing;Optical character recognition software;Layout;Rendering (computer graphics);Engines;Printers;Cameras,document image processing;pattern clustering,active document layout synthesis;document layout analysis;block position adjustment;nonlinear height-width curve;two-pass simplex algorithm;variable data printing;automatic table formatting,,3,,10,,16-Jan-06,,,IEEE,IEEE Conferences
Finding Good XML Fragments Based on k-Medoid Cluster Number Optimization and Ranking Model for Feedback,基於k-Medoid簇數優化和排序模型的反饋找到好的XML片段,Z. Minjuan,NA,2013 International Conference on Information Technology and Applications,11-Jan-14,2013,,,333,337,"Due to low quality feedback set, traditional pseudo relevance feedback may bring into topic drift. This paper studies how to identify or find good xml documents(fragments) for feedback. We propose an effective method, in which xml element search results clustering is performed firstly by k-mediod cluster number optimization, and then those fragments with high relevant to the query are identified and ranked in the top position by ranking model. The final experimental results show that the proposed approach produces better performance and achieves high quality feedback set.",,978-1-4799-2877-4,10.1109/ITA.2013.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6709999,Pseudo Relevance Feedback;XML feedback fragment;cluster number optimization;ranking model,XML;Optimization;Clustering algorithms;Mathematical model;Information retrieval;Frequency measurement;Presses,document handling;pattern clustering;relevance feedback;XML,high quality feedback set;XML element search results clustering;XML documents;topic drift;pseudo relevance feedback;low quality feedback set;ranking model;k-medoid cluster number optimization;XML fragments,,1,,9,,11-Jan-14,,,IEEE,IEEE Conferences
Class clustering with ant colony rank optimization (CCACRO) for data categorization,使用蟻群排名優化（CCACRO）進行類聚類以進行數據分類,D. Kourav; A. Khilrani; R. Nigam,"CSE, TIT & Science Bhopal, India; AP, Department of CSE, TIT & Science Bhopal, India; Department of CSE, TIT & Science Bhopal, India",2015 International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT),21-Apr-16,2015,,,201,206,Self-Organizing Map (SOM) is in the more prominent concern nowadays because of the chain of importance making in document association. It will be better for those issues where we required bunching and representation. This paper provides an efficient direction for better clustering and classification techniques which will be significant in archive association. In this paper an efficient Class Clustering with Ant Colony Rank Optimization (CCACRO) for Data Categorization have been proposed. In this method first the data is classified and parsed numerically and then alphabetical parsing is done so that grammatical terms have been removed for proper data filtration. Ant Colony Optimization have then applied for finding the threshold ranking for finding the optimize rank in the similar class group. For experimentation we have connected our methodology on distinctive information and accomplish better results in correlation to the past strategies.,,978-1-4673-9223-5,10.1109/ICATCCT.2015.7456882,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456882,Ant Colony Optimization (ACO);Clustering;Parser;data categorization,Communications technology;Ant colony optimization;Clustering algorithms;Optimization;Information filters;Particle swarm optimization,ant colony optimisation;document handling;grammars;pattern classification;pattern clustering;self-organising feature maps,class clustering-with-ant colony rank optimization;CCACRO;data categorization;self-organizing map;SOM;document association;clustering technique;classification technique;archive association;data classification;alphabetical parsing;numerical parsing;grammatical terms;data filtration;threshold ranking;rank optimization;similar class group,,1,,36,,21-Apr-16,,,IEEE,IEEE Conferences
Hierarchical clustering model for pixel-based classification of document images,基於像素的文檔圖像分類的層次聚類模型,R. Vieux; J. Domenger,"Univ. Bordeaux, LaBRI, UMR5800, F-33400 Talence, France; Univ. Bordeaux, LaBRI, UMR5800, F-33400 Talence, France",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,290,293,"We propose a method to learn and classify pixels in document images, e.g., to separate text from illustrations or other predefined classes. We extract texture information using a bank of Gabor filters, and learn a hierarchical clustering model that can be used as a K-Nearest Neighbours (KNN) classifier. The model has advantages over other local document image classification methods, making it efficient for real industrial applications: we do not rely on the accuracy of preprocessing steps such as binarisation or segmentation, the model can be efficiently trained using zone level annotations and it seamlessly supports multi-class classification. We demonstrate the performance of the method on a public dataset containing complex documents from magazines and technical journals.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460129,,Training;Image segmentation;Accuracy;Vectors;Data models;Optical character recognition software;Graphics,document image processing;feature extraction;Gabor filters;image classification;image texture;pattern clustering,hierarchical clustering model;pixel-based classification;document image;pixel classification;text separation;texture information extraction;Gabor filter;K-nearest neighbours classifier;KNN classifier;industrial application;zone level annotation;multiclass classification,,1,,11,,14-Feb-13,,,IEEE,IEEE Conferences
A Genetic-Frog Leaping Algorithm for Large Dataset Document Clustering,大數據集文檔聚類的遺傳蛙跳算法,L. Alhenaki; M. Hosny,"Computer Science Department, King Saud University, Riyadh, Saudi Arabia; Computer Science Department, King Saud University, Riyadh, Saudi Arabia",2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA),16-Mar-20,2019,,,1,4,"This study describes the development of a text document clustering optimization model using a novel Genetic Algorithm-Shuffled Frog Leaping Algorithm (GA-SFLA), which clusters text documents based on selected features in an efficient way. In the approach proposed in this study, a Genetic Algorithm (GA) handles the feature selection task, while a Shuffled Frog-Leaping Algorithm (SFLA) handles the clustering task. The effectiveness of the proposed approach was evaluated by testing it on the popular ""20Newsgroup"" text document dataset. After multiple experiments, it was found that using GA-SFLA on the 20Newsgroup dataset considerably helped to enhance the text document clustering task, compared to classical K-means clustering. In addition, the feature selection stage greatly contributed to improving the results of clustering. Nevertheless, this improvement comes at the expense of longer computational time.",2161-5330,978-1-7281-5052-9,10.1109/AICCSA47632.2019.9035266,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035266,Text Documents Clustering;Shuffled Frog-Leaping Algorithm;Genetic Algorithm;Feature Selection,Feature extraction;Clustering algorithms;Genetic algorithms;Task analysis;Biological cells;Mathematical model;Optimization,feature selection;genetic algorithms;pattern clustering;search problems;text analysis,GA-SFLA;20Newsgroup dataset;feature selection stage;genetic algorithm-shuffled frog leaping algorithm;text document clustering optimization model;20Newsgroup text document dataset;K-means clustering,,,,15,,16-Mar-20,,,IEEE,IEEE Conferences
A krill herd algorithm for efficient text documents clustering,一種高效的文本文檔聚類的磷蝦群算法,L. M. Abualigah; A. T. Khader; M. A. Al-Betar; M. A. Awadallah,"School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; Department of information technology, Al-Huson University College, Al-Balqa Applied University, Al-Huson, Irbid-Jordan; Department of Computer Science, Al-Aqsa University, Gaza, Palestine",2016 IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE),26-Sep-16,2016,,,67,72,"Recently, due to the huge growth of web pages, social media and modern applications, text clustering technique has emerged as a significant task to deal with a huge amount of text documents. Some web pages are easily browsed and tidily presented via applying the clustering technique in order to partition the documents into a subset of homogeneous clusters. In this paper, two novel text clustering algorithms based on krill herd (KH) algorithm are proposed to improve the web text documents clustering. In the first method, the basic KH algorithm with all its operators is utilized while in the second method, the genetic operators in the basic KH algorithm are neglected. The performance of the proposed KH algorithms is analyzed and compared with the k-mean algorithm. The experiments were conducted using four standard benchmark text datasets. The results showed that the proposed KH algorithms outperformed the k-mean algorithm in term of clusters quality that is evaluated using two common clustering measures, namely, Purity and Entropy.",,978-1-5090-1543-6,10.1109/ISCAIE.2016.7575039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575039,krill herd algorithm;web text document clustering;evolutionary computing and global optimization,Clustering algorithms;Optimization;Mathematical model;Algorithm design and analysis;Genetics;Partitioning algorithms;Clustering methods,evolutionary computation;pattern clustering;text analysis,krill herd algorithm;text document clustering technique;Web pages;KH algorithm;purity;entropy;clustering measures;evolutionary computing,,23,,21,,26-Sep-16,,,IEEE,IEEE Conferences
Measuring Similarities between XML Documents Based on Content and Structure,根據內容和結構衡量XML文檔之間的相似性,X. Xia; Y. Guo; J. Le,"Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China",2009 Asia-Pacific Conference on Information Processing,7-Aug-09,2009,1,,459,462,"Extended Marked-up Language (XML) has become a de facto standard for information representation and data exchange over the Web. As a result, large amounts of XML documents emerge in many application areas, such as Digital library, patent retrieval and Intranet search. Effectively measuring similarities between XML documents plays an important role in these application areas. However, most of current research work focus on measuring the structural similarities between XML documents, and not or less taking into account the content of the documents and links between documents. This paper develops a novel similarity measure model which is based on Extended Vector Space Model. This model can effectively measure similarities between XML documents by combining content, structure and links. In order to evaluate this similarity measure model, we adopt k-means algorithm to cluster XML documents. Experiments show that this model gains better clustering quality compared to the classical vector space model.",,978-0-7695-3699-6,10.1109/APCIP.2009.119,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197093,XML;similarity measure;document clustering;Extended Vector Space Model,XML;Extraterrestrial measurements;Couplings;Measurement standards;Information representation;Software libraries;Area measurement;Current measurement;Clustering algorithms;Information processing,document handling;XML,XML document similarity;Extended Marked-up Language;information representation;data exchange;World Wide Web;digital library;patent retrieval;Intranet search;similarity measure model;extended vector space model;k-means algorithm;eXtensible Markup Language,,,,17,,7-Aug-09,,,IEEE,IEEE Conferences
A new approach for fuzzy clustering of Web documents,Web文檔模糊聚類的新方法,M. Friedman; M. Last; O. Zaafrany; M. Schneider; A. Kandel,"Dept. of Phys., Nucl. Res. Center-Negev, Beer-Sheva, Israel; NA; NA; NA; NA",2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542),10-Jan-05,2004,1,,377,381 vol.1,"Most existing methods of document clustering are based on the classical vector-space model, which represents each document by a fixed-size vector of key terms or key phrases. In large and diverse document collections such as the World Wide Web, this approach suffers from a tremendous computational overload, since the constant size of the term vector equals to the total number of key terms in all documents. We propose a new fuzzy-based approach to clustering documents that are represented by vectors of variable size. Each entry in a vector consists of two fields. The first field is the name of a key phrase in the document and the second denotes an importance weight associated with this key phrase within the particular document. We will describe the proposed approach in detail and show how it is implemented in a real world application from the area of web monitoring.",1098-7584,0-7803-8353-2,10.1109/FUZZY.2004.1375752,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375752,,Clustering methods;Clustering algorithms;Information systems;Systems engineering and theory;Computer science;Fuzzy systems;Physics;Electronic mail;Educational institutions;Web sites,fuzzy set theory;Internet;document handling;pattern clustering;statistical analysis,fuzzy clustering;Web documents;document clustering;World Wide Web,,2,,10,,10-Jan-05,,,IEEE,IEEE Conferences
Document clustering and topic discovery based on semantic similarity in scientific literature,科技文獻中基於語義相似度的文檔聚類與主題發現,J. Jayabharathy; S. Kanmani; A. A. Parveen,"Department of Computer Science & Engineering; Department of Information Technology, Pondicherry Engineering College, Puducherry, India; Department of Computer Science & Engineering",2011 IEEE 3rd International Conference on Communication Software and Networks,8-Sep-11,2011,,,425,429,"Unlabeled document collections are becoming increasingly common and mining such databases becomes a major challenge. It is a major issue to retrieve relevant documents from the larger document collection. By clustering the text documents, the documents sharing similar topics are grouped together. Incorporating semantic features will improve the accuracy of document clustering methods. In order to determine at a sight whether the content of a cluster are of user interest or not, topic discovery methods are required to tag each clusters identifying distinct and representative topic of each cluster. Most of the existing topic discovery methods often assign labels to clusters based on the terms that the clustered documents contain. In this paper a modified semantic-based model is proposed where related terms are extracted as concepts for concept-based document clustering by bisecting k-means algorithm and topic detection method for discovering meaningful labels for the document clusters based on semantic similarity by Testor theory. The proposed method is compared to the Topic Detection by Clustering Keywords method using F-measure and purity as evaluation metrics. Experimental results prove that the proposed semantic-based model outperforms the existing work.",,978-1-61284-486-2,10.1109/ICCSN.2011.6014600,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014600,Document clustering;Topic discovery;Semantic similarity;Concept;Testor theory,Internet;Data mining;Information retrieval;Information services;Electronic publishing,data mining;information retrieval;pattern clustering;text analysis,semantic similarity;scientific literature;unlabeled document collection;database mining;text document clustering;topic discovery method;representative topic identification;distinct topic identification;concept-based document clustering;k-means algorithm;topic detection method;Testor theory,,6,,13,,8-Sep-11,,,IEEE,IEEE Conferences
Enhancing degraded document images via bitmap clustering and averaging,通過位圖聚類和平均來增強降級的文檔圖像,J. D. Hobby; Tin Kam Ho,"AT&T Bell Labs., Murray Hill, NJ, USA; NA",Proceedings of the Fourth International Conference on Document Analysis and Recognition,6-Aug-02,1997,1,,394,400 vol.1,Proper display and accurate recognition of document images are often hampered by degradations caused by poor scanning or transmission conditions. The authors propose a method to enhance such degraded document images for better display quality and recognition accuracy. The essence of the method is in finding and averaging bitmaps of the same symbol that are scattered across a text page. Outline descriptions of the symbols are then obtained that can be rendered at arbitrary solution. The paper describes details of the algorithm and an experiment to demonstrate its capabilities using fax images.,,0-8186-7898-4,10.1109/ICDAR.1997.619877,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619877,,Degradation;Clustering algorithms;Displays;Scattering;Optical character recognition software;Image segmentation;Shape;Image recognition;Rendering (computer graphics);Document image processing,document image processing;rendering (computer graphics);bit-mapped graphics;facsimile,degraded document image enhancement;bitmap clustering;averaging;scanning conditions;transmission conditions;document image recognition accuracy;document image display quality;symbol;text page;outline descriptions;rendering;arbitrary solution;algorithm;fax images,,36,,13,,6-Aug-02,,,IEEE,IEEE Conferences
Examining the impact of stemming on clustering Turkish texts,研究詞幹對土耳其文本集聚的影響,V. Tunali; T. T. Bilgin,"Faculty of Engineering, Maltepe University, Istanbul, Turkey; Faculty of Engineering, Maltepe University, Istanbul, Turkey",2012 International Symposium on Innovations in Intelligent Systems and Applications,23-Jul-12,2012,,,1,4,"Preprocessing is an important step in information retrieval and text mining. In this study, we examined the impact of stemming on clustering Turkish texts. We used two datasets compiled from web sites of Turkish news agencies, and performed extensive experiments. We empirically show that there is no significant evidence that stemming always improves the quality of clustering for texts in Turkish. However, when stemming is used, dimensionality of the document-term matrix dramatically decreases without inversely affecting the clustering performance. As a result, it is highly recommended to apply stemming for clustering Turkish texts.",,978-1-4673-1448-0,10.1109/INISTA.2012.6246966,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6246966,data mining;text mining;document clustering;preprocessing;stemming,Entropy;Text mining;Clustering algorithms;Web sites;Educational institutions,data mining;information retrieval;pattern clustering;text analysis;Web sites,Turkish text clustering;information retrieval;text mining;stemming impact;Web sites;Turkish news agencies;text clustering quality;document-term matrix dimensionality reduction,,2,,14,,23-Jul-12,,,IEEE,IEEE Conferences
Self-organising maps for tree view based hierarchical document clustering,自組織地圖，用於基於樹視圖的分層文檔聚類,R. Freeman; Hujun Yin; N. M. Allinson,"Dept. of Electr. Eng. & Electron., Univ. of Manchester Inst. of Sci. & Technol., UK; Dept. of Electr. Eng. & Electron., Univ. of Manchester Inst. of Sci. & Technol., UK; Dept. of Electr. Eng. & Electron., Univ. of Manchester Inst. of Sci. & Technol., UK",Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),7-Aug-02,2002,2,,1906,1911 vol.2,"In this paper, we investigate the use of self-organising maps (SOMs) for document clustering. Previous methods using SOMs to cluster documents have used 2D maps. This paper presents a hierarchical and growing method using a series of 1D maps instead. Using this type of SOM is an efficient method for clustering documents and browsing them in a dynamically generated tree of topics. These topics are automatically discovered for each cluster, based on the set of documents in a particular cluster. We demonstrate the efficiency of the method using different sets of real-world Web documents.",1098-7576,0-7803-7278-6,10.1109/IJCNN.2002.1007810,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007810,,Frequency;Indexing;Equations;Information theory;Clustering methods;Clustering algorithms,document handling;classification;self-organising feature maps;tree data structures;pattern clustering;information resources,self-organising maps;tree view-based hierarchical document clustering;growing method;1D map series;document browsing;dynamically generated topic tree;automatic topic discovery;World Wide Web documents,,8,,24,,7-Aug-02,,,IEEE,IEEE Conferences
A Clustering Algorithm for Short Documents Based On Concept Similarity,基於概念相似度的短文檔聚類算法,J. Peng; D. Yang; J. Wang; M. Wu; J. Wang,"School of Electronics Engineering and Computer Science, Peking University, Beijing, China. e-mail: pjxxlpsj@hotmail.com; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China","2007 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing",24-Sep-07,2007,,,42,45,"In recent years, there has been an increasing interest in data clustering of short documents. Existing works consider seldom the concept similarity between the words, so the quality of clustering is often very low. This paper proposes a new document-clustering algorithm based on concept similarity in Chinese text processing. Different from tradition method, the algorithm converts text into a words vector space model at first; it splits words into a set of concepts at second; 3rd, it gets the similarity between words through computing the inner products between concepts; 4th, it computes the similarity of text based on the similarity of words. Finally, through two-phased steps, the algorithm finishes the clustering of a specified set of document. The extensive experiments prove the validity and performance of the algorithm.",2154-5952,978-1-4244-1189-4,10.1109/PACRIM.2007.4313172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4313172,,Clustering algorithms;Natural languages;Data engineering;Computer science;Text processing;Data mining;Books;Web pages;Research and development,natural languages;pattern clustering;text analysis,data clustering;short document clustering algorithm;concept similarity;Chinese text processing,,3,,9,,24-Sep-07,,,IEEE,IEEE Conferences
Generic multi-document summarization using cluster refinement and NMF,使用聚類細化和NMF進行通用的多文檔摘要,S. Park; D. U. An; Y. J. Cho,"Advanced Graduate Education Center of Jeonbuk for Electronics and Information Technology-BK21, Chonbuk National University, South Korea; Division of Electronic & Infomation Engineering, Chonbuk National University, South Korea; Division of Electronic & Infomation Engineering, Chonbuk National University, South Korea",2009 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT),5-Feb-10,2009,,,65,70,"In this paper, a generic summarization method that uses cluster refinement and NMF is introduced to extract meaningful sentences from documents. The proposed method uses cluster refinement to improve the quality of document clustering since it helps us to remove dissimilarity information easily and avoid biased inherent semantics of documents to be reflected in clusters by NMF. In addition, it uses the weighted semantic variable to select meaningful sentences because the extracted sentences are well covered with the major topics of document. The experimental results demonstrate that the proposed method has better performance than other methods that use the other methods.",2162-7843,978-1-4244-5950-6,10.1109/ISSPIT.2009.5407492,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407492,geneirc multi-document summarization;cluster refinement;NMF;cluster coherence,Data mining;Diversity reception;Feature extraction;Vectors;Sun;Educational technology;Clustering algorithms;Diversity methods;Internet,feature extraction;pattern clustering;text analysis,generic multidocument summarization;cluster refinement;meaningful sentence extraction;document clustering,,,,26,,5-Feb-10,,,IEEE,IEEE Conferences
An Efficient Pretopological Approach for Document Clustering,用於文檔聚類的高效的前拓撲方法,T. V. Le; T. N. Truong; H. N. Nguyen; T. V. Pham,"NA; HCMC Univ. of Technol., Ho Chi Minh City, Vietnam; HCMC Univ. of Technol., Ho Chi Minh City, Vietnam; HCMC Univ. of Technol., Ho Chi Minh City, Vietnam",2013 5th International Conference on Intelligent Networking and Collaborative Systems,15-Oct-13,2013,,,114,120,"In this paper we propose a new document clustering approach that does not require distance metric for aggregating multi variables in order to measure the similarity between documents. Based on calculated coherence (or pseudoclosure) function, closure set of pretopology concepts, we suggest a new proposition for cluster exploring when the connection between data is represented by one/many equivalent relations. Our approach also shows the data structure aggregating multi-criteria by viewing subsequent levels of pseudoclosure function which could represent data expansions. Furthermore, noisy data could be automatically detected by using our work as all elementary closures that have very small size of cardinality signifies their limit connections to the others. We also compare our approach with the popular K-Means and Fast Pair Nearest Neighbor with a given document collection for evaluating the efficiency and performance.",,978-0-7695-4988-0,10.1109/INCoS.2013.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630395,document clustering;pretopology;pseudoclosure;closure;multi-criteria,Abstracts;Vectors;Error analysis;Coherence;Topology;Measurement;Data mining,document handling;pattern clustering;unsupervised learning,pretopological approach;document clustering approach;document similarity;calculated coherence function;cluster exploration;data connection;data structure;pseudoclosure function;data expansions;cardinality size;k-means approach;fast pair nearest neighbor approach,,1,,15,,15-Oct-13,,,IEEE,IEEE Conferences
Robust Vanishing Point Detection for MobileCam-Based Documents,基於MobileCam的文檔的可靠消失點檢測,X. Yin; H. Hao; J. Sun; S. Naoi,"Dept. of Comput. Sci., Univ. of Sci. & Technol. Beijing, Beijing, China; Dept. of Comput. Sci., Univ. of Sci. & Technol. Beijing, Beijing, China; Fujitsu R&D Center Co. Ltd., Beijing, China; Fujitsu R&D Center Co. Ltd., Beijing, China",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,136,140,"Document images captured by a mobile phone camera often have perspective distortions. In this paper, fast and robust vanishing point detection methods for such perspective documents are presented. Most of previous methods are either slow or unstable. Based on robust detection of text baselines and character tilt orientations, our proposed technology is fast and robust with the following features: (1) quick detection of vanishing point candidates by clustering and voting on the Gaussian sphere space, and (2) precise and efficient detection of the final vanishing points using a hybrid approach, which combines the results from clustering and projection analysis. The rectified image acceptance rate for Mobile Cam-based documents, signboards and posters is more than 98% with an average speed of about 100ms.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065291,Vanishing point detection;Perspective document rectification;clustering;MobileCam-based documents;the Gaussian sphere,Robustness;Mobile handsets;Accuracy;Text analysis;Fitting;Estimation;Cameras,document image processing;Gaussian processes;mobile computing;object detection;pattern clustering;statistical analysis;text analysis,robust vanishing point detection method;mobilecam-based documents;mobile phone camera;text baselines;character tilt orientations;Gaussian sphere space;clustering analysis;projection analysis,,17,,19,,3-Nov-11,,,IEEE,IEEE Conferences
Skew Estimation of Sparsely Inscribed Document Fragments,稀疏題寫文檔片段的偏斜估計,M. Diem; F. Kleber; R. Sablatnig,"Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,292,296,"Document analysis is done to analyze entire forms (e.g. intelligent form analysis, table detection) or to describe the layout/structure of a document for further processing. A pre-processing step of document analysis methods is a skew estimation of scanned or photographed documents. Current skew estimation methods require the existence of large text areas, are dependent on the text type and can be limited on a specific angle range. The proposed method is gradient based in combination with a Focused Nearest Neighbor Clustering of interest points and has no limitations regarding the detectable angle range. The upside/down decision is based on statistical analysis of ascenders and descenders. It can be applied to entire documents as well as to document fragments containing only a few words. Results show that the proposed skew estimation is comparable with state-of-the-art methods and outperforms them on a real dataset consisting of 658 snippets.",,978-0-7695-4661-2,10.1109/DAS.2012.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195381,skew estimation;rotation;document fragments,Estimation;Histograms;Text analysis;Vectors;Transforms;Algorithm design and analysis;Robustness,document image processing;estimation theory;optical character recognition;pattern clustering;statistical analysis;text analysis,document processing;document analysis method;photographed document;scanned document;skew estimation method;gradient based method;focused nearest neighbor clustering;upside-down decision;statistical analysis;ascenders;descenders;sparsely inscribed document fragment;OCR method,,9,,19,,7-May-12,,,IEEE,IEEE Conferences
Semantic Smoothing for Model-based Document Clustering,基於模型的文檔聚類的語義平滑,X. Zhang; X. Zhou; X. Hu,Drexel University; Drexel University; Drexel University,Sixth International Conference on Data Mining (ICDM'06),8-Jan-07,2006,,,1193,1198,"A document is often full of class-independent ""general"" words and short of class-specific ""core "" words, which leads to the difficulty of document clustering. We argue that both problems will be relieved after suitable smoothing of document models in agglomerative approaches and of cluster models in partitional approaches, and hence improve clustering quality. To the best of our knowledge, most model-based clustering approaches use Laplacian smoothing to prevent zero probability while most similarity-based approaches employ the heuristic TF*IDF scheme to discount the effect of ""general"" words. Inspired by a series of statistical translation language model for text retrieval, we propose in this paper a novel smoothing method referred to as context-sensitive semantic smoothing for document clustering purpose. The comparative experiment on three datasets shows that model-based clustering approaches with semantic smoothing is effective in improving cluster quality.",2374-8486,978-0-7695-2701-7,10.1109/ICDM.2006.142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053178,,Smoothing methods;Context modeling;Laplace equations;Clustering algorithms;Educational institutions;Information science;Probability;Information retrieval;Nearest neighbor searches;Vocabulary,document handling;information retrieval;pattern clustering;probability;smoothing methods,model-based document clustering;document models;clustering quality;model-based clustering;Laplacian smoothing;zero probability;statistical translation language model;text retrieval;context-sensitive semantic smoothing;cluster quality,,12,,13,,8-Jan-07,,,IEEE,IEEE Conferences
Order-based clustering using formal concept analysis,使用形式概念分析的基於訂單的聚類,A. Moriki; S. Yoshida,"Information Systems Engineering Course, Graduate School of Engineering, Kochi University of Technology, Japan; School of Information, Kochi University of Technology, 185 Miyanokuchi, Tosayamada-cho, Kami-shi, 782-8502, Japan",2010 World Automation Congress,10-Dec-10,2010,,,1,6,"In this paper, we propose an document clustering algorithm based on formal concept analysis. In conventional clustering methods, numeric data are required and numeric processing is performed by cosine distance of numeric data as word or document vector. However, several documents of a cluster are not similar as a result of classification using conventional methods. In this paper, a novel clustering method is proposed by an application of formal concept analysis. Documents are classified into sets of documents shared same features by formal concept analysis. In addition, each set of documents can be selected in the method. We, thereby, propose document clustering which is suitable for expressing themes of documents based on information of documents as words. In this paper, formal concept analysis is applied to 100 documents of English news articles selected from Reuters-21578 database. Then, the document clustering is performed by selecting each concept on concept lattice. Elements of each article are included in all concepts connecting to lower layers of a selected concept. Those elements are set as a cluster. Each cluster has a shared topic. In addition, clusters of low-level connecting layers are set as a cluster by selecting concept on higher layers. Proposed clustering technique can be applied to text classification and summarization.",2154-4824,978-1-889335-42-1,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665287,Formal Concept Analysis;Order Relation;Document Clustering,Biomedical monitoring;Biosensors;Temperature sensors;Temperature measurement,formal concept analysis;pattern classification;pattern clustering;text analysis;word processing,order based clustering;formal concept analysis;document clustering algorithm;numeric data processing;cosine distance;word vector;English news article;text classification,,1,,4,,10-Dec-10,,,IEEE,IEEE Conferences
Measuring documents similarity in large corpus using MapReduce algorithm,使用MapReduce算法測量大型語料庫中的文檔相似度,M. Birjali; A. Beni-Hssane; M. Erritali,"Department of Computer Sciences, University of Chouaib Doukkali, Faculty of Sciences, Eljadida, Morocco; Department of Computer Sciences, University of Chouaib Doukkali, Faculty of Sciences, Eljadida, Morocco; Department of Computer Sciences, University of Sultan Moulay Slimane, Faculty of Science and Technology, Beni Mellal, Morocco",2016 5th International Conference on Multimedia Computing and Systems (ICMCS),24-Apr-17,2016,,,24,28,"Document similarity measures between documents and queries has been extensively studied in information retrieval. Measuring the similarity of documents are crucial components of many text-analysis tasks, including information retrieval, document classification, and document clustering. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. There exist a large number of composed documents in a large amount of corpus. Most of them are required to compute the similarity for validation. In this paper, we propose our approach of measuring similarity between documents in large amount of corpus. For evaluation, we compare the proposed approach with other approaches previously presented by using our new MapReduce algorithm. Simulation results, on Hadoop framework, show that our new MapReduce algorithm outperforms the classical ones in term of running time performance and increases the value of the similarity.",2472-7652,978-1-5090-5146-5,10.1109/ICMCS.2016.7905587,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905587,Hadoop cluster;document similarity;MapReduce programming model;similarity measure,Semantics;Ontologies;Frequency measurement;Algorithm design and analysis;Time measurement;Information retrieval;Indexing,data handling;parallel processing;pattern classification;pattern clustering;query processing;text analysis,document similarity measurement;MapReduce algorithm;information retrieval;text-analysis tasks;information retrieval;document classification;document clustering;Hadoop framework;running time performance,,3,,15,,24-Apr-17,,,IEEE,IEEE Conferences
A Semantic-Based Approach to Service Clustering from Service Documents,基於語義的服務文檔服務聚類方法,B. Jiang; L. Ye; J. Wang; Y. Wang,"Sch. of Comput. & Inf. Eng., Zhejiang Gongshang Univ., Hangzhou, China; Sch. of Comput. & Inf. Eng., Zhejiang Gongshang Univ., Hangzhou, China; Sch. of Comput. & Inf. Eng., Zhejiang Gongshang Univ., Hangzhou, China; Sch. of Comput. & Inf. Eng., Zhejiang Gongshang Univ., Hangzhou, China",2017 IEEE International Conference on Services Computing (SCC),14-Sep-17,2017,,,265,272,"With the rapid growth of service volumes and types, discovering services in an efficient and accurate manner has become a significant challenge in service computing. Service clustering is an important technology to improve the efficiency of service discovery. In this paper, we propose a new service clustering approach, which starts from service documents and is based on the functional semantics of services. This approach, first, extracts service goals from service description documents by using natural language processing technologies. Then it obtains the semantic similarity between two service goals and clusters the services by the K-means algorithm. Experiments conducted on a real-world service dataset crawled from ProgrammableWeb demonstrate the feasibility and the effectiveness of the proposed approach.",2474-2473,978-1-5386-2005-2,10.1109/SCC.2017.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8034994,Service discovery;service clustering;natural language processing;service goals;service similarity,Semantics;Clustering algorithms;Natural language processing;Web services;Clustering methods;Data mining;Algorithm design and analysis,document handling;natural language processing;pattern clustering;Web services,semantic-based approach;service documents;service volumes;service computing;service discovery;service clustering approach;functional semantics;extracts service goals;service description documents;real-world service dataset;natural language processing technologies;K-means algorithm,,,,18,,14-Sep-17,,,IEEE,IEEE Conferences
Genetic Algorithms Revisited for Improved Clustering of Text Documents,重新研究遺傳算法以改進文本文檔的聚類,M. Abdullah-Al-Wadud,"Department of Software Engineering, King Saud University, Riyadh, Saudi Arabia",2018 International Conference on Innovation in Engineering and Technology (ICIET),7-Mar-19,2018,,,1,5,"Text document clusters are usually represented with respect to cluster centers called centroids. A clustering algorithm tries to find a good combination of centroids that satisfies the objective of clustering, i.e., assigning similar documents together and dissimilar documents in different clusters. The success of such an algorithm mainly depends on the ability to generate different potential solutions, i,e., combinations of centroids. However, most of the existing approaches such as k-means clustering and genetic algorithm (GA) depends heavily on the randomly generated initial solution(s). At times, such methods are found to reach premature convergences due to getting stuck at local extrema, from which no more potential solutions can be explored. This paper proposes a modification to the crossover operations, which is the heart of GA to generate potential solutions, to overcome such limitations. Experimental evaluation also shows the performance improvements of available GA approaches when using the modified crossover operation.",,978-1-5386-5229-9,10.1109/CIET.2018.8660827,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660827,Text document clustering;genetic algorithm;crossover;premature convergence,Genetic algorithms;Biological cells;Clustering algorithms;Mathematical model;Search engines;Partitioning algorithms;Convergence,genetic algorithms;pattern clustering;text analysis,text document clustering;similar documents;clustering algorithm;genetic algorithm;centroids,,,,16,,7-Mar-19,,,IEEE,IEEE Conferences
Determining Document Skew Using Inter-line Spaces,使用行間空格確定文檔偏斜,B. Epshtein,"Google Inc., Mountain View, CA, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,27,31,"We present a novel method of determining a global text page orientation. The method is based on Hough transform, but, unlike the existing methods, it does not use the letters themselves and relies on establishing the orientation of interline regions. The method is robust and also works even when a single line of text is present. Experimental evaluation is shown, comparing the method to other methods proposed in the literature.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065270,document analysis;skew;orientation;Hough transform;inter-line regions,Transforms;Algorithm design and analysis;Optical character recognition software;Robustness;Databases;Pattern recognition;Clustering algorithms,document image processing;Hough transforms,document skew;interline space;global text page orientation;Hough transform;interline region,,9,,16,,3-Nov-11,,,IEEE,IEEE Conferences
A weighted topical document embedding based clustering method for news text,基於加權主題文檔嵌入的新聞文本聚類方法,S. Hui; Z. Dechao,"School of Computer Science, Donghua University, Shanghai, China; School of Computer Science, Donghua University, Shanghai, China","2016 IEEE Information Technology, Networking, Electronic and Automation Control Conference",5-Sep-16,2016,,,1060,1065,"As an unsupervised machine learning method, clustering can preliminarily group text without artificial labeling, which effectively accelerates the organization, abstraction and navigation on large news set. The length of news is long, and the text contains many homonymy and polysemy, that is one of the reason that traditional text clustering methods perform weaker on grouping news text. This paper presents a novel text representation method based on topical document embedding (TDE) to capture the semantic features of different topics. In TDE representation, document embedding of news texts is obtained by adding up word vector from Skip-Gram model weighted by TF-IDF score of all the key words in the text. While the topical document embedding is learned by joining the topic vectors obtained from LDA model and the document vectors in document embedding. By using topical document embedding to perform clustering, we implement a novel text clustering method (TDE-TC). The experimental results show that the effect of news clustering based on TDE representation is better than that of bag of words model and LDA model.",,978-1-4673-9194-8,10.1109/ITNEC.2016.7560526,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560526,Text Clustering;Skip-Gram;LDA;TF-IDF,Semantics;Computational modeling;Clustering methods;Feature extraction;Clustering algorithms;Resource management;Vocabulary,information resources;pattern clustering;statistical analysis;text analysis;unsupervised learning;vectors,weighted topical document embedding based clustering method;news text;unsupervised machine learning method;homonymy;polysemy;text representation method;TDE representation;word vector;skip-gram model;TF-IDF score;topic vectors;LDA model;document vectors;text clustering method;TDE-TC,,2,,14,,5-Sep-16,,,IEEE,IEEE Conferences
Automatic text categorization of marathi documents using clustering technique,使用聚類技術對馬拉地語文檔進行自動文本分類,S. R. Vispute; M. A. Potey,"DYPCOE, Akurdi, Pune, India; DYPCOE, Akurdi, Pune, India",2013 15th International Conference on Advanced Computing Technologies (ICACT),16-Jan-14,2013,,,1,5,"The purpose of the present work is creating an intelligent system to retrieve desired documents in Marathi language. The system also focuses on providing the personalized documents in Marathi language to the end user based on their interests identified from the browsing history. This paper presents the automatic categorization of Marathi documents and the literature survey of the related work done in automatic categorization of text documents. Several supervised learning techniques are exists for the classification of text documents namely Decision trees, Support Vector machine (SVM), Neural Network, Ada Boost and Na簿ve Bayes etc. Several clustering techniques are also available for text categorization namely K-means, Suffix Tree Clustering (STC), Semantic Online Hierarchical Clustering (SHOC), Label Induction Grouping Algorithm (LINGO) etc. In the literature survey it is found that vector space model (VSM) gives better result than probabilistic model. This paper presents categorization of the Marathi text documents using Lingo Clustering algorithm based on VSM. The data set consists of 107 Marathi documents of 3 different categories-Tourism, Health Programmes and Maharashtra festivals. The result shows that the performance of the LINGO clustering algorithm is good for categorizing the Marathi text documents. For the Marathi documents overall accuracy of the system is 91.10%.",,978-1-4673-2818-0,10.1109/ICACT.2013.6710543,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710543,Text categorization;Clustering;Information filtering;Internet search;Information retrieval,Clustering algorithms;Classification algorithms;Text categorization;Support vector machines;Vectors;Matrix decomposition;Internet,information retrieval;natural language processing;pattern classification;pattern clustering;text analysis,clustering technique;intelligent system;document retrieval;Marathi documents automatic categorization;text documents automatic categorization;Lingo clustering algorithm;VSM;vector space model;tourism;health programmes;Maharashtra festivals;Marathi text documents,,6,,17,,16-Jan-14,,,IEEE,IEEE Conferences
Document distribution algorithm for load balancing on an extensible Web server architecture,用於可擴展Web服務器體系結構上的負載平衡的文檔分發算法,Ben Chung-Pun Ng; Ch-Li Wang,"Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China; NA",Proceedings First IEEE/ACM International Symposium on Cluster Computing and the Grid,7-Aug-02,2001,,,140,147,"Access latency and load balancing are the two main issues in the design of clustered Web server architecture for achieving high performance. We propose a novel document distribution algorithm for load balancing on a cluster of distributed Web servers. We group Web pages that are likely to be accessed during a request session into a migrating unit, which is used as the basic unit of document placement. A modified binning algorithm is developed to distribute the migrating units among the Web servers to fulfil the load balancing. We also present a redirection mechanism, which makes use of a migrating unit's property, to reduce the cost of request redirections. The distribution of Web documents would be recomputed periodically to adapt to the changes in client request patterns and system configuration. Simulation results show that our solution can reduce the amount of request redirection and document migration, and it can distribute workload properly among Web servers.",,0-7695-1010-8,10.1109/CCGRID.2001.923186,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=923186,,Load management;Web server;Service oriented architecture;Delay;Computer architecture;Web pages;File servers;Clustering algorithms;Computer science;Information systems,Internet;file servers;document handling;resource allocation;workstation clusters;client-server systems,document distribution algorithm;load balancing;extensible Web server architecture;access latency;clustered Web server architecture;distributed Web server cluster;Web pages;request session;migrating unit;document placement;modified binning algorithm;migrating units;redirection mechanism;request redirections;Web documents;client request patterns;system configuration;request redirection;document migration,,3,,15,,7-Aug-02,,,IEEE,IEEE Conferences
Medication recommendation system based on clinical documents,基於臨床文獻的用藥推薦系統,A. John; M. I. H.; V. Vasudevan,"Department of Computer Science and Engineering, College of Engineering, Cherthala, Kerala, India-688541; Dept. of Comput. Sci. & Eng., Coll. of Eng., Cherthala, Cherthala, India; Department of Computer Science and Engineering, College of Engineering, Cherthala, Kerala, India-688541",2016 International Conference on Information Science (ICIS),9-Feb-17,2016,,,180,184,"Designing medication recommendation system is a need for the fast growing world. In this fast growing world, the need for the application which recommend a medication led to a doctor friendly and hospital free atmosphere for all users all over the world. In this paper an unified extraction system with stanford parser is used for extraction of medical terms. Then K-means clustering algorithm clusters the diseases and a filtering method find out the desired medication.",,978-1-5090-1987-8,10.1109/INFOSCI.2016.7845323,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845323,NER;NE class;SVM;FuzzyClinical text;Recommendation;Clustering;Extraction,Diseases;Medical diagnostic imaging;Data mining;Clustering algorithms;Drugs;Databases;Collaboration,collaborative filtering;diseases;document handling;medical computing;pattern clustering;recommender systems,medication recommendation system;clinical documents;doctor friendly atmosphere;hospital free atmosphere;unified extraction system;Stanford parser;medical term extraction;K-means clustering algorithm;diseases;filtering method,,1,,14,,9-Feb-17,,,IEEE,IEEE Conferences
Concept Factorization With Adaptive Neighbors for Document Clustering,用於文檔聚類的帶有自適應鄰居的概念分解,X. Pei; C. Chen; W. Gong,"School of Software, Huazhong University of Science and Technology, Wuhan, China; School of Software, Huazhong University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",IEEE Transactions on Neural Networks and Learning Systems,17-Jan-18,2018,29,2,343,352,"In this paper, a novel concept factorization (CF) method, called CF with adaptive neighbors (CFANs), is proposed. The idea of CFAN is to integrate an ANs regularization constraint into the CF decomposition. The goal of CFAN is to extract the representation space that maintains geometrical neighborhood structure of the data. Similar to the existing graph-regularized CF, CFAN builds a neighbor graph weights matrix. The key difference is that the CFAN performs dimensionality reduction and finds the neighbor graph weights matrix simultaneously. An efficient algorithm is also derived to solve the proposed problem. We apply the proposed method to the problem of document clustering on the 20 Newsgroups, Reuters-21578, and TDT2 document data sets. Our experiments demonstrate the effectiveness of the method.",2162-2388,,10.1109/TNNLS.2016.2626311,"Fundamental Research Funds for the Central Universities, HUST; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748469,Concept factorization (CF);document clustering,Optimization;Matrix decomposition;Clustering algorithms;Sparse matrices;Adaptation models;Encoding;Symmetric matrices,document handling;graph theory;matrix algebra;pattern clustering,document clustering;concept factorization method;CFAN;ANs regularization constraint;CF decomposition;neighbor graph weights matrix;TDT2 document data sets;graph-regularized CF;CF with adaptive neighbors;data geometrical neighborhood structure;dimensionality reduction;Reuters-21578;20 Newsgroups,,5,,46,,18-Nov-16,,,IEEE,IEEE Journals
Machine learning algorithms for document clustering and fraud detection,用於文檔聚類和欺詐檢測的機器學習算法,S. Yaram,"Data Architect, CSC India Limited, Hyderabad, India",2016 International Conference on Data Science and Engineering (ICDSE),19-Jan-17,2016,,,1,6,"Machine Learning plays very important role in processing of large amounts of structured and unstructured data. A set of algorithms can be used to get meaningful insights into the data that are helpful in making effective business decisions. Document clustering is one of the popular machine learning technique used to group unstructured data (text documents) based on its content and further analyze the data to understand the patterns in it. The unstructured data gets transformed into semi-structured data and structured data in stages by using text mining and clustering (k-means) techniques. Classification is another machine learning technique that can be implemented for use cases like ""fraud detection and cross-sell & up-sell opportunity identification"" in banking, financial services and insurance industry. This paper focuses on the implementation of both document clustering algorithm and a set of classification algorithms (Decision Tree, Random Forest and Na簿ve Bayes), along with appropriate industry use cases. Also, the performance of three classification algorithms will be compared by calculation of ""Confusion Matrix"" which in turn helps us to calculate performance measures such as, ""accuracy"", ""precision"", and ""recall"".",,978-1-5090-1281-7,10.1109/ICDSE.2016.7823950,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823950,machine learning;supervised learning;unsupervised learning;clustering;classification;decision tree;random forest;na簿ve bayes;class variable or dependent variable;feature variable or independent variable;document-term matrix;term frequency;inverse document frequency;euclidean distance;information gain;entropy;confusion matrix;accuracy;precision;recall,Classification algorithms;Feature extraction;Decision trees;Entropy;Prediction algorithms;Insurance;Machine learning algorithms,Bayes methods;data mining;decision making;decision trees;fraud;learning (artificial intelligence);matrix algebra;pattern classification;pattern clustering;text analysis,machine learning algorithms;document clustering;fraud detection;unstructured data;business decision making;text documents;semistructured data;clustering techniques;text mining;up-sell opportunity identification;classification algorithm set;decision tree;random forest;Naive Bayes;confusion matrix,,5,,17,,19-Jan-17,,,IEEE,IEEE Conferences
Binarization of Colored Document Images using Spectral Clustering,使用光譜聚類對彩色文檔圖像進行二值化,E. M. Elgbbas; M. I. Khalil; H. Abbas,"Faculty of Engineering, Ain Shams University, Egypt; Faculty of Engineering, Ain Shams University, Egypt; Faculty of Engineering, Ain Shams University, Egypt",2018 13th International Conference on Computer Engineering and Systems (ICCES),14-Feb-19,2018,,,411,416,"In this paper, we propose a hybrid method for text binarization of historical documents. Proposed method incorporates the advantages of Otsu and spectral clustering algorithm. In text binarization problem, there are noise and faint text challenges. To overcome the noise problem, a preprocessing step is applied to the colored document. After that, the resulted image is binarized using Otsu, producing a global binary image. As a final step, the spectral clustering algorithm is locally applied to the original image, with the aid of the global binary image, in order to retrieve faint text. The proposed design of spectral clustering provides a significant reduction of the similarity matrix computing time and size use, without affecting the quality of clustering. This method is suitable for colored document images. The efficiency of this method is illustrated by experiments using DIBCO images.",,978-1-5386-5111-7,10.1109/ICCES.2018.8639367,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8639367,,Clustering algorithms;Gray-scale;Microsoft Windows;Mathematical model;Clustering methods;Laplace equations;Complexity theory,document image processing;image colour analysis;image retrieval;matrix algebra;pattern clustering,hybrid method;historical documents;spectral clustering algorithm;text binarization problem;noise problem;global binary image;DIBCO images;colored document image binarization;Otsu clustering algorithm;faint text retrieval;similarity matrix,,,,19,,14-Feb-19,,,IEEE,IEEE Conferences
Hyperspherical Fuzzy clustering for online document categorization,超球形模糊聚類用於在線文檔分類,J. Mei; Y. Wang,"College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou 310023, China; Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore 138632",2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),10-Nov-16,2016,,,1487,1493,"For document data which are typically represented as high dimensional and sparse vectors, cosine distance based Hyperspherical Fuzzy C-Means(HFCM) has been shown to be more effective than classic Euclidean distance based fuzzy c-means(FCM) for document categorization. The existing HFCM approach assumes a static dataset and performs clustering in a batch mode. This design makes HFCM no more suitable when the dataset keeps increasing in its size or is too large to be loaded wholly. In this paper, we work on fuzzy clustering approaches for online document clustering. Specifically, we propose to perform hyperspherical fuzzy c-means in an online manner based on the stochastic gradient method. In this Online Hyperspherical Fuzzy C-Means(OnHFCM), documents are assumed to come one by one and centroids of clusters are updated immediately when each document is arrived. Such a formulation allows OnHFCM to be applicable to large datasets which may be incrementally changing in size. Two variants of OnHFCM with different objective functions are presented in this paper. In addition to the classic online algorithm which processes documents one by one, mini-batch algorithm that handles a small batch of documents at a time is also provided. Experimental results on real-world benchmarks demonstrate that the proposed OnHFCM algorithms achieved better performance than existing ones in online document categorization.",,978-1-5090-0626-7,10.1109/FUZZ-IEEE.2016.7737866,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737866,,Clustering algorithms;Linear programming;Gradient methods;Electronic mail;Benchmark testing;Extraterrestrial measurements;Scalability,document handling;fuzzy set theory;pattern clustering,hyperspherical fuzzy clustering;online document categorization;document data;sparse vectors;cosine distance;HFCM;classic Euclidean distance;static dataset;batch mode;online document clustering;online manner;online hyperspherical Fuzzy C-means;OnHFCM;classic online algorithm,,,,17,,10-Nov-16,,,IEEE,IEEE Conferences
A Semantic Collaborative Clustering Approach Based on Confusion Matrix,基於混淆矩陣的語義協同聚類方法,D. E. Zomahoun,University of Burgundy,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),16-Apr-20,2019,,,688,692,"In this paper we discuss about a new images retrieval technique based on clustering. We argue that images don't have an intrinsic meaning, but they can receive different interpretation. These images can complicate documents retrieval. However, users need a quick and direct access to documents. To answer this requirement, we propose a retrieval approach which use a collaborative clustering technique based on Confusion matrix.",,978-1-7281-5686-6,10.1109/SITIS.2019.00112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067937,Images;Collaborative-Clustering;Retrieval;Confusion-matrix;Mapping-Function,Semantics;Collaboration;Clustering algorithms;Image retrieval;Training;Ontologies,document handling;groupware;image representation;image retrieval;information retrieval;matrix algebra;pattern clustering,collaborative clustering;confusion matrix;image retrieval;documents retrieval;image representation,,,,14,,16-Apr-20,,,IEEE,IEEE Conferences
Effect of multi-word features on the hierarchical clustering of web documents,多字功能對Web文檔層次聚類的影響,S. Karthick; S. M. Shalinie; A. Eswarimeena; P. Madhumitha; T. N. Abhinaya,"Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Dept. of Comput. Sci. & Eng, Thiagarajar Coll. of Eng., Madurai, India",2014 International Conference on Recent Trends in Information Technology,29-Dec-14,2014,,,1,6,"Contemporary search engines and other automated web tools are faced with the task of extracting relevant information from huge web archives. This is supposed to be a difficult task due to the semi-structured and unstructured nature of the web documents. Users need automated ways of organizing and cataloging the web documents so that they can be queried efficiently. Clustering is typically employed to organize web archives and to subsequently handle user queries. This paper analyzes the effect of including multi-word features on the performance of a hierarchical clustering algorithm. Noun sequences are the predominant features considered in our work, while most of the previous research uses n-grams as features. The paper also analyzes the effect of combining link and content based representations for the web documents and their inter-relationships on the clustering performance. Empirical evaluation of the hierarchical clustering engine suggests that including multi-word features enhances the performance of the hierarchical clustering algorithm with respect to precision.",,978-1-4799-4989-2,10.1109/ICRTIT.2014.6996185,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996185,Multi-words;Part of Speech Tagger;Clustering;Web Mining;Hierarchical Clustering;Information retrieval;Feature Extraction,Clustering algorithms;Web pages;Measurement;Algorithm design and analysis;Equations;Mathematical model;Speech,cataloguing;Internet;pattern clustering;query processing;search engines;text analysis,contemporary search engines;automated Web tools;information extraction;Web archives;Web documents organization;Web documents cataloging;user queries;multiword features;hierarchical clustering algorithm;noun sequences;link based representations;content based representations;clustering performance;hierarchical clustering engine,,2,,16,,29-Dec-14,,,IEEE,IEEE Conferences
Unsupervised clustering of text entities in heterogeneous grey level documents,異構灰度文檔中文本實體的無監督聚類,S. Bres; W. Eglin; A. Gagneux,"Lab. Reconnaissance de Formes et Vision, Inst. Nat. des Sci. Appliquees de Lyon, Villeurbanne, France; Lab. Reconnaissance de Formes et Vision, Inst. Nat. des Sci. Appliquees de Lyon, Villeurbanne, France; Lab. Reconnaissance de Formes et Vision, Inst. Nat. des Sci. Appliquees de Lyon, Villeurbanne, France",Object recognition supported by user interaction for service robots,10-Dec-02,2002,3,,224,227 vol.3,"Presents a new method of functional classification of text blocks on a document. It is based on texture analysis and unsupervised classification. Texture is used to define different classes of text blocks in the document and to direct a possible way of exploration from the most eye-catching data to the less significant text block. The typographical properties of blocks are characterized by two main discriminating primitives : the complexity of the text drawing and the structural relief of the block. This analysis is the starting point of a three-classes categorization into functional families (main headings, sub-headings and text paragraphs). Each block of text is described and classified through a labeling process based on a 3D-feature space using the two previous features (complexity and structural relief) and a third one among pattern primitives, blocks size and location in the document. This method allows a first approach to a global context free classification of documents.",1051-4651,0-7695-1695-X,10.1109/ICPR.2002.1047835,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1047835,,Labeling;Reconnaissance;Electronic mail;Humans;Decoding;Neural networks;Writing;Frequency;Abstracts;Entropy,text analysis;document image processing;image classification;pattern clustering;statistical analysis,unsupervised clustering;text entities;heterogeneous grey level documents;functional classification;text blocks;texture analysis;most eye-catching data;typographical properties;discriminating primitives;complexity;text drawing;structural relief;three-classes categorization;main headings;sub-headings;text paragraphs;3D-feature space;pattern primitives;global context free classification,,,,9,,10-Dec-02,,,IEEE,IEEE Conferences
Document space dimension reduction by Latent Semantic Analysis and Hebbian neural network,基於潛在語義分析和Hebbian神經網絡的文檔空間降維,I. Mokris; L. Skovajsova,"Institute of Informatics, Slovak Academy of Sciences, D繳bravsk獺 cesta 9, 84507 Bratislava, Slovakia; Institute of Informatics, Slovak Academy of Sciences, D繳bravsk獺 cesta 9, 84507 Bratislava, Slovakia",2008 6th International Symposium on Intelligent Systems and Informatics,5-Nov-08,2008,,,1,4,This paper presents the comparison of the text document space dimension reduction and the text document clustering and also the keyword space dimension reduction and keyword clustering by the latent semantic analysis and by the Hebbian neural network with Oja learning rule. Results of this neural network are compared with the results of the latent semantic analysis which uses the Singular value decomposition for dimension space reduction of the text documents in natural language.,1949-0488,978-1-4244-2406-1,10.1109/SISY.2008.4664910,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664910,,Neural networks;Feedforward neural networks;Self organizing feature maps;Clustering algorithms;Singular value decomposition;Informatics;Natural languages;Computational complexity;Functional analysis;Text analysis,document handling;Hebbian learning;information retrieval;neural nets;pattern clustering;singular value decomposition,text document space dimension reduction;latent semantic analysis;Hebbian neural network;text document clustering;keyword space dimension reduction;keyword clustering;Oja learning rule;singular value decomposition;natural language,,2,,19,,5-Nov-08,,,IEEE,IEEE Conferences
A Novel Partitioning-Based Clustering Method and Generic Document Summarization,一種新的基於分區的聚類方法和通用文檔摘要,R. M. Aliguliyev,"National Academy of Sciences, Azerbaijan",2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops,8-Jan-07,2006,,,626,629,This paper proposed the generic summarization method that extracts the most relevance sentences from the source document to form a summary. This method is based on clustering of sentences. The specificity of this approach is that the generated summary can contain the main contents of different topics as many as possible and reduce its redundancy at the same time. The clustering method satisfies as much homogeneity within each cluster as well as much separability between the clusters as possible,,0-7695-2749-3,10.1109/WI-IATW.2006.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053329,,Clustering methods;Data mining;Clustering algorithms;Web pages;Transmission line measurements;Intelligent agent;Information technology;Abstracts;Genetics;Paper technology,abstracting;pattern clustering;text analysis,generic document summarization;partitioning-based clustering method;sentence extraction,,15,,17,,8-Jan-07,,,IEEE,IEEE Conferences
Improving web clustering through a new modeling for web documents,通過新的Web文檔建模來改善Web群集,I. Agavriloaei; A. Alexandrescu; M. Craus,"Faculty of Automatic Control and Computer Engineering, ?Gheorghe Asachi??Technical University of Iasi, Romania; Faculty of Automatic Control and Computer Engineering, ?Gheorghe Asachi??Technical University of Iasi, Romania; Faculty of Automatic Control and Computer Engineering, ?Gheorghe Asachi??Technical University of Iasi, Romania","15th International Conference on System Theory, Control and Computing",28-Nov-11,2011,,,1,6,"The constant and rapid growth of the Web complexity and the Web size generates new challenges regarding the approaches in efficient processing of Web searched results. Due to the dynamic Web content and the huge amount of information returned by search engines, it is necessary to find new methods and ways for better organizing and modelling the information spread on the Web. In this paper, we propose the structuring of the Web content as a hierarchical environment, taking into account the site content and structure, the HTML document structure and the term importance. Furthermore, we propose an effective partitional clustering algorithm for a Web site. The preliminary results prove the effectiveness of the new Web content representation and the accuracy of the Web clustering algorithm.",,978-973-621-321-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085702,World Wide Web;Web document model;Web clustering;data mining;information retrieval,Clustering algorithms;HTML;Vectors;Accuracy;Algorithm design and analysis;Internet;Partitioning algorithms,content management;document handling;hypermedia markup languages;information management;information retrieval;Internet;pattern clustering;search engines;Web sites,Web clustering algorithm;Web documents;Web complexity;Web size;Web search;dynamic Web content;search engine;information organization;information modelling;HTML document structure;Web site;Web content representation,,,,21,,28-Nov-11,,,IEEE,IEEE Conferences
Document page decomposition by the bounding-box project,邊界框項目對文檔頁面的分解,Jaekyu Ha; R. M. Haralick; I. T. Phillips,"Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,1119,1122 vol.2,"This paper describes a method for extracting words, textlines and text blocks by analyzing the spatial configuration of bounding boxes of connected component on a given document image. The basic idea is that connected components of black pixels can be used as computational units in document image analysis. In this paper, the problem of extracting words, textlines and text blocks is viewed as a clustering problem in the 2-dimensional discrete domain. Our main strategy is that profiling analysis is utilized to measure horizontal or vertical gaps of (groups of) components during the process of image segmentation. For this purpose, we compute the smallest rectangular box, called the bounding box, which circumscribes a connected component. Those boxes are projected horizontally and/or vertically, and local and global projection profiles are analyzed for word, textline and text-block segmentation. In the last step of segmentation, the document decomposition hierarchy is produced from these segmented objects.",,0-8186-7128-9,10.1109/ICDAR.1995.602115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=602115,,Image analysis;Image segmentation;Protocols;Image databases;Computer science;Pixel;Text analysis;Printing;Production;Optical character recognition software,image segmentation;document image processing,document page decomposition;bounding-box project;words extraction;textlines;text blocks;spatial configuration;document image;black pixels;document image analysis;clustering problem;2-dimensional discrete domain;image segmentation;bounding box;document decomposition hierarchy,,14,,6,,6-Aug-02,,,IEEE,IEEE Conferences
A parallel XML documents placement algorithm based on adaptive ant clustering of chaos,基於混沌自適應螞蟻聚類的並行XML文檔放置算法,Bo Liu; LuMing Yang; XueMin Zhai; YunLong Deng,"College of Information Science and Engineering, Central-south University, ChangSha, Hunan 410083 China; College of Information Science and Engineering, Central-south University, ChangSha, Hunan 410083 China; College of Information Engineering of Jiangnan University, Wuxi Jiangsu 214122 China; The 3rd Xiangya Hospital of Central-south, University, ChangSha, Hunan 410013 China",2008 3rd IEEE Conference on Industrial Electronics and Applications,1-Aug-08,2008,,,84,89,"To improve the placement quality of massive extensible markup language (XML) document clustering, this paper proposes a parallel xml documents placement algorithm based on the chaos principle and an ant clustering model (ACC). With the XML key the algorithm combines the ACC clustering algorithm to create a new parallel XML documents placement algorithm. The whole manual ant-swarm move dynamically by defining relevant chaos fitness function to weigh the similarity of ants and neighbor- hoods, form many independent sub populations. In the mean time, it only requires partial information to adjust the parameters of the function automatically to drive the manual ants' movement. This series executions result in the acceleration of the XML document syncopation and improvement of the placement quality. Contrasted with other XML placement algorithms, a series of emulation experiments showed that this algorithm had a model with more visibility and less computation in XML documents.",2158-2297,978-1-4244-1717-9,10.1109/ICIEA.2008.4582485,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4582485,Parallel XML Databases;XML Key;Chaos;Ant Clustering,,document handling;optimisation;pattern clustering;XML,parallel XML documents placement algorithm;adaptive ant clustering;placement quality;massive extensible markup language;document clustering;chaos principle;ACC clustering algorithm;chaos fitness function;XML document syncopation,,,,11,,1-Aug-08,,,IEEE,IEEE Conferences
The Determination of Cluster Number at k-Mean Using Elbow Method and Purity Evaluation on Headline News,用肘法確定k均值的簇數和頭條新聞的純度評估,D. Marutho; S. Hendra Handaka; E. Wijaya; Muljono,"Computer Science Faculty of Dian, Nuswantoro University, Semarang, Indonesia; Computer Science Faculty of Dian, Nuswantoro University, Semarang, Indonesia; Computer Science Faculty of Dian, Nuswantoro University, Semarang, Indonesia; Computer Science Faculty of Dian, Nuswantoro University, Semarang, Indonesia",2018 International Seminar on Application for Technology of Information and Communication,29-Nov-18,2018,,,533,538,"Information is one of the most important thing in our lives, while humans is naturally impatient when searching for information from the internet. Users want to get the right answer instantaneously with minimal effort. News headlines can be used to categorize news types, as appropriate. The appropriate type of news can make it easier for us to choose the particular topic we want. Similarity in a title can be used to clustering news based on news title. From those reason this dataset research contain the title of online news site. TFIDF used as Document Preprocessing method, K-Means as clustering method, and elbow method used to optimize number of cluster. Purity method applied to evaluate news title clustering as internal evaluation. SSE (Sum Square Error) of each cluster are calculate and compared to optimize number of cluster in the elbow method, the result of those comparison evaluate using internal evaluation called purity, purity value is conformity between cluster and ideal cluster. From the calculation of elbow method, the most optimal number of cluster are 8 cluster, there is 0.228 point between 7cluster and 8 cluster SSE value so the elbow form are made. Purity evaluation method generates value 0.514 in the number of cluster are 8, this is the highest value and the one closest to one rather than the other number of cluster which mean the most ideal. The conclusion is the elbow method can be used to optimize number of cluster on K-Mean clustering method.",,978-1-5386-7486-4,10.1109/ISEMANTIC.2018.8549751,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8549751,Clustering;K-Means;ElbowM ethod;Purity;Tf-idf,Elbow;Clustering algorithms;Principal component analysis;Entropy;Seminars;Clustering methods;Genetic algorithms,information retrieval;Internet;pattern clustering;text analysis,sum square error;cluster SSE value;document preprocessing method;K-mean clustering method;internal evaluation;purity method;online news site;news title;clustering news;news headlines;cluster number;purity evaluation method;optimal number;elbow method,,9,,17,,29-Nov-18,,,IEEE,IEEE Conferences
Perspective Rectification of Document Images Based on Morphology,基於形態學的文檔圖像透視校正,L. Miao; S. Peng,"National ASIC Design Engineering Center, Institute of Automation, Chinese Academy of Sciences, Beijing 100080, China. ligang.miao@ia.ac.cn; National ASIC Design Engineering Center, Institute of Automation, Chinese Academy of Sciences, Beijing 100080, China. silong.peng@ia.ac.cn",2006 International Conference on Computational Intelligence and Security,29-Jan-07,2006,2,,1805,1808,"Documents may suffer from perspective distortion when captured with hand-held digital cameras. A morphology based rectification method is proposed to recover fronto-parallel views of perspectively distorted documents images. Firstly we extend the nearest-neighbor (NN) clustering technique in document skew rectification to locate the horizontal vanishing point of the text plane. Secondly we partition the image into multiple overlapping blocks centered with the centroid of each connected component (CC), and propose a run-length opening algorithm (RLOA) to compute the local orientation of vertical character stroke (VSB), which is used to locate the document's vertical vanishing point. Finally, a three-step hierarchical rectification method is proposed to rectify the distorted images. Experiment results on various types of documents show that the proposed rectification method has achieved accurate recovery of document images",,1-4244-0604-8,10.1109/ICCIAS.2006.295374,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076280,,Morphology;Carbon capture and storage;Neural networks;Digital cameras;Tin;Application specific integrated circuits;Design engineering;Design automation;Clustering algorithms;Partitioning algorithms,character recognition;document image processing;image reconstruction;pattern clustering,perspective rectification;document images;morphology;perspective distortion;handheld digital camera;fronto-parallel view recovery;nearest-neighbor clustering;document skew rectification;horizontal vanishing point;run-length opening algorithm;vertical character stroke orientation;hierarchical rectification,,2,,10,,29-Jan-07,,,IEEE,IEEE Conferences
Coordinate systems reconstruction for graphical documents by Hough-feature clustering and geometric analysis,基於霍夫特徵聚類和幾何分析的圖形文檔坐標系統重構,Z. Yanping; T. C. Lim,"Sch. of Comput., Nat. Univ. of Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",20-Sep-04,2004,1,,376,379 Vol.1,"Two-dimensional and three-dimensional coordinate systems are the basic graphics symbols in many graphical documents. A robust coordinate system detection scheme is needed in order to further decompose other graphics elements accurately. In this paper, we propose a robust coordinate systems reconstruction approach that combines the geometric information of the coordinate systems with feature clustering and line segments verification techniques in the Hough space. Experiments on both 2D and 3D scanned diagram images show that the proposed approach is robust and applicable in constructing the coordinate systems when comparing with other projection-based coordinate systems detection approaches.",1051-4651,0-7695-2128-2,10.1109/ICPR.2004.1334130,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334130,,Graphics;Robustness;Image reconstruction;Image segmentation;Reconstruction algorithms;Transforms;Equations;Testing;Data mining;Inspection,document image processing;image reconstruction;image segmentation;pattern clustering;Hough transforms;geometry,robust coordinate systems reconstruction;graphical documents;Hough feature clustering technique;geometric analysis;two dimensional coordinate systems;three dimensional coordinate systems;robust coordinate system detection;geometric information;line segment verification techniques;Hough space;2D scanned diagram images;3D scanned diagram images;projection based coordinate systems,,,,8,,20-Sep-04,,,IEEE,IEEE Conferences
Automated forensic ink determination in handwritten documents by clustering,通過聚類自動確定手寫文檔中的法醫墨水,M. Kalbitz; C. Vielhauer,"Otto-von-Guericke-University Magdeburg, Universit穡atsplatz 2,Faculty of Computer Science,Magdeburg,Germany,39106; University of Applied Sciences Brandenburg,Department of Informatics and Media,Brandenburg an der Havel,Germany,14770",2019 27th European Signal Processing Conference (EUSIPCO),18-Nov-19,2019,,,1,5,"Even in today`s highly digitalized world, the use of handwriting is still widely in use for legal documents such as testaments, contracts, bank cheques or professional certificates. Thus, forgery analysis of handwriting still poses challenges for criminalistics forensic document examiners and one of the investigation questions is, if a questioned document has been written with more than one ink. If so, this may indicate a forgery by manipulations of a possible counterfeiter after production of the genuine original. By means of chemical analysis, it is possible today to identify an ink with almost 100% certainty. However, this process is manual, tedious and needs an initial suspicion by a human expert, that more than one ink was applied on a specific area on the document. Further, most chemical approaches are destructive and limited to very small areas. To improve and to automate the initial investigation on the use of multiple inks, this work proposes a pattern recognition approach based on signal processing, feature extraction and classification by data clustering, which is based on spectral imaging, acquired in almost non-destructive and contact-less manner. The goal is to support forensic examiners by an automated digital detection of regions, which have been written using different ink, which they then can further examine. For experimental evaluation, a benchmark is introduced to evaluate the accuracy of detection results on a specifically created test set, which is also presented. Test results indicate that the best clustering in our investigation has been achieved by the expectation maximisation (EM) approach, with a correct ink cover rate of above 80% for the first and 73% for the second ink in average. Even more relevant for forensic experts is the observation, that false detections occurred in less than 1% of the cases in average. Future work will include extension of data sets and automatic analysis and parameter adjustments in the clustering process.",2076-1465,978-9-0827-9703-9,10.23919/EUSIPCO.2019.8903107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903107,Forensics;Pattern clustering;Handwriting recognition;Forgery;Spectral analysis;spectroscopy;ink,Ink;Signal processing;Forensics;Wavelength measurement;Feature extraction;Benchmark testing;Image color analysis,document image processing;feature extraction;handwriting recognition;handwritten character recognition;image forensics;ink;pattern clustering,automated forensic ink determination;handwritten documents;handwriting;legal documents;bank cheques;professional certificates;forgery analysis;criminalistics forensic document examiners;investigation questions;questioned document;possible counterfeiter;chemical analysis;initial suspicion;chemical approaches;initial investigation;multiple inks;pattern recognition approach;data clustering;contact-less;forensic examiners;different ink;specifically created test set;correct ink cover rate;forensic experts;automatic analysis;parameter adjustments;clustering process,,,,16,,18-Nov-19,,,IEEE,IEEE Conferences
Analysis of book documents' table of content based on clustering,基於聚類的圖書文檔目錄分析,L. Gao; Z. Tang; X. Lin; X. Tao; Y. Chu,"Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; NA; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,911,915,"Table of contents (TOC) recognition has attracted a great deal of attention in recent years. After reviewing the merits and drawbacks of the existing TOC recognition methods, we have observed that book documents are multi-page documents with intrinsic local format consistency. Based on this finding we introduce an automatic TOC analysis method through clustering. This method first detects the decorative elements in TOC pages. Then it learns a layout model used in the TOC pages through clustering. Finally, it generates TOC entries and extracts their hierarchical structure under the guidance of the model. More specifically, broken lines are taken into account in the method. Experimental results show that this method achieves high accuracy and efficiency. In addition, this method has been successfully applied in a commercial e-book production software package.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277548,,Books;Layout;Data mining;Clustering algorithms;Adaptation model;Algorithm design and analysis,document handling;electronic publishing;pattern clustering,book documents;table of contents recognition;multipage documents;intrinsic local format consistency;decorative element detection;layout model;commercial e-book production software package;clustering technique,,3,,12,,2-Oct-09,,,IEEE,IEEE Conferences
A probabilistic hierarchical clustering method for organising collections of text documents,一種組織文本文檔集合的概率層次聚類方法,A. Vinokourov; M. Girolami,"Dept. of Comput. & Inf. Syst., Paisley Univ., UK; NA",Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,6-Aug-02,2000,2,,182,185 vol.2,A generic probabilistic framework for the unsupervised hierarchical clustering of large-scale sparse high-dimensional data collections is proposed. The framework is based on a hierarchical probabilistic mixture methodology. Two classes of models emerge from the analysis and these have been called symmetric and asymmetric models. For text data specifically both asymmetric and symmetric models based on multinomial and binomial distributions are most appropriate. An expectation maximisation parameter estimation method is provided for all of these models. An experimental comparison of the models is obtained for two extensive online document collections.,1051-4651,0-7695-0750-6,10.1109/ICPR.2000.906043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=906043,,Clustering methods;Parameter estimation;Databases;Computational intelligence;Information systems;Large-scale systems;Internet;Information retrieval;Costs;Topology,binomial distribution;parameter estimation;pattern clustering;text analysis;unsupervised learning,probabilistic hierarchical clustering method;text documents;unsupervised hierarchical clustering;large-scale sparse high-dimensional data collections;hierarchical probabilistic mixture methodology;asymmetric models;symmetric models;binomial distributions;multinomial distributions;expectation maximisation parameter estimation method;online document collections,,9,,13,,6-Aug-02,,,IEEE,IEEE Conferences
Improved nearest neighbor based approach to accurate document skew estimation,改進的基於最近鄰的方法可準確估計文檔偏斜,Yue Lu; Chew Lim Tan,"Dept. of Comput. Sci., Nat. Univ. of Singapore, Kent Ridge, Singapore; Dept. of Comput. Sci., Nat. Univ. of Singapore, Kent Ridge, Singapore","Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,503,507 vol.1,"The nearest-neighbor based document skew detection methods do not require the presence of a predominant text area, and are not subject to skew angle limitation. However, the accuracy of these methods is not perfect in general. In this paper, we present an improved nearest-neighbor based approach to perform accurate document skew estimation. Size restriction is introduced to the detection of nearest-neighbor pairs. Then the chains with a largest possible number of nearest-neighbor pairs are selected, and their slopes are computed to give the skew angle of document image. Experimental results on various types of documents containing different linguistic scripts and diverse layouts show that the proposed approach has achieved an improved accuracy for estimating document image skew angle and has an advantage of being language independent.",,0-7695-1960-1,10.1109/ICDAR.2003.1227716,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227716,,Nearest neighbor searches;Histograms;Text analysis;Image analysis;Clustering algorithms;Layout;Computer science;Character recognition;Graphics,document image processing;image recognition,nearest neighbor based approach;document skew estimation;document skew detection;skew angle limitation;size restriction;document image;linguistic script,,11,,16,,8-Sep-03,,,IEEE,IEEE Conferences
Clustering aggregation based on genetic algorithm for documents clustering,基於遺傳算法的文檔聚類聚類,Zhenya Zhang; Hongmei Cheng; Shuguang Zhang; Wanli Chen; Qiansheng Fang,"School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China; Management Engineering Department of ALAI, Hefei 230022, China; Department of Statistics & Finance, University of Science and Technology of China, China; School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China; School of Electrical and Information Engineering, Anhui Institute of Architecture&Industry (AIAI), Hefei 230022, China",2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence),23-Sep-08,2008,,,3156,3161,"Clustering aggregation problem is a kind of formal description for clustering ensemble problem and technologies for the solving of clustering aggregation problem can be used to construct clustering division with better clustering performance when the clustering performances of each original clustering division are fluctuant or weak. In this paper, an approach based on genetic algorithm for clustering aggregation problem, named as GeneticCA, is presented To estimate the clustering performance of a clustering division, clustering precision is defined and features of clustering precision are discussed In our experiments about clustering performances of GeneticCA for document clustering, hamming neural network is used to construct clustering divisions with fluctuant and weak clustering performances. Experimental results show that the clustering performance of clustering division constructed by GeneticCA is better than clustering performance of original clustering divisions with clustering precision as criterion.",1941-0026,978-1-4244-1822-0,10.1109/CEC.2008.4631225,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4631225,,Evolutionary computation,document handling;genetic algorithms;neural nets;pattern clustering,genetic algorithm;documents clustering;clustering aggregation problem;formal description;clustering division;clustering precision;hamming neural network,,16,,14,,23-Sep-08,,,IEEE,IEEE Conferences
XML Document Co-clustering via Non-negative Matrix Tri-factorization,通過非負矩陣三因子分解進行XML文檔共聚,G. Costa; R. Ortale,"ICAR, Rende, Italy; ICAR, Rende, Italy",2014 IEEE 26th International Conference on Tools with Artificial Intelligence,15-Dec-14,2014,,,607,614,"XML co-clustering is a promising method to overcome the effectiveness of traditional XML clustering approaches, due to the exploitation of the mutual relationships between XML documents and their respective XML features while clustering both simultaneously. To shed light on this so far unexplored research direction, we conduct a systematic study of the effectiveness of XML co-clustering, by viewing the task as parametric with respect to the XML features. Thus, the definition and exploitation of three distinct types of XML features, which are respectively informative of the content, structure and both aspects of the XML documents, allows an in-depth investigation of all three different instances of the XML co-clustering task, i.e., XML co-clustering by content alone, structure alone as well as both structure and content. XML co-clustering relies on a non-negative matrix trifactorization technique, that efficiently processes large-scale input data, which is especially useful with large corpora of text-centric XML documents. The relevance of the structural and content features of the XML documents is assessed through a new weighting scheme. An intensive experimental evaluation on real-world benchmark XML corpora reveals a higher effectiveness of XML co-clustering in comparison with state-of-the-art approaches to XML clustering. Insights are also provided on the effectiveness of XML feature clustering.",2375-0197,978-1-4799-6572-4,10.1109/ICTAI.2014.96,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984532,XML Analysis;Semistructured Data Mining;XML Co-Clustering,XML;Context;Matrix decomposition;Vegetation;Encyclopedias;Electronic publishing,document handling;matrix decomposition;pattern clustering;XML,XML document coclustering;nonnegative matrix trifactorization;XML clustering approach;XML co-clustering;text-centric XML document;structural feature;content feature;XML corpora;XML coclustering;XML feature clustering,,6,,41,,15-Dec-14,,,IEEE,IEEE Conferences
Implementation of MCL algorithm in clustering digital news with graph representation,圖表示法在數字新聞聚類中的MCL算法實現,A. M. Ubaidillah Al-Fath; W. K. R. Saleh; S. Sa'adah,"Informatics Engineering program, Informatics Faculty, Telkom University, Bandung, Indonesia; Informatics Engineering program, Informatics Faculty, Telkom University, Bandung, Indonesia; M.T. Informatics Engineering program, Informatics Faculty, Telkom University, Bandung, Indonesia",2016 4th International Conference on Information and Communication Technology (ICoICT),22-Sep-16,2016,,,1,6,"Digital news will continue to grow up and evolve, it bring up the new issue for modeling digital news data that are stored in the database so that it is easier to understand and be able to take some important information thoroughly. To simplify the information processing in the database it is require a model and a specific method for clustering the news based on proximity and characteristics of the digital news. Using a graph database models and methods, MCL graph clustering algorithm (Markov Cluster Algorithm) can simplify information processing by identifying the characteristics of each vertex in the graph so that it will establish cluster vertices with a specific label. In the process of identifying a clusters of graph, the digital news documents will be stored into one vertex to be connected with other vertex in common category of news. The process of expanding and inflating matrix will be the main process in the clustering of digital news that has been transformed into a graph database models that expand aims to show a new edge and remove the old edge in common not needed in the graph. Meanwhile the process of inflating aims to strengthen the strong edge and weaken the weak edge. So that with the clustering of digital news, process of inflating matrix is very influential for execution time in MCL algorithm and process of inflating matrix influence the number of cluster will be formed.",,978-1-4673-9879-4,10.1109/ICoICT.2016.7571917,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571917,Digital news;graph database;clustering graph;MCL algorithm,Clustering algorithms;Databases;Algorithm design and analysis;Informatics;Testing;Data models;Markov processes,database management systems;document handling;graph theory;Internet;Markov processes;pattern clustering,Markov cluster algorithm;MCL graph clustering algorithm;digital news clustering;graph representation;matrix expansion;matrix inflation;graph database model;Internet,,,,9,,22-Sep-16,,,IEEE,IEEE Conferences
Topic Modeling Technique for Text Mining Over Biomedical Text Corpora Through Hybrid Inverse Documents Frequency and Fuzzy K-Means Clustering,混合逆文檔頻率和模糊K-均值聚類的生物醫學文本語料文本挖掘主題建模技術,J. Rashid; S. M. Adnan Shah; A. Irtaza; T. Mahmood; M. W. Nisar; M. Shafiq; A. Gardezi,"Department of Computer Science, University of Engineering and Technology Taxila, Punjab, Pakistan; Department of Computer Science, University of Engineering and Technology Taxila, Punjab, Pakistan; Department of Computer Science, University of Engineering and Technology Taxila, Punjab, Pakistan; Department of Computer Science, University of Engineering and Technology Taxila, Punjab, Pakistan; Department of Computer Science, COMSATS University Islamabad, Wah Campus, Wah Cantt, Pakistan; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Computer Science, COMSATS University Islamabad, Islamabad, Pakistan",IEEE Access,16-Oct-19,2019,7,,146070,146080,"Text data plays an imperative role in the biomedical domain. As patient's data comprises of a huge amount of text documents in a non-standardized format. In order to obtain the relevant data, the text documents pose a lot of challenging issues for data processing. Topic modeling is one of the popular techniques for information retrieval based on themes from the biomedical documents. In topic modeling discovering the precise topics from the biomedical documents is a challenging task. Furthermore, in biomedical text documents, the redundancy puts a negative impact on the quality of text mining as well. Therefore, the rapid growth of unstructured documents entails machine learning techniques for topic modeling capable of discovering precise topics. In this paper, we proposed a topic modeling technique for text mining through hybrid inverse document frequency and machine learning fuzzy k-means clustering algorithm. The proposed technique ameliorates the redundancy issue and discovers precise topics from the biomedical text documents. The proposed technique generates local and global term frequencies through the bag-of-words (BOW) model. The global term weighting is calculated through the proposed hybrid inverse documents frequency and Local term weighting is computed with term frequency. The robust principal component analysis is used to remove the negative impact of higher dimensionality on the global term weights. Afterward, the classification and clustering for text mining are performed with a probability of topics in the documents. The classification is performed through discriminant analysis classifier whereas the clustering is done through the k-means clustering. The performance of clustering is evaluated with Calinsiki-Har-abasz (CH) index internal validation method. The proposed toping modeling technique is evaluated on six standard datasets namely Ohsumed, MuchMore Springer Corpus, GENIA corpus, Bioxtext, tweets and WSJ redundant corpus for experimentation. The proposed topic modeling technique exhibits high performance on classification and clustering in text mining compared to baseline topic models like FLSA, LDA, and LSA. Moreover, the execution time of the proposed topic modeling technique remains stable for different numbers of topics.",2169-3536,,10.1109/ACCESS.2019.2944973,National Research Foundation of Korea; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854068,Topic modeling;biomedical;text mining;bag-of-words;classification;clustering;LDA;LSA,Computational modeling;Biological system modeling;Text mining;Probabilistic logic;Task analysis;Redundancy;Biomedical imaging,data mining;document handling;learning (artificial intelligence);medical computing;natural language processing;pattern classification;pattern clustering;principal component analysis;query processing;text analysis,topic modeling technique;text mining;biomedical text corpora;hybrid inverse documents frequency;text data;biomedical text documents;unstructured documents;hybrid inverse document frequency;local term frequencies;global term frequencies;bag-of-words model;global term weighting;baseline topic models;machine learning fuzzy k-means clustering algorithm;information retrieval;robust principal component analysis;Calinsiki-Har-abasz index internal validation method;Ohsumed;MuchMore Springer Corpus;GENIA corpus;Bioxtext,,,,56,CCBY,1-Oct-19,,,IEEE,IEEE Journals
Automatic text categorization: Marathi documents,自動文本分類：Marathi文檔,J. J. Patil; N. Bogiri,"Department of Computer Engineering (Computer Networks), K. J. College of Engineering & Management, Research Pune, India; Department of Computer Engineering (Computer Networks), K. J. College of Engineering & Management, Research Pune, India",2015 International Conference on Energy Systems and Applications,4-Jul-16,2015,,,689,694,"Information technology generated huge data on the internet. Initially this data is mainly in English language so majority of data mining research work is on the English text documents. As the internet usage increased, data in other languages like Marathi, Tamil, Telugu and Punjabi etc. increased on the internet. This paper presents the retrieval system for Marathi language documents based on the user profile. User profile considers the user's interests, user's browsing history. The system shows the Marathi documents to the end user based on the user profile. Automatic text categorization is useful in better management and retrieval of these text documents and also makes document retrieval as simple task. This paper discusses the automatic text categorization of Marathi documents and literature survey of the related work done in automatic text categorization of Marathi documents. Various learning techniques exist for the classification of text documents like Na簿ve Bayes, Support Vector Machine and Decision Trees etc. There are different clustering techniques used for text categorization like Label Induction Grouping Algorithm, Suffix Tree Clustering, and K- means etc. Literature survey shows that for non-English documents VSM [Vector Space Model] gives the better results than any other models. The system provides text categorization of Marathi documents by using the LINGO [Label Induction Grouping] algorithm. LINGO is based on the VSM [Vector Space Model]. The system uses the dataset which contains 200 documents of 20 different categories. The result represents that for Marathi text documents LINGO clustering algorithm is efficient.",,978-1-4673-6817-9,10.1109/ICESA.2015.7503438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503438,Categorization;Clustering;Marathi Documents;LINGO;User profile,Clustering algorithms;Matrix decomposition;Text categorization;Internet;Algorithm design and analysis;Classification algorithms;Search engines,Bayes methods;decision trees;information retrieval;learning (artificial intelligence);natural language processing;pattern clustering;support vector machines;text analysis,automatic text categorization;Marathi documents;Internet usage;English language;English text documents;Tamil;Telugu;Punjabi;document retrieval;learning techniques;na簿ve Bayes;support vector machine;decision trees;clustering techniques;label induction grouping algorithm;suffix tree clustering;K-means;nonEnglish documents VSM;vector space model;label induction grouping;LINGO clustering algorithm,,13,,10,,4-Jul-16,,,IEEE,IEEE Conferences
Fuzzy discrete correlation for document clustering,用於文檔聚類的模糊離散相關,M. Danesh; M. Naghibzadeh; A. Harati,"Computer Engineering Department, Ferdowsi University of Mashhad, Mashhad, Iran; Computer Engineering Department, Ferdowsi University of Mashhad, Mashhad, Iran; Computer Engineering Department, Ferdowsi University of Mashhad, Mashhad, Iran",2011 International Symposium on Artificial Intelligence and Signal Processing (AISP),25-Jul-11,2011,,,59,65,"Nowadays, there is an enormous growth in the quantity of text documents on the Internet, digital libraries and news sources. This has led to an increased interest in developing methods that help users to effectively navigate, summarize, and organize this information. A new method that uses neighbor and link concepts has more suitable performance than previous methods in this field. Two documents are neighbors if their similarity is more than a defined threshold. If they are neighbors, neighbor matrix element is set to one, otherwise it is set to zero. So we lose some information about documents similarity in it and therefore decrease of accuracy. To overcome this problem, we propose two methods of ?discrete correlation??and ?fuzzy correlation?? which both of them attempt to accurate neighbor definition more and more and so reach better clustering results. To evaluate our work, we used k-means algorithm to determine the initial cluster centers and similarity criteria between documents and centers. The results of applying proposed method on real-world document data sets by information retrieval factors show better performance than traditional algorithms and previous works.",,978-1-4244-9834-5,10.1109/AISP.2011.5960974,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960974,document clustering;neighbor;link;similarity;correlation;fuzzy;discrete,Correlation;Clustering algorithms;Polynomials;Fuzzy sets;Fuzzy systems;Accuracy;Software,digital libraries;fuzzy reasoning;information retrieval;Internet;pattern clustering;text analysis,fuzzy discrete correlation;document clustering;text document;Internet;digital libraries;news sources;neighbor matrix element;discrete correlation;fuzzy correlation;pattern clustering;k-means algorithm;information retrieval factors,,1,,12,,25-Jul-11,,,IEEE,IEEE Conferences
A Content Spotting System for Line Drawing Graphic Document Images,線圖圖形文檔圖像的內容髮現系統,M. M. Luqman; T. Brouard; J. Ramel; J. Llod籀s,"Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France; Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France; Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France; Comput. Vision Center, Univ. Autonoma de Barcelona, Barcelona, Spain",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,3420,3423,"We present a content spotting system for line drawing graphic document images. The proposed system is sufficiently domain independent and takes the keyword based information retrieval for graphic documents, one step forward, to Query By Example (QBE) and focused retrieval. During offline learning mode: we vectorize the documents in the repository, represent them by attributed relational graphs, extract regions of interest (ROIs) from them, convert each ROI to a fuzzy structural signature, cluster similar signatures to form ROI classes and build an index for the repository. During online querying mode: a Bayesian network classifier recognizes the ROIs in the query image and the corresponding documents are fetched by looking up in the repository index. Experimental results are presented for synthetic images of architectural and electronic documents.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.835,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597525,content spotting;graphic document retrieval;query by example;fuzzy structural signature,Graphics;Bayesian methods;Pattern recognition;Context;Technical drawing;Indexing,belief networks;content-based retrieval;document image processing;image classification;pattern clustering,content spotting system;line drawing graphic document images;keyword based information retrieval;query by example;offline learning mode;attributed relational graphs;regions of interest extraction;fuzzy structural signature;similar signature clustering;online querying mode;Bayesian network classifier,,8,,24,,7-Oct-10,,,IEEE,IEEE Conferences
Managing the knowledge contained in electronic documents: a clustering method for text mining,管理電子文檔中包含的知識：用於文本挖掘的聚類方法,S. Iiritano; M. Ruffolo,"Getrronics SpA, Rende, Italy; NA",12th International Workshop on Database and Expert Systems Applications,7-Aug-02,2001,,,454,458,"The huge amount of unstructured data available on the Web and the intranets creates an information overloading problem. So, managing the knowledge contained in the textual documents is an important problem of Knowledge Management. Knowledge Extraction from collections of data is possible by Knowledge Discovery in Database (KDD), an interactive and iterative process focused on the exploration of data to discover new and interesting patterns within them. The fundamental phase of KDD process is Data Mining if data are in structured form and Text Mining when they are unstructured. This paper describes a prototype of a vertical corporate portal that implements a KDD process for knowledge extraction from unstructured data contained in textual documents. Text mining is realized through a clustering method that produces a partition of a set of documents on the basis of their contents characterized through the frequency of the words.",,0-7695-1230-5,10.1109/DEXA.2001.953103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953103,,Knowledge management;Clustering methods;Text mining;Data mining;Portals;Databases;Prototypes;Frequency;Refining;Instruments,data mining;pattern clustering;intranets;information resources,textual documents;knowledge management;web portal;knowledge extraction;knowledge discovery in database;KDD;Web;vertical corporate portal;intranets;electronic documents;clustering method;data mining;text mining;unstructured data,,4,,22,,7-Aug-02,,,IEEE,IEEE Conferences
Document clustering based on time series,基於時間序列的文檔聚類,L. S. Matei; ?. Tr?u?an-Matu,"University Politehnica of Bucharest, Faculty of Automatic Control and Computer Science, Bucharest, Romania; University Politehnica of Bucharest, Faculty of Automatic Control and Computer Science, Romania","2015 19th International Conference on System Theory, Control and Computing (ICSTCC)",9-Nov-15,2015,,,128,133,"This paper presents a novel document clustering algorithm that represents documents as a time series of words. Document clustering is very important due to the fact that it permits us to group them based on some certain criteria, especially nowadays when a large number of articles are available. The timed series representation of the document instead of the vector model permits us to consider a new algorithm for the computation of the distance between documents: dynamic time warping. This novel representation together with the dynamic time warping algorithm represents the foundation for computing the similarity and the clustering of the documents. The clustering algorithm used is hierarchical clustering. This novel clustering method of texts is applied on named entities and on the parts of speech of the words that compose the documents. As test data we are using the Reuters corpus of newspaper articles.",,978-1-4799-8481-7,10.1109/ICSTCC.2015.7321281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321281,time series;clustering;words;natural language processing,Time series analysis;Clustering algorithms;Speech;Heuristic algorithms;Signal processing algorithms;Computational modeling;Algorithm design and analysis,document handling;pattern clustering;time series,novel document clustering algorithm;timed series representation;dynamic time warping algorithm;hierarchical clustering;Reuters corpus;newspaper articles,,2,,9,,9-Nov-15,,,IEEE,IEEE Conferences
Exploring diseases based biomedical document clustering and visualization using self-organizing maps,使用自組織圖探索基於疾病的生物醫學文檔聚類和可視化,S. Shah; X. Luo,"Purdue School of Engineering and Technology, Indiana University-Purdue University Indianapolis, Indianapolis, USA; Purdue School of Engineering and Technology, Indiana University-Purdue University Indianapolis, Indianapolis, USA","2017 IEEE 19th International Conference on e-Health Networking, Applications and Services (Healthcom)",18-Dec-17,2017,,,1,6,"Document clustering is a text mining technique used to provide better document search and browsing in digital libraries or online corpora. In this research, a vector representation of concepts of diseases and similarity measurement between concepts are proposed. They identify the closest concepts of diseases in the context of a corpus. Each document is represented by using the vector space model. A weight scheme is proposed to consider both local content and associations between concepts. Self-Organizing Maps (SOM) are often used as document clustering algorithm. The vector projection and visualization features of SOM enable visualization and analysis of the cluster distribution and relationships on the two dimensional space. The Davies-Bouldin index is used to validate the clusters based on the visualized cluster distributions. The results show that the proposed document clustering framework generates meaningful clusters and can facilitate clustering visualization and information retrieval based on the concepts of diseases.",,978-1-5090-6704-6,10.1109/HealthCom.2017.8210791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8210791,,Diseases;Biomedical measurement;Visualization;Unified modeling language;Self-organizing feature maps;Semantics;Ontologies,data mining;data visualisation;digital libraries;diseases;information retrieval;medical computing;pattern clustering;self-organising feature maps;text analysis,text mining technique;document search;digital libraries;vector representation;vector space model;SOM;document clustering algorithm;vector projection;visualization features;cluster distribution;dimensional space;visualized cluster distributions;document clustering framework;clustering visualization;information retrieval;self-organizing maps;diseases based biomedical document clustering;diseases based biomedical document visualization;Davies-Bouldin index,,,,21,,18-Dec-17,,,IEEE,IEEE Conferences
Incremental fuzzy clustering for document categorization,增量模糊聚類用於文檔分類,J. Mei; Y. Wang; L. Chen; C. Miao,"College of Computer Science and Technology, Zhejiang University of Technology, 310023 Hangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore",2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),8-Sep-14,2014,,,1518,1525,"Incremental clustering has been proposed to handle large datasets which can not fit into memory entirely. Single pass fuzzy c-means (SpFCM) and Online fuzzy c-means (OFCM) are two representative incremental fuzzy clustering methods. Both of them extend the scalability of fuzzy c-means (FCM) by processing the dataset chunk by chunk. However, due to the data sparsity and high-dimensionality, SpFCM and OFCM fail to produce reasonable results for document data. In this study, we work on clustering approaches that take care of both the large-scale and high-dimensionality issues. Specifically, we propose two methods for incrementally clustering of document data. The first method is a modification of the existing FCM-based incremental clustering with a step to normalize the centroids in each iteration, while the other method is incremental clustering, i.e., Single-Pass or Online, with weighted fuzzy co-clustering. We use several benchmark document datasets for experimental study. The experimental results show that the proposed approaches achieved significant improvements over existing SpFCM and OFCM in document clustering.",1098-7584,978-1-4799-2072-3,10.1109/FUZZ-IEEE.2014.6891554,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891554,,Clustering algorithms;Vectors;Scalability;Educational institutions;Electronic mail;Computers;Atmospheric measurements,document handling;fuzzy set theory;pattern clustering,benchmark document datasets;weighted fuzzy co-clustering;document data;data high-dimensionality;data sparsity;OFCM;online fuzzy c-means;SpFCM;fuzzy c-means;document categorization;incremental fuzzy clustering method,,5,,22,,8-Sep-14,,,IEEE,IEEE Conferences
Document Clustering and Topic Modeling: A Unified Bayesian Probabilistic Perspective,文檔聚類和主題建模：統一的貝葉斯概率觀點,G. Costa; R. Ortale,"ICAR-CNR, Italy; ICAR-CNR, Italy",2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),13-Feb-20,2019,,,278,285,"Document clustering and topic modeling are fundamental tasks in text mining, that can be unified to reciprocally enhance each other. In this paper, we present a machine learning approach to the joint modeling and interdependent fulfilment of both tasks. In particular, document clustering and topic modeling are seamlessly interrelated under an innovative Bayesian generative model of clusters, topics and contents in text corpora. Such a model assumes that text corpora result from a generative process, in which clusters and topics act as connected latent factors. Essentially, clusters are initially associated with descriptive and actionable topic distributions, that enforce cluster coherence. The individual documents are then assigned to one respective cluster and worded accordingly. Under the devised model, document clustering and topic modeling can be simultaneously performed in an interdependent manner simply by Bayesian reasoning. For this purpose, the mathematical details regarding collapsed Gibbs sampling as well as parameter estimation are derived and implemented into an approximate inference algorithm. Comparative experiments on standard benchmark text corpora reveal the effectiveness of our approach at jointly clustering text documents and unveiling their semantics in terms of coherent topics.",2375-0197,978-1-7281-3798-8,10.1109/ICTAI.2019.00047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995423,Document Clustering;Topic Modeling;Bayesian Text Analysis,,Bayes methods;belief networks;data mining;inference mechanisms;learning (artificial intelligence);pattern clustering;probability;text analysis,document clustering;topic modeling;unified Bayesian probabilistic perspective;joint modeling;innovative Bayesian generative model;text corpora result;actionable topic distributions;cluster coherence;respective cluster;devised model;text documents;coherent topics,,,,32,,13-Feb-20,,,IEEE,IEEE Conferences
The research of theme identification in scientific documents,科技文獻主題識別研究,Y. Chunlei; F. Lu,"National Science of Library, Chinese Academy of Science, Beijing, China; Information Department, Beijing City University, Beijing, China",2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE),20-Aug-12,2012,3,,715,718,"There is abundant thematic information in the technical documentations which can reveal the content of the subject. Co-word analysis is an important method for Scientometrics analysis. And the theme clustering analysis based on co-word has become one of the most active research fields. Co-word clustering analysis forms a series of paper clusters which consists of scientific and technological documents. These theme clustering reflect the evolution of the development trend which contribute to grasp the development of science for researchers. So, It is necessary to identify the theme of these clusters. This paper analyses some typical approaches of theme identification in co-word analysis and their drawbacks, and advances an improved method that combines Latent Dirichlet Allocation model for theme identification. The experimental results prove that the advanced approach can utilize the merits of improved co-word analysis, especially in enhancing the thematic characteristic and coherency among the descriptors. And thus the advanced approach can be better used in theme identification of scientific documents.",,978-1-4673-0089-6,10.1109/CSAE.2012.6273049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273049,co-word;theme identification;Latent Dirichlet Allocation;cluster analysis,Indexes;Software engineering;Software;Cities and towns;Educational institutions;Algorithm design and analysis,document handling;pattern clustering;scientific information systems,theme identification;scientific documents;thematic information;technical documentations;scientometrics analysis;theme clustering analysis;coword clustering analysis;latent Dirichlet allocation model,,,,7,,20-Aug-12,,,IEEE,IEEE Conferences
Web document categorization by Support Vector Clustering,通過支持向量聚類對Web文檔進行分類,Daming Shi; M. H. Tsui; Jigang Liu,"School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798; School of Computer Engineering, Nanyang Technological University, Singapore 639798","2008 IEEE International Conference on Systems, Man and Cybernetics",7-Apr-09,2008,,,1483,1488,"Search Engine has proven its effectiveness for retrieval of information from World Wide Web. Traditionally, the search results are arranged in an ordered list by popularity and relevancy. However, the enormous size of matched Web pages causes inefficiency for users to locate the most relevant Web pages. A proper organization of the search result is important to improve its browsability of Web searching. In this paper, we proposed by performing Support Vector Clustering (SVC) on the search result to reorganize results in groups of similar context to facilitate effective browsing of search result by the users. SVC is a nonparametric clustering algorithm that can group clusters with arbitrary shapes and without the need to specify the number of clusters. It is a kernel clustering method that maps via a nonlinear function to a high dimension feature space. To obtain the optimal clustering result, choosing of the accurate parameters (kernel width and penalty coefficient) for SVC is crucial. In this paper, it proposed an automatic tuning method for SVC parameters to obtain the optimal result. The results from the experiment have proven the effectiveness and usefulness of above mentioned method. The performance is comparable to other popular clustering techniques.",1062-922X,978-1-4244-2383-5,10.1109/ICSMC.2008.4811495,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811495,Document clustering;support vector clustering;simulated annealing,Static VAr compensators;Web pages;Search engines;Support vector machines;Web sites;Clustering algorithms;Kernel;Clustering methods;Simulated annealing;Information retrieval,information retrieval;Internet;nonlinear functions;online front-ends;pattern clustering;search engines;support vector machines;text analysis,Web document categorization;support vector clustering;search engine;information retrieval;automatic tuning method;Web browsability;nonlinear function,,1,,17,,7-Apr-09,,,IEEE,IEEE Conferences
Clustering improvement via integrating with sparse topical coding,通過與稀疏主題編碼集成來改善聚類,P. Ahmadi; R. Kaviani; I. Gholampour; M. Tabandeh,"Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran",2015 23rd Iranian Conference on Electrical Engineering,2-Jul-15,2015,,,466,471,"Topic modeling can improve document clustering by projecting documents into a topic space. By document, we mean a general concept. Document can be an image, a video, a textual document or each data which can be described in bag-of-words model based on the histogram of its features. In this paper, we introduce a clustering method based on Sparse Topical Coding (STC). In the proposed method, document clustering and topic modeling are integrated into a unified framework and jointly performed to achieve the best clustering performance. Our method clusters the documents based on STC topic modeling used for mining the topics and K-means clustering used for discovering latent groups in document collection. Experimental results show the effectiveness of our proposed clustering approach.",2164-7054,978-1-4799-1972-7,10.1109/IranianCEE.2015.7146260,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146260,Document clustering;topic model;Sparse Topical Coding (STC);K-means,Decision support systems;Electrical engineering;Conferences,compressed sensing;document image processing;image coding;pattern clustering;text analysis;video signal processing;visual databases;word processing,clustering improvement;sparse topical coding;STC topic modeling;document clustering;topic space;textual document;bag-of-words model;K-means clustering;document collection,,1,,15,,2-Jul-15,,,IEEE,IEEE Conferences
Scanner-model-based document image improvement,基於掃描儀模型的文檔圖像改進,M. Bern; D. Goldberg,"Xerox Palo Alto Res. Center, CA, USA; NA",Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),6-Aug-02,2000,2,,582,585 vol.2,"We describe a method for improving scanned or faxed document images. Our method assumes a probabilistic model of the scanning process, and uses this model to cluster instances of the same letter and to compute super-resolved representatives of the clusters. The approach also enables Bayesian prior distributions and reversal of scanner distortions such as gain.",1522-4880,0-7803-6297-7,10.1109/ICIP.2000.899497,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899497,,Optical character recognition software;Image sensors;Clustering algorithms;Bayesian methods;Histograms;Pixel;Sensor phenomena and characterization;Character recognition;Image restoration;Degradation,document image processing;image scanners;facsimile;image enhancement;Bayes methods;pattern clustering;probability;image resolution,scanner-model-based document image improvement;faxed document images;scanned document images;probabilistic model;scanning process;super-resolved cluster representatives;Bayesian prior distribution;scanner distortions;gain,,6,,7,,6-Aug-02,,,IEEE,IEEE Conferences
Improving Online Clustering of Chinese Technology Web News With Bag-of-Near-Synonyms,用近義詞袋改善中文技術網絡新聞的在線聚類,Z. Zhang; L. Chen; F. Yin; X. Zhang; L. Guo,"Science and Technology on Information Systems Engineering Laboratory, College of Systems Engineering, National University of Defense Technology, Changsha, China; Institute of Systems Engineering, Academy of Military Science, Beijing, China; Science and Technology on Information Systems Engineering Laboratory, College of Systems Engineering, National University of Defense Technology, Changsha, China; Science and Technology on Information Systems Engineering Laboratory, College of Systems Engineering, National University of Defense Technology, Changsha, China; Science and Technology on Information Systems Engineering Laboratory, College of Systems Engineering, National University of Defense Technology, Changsha, China",IEEE Access,28-May-20,2020,8,,94245,94257,"In the Internet era, online clustering of technology web news can help discover scientific breakthroughs and grasp technology trends. To do that automatically, the news documents to be clustered must be represented appropriately with numerical vectors. However, traditional representations such as Term Frequency-Inverse Document Frequency (TF-IDF) cannot distinguish near-synonyms and may cause ?dimension disaster.??To overcome these problems, this article proposes the Bag-of-Near-Synonyms (BoNS) model based on the idea to construct near-synonym sets using word embeddings and agglomerative clustering, and then to represent a document with a Set Frequency-Inverse Document Frequency (SF-IDF) vector in which each dimension corresponds to a near-synonym set rather than a single word. To speed up computation, we further propose the hashed version of SF-IDF and name it hSF-IDF, which employs a hash function to map each near-synonym set to a unique number as the key and hence reduces the computation of SF to linear time. In addition, we apply hSF-IDF to online clustering of Chinese technology web news and propose an improved batch-based method. Extensive experiments have been conducted on a real-world dataset. The results show that our model outperforms some strong baselines including TF-IDF, average pooling of word or character embeddings, Latent Dirichlet Allocation (LDA), and bag-of-concepts in terms of both accuracy and efficiency.",2169-3536,,10.1109/ACCESS.2020.2995516,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095310,Information retrieval;single-pass algorithm;document representation;word embeddings;agglomerative clustering,Computational modeling;Semantics;Vocabulary;Clustering algorithms;Facebook;Licenses;Artificial intelligence,document handling;Internet;pattern clustering;text analysis,BoNS;Chinese technology Web news;word embeddings;bag-of-near-synonym sets;Latent Dirichlet Allocation;bag-of-near-synonyms model;set frequency-inverse document frequency vector;hSF-IDF;SF-IDF;agglomerative clustering;term frequency-inverse document frequency;news documents;Internet;online clustering;TF-IDF;batch-based method,,,,37,CCBY,18-May-20,,,IEEE,IEEE Journals
A clustering algorithm based on latent semantic model,基於潛在語義模型的聚類算法,Bu-Yu Wang; Mei-An Li; Yong-Jiang Wang,"Collage of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot 010018, China; Collage of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot 010018, China; Collage of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot 010018, China",2009 International Conference on Apperceiving Computing and Intelligence Analysis,28-Dec-09,2009,,,44,48,"In order to precisely procure the Chinese person information on the web, especially distinguish from the namesake, this paper propose a clustering algorithm based on latent semantic model. It establishes for every document a latent semantic model of sentence-word matrix based on central distance, central segment, document length, etc, by building the central word library of person attributes. It clusters the similar documents by means of dynamic-extending clustering algorithm. Experiments prove that the algorithm gives high accuracy to documents clustering as well as maintaining the coherence of the person's semantic information and highlighting the importance of semantic information under different sequences.",,978-1-4244-5204-0,10.1109/ICACIA.2009.5361155,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361155,Latent semantic model;central word set;central word position;center word distance,Clustering algorithms;Libraries;Coherence;Educational institutions;Engineering profession;Agricultural engineering;Electronic mail;Heuristic algorithms;Stress;Data mining,document handling;information analysis;Internet;natural language processing;pattern clustering,latent semantic model;Chinese person information;World Wide Web;sentence-word matrix;central distance;central segment;document length;central word library;person attributes;dynamic-extending clustering algorithm;documents clustering,,,,6,,28-Dec-09,,,IEEE,IEEE Conferences
An intuitive based clustering on document mining using dirichlet process mixture models and its kernels,使用Dirichlet混合流程模型及其內核的基於文檔挖掘的直觀集群,D. Ratnam; B. V. S. Rao; J. R. Prasad; S. S. Kumar,"P V P Siddhartha Institute of Technology, Vijayawada. A.P, India; P V P Siddhartha Institute of Technology, Vijayawada. A.P, India; P V P Siddhartha Institute of Technology, Vijayawada. A.P, India; P V P Siddhartha Institute of Technology, Vijayawada. A.P, India","2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)",26-Jun-17,2016,,,1273,1279,"In machine learning and data mining tasks, Clustering is considered to be one of the most important techniques. The same sorts of documents are grouped by performing clustering techniques. Similarity measuring is used to determine transaction relationships. Hierarchical clustering model generates tree structured results. Partitioned based clustering produces the result in grid format. Text documents are unstructured data values with high dimensional attributes. Document clustering group transforms unlabeled text documents into meaningful clusters. In the event of document grouping process, traditional clustering methods require cluster count (K). Clustering accuracy degrades drastically with reference to the unsuitable cluster count. It is observed that document features are automatically partitioned into two groups namely - discriminative words and non-discriminative words. In particular, discriminative words are only useful for grouping documents. The involvement of non-discriminative words confuses the clustering process and leads to poor clustering solution in return. A variation inference algorithm is used to infer the document collection structure and partition of document words simultaneously. Dirichlet Process Mixture (DPM) model is used to partition documents in a way utilizing both the data likelihood and the clustering property of the Dirichlet Process (DP). Dirichlet Process Mixture Model for Feature Extraction (DPMFE) is used to discover the latent cluster structure based on the DPM model and it is performed without involving the number of clusters as input.",,978-1-5090-4620-1,10.1109/SCOPES.2016.7955646,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955646,dirichlet process mixture model;Chinese restaurant process;stick breaking process,Mixture models;Data models;Signal processing;Embedded systems;Bayes methods;Clustering algorithms;Inference algorithms,data mining;feature extraction;inference mechanisms;learning (artificial intelligence);mixture models;pattern clustering;statistical analysis;text analysis,intuitive based clustering;document mining;Dirichlet process mixture model for feature extraction;DPMFE;machine learning;data mining;similarity measuring;hierarchical clustering model;partitioned based clustering;text documents;document grouping process;variation inference algorithm;latent cluster structure discovery,,,,,,26-Jun-17,,,IEEE,IEEE Conferences
An innovative approach to classify and retrieve text documents using feature extraction and Hierarchical clustering based on ontology,使用特徵提取和基於本體的層次聚類對文本文檔進行分類和檢索的創新方法,A. R. Patil; A. A. Manjrekar,"Computer science and technology department, Department of technology, Kolhapur, India; Computer science and technology department, Department of technology, Kolhapur, India","2016 International Conference on Computing, Analytics and Security Trends (CAST)",1-May-17,2016,,,371,376,"Data retrieval is a key process of acquiring information as per requirement. The necessity of proper information has increased. The most basic tools which provide this service are browser. It traverses the data as per user's query and gives the search results of all related information. Hence, it becomes a time consuming process to find required information. In this paper, the focus is done on content based data mining using ontology and text feature extraction. Content based data mining process focuses on domain of the data. Ontology, itself is a domain based data set information system that will help to achieve required data retrieval in a more appropriate way. The proposed system uses k means clustering algorithm for creation of flat clusters. Flat clusters are the primary classification or clusters of data that are used for Hierarchical clustering. For the proposed system Hierarchical Fuzzy Relational Eigenvector Centrality-based Clustering Algorithm (HFRECCA) is used. This technique of clustering is very fast and gives more accurate results. For more appropriate data retrieval, this system uses text feature extraction algorithm. This algorithm will help to reduce the noisy data from data sets. A noise free data will help to perform better data retrieval process. Implemented system works over various types of text file such as PDF, .txt, DOC, DOCX. This system is also compatible with other types of files like WebPages, images etc.",,978-1-5090-1338-8,10.1109/CAST.2016.7914997,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7914997,Domain ontology;Text data;Feature extraction;Flat clusters;Hierarchical clustering;information retrieval,Feature extraction;Ontologies;Clustering algorithms;Data mining;Partitioning algorithms;Algorithm design and analysis;Browsers,data mining;feature extraction;fuzzy set theory;information retrieval;ontologies (artificial intelligence);pattern clustering;text analysis,text document classification;text document retrieval;data retrieval;content based data mining;ontology;text feature extraction;domain based data set information system;k means clustering algorithm;flat clusters;primary data cluster classification;hierarchical fuzzy relational eigenvector centrality-based clustering algorithm;HFRECCA;textfeature extraction;text file,,1,,19,,1-May-17,,,IEEE,IEEE Conferences
Parallel hierarchical subspace clustering for segmenting large text corpuses,用於分割大型文本語料庫的並行層次子空間聚類,S. Karthick; S. M. Shalinie; S. Umabharathi; S. K. Saroja,"Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science and Engineering, Thiagarajar College of Engineering, Madurai, India",2017 International Conference on Trends in Electronics and Informatics (ICEI),22-Feb-18,2017,,,1,6,"Hierarchical Document Clustering helps in grouping similar documents and facilitates the subsequent extraction of useful information. High dimensionality of documents and the scale of the document-corpus often results in a heavy demand for both memory and compute-power. In this paper, we have designed and implemented a parallelization scheme for a hierarchical document clustering algorithm and also study the effect of the popular dimensionality reduction techniques on the performance of the algorithm. The parallelization scheme is amenable to be implemented on distributed parallel programming frameworks like Map Reduce. The parallelized hierarchical algorithm produces clusters of good quality as measured by the cluster-validation indices and also achieves a considerable speed-up when run on the HaLoop Map-Reduce framework.",,978-1-5090-4257-9,10.1109/ICOEI.2017.8300713,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300713,Big Data;Document Clustering;Parallel Clustering;Dimensionality Reduction;Distributed Algorithm;Parallel Programming Frameworks;Map Reduce Framework,Clustering algorithms;Dimensionality reduction;Matrix decomposition;Sparse matrices;Principal component analysis;Task analysis;Parallel algorithms,data reduction;information retrieval;parallel programming;pattern clustering;text analysis,parallelized hierarchical algorithm;cluster-validation indices;parallel hierarchical subspace clustering;text corpuses;subsequent extraction;parallelization scheme;hierarchical document clustering algorithm;popular dimensionality reduction techniques;distributed parallel programming frameworks;similar document grouping;document-corpus;HaLoop MapReduce framework,,,,14,,22-Feb-18,,,IEEE,IEEE Conferences
Clustering of Symbols Using Minimal Description Length,使用最小描述長度的符號聚類,O. M. Tataw; T. Rakthanmanon; E. J. Keogh,"Univ. of California, Riverside, Riverside, CA, USA; Univ. of California, Riverside, Riverside, CA, USA; Univ. of California, Riverside, Riverside, CA, USA",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,180,184,"The clustering of glyphs (individual letters/characters/symbols) is typically the first step in document processing algorithms and a critical enabling technology for most historical document indexing techniques. In this work, we take a step back from current domain/language specialized research efforts to consider the problem from an agnostic perspective. In particular, we claim that, independent of the distance measure used, any method that attempts to cluster all the data is almost certainly doomed to failure. We explain this observation, and introduce a clustering method based on Minimum Description Length (MDL) that can overcome it.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628608,Image Similarity;Clustering;MDL,Clustering algorithms;Algorithm design and analysis;Encoding;Character recognition;Accuracy;Approximation algorithms;Clustering methods,document image processing;image classification;pattern clustering,symbol clustering;minimal description length;glyphs clustering;document processing algorithms;MDL,,2,,21,,15-Oct-13,,,IEEE,IEEE Conferences
A Self-Organizing Map Based Approach for Document Clustering and Visualization,基於自組織圖的文檔聚類和可視化方法,G. G. Yen; Zheng Wu,"Senior Member, IEEE, School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK 74078-5032, USA. email: gyen@okstate.edu; Oklahoma State Univ., Stillwater",The 2006 IEEE International Joint Conference on Neural Network Proceedings,30-Oct-06,2006,,,3279,3286,"In this paper, the clustering and visualization capabilities of the SOM, specifically tailored for the analysis of textual data, are reviewed and further developed. A novel clustering and visualization approach is proposed for the task of textual data mining. The proposed approach first transforms the document space into a multi-dimensional vector space by means of citation patterns. An intuitive and effective projection method, namely the ranked centroid projection (RCP), is then applied in conjunction with a dynamic SOM model, the growing hierarchical self-organizing map, which automatically produces document maps with various levels of details. The RCP is used both as a data analysis tool as well as a direct interface to the data. We also extend the RCP to address the problem of the incremental clustering of dynamic document collections. In a set of simulations, the proposed approach is applied to a synthetic data set and two real-world scientific document collections, to demonstrate its applicability.",2161-4407,0-7803-9490-9,10.1109/IJCNN.2006.247324,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716546,,Data visualization;Data analysis;Prototypes;Data mining;Clustering algorithms;Neurons;Pattern recognition;Clustering methods;Data engineering;Displays,data analysis;data mining;data visualisation;document handling;pattern clustering;self-organising feature maps,document clustering;textual data analysis;textual data mining;multi-dimensional vector space;citation patterns;ranked centroid projection;growing hierarchical self-organizing map;scientific document collections;document visualization,,2,,20,,30-Oct-06,,,IEEE,IEEE Conferences
"Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool for Investigative Journalists",概述：供調查記者使用的可視文檔挖掘工具的設計，採用和分析,M. Brehmer; S. Ingram; J. Stray; T. Munzner,University of British Columbia; University of British Columbia; Columbia Journalism School and the Associated Press; University of British Columbia,IEEE Transactions on Visualization and Computer Graphics,6-Nov-14,2014,20,12,2271,2280,"For an investigative journalist, a large collection of documents obtained from a Freedom of Information Act request or a leak is both a blessing and a curse: such material may contain multiple newsworthy stories, but it can be difficult and time consuming to find relevant documents. Standard text search is useful, but even if the search target is known it may not be possible to formulate an effective query. In addition, summarization is an important non-search task. We present Overview, an application for the systematic analysis of large document collections based on document clustering, visualization, and tagging. This work contributes to the small set of design studies which evaluate a visualization system ?in the wild?? and we report on six case studies where Overview was voluntarily used by self-initiated journalists to produce published stories. We find that the frequently-used language of ?exploring??a document collection is both too vague and too narrow to capture how journalists actually used our application. Our iterative process, including multiple rounds of deployment and observations of real world usage, led to a much more specific characterization of tasks. We analyze and justify the visual encoding and interaction techniques used in Overview's design with respect to our final task abstractions, and propose generalizable lessons for visualization design methodology.",1941-0506,,10.1109/TVCG.2014.2346431,Knight News Challenge; NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875900,Design study;investigative journalism;task and requirements analysis;text and document data;text analysis,Document handling;Data visualization;Encoding;Text mining;Text analysis,data mining;data visualisation;graphical user interfaces;pattern clustering;text analysis,visual document mining tool design;visual document mining tool adoption;visual document mining tool analysis;investigative journalists;Freedom of Information Act request;multiple newsworthy stories;standard text search;query processing;data summarization;Overview;document collection analysis;document clustering;document visualization;document tagging;in-the-wild visualization system;self-initiated journalists;published story production;frequently-used language;iterative process;visual encoding;interaction techniques;task abstractions;visualization design methodology,"Cluster Analysis;Computer Graphics;Data Mining;Humans;Journalism;Models, Theoretical",24,,53,,6-Nov-14,,,IEEE,IEEE Journals
Clustering low quality Farsi sub-words for word recognition,聚類低質量的波斯波斯詞以進行單詞識別,H. Arab Yarmohammadi; A. Ahmady Fard; H. Khosravi,"Faculty of Electrical and Robotic Engineering Shahrood University of Technology Shahrood, Iran; Faculty of Electrical and Robotic Engineering Shahrood University of Technology Shahrood, Iran; Faculty of Electrical and Robotic Engineering Shahrood University of Technology Shahrood, Iran",2014 Iranian Conference on Intelligent Systems (ICIS),21-Apr-14,2014,,,1,5,"OCR of low resolution documents is not so common, because it has a lot of problems. However, today there are several archives of digital documents which are scanned at low resolution, to consume less storage. These documents which usually have a resolution of 100 to 150 dpi, require to be converted to searchable documents. In this paper presents a new method for clustering of low quality printed Persian sub-words. This is necessary to reduce the number of classes of sub-words in order to improve the overall recognition rate. Two popular clustering methods, hierarchical and k-means implemented and compared. Local binary patterns (LBP) and zoning algorithms used for feature extraction. Both features are fast and represent the global shape information very well. Moreover, we used different distance measures to find the similarity of feature vectors. We applied our algorithms on a dataset of 10,700 images of distinct Persian sub-words with 96 dpi resolution. Experimental results show that the hierarchical clustering with the correlation distance measure has the best performance over other clustering methods and distance measures.",,978-1-4799-3351-8,10.1109/IranianCIS.2014.6802518,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802518,Clustering;Persian Typing Sub-Words;Low Resolution;Hierarchical;K-meams;Local Binary Patterns;Zoning,Feature extraction;Image resolution;Clustering algorithms;Vectors;Clustering methods;Image recognition;Correlation,document image processing;feature extraction;image resolution;optical character recognition;pattern clustering,low quality Farsi sub-word clustering;word recognition;low resolution documents;OCR;digital documents;low quality printed Persian sub-word clustering;k-means clustering;local binary patterns;LBP;zoning algorithms;feature extraction;global shape information;distance measures;feature vector similarity;hierarchical clustering;correlation distance measure,,2,,9,,21-Apr-14,,,IEEE,IEEE Conferences
Enhanced document clustering using fusion of multiscale wavelet decomposition,使用多尺度小波分解融合增強文檔聚類,M. F. Hussin; I. El Rube; M. S. Kamel,"Arab Academy for Science and Technology and Maritime Transport, Alexandria, Egypt; Arab Academy for Science and Technology and Maritime Transport, Alexandria, Egypt; Dept. of Electrical and Computer Engineering, University of waterloo, Ontario, Canada",2008 IEEE/ACS International Conference on Computer Systems and Applications,22-Apr-08,2008,,,870,874,"Most term weighting schemes for text document clustering depend on the term frequency based analysis of the text contents. A shortcoming of these indexing schemes, which consider only the occurrences of the terms in a document, is that they have some limitations in filtering out noise in most cases. In this paper, we propose a novel weighting approach using fusion technique that can be combined with wavelet-based estimation to achieve consistent improvements in the clustering. Our approach involves three steps: (1) term frequency (TF) weighting scheme, (2) multiple wavelets estimating, and (3) data fusion. Specifically, we apply the wavelet with different scales to produce different estimation values of the original TF, and use the fusion of these different values as new features for clustering the documents. The conducted experiments of clustering the documents from RETURES corpus verify that our weighting schemes using wavelet and fusion techniques reduces effectively the noise and improves clustering performance evaluated using the entropy and F_measure.",2161-5330,978-1-4244-1967-8,10.1109/AICCSA.2008.4493632,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493632,,Noise reduction;Subspace constraints;Wavelet analysis;Frequency estimation;Wavelet transforms;Indexing;Filtering;Entropy;Internet;Explosions,document image processing;image fusion;text analysis;wavelet transforms,enhanced document clustering;multiscale wavelet decomposition fusion;text document clustering;noise filtering;weighting approach;wavelet-based estimation;term frequency methods;multiple wavelets estimation;data fusion,,,,12,,22-Apr-08,,,IEEE,IEEE Conferences
A Genetic Niching Algorithm with Self-Adaptating Operator Rates for Document Clustering,具有自適應算子速率的遺傳小生境文檔聚類算法,E. Le籀n; J. G籀mez; O. Nasraoui,NA; NA; NA,2012 Eighth Latin American Web Congress,24-Dec-12,2012,,,79,86,"We propose a Genetic algorithm for document clustering, where an evolutionary multimodal optimization algorithm evolves candidate cluster representative solutions to search for dense regions in the sparse high dimensional vector space of text documents. The evolution affects not only the document cluster representatives but also the genetic operator rates which are evolved simultaneously with the document cluster representative solutions. The evolving population consists of candidate document cluster representatives that are encoded in the form of a sparse index and sparse index/frequency variable length vectors. In addition, specialized sparse genetic operators are defined for this special representation. The proposed specialized genetic operators achieve different degrees of exploitation and exploration in searching for the optimal document cluster prototypes, in particular the most specialized operator for the document clustering problem is the Sparse Top-K-Addition operator, which can be seen as an incentive towards a more aggressive exploitation of the local context in a small subset of documents, whereas the simple Sparse Real Addition operator works more in an exploratory manner. As shown in our experiments on two well-known document data sets, taking into account associated terms within a local context adds the benefit of an explicit latent semantic consideration in the search for optimal term lists to describe the cluster prototypes.",,978-1-4673-4473-9,10.1109/LA-WEB.2012.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392142,Genetic Clustering;Text Mining,Indexes;Vectors;Clustering algorithms;Genetics;Prototypes;Mathematical model;Frequency measurement,genetic algorithms;pattern clustering;text analysis,genetic niching algorithm;self-adaptating operator rates;document clustering;evolutionary multimodal optimization algorithm;text document;sparse index;frequency variable length vector;specialized sparse genetic operator;specialized genetic operator;optimal document cluster prototype;sparse top-K-addition operator;explicit latent semantic,,1,,24,,24-Dec-12,,,IEEE,IEEE Conferences
A Similarity Measure for Text Classification and Clustering,文本分類和聚類的相似性度量,Y. Lin; J. Jiang; S. Lee,"Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan",IEEE Transactions on Knowledge and Data Engineering,9-Jul-14,2014,26,7,1575,1590,"Measuring the similarity between documents is an important operation in the text processing field. In this paper, a new similarity measure is proposed. To compute the similarity between two documents with respect to a feature, the proposed measure takes the following three cases into account: a) The feature appears in both documents, b) the feature appears in only one document, and c) the feature appears in none of the documents. For the first case, the similarity increases as the difference between the two involved feature values decreases. Furthermore, the contribution of the difference is normally scaled. For the second case, a fixed value is contributed to the similarity. For the last case, the feature has no contribution to the similarity. The proposed measure is extended to gauge the similarity between two sets of documents. The effectiveness of our measure is evaluated on several real-world data sets for text classification and clustering problems. The results show that the performance obtained by the proposed measure is better than that achieved by other measures.",1558-2191,,10.1109/TKDE.2013.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420834,Document classification;document clustering;entropy;accuracy;classifiers;clustering algorithms;Document classification;document clustering;entropy;accuracy;classifiers;clustering algorithms,Vectors;Clustering algorithms;Text processing;Euclidean distance;Approximation methods;Educational institutions,pattern clustering;text analysis,similarity measure;text classification;text clustering;text processing field;feature values;fixed value;real-world data sets,,130,,52,,25-Jan-13,,,IEEE,IEEE Journals
An analysis of efficient clustering methods for estimates similarity measures,估計相似性度量的有效聚類方法分析,G. Jagatheeshkumar; S. S. Brunda,"R&D Centre, Bharathiar University, Coimbatore-46, Tamilnadu, India; Academic and Admission Co-ordinator, Cheren College of Engineering, Karur, Tamilnadu, India",2017 4th International Conference on Advanced Computing and Communication Systems (ICACCS),24-Aug-17,2017,,,1,3,"The main objective of clustering to form a group of similar/dissimilar data object into cluster. Cluster analysis aim to group a collection of patterns in to cluster based on similarity. Cluster is the unsupervised learning technique which is used to looping a set of unordered data object in to a smaller number of meaning full cluster. The relation between cluster either intra or inter. Clustering is mostly analysis for field of text document. In this domain problem finds many applications in Market Analysis, web mining and indexing. In this analysis covers of clustering methods similarity measures based on distance. To discover related work this cluster technique find a new proposal for our further work in text documents, similarity meaning data mining.",,978-1-5090-4559-4,10.1109/ICACCS.2017.8014710,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014710,Text documents;similarity measures;Data mining,Clustering algorithms;Partitioning algorithms;Clustering methods;Data mining;Euclidean distance;Algorithm design and analysis,data mining;pattern clustering,clustering methods;similarity measures;similar/dissimilar data object;cluster analysis;unsupervised learning;unordered data object;market analysis;Web mining;indexing;text documents;similarity meaning data mining,,,,14,,24-Aug-17,,,IEEE,IEEE Conferences
Inter-document reference detection as an alternative to full text semantic analysis in document clustering,文檔間引用檢測是文檔聚類中全文語義分析的替代方法,P. A. De Mazi癡re; M. M. Van Hulle,"Dept. Healthcare & Technology, KHLeuven Herestraat 49, 3000 Leuven, Belgium; Lab. Neuro- & Psychofysiologie, KU Leuven Herestraat 49, PO Box 1021, 3000 Leuven, Belgium",2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),14-Nov-13,2013,,,1,6,"We discuss here the search for inter-document references as an alternative to the grouping of document inventories based on a full text semantic analysis. The used document inventory, which is not publicly available, was provided to us by the European Union (EU) in the framework of an EU project, the aim of which was to analyse, classify, and visualise EU funded research in social sciences and humanities in EU framework programmes FP5 and FP6. This project, called the SSH project for short, was aimed at the evaluation of the contributions of research to the development of EU policies. For the semantic based grouping, we start from a Multi-Dimensional Scaling analysis of the document vectors, which is the result of a prior semantic analysis. As an alternative to a semantic analysis, we searched for inter-document references or direct references. Direct references are defined as terms that explicitly refer to other documents present in the inventory. We show that the grouping based on references is largely similar to the one based on semantics, but with considerably less computational efforts. In addition, the non-expert can make better use of the results, since the references are displayed as graphical webpages with hyperlinks pointing to both the referenced and the referencing document(s), and the reason of linkage. Finally, we show that the combination of a database, to store the data and the (intermediate) results, and a webserver, to visualise the results, offers a powerful platform to analyse the document inventory and to share the results with all participants/collaborators involved in a data- and computation intensive EU-project, thereby guaranteeing both data- and result-consistency.",2378-928X,978-1-4799-1180-6,10.1109/MLSP.2013.6661952,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661952,Text Mining;HPC;Semantic Analysis;client-server infrastructure,Semantics;Vectors;Databases;Text analysis;Terminology;Servers;Europe,data visualisation;database management systems;document handling;file servers;pattern clustering,interdocument reference detection;full text semantic analysis;document clustering;document inventories;European Union;EU project;EU funded research;social sciences;humanities;FP5;FP6;semantic based grouping;multidimensional scaling analysis;document vectors;graphical webpages;hyperlinks;Webserver;database;result-consistency;data-consistency,,,,10,,14-Nov-13,,,IEEE,IEEE Conferences
Geodesic distances for web document clustering,Web文檔聚類的測地距離,S. Tekir; F. Mansmann; D. Keim,"Dept. of Computer Engineering, Izmir Institute of Technology, Turkey 35430; Faculty of Computer Science, Box 78, University of Konstanz, Germany 78457; Faculty of Computer Science, Box 78, University of Konstanz, Germany 78457",2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM),11-Jul-11,2011,,,15,21,"While traditional distance measures are often capable of properly describing similarity between objects, in some application areas there is still potential to fine-tune these measures with additional information provided in the data sets. In this work we combine such traditional distance measures for document analysis with link information between documents to improve clustering results. In particular, we test the effectiveness of geodesic distances as similarity measures under the space assumption of spherical geometry in a 0-sphere. Our proposed distance measure is thus a combination of the cosine distance of the term-document matrix and some curvature values in the geodesic distance formula. To estimate these curvature values, we calculate clustering coefficient values for every document from the link graph of the data set and increase their distinctiveness by means of a heuristic as these clustering coefficient values are rough estimates of the curvatures. To evaluate our work, we perform clustering tests with the k-means algorithm on the English Wikipedia hyperlinked data set with both traditional cosine distance and our proposed geodesic distance. The effectiveness of our approach is measured by computing micro-precision values of the clusters based on the provided categorical information of each article.",,978-1-4244-9927-4,10.1109/CIDM.2011.5949449,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949449,,Level measurement;Clustering algorithms;Equations;Internet;Encyclopedias,document handling;information retrieval systems;pattern clustering,geodesic distance;Web document clustering;document analysis;term-document matrix;clustering coefficient value;k-means algorithm;English Wikipedia hyperlinked data;cosine distance;information retrieval systems,,1,,22,,11-Jul-11,,,IEEE,IEEE Conferences
FMODC: Fuzzy guided multi-objective document clustering by GA,FMODC：基於GA的模糊指導多目標文檔聚類,A. S. Rao; S. Ramakrishna,"Department of Computer Science and engineering, MRCE, Hyderabad, Telangana State, India; Department of Computer Science, Sri Venkateswara University, Tirupathi, Andhra Pradesh, India",2016 2nd International Conference on Contemporary Computing and Informatics (IC3I),4-May-17,2016,,,650,655,"In the unsuprevised learning method of text mining process that is in prevalence, the issues pertaining to multi dimensionality is turning out to be a major factor, as the clustering is not focusing on optimal evaluation of concept, context and semantic relevancy, which are also very essential in terms of clustering process. In majority of the models that are proposed earlier, the factors like the term frequency, were considered and the clustering has been focusing only on one factor of Semantic, whereas as context and conceptual factors also play a significant importance. In extension to the earlier model of MODC, and DC3SR, the proposed model of Multi-objective Distance based Optimal Document Clustering (MODC) by GA has been proposed in the study. Among the lessons that are learnt from the review of earlier models., the scope for fuzzy guided multi-objective optimal document clustering (FMODC) approach which shall support in more effective computation and clustering using the Genetic Algorithm is discussed in the case scenario. From the experimentation process that is focused in the study, using the meta-text data gathered from the same publisher, the model has been tested in comparative analysis to other two models, BADC and AC-DCO and the outcome in terms of optimum clustering that has been achieved with FMODC model depicts the kind of accuracy in the model and the system. An unsupervised learning approach to form the initial clusters that estimates similarity between any two documents by concept, context and semantic relevance score and further optimizes by fuzzy genetic algorithm is proposed. This novel method represents the concept as correlation between arguments and activities in given documents, context as correlation between meta-text of the documents and the semantic relevance is assessed by estimating the similarity between documents through the hyponyms of the arguments. The meta-text of the documents considered for context assessment contains the authors list, keywords list and list of document versioning time schedules. The experiments were conducted to assess the significance of the proposed model.",,978-1-5090-5256-1,10.1109/IC3I.2016.7918043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918043,Context similarity;Document clustering;concept similarity;DC3SR;Meta-text;DCCR;DCCSR;MODC;FMODC;text mining;unsupervised learning;Semantic Similarity,Conferences;Informatics,data mining;document handling;fuzzy set theory;genetic algorithms;learning (artificial intelligence);pattern clustering,FMODC;fuzzy guided multi-objective document clustering;GA;unsuprevised learning;text mining;concept relevancy;context relevancy;semantic relevancy;genetic algorithm,,,,35,,4-May-17,,,IEEE,IEEE Conferences
Document Clustering for Forensic Analysis: An Approach for Improving Computer Inspection,用於法醫分析的文檔聚類：一種改進計算機檢查的方法,L. F. d. C. Nassif; E. R. Hruschka,"Brazilian Federal Police Department, S瓊o Paulo, Brazil; University of Brasilia (UnB), Brazil",IEEE Transactions on Information Forensics and Security,24-Dec-12,2013,8,1,46,54,"In computer forensic analysis, hundreds of thousands of files are usually examined. Much of the data in those files consists of unstructured text, whose analysis by computer examiners is difficult to be performed. In this context, automated methods of analysis are of great interest. In particular, algorithms for clustering documents can facilitate the discovery of new and useful knowledge from the documents under analysis. We present an approach that applies document clustering algorithms to forensic analysis of computers seized in police investigations. We illustrate the proposed approach by carrying out extensive experimentation with six well-known clustering algorithms (K-means, K-medoids, Single Link, Complete Link, Average Link, and CSPA) applied to five real-world datasets obtained from computers seized in real-world investigations. Experiments have been performed with different combinations of parameters, resulting in 16 different instantiations of algorithms. In addition, two relative validity indexes were used to automatically estimate the number of clusters. Related studies in the literature are significantly more limited than our study. Our experiments show that the Average Link and Complete Link algorithms provide the best results for our application domain. If suitably initialized, partitional algorithms (K-means and K-medoids) can also yield to very good results. Finally, we also present and discuss several practical results that can be useful for researchers and practitioners of forensic computing.",1556-6021,,10.1109/TIFS.2012.2223679,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327658,Clustering;forensic computing;text mining,Clustering algorithms;Algorithm design and analysis;Text mining;Pattern clustering;Digital forensics;Text analysis,data mining;digital forensics;pattern clustering;text analysis,forensic analysis;computer forensic analysis;unstructured text;document clustering algorithms;police investigations;k-mean clustering;K-medoids clustering;single link clustering;complete link clustering;average link clustering;CSPA;text mining,,32,,29,,9-Oct-12,,,IEEE,IEEE Journals
Development of a Topic-Centered Adaptive Document Management System,以主題為中心的自適應文檔管理系統的開發,S. Nakamura; S. Chiba; H. Kaminaga; S. Yokoyama; Y. Miyadera,"Dept. Comput. Sci. & Math., Fukushima Univ., Fukushima, Japan; Dept. Comput. Sci. & Math., Fukushima Univ., Fukushima, Japan; Dept. Comput. Sci. & Math., Fukushima Univ., Fukushima, Japan; Div. of Natural Sci., Tokyo Gakugei Univ., Tokyo, Japan; Div. of Natural Sci., Tokyo Gakugei Univ., Tokyo, Japan",2009 Fourth International Conference on Computer Sciences and Convergence Information Technology,31-Dec-09,2009,,,109,115,"Skillful management of the various types of documents used in intelligent activities and their efficient utilization are important. However, most available systems target only a single type of document (e-mails, Web pages, etc.) or are not adaptive enough. A more promising approach is topic-centered document management. A topic-centered document management system is described that changes its own functions and interfaces adaptively.",,978-1-4244-5244-6,10.1109/ICCIT.2009.289,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5369971,Topic extraction;Document management;Document clustering;Topic-centered document management;Adaptive system construction,Adaptive systems;Electronic mail;Web pages;Data mining;Conference management;Technology management;Intelligent networks;Programming;Application software;Information technology,document handling,document management system;topic-centered document management;intelligent activities usage,,1,,13,,31-Dec-09,,,IEEE,IEEE Conferences
Active Learning of Instance-Level Constraints for Semi-supervised Document Clustering,主動學習半監督文檔聚類的實例級約束,W. Zhao; Q. He; H. Ma; Z. Shi,NA; NA; NA; NA,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,13-Oct-09,2009,1,,264,268,"This paper presents a framework that actively selects informative documents pairs for semi-supervised document clustering. The semi-supervised document clustering algorithm is a Constrained DBSCAN (Cons-DBSCAN), which incorporates instance-level constraints to guide the clustering process in DBSCAN. By obtaining user feedbacks, our proposed active learning algorithm can get informative instance level constraints to aid clustering process. Experimental results show that Cons-DBSCAN with the proposed active learning approach can provide an appealing clustering performance.",,978-0-7695-3801-3,10.1109/WI-IAT.2009.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286064,Semi-supervised Clustering;Document Clustering;Active Learning;Instance-level Constraint,Intelligent agent;Clustering algorithms;Feedback;Machine learning;Conferences;Information processing;Computers;Data mining;Learning systems,,,,,,13,,13-Oct-09,,,IEEE,IEEE Conferences
An improved method in clustering Web retrieval result based on relevance feedback,基於相關反饋的Web檢索結果聚類的一種改進方法,Xinye Li,"Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China",2011 International Conference on Computer Science and Service System (CSSS),4-Aug-11,2011,,,3000,3003,"Since the number of Web retrieval result is very large, the performance and reasonableness of clustering Web retrieval result are important. Existed methods cost much time while clustering all retrieval result and there were many unrelated document in their clustering result. To avoid the disadvantage, this paper proposed an improved k-means algorithm by using a few of related and unrelated feedback to guide clustering Web retrieval result. The improved algorithm first selected initial cluster metroid based on feedback messages, then during the clustering process, it removed large unrelated documents which increased the clustering speed and optimized the clustering result. During the clustering process, the metroids of clusters including unrelated documents needn't be modified in order to avoid noise influence. Experiment result illustrate that our algorithm is superior to the traditional k-means algorithm.",,978-1-4244-9763-8,10.1109/CSSS.2011.5974767,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974767,Web retrieval result;clustering;relevance feedback;improved k-means algorithm,Clustering algorithms;Information retrieval;Computers;Medical services;Ontologies;Research and development;Proposals,Internet;pattern clustering;relevance feedback,Web retrieval clustering;relevance feedback;unrelated document;k-means algorithm;unrelated feedback;cluster metroid;feedback messages,,,,,,4-Aug-11,,,IEEE,IEEE Conferences
A Comparable Study Employing WEKA Clustering/Classification Algorithms for Web Page Classification,使用WEKA聚類/分類算法進行網頁分類的比較研究,I. Charalampopoulos; I. Anagnostopoulos,"Dept. of Inf. & Commun. Syst. Eng., Univ. of the Aegean, Karlovassi, Greece; Dept. of Comput. Sci. & Biomed. Inf., Univ. of Central Greece, Lamia, Greece",2011 15th Panhellenic Conference on Informatics,3-Nov-11,2011,,,235,239,"Documents and web pages share many similarities. Thus classification methods used in documents can be applied to advanced web content, with or even without modifications. Algorithms for document and web classification are presented as an introduction. One out of many tools that can be used in method evaluation, application and modification is WEKA (Waikato Environment for Knowledge Analysis). Testing results and conclusions strengthen the principles and bases of classification, while demonstrating the need for a new interlayer in the evaluation of classification methods.",,978-1-61284-962-1,10.1109/PCI.2011.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065094,Classification;Clustering;WEKA,Classification algorithms;Algorithm design and analysis;Web pages;Clustering algorithms;Indexing;Internet;Machine learning,document handling;Internet;pattern classification;pattern clustering,WEKA clustering algorithm;WEKA classification algorithm;Web page classification;Waikato environment for knowledge analysis;document classification method,,8,,17,,3-Nov-11,,,IEEE,IEEE Conferences
Enhanced distributed document clustering algorithm using different similarity measures,使用不同相似性度量的增強型分佈式文檔聚類算法,N. Narayanan; J. E. Judith; J. Jayakumari,"Noorul Islam Centre for Higher Education Kumaracoil, Tamilnadu, India; Noorul Islam Centre for Higher Education Kumaracoil, Tamilnadu, India; Noorul Islam Centre for Higher Education Kumaracoil, Tamilnadu, India",2013 IEEE Conference on Information & Communication Technologies,15-Jul-13,2013,,,545,550,"Many of the distributed environments like internets, intranets, local area networks and wireless networks have different distributed data sources. Inorder to analyze and monitor these distributed data sources specialized data mining technologies for distributed applications are required. A variety of distributed document clustering algorithms exists for this purpose. This paper presents an Enhanced Distributed Algorithm (EDA) for document clustering. This paper presents the performance analysis of the algorithm using different similarity measures like cosine similarity, Jaccard and Pearson coefficient. The test was performed on standard document corpora like 20NG (News Group), Reuters, WebKB. The performance of this proposed EDA algorithm is also evaluated using different performance factors in order to determine its accuracy and clustering quality.",,978-1-4673-5758-6,10.1109/CICT.2013.6558155,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6558155,Distributed document clustering;similarity measures;Cosine similarity;Jaccard coefficient;Pearson coefficient,Clustering algorithms;Peer-to-peer computing;Accuracy;Algorithm design and analysis;Computational modeling;Approximation algorithms;Measurement,data mining;distributed processing;document handling;pattern clustering,enhanced distributed document clustering;different similarity measures;distributed environments;Internets;intranets;local area networks;wireless networks;distributed data sources;data mining technologies;enhanced distributed algorithm;EDA,,2,,22,,15-Jul-13,,,IEEE,IEEE Conferences
A neural visualization method for WWW document clusters,WWW文檔簇的神經可視化方法,T. Yoshioka; Y. Takata; M. Ito; S. Ishii,"Nara Inst. of Sci. & Technol., Japan; NA; NA; NA",IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),7-Aug-02,2001,3,,2270,2275 vol.3,"Search engines are widely used for retrieving documents on the WWW. Visualization is useful for users to understand the retrieval results. When the retrieved documents are represented as document vectors, neural networks can be employed to visualize them. In this study, we consider the following two requirements for the visualization algorithm. One is that the cluster structure of document vectors is preserved. The other is that the visualization algorithm is fast. For these requirements, we employ basis function networks. Basis functions detect the cluster structure and weight parameters are adjusted by a fast algorithm so that the distance structure of the document vectors is preserved. Experiments show that our method is fast enough as an interface system.",1098-7576,0-7803-7044-9,10.1109/IJCNN.2001.938520,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938520,,World Wide Web;Data visualization;Clustering algorithms;Principal component analysis;Search engines;Neural networks;Electronic mail;Web sites;Clustering methods;Marine vehicles,information resources;Internet;neural nets;search engines;data visualisation,neural visualization method;World Wide Web document clusters;search engines;documents retrieval;document vectors;neural networks;cluster structure;basis function networks;weight parameters;interface system,,2,,9,,7-Aug-02,,,IEEE,IEEE Conferences
The Effects of Documents Lineage on Use of Explanation in Document-Driven DSS,文檔沿襲對文檔驅動的DSS中使用解釋的影響,Z. Chen; H. Dong,"State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; Int. Sch. of Software, Wuhan Univ., Wuhan, China",2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics,30-Sep-10,2010,1,,243,248,"In document-driven DSS, the decisions are both based on the inheritance among the documents and the acceptance of advices for users. Research in the field of DSS has shown that providing explanations may improve acceptance of decision makers. The most important part of explanations in document-driven DSS lies in tracing the contents and the classification of interrelated documents. But current document-driven DSS is lack of a mechanism to record the citation and cluster the related documents. This paper tries to find out the trace routes among documents to improve the explanations. First, a document lineage model is established to present the citation and delivery mechanism in documents. Second, the document DNA is used to build the routes of documents transferences. Third, the whole lineage in documents is integrated by routes. Finally, a system frame for explanations mechanism in document-driven DSS was described.",,978-1-4244-7869-9,10.1109/IHMSC.2010.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590681,Document-driven DSS;Explanation;Document lineage,Decision support systems;DNA;Decision making;Blood;Face;Computers,data mining;decision making;decision support systems;information retrieval;pattern clustering;text analysis,document driven DSS;decision support system;decision making;document lineage model;document transference,,,,12,,30-Sep-10,,,IEEE,IEEE Conferences
Clustering analysis by Improved Particle Swarm Optimization and K-means algorithm,改進的粒子群算法和K-means算法進行聚類分析,A. J. M. Rani; L. Parthipan,"Department of CSE, Maamallan Institute of Technology, Chennai, INDIA; Department of CSE, Pondicherry Engineering College, Chennai, INDIA",IET Chennai 3rd International on Sustainable Energy and Intelligent Systems (SEISCON 2012),23-Jan-14,2012,,,1,6,"This paper presents an Improved Particle Swarm Optimization (IPSO) and K-means algorithm for solving clustering problems for document and avoid trapping in a local optimal solution. Recent studies have shown that partitional clustering algorithms are more suitable for clustering large datasets. The K-means algorithm is the most commonly used partitional clustering algorithm because it can be easily implemented and is the most efficient one in terms of the execution time. The major problem with this algorithm is that it is sensitive to the selection of the initial partition and may converge to local optima. So here used Improved Particle Swarm Optimization (IPSO) +K-means document clustering algorithm. The proposed solution generates more accurate, robust and better clustering results when compared with K-means and PSO. IPSO algorithm is applied for four different text document datasets. The number of documents in the datasets range from 204 to over 800, and the number of terms range from over 5000 to over 7000 are take for analysis.",,978-1-84919-797-7,10.1049/cp.2012.2195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719101,Clustering;Improved Particle Swarm Optimization;Data mining;K-Means;Centroid,,particle swarm optimisation;pattern clustering;text analysis,clustering analysis;improved particle swarm optimization;k-means document clustering algorithm;IPSO;text document datasets;local optimal solution;partitional clustering algorithm;execution time;initial partition selection,,1,,,,23-Jan-14,,,IET,IET Conferences
Fuzzy named entity-based document clustering,基於模糊命名實體的文檔聚類,T. H. Cao; H. T. Do; D. T. Hong; T. T. Quan,"Faculty of Computer Science and Engineering, Ho Chi Minh City, University of Technology, Vietnam; Faculty of Computer Science and Engineering, Ho Chi Minh City, University of Technology, Vietnam; Faculty of Computer Science and Engineering, Ho Chi Minh City, University of Technology, Vietnam; Faculty of Computer Science and Engineering, Ho Chi Minh City, University of Technology, Vietnam",2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),23-Sep-08,2008,,,2028,2034,"Traditional keyword-based document clustering techniques have limitations due to simple treatment of words and hard separation of clusters. In this paper, we introduce named entities as objectives into fuzzy document clustering, which are the key elements defining document semantics and in many cases are of user concerns. First, the traditional keyword-based vector space model is adapted with vectors defined over spaces of entity names, types, name-type pairs, and identifiers, instead of keywords. Then, hierarchical fuzzy document clustering can be performed using a similarity measure of the vectors representing documents. For evaluating fuzzy clustering quality, we propose a fuzzy information variation measure to compare two fuzzy partitions. Experimental results are presented and discussed.",1098-7584,978-1-4244-1818-3,10.1109/FUZZY.2008.4630648,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4630648,,Fuzzy systems;Conferences,document handling;fuzzy set theory;pattern clustering;vectors,keyword-based document clustering technique;fuzzy document clustering;entity-based document clustering;document semantics;keyword-based vector space model;fuzzy information variation,,14,,24,,23-Sep-08,,,IEEE,IEEE Conferences
An Improved Genetic Algorithm for Document Clustering with Semantic Similarity Measure,一種改進的基於語義相似度的文檔聚類遺傳算法,W. Song; S. C. Park,"Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju; Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju",2008 Fourth International Conference on Natural Computation,7-Nov-08,2008,1,,536,540,"This paper proposes a self-organized genetic algorithm for document clustering based on semantic similarity measure. The traditional method to represent text is that the document is organized as a string of words, while the conceptual similarity is ignored. We take advantage of thesaurus-based ontology to overcome this problem. To investigate how ontology method could be used effectively in document clustering, a hybrid strategy which combines the thesaurus-based semantic similarity measure and vector space model (VSM) measure to provide more accurate assessment of similarity between documents are implemented. Considering the influence between the diversity of the population and the selective pressure, an approach of dynamic evolution operators is put forward in this article. In our experiment two data sets of 200 and 600 documents from Reuter-21578 corpus are excerpted for test and the experiment results show that our method of genetic algorithm in conjunction with the hybrid semantic strategy, the combination of the thesaurus-based measure and VSM-based measure, outperforms that with the sole VSM measure. Our clustering algorithm also efficiently enhances the performance of precision and recall in comparison with k-means in the same similarity environments.",2157-9563,978-0-7695-3304-9,10.1109/ICNC.2008.374,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666903,genetic algorithm;clustering;Wordnet;semantic similarity measure,Genetic algorithms;Clustering algorithms;Ontologies;Clustering methods;Partitioning algorithms;Genetic engineering;Extraterrestrial measurements;Testing;Vocabulary;Web sites,document handling;genetic algorithms;ontologies (artificial intelligence);pattern clustering;thesauri,improved genetic algorithm;document clustering;semantic similarity measure;thesaurus-based ontology;vector space model;Reuter-21578 corpus,,3,,15,,7-Nov-08,,,IEEE,IEEE Conferences
A weighted seeds affinity propagation clustering for efficient document mining,加權種子親和力傳播聚類，用於有效的文檔挖掘,P. Kashyap; S. K. Shrivastava; B. Ujjainiya,"Dept. of Information Technology, SATI, Vidisha, India; Dept. of Information Technology, SATI, Vidisha, India; Dept. of Information Technology, SATI, Vidisha, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)",30-Jan-14,2013,,,1,7,"Clustering is widely used in data mining and learning systems. It is not one specific algorithm, but a general task to be solved which can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. However the clustering is not easy task especially for the complex datasets like text mining where the information does not depends only on terms frequency. This paper presents an effective approach for dealing with similar problems. The proposed algorithm is a category dependent weighted seeds affinity clustering algorithm. The advantage of the proposed algorithm is that clusters can be easily modified according to the field of interest of the user. The superiority of the proposed algorithm is also validated by the simulation results comparison using Reuters-21578 dataset. Results shows improvement over k-means, Affinity and Seeds Affinity Algorithm.",,978-1-4799-3926-8,10.1109/ICCCNT.2013.6726723,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726723,Affinity Propagation Clustering;Clustering;K-Means;Text Mining,Clustering algorithms;Entropy;Algorithm design and analysis;Availability;Text mining;Indexes,data mining;document handling;learning systems;pattern clustering,weighted seeds affinity propagation clustering;efficient document mining;data mining;learning systems;complex datasets;text mining;terms frequency;category dependent weighted seeds affinity clustering algorithm;Reuters-21578 dataset;k-means algorithm,,,,18,,30-Jan-14,,,IEEE,IEEE Conferences
Document Clustering Method Based on Visual Features,基於視覺特徵的文檔聚類方法,Y. Liu; B. Zhang; K. Xing; B. Zhou,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China","2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing",2-Feb-12,2011,,,458,462,"There are two important problems worth conducting research in the fields of personalized information services based on user model. One is how to get and describe user personal information, i.e. building user model, the other is how to organize the information resources, i.e. document clustering. It is difficult to find out the desired information without a proper clustering algorithm. Several new ideas have been proposed in recent years. But most of them only took into account the text information, but some other useful information may have more contributions for documents clustering, such as the text size, font and other appearance characteristics, so called visual features. This paper proposes a method to cluster the scientific documents based on visual features, so called VF-Clustering algorithm. Five kinds of visual features of documents are de-fined, including body, abstract, subtitle, keyword and title. The thought of crossover and mutation in genetic algorithm is used to adjust the value of k and cluster center in the k-means algorithm dynamically. Experimental result supports our approach as better concept. In the five visual features, the clustering accuracy and steadiness of subtitle are only less than that of body, but the efficiency is much better than body because the subtitle size is much less than body size. The accuracy of clustering by combining subtitle and keyword is better than each of them individually, but is a little less than that by combining subtitle, keyword and body. If the efficiency is an essential factor, clustering by combining subtitle and keyword can be an optimal choice.",,978-1-4577-1976-9,10.1109/iThings/CPSCom.2011.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142293,document clustering;k-means;visual features;genetic algorithm,Visualization;Clustering algorithms;Algorithm design and analysis;Genetic algorithms;Heuristic algorithms;Vectors;Feature extraction,genetic algorithms;information resources;pattern clustering;text analysis,document clustering method;visual features;personalized information services;user personal information;information resources;text information;text size;text font;appearance characteristics;scientific documents;body;abstract;subtitle;keyword;title;crossover;mutation;genetic algorithm;k-means algorithm,,,,16,,2-Feb-12,,,IEEE,IEEE Conferences
Invariants Extraction Method Applied in an Omni-language Old Document Navigating System,不變量提取方法在全語言舊文檔導航系統中的應用,Q. A. Bui; M. Visani; R. Mullot,"Lab. L3i, Univ. of La Rochelle, La Rochelle, France; Lab. L3i, Univ. of La Rochelle, La Rochelle, France; Lab. L3i, Univ. of La Rochelle, La Rochelle, France",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,1325,1329,"We are currently working on the concept of an omni script and interactive word retrieval system for ancient document collection navigation, based on query composition for non-expert users. To make the query, the user selects and composes writing pieces, which are invariants automatically extracted from the old document collection. In order to extract invariants from documents, strokes must be first extracted and clustered. Stroke extraction raises two main difficulties: detecting the ambiguous zones so as to extract primary strokes (writing pieces which do not contain any ambiguous zone) and grouping the primary strokes so as to form invariants. In this paper, we present existing methods for ambiguity zones detection and compare these methods on documents of different languages and periods to find out which one is more adapted in our context. Once ambiguous zones have been extracted, some neighboring primary strokes are grouped so as to obtain strokes and our clustering algorithm is applied over these strokes to find their representatives, i.e. the invariants. These invariants can further be used by the user to compose his/her query and to retrieve words from the document collection.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628829,Word Retrieval;Invariant Extraction;Stroke Extraction;Clustering;Ambiguous Zones Detection,Databases;Writing;Shape;Feature extraction;Clustering algorithms;Visualization;Algorithm design and analysis,document handling;history;pattern clustering;query processing,invariants extraction method;omni-language old document navigation system;omni-script language;interactive word retrieval system;ancient document collection navigation;query composition;user selection;user composition;stroke extraction;ambiguity zones detection;primary strokes;clustering algorithm,,1,,18,,15-Oct-13,,,IEEE,IEEE Conferences
Study on meaningful string extraction algorithm for improving webpage classification,改進網頁分類的有意義的字符串提取算法研究,J. Chen; J. Li; H. Liao; Q. Yuan; X. Bao,"Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China",2011 7th International Conference on Natural Language Processing and Knowledge Engineering,26-Jan-12,2011,,,140,145,"This paper shows that the accuracy of webpage classifiers can be improved by extracting meaningful strings with an unsupervised clustering method. Since webpage classification is different from original document classification with its words and phrases irregular, massive and unlabeled features, to cope with these features, we introduce two scenarios for extracting meaningful strings based on document clustering and term clustering with multi-strategies to optimize a vector space model (VSM). First, some candidate strings are used to build VSM; then, two scenarios performance on VSM respectively; last, we can extract meaningful strings from each cluster. So, theses meaningful string may represent documents comprehensively. The proposed method has been applied to webpage document classification and the results show that document clustering works better than term clustering. They also demonstrate that spectral clustering method outperforms k-means in document clustering case, conversely, make reduce performance in term clustering. However, a better synthetic performance can be obtained by spectral clustering with document clustering from some experiments.",,978-1-61284-729-0,10.1109/NLPKE.2011.6138182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138182,webpage classification;document clustering;term clustering;spectral clustering;k-means,Dictionaries;Computational modeling,document handling;Internet;pattern classification;pattern clustering,string extraction algorithm;Web page classification;unsupervised clustering method;document classification;document clustering;term clustering;vector space model;spectral clustering method,,,,14,,26-Jan-12,,,IEEE,IEEE Conferences
Enhancing an Evolving Tree-based text document visualization model with Fuzzy c-Means clustering,通過模糊c均值聚類增強基於樹的文本文檔可視化模型,W. L. Chang; K. M. Tay; C. P. Lim,"Faculty of Engineering, Universiti Malaysia Sarawak, Kota Samarahan, Malaysia; Faculty of Engineering, Universiti Malaysia Sarawak, Kota Samarahan, Malaysia; Centre for Intelligent Systems Research, Deakin University, Australia",2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),7-Oct-13,2013,,,1,6,"An improved evolving model, i.e., Evolving Tree (ETree) with Fuzzy c-Means (FCM), is proposed for undertaking text document visualization problems in this study. ETree forms a hierarchical tree structure in which nodes (i.e., trunks) are allowed to grow and split into child nodes (i.e., leaves), and each node represents a cluster of documents. However, ETree adopts a relatively simple approach to split its nodes. Thus, FCM is adopted as an alternative to perform node splitting in ETree. An experimental study using articles from a flagship conference of Universiti Malaysia Sarawak (UNIMAS), i.e., Engineering Conference (ENCON), is conducted. The experimental results are analyzed and discussed, and the outcome shows that the proposed ETree-FCM model is effective for undertaking text document clustering and visualization problems.",1098-7584,978-1-4799-0022-0,10.1109/FUZZ-IEEE.2013.6622363,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622363,Evolving tree;text document clustering;visualization;online learning;fuzzy c-means,Vectors;Clustering methods;Radiation detectors;Visualization;Abstracts;Numerical models;Computational complexity,data visualisation;fuzzy set theory;pattern clustering;text analysis,evolving tree based text document visualization model;fuzzy c-means clustering;ETree;FCM;hierarchical tree structure;documents cluster;Universiti Malaysia Sarawak;UNIMAS;Engineering Conference,,2,,30,,7-Oct-13,,,IEEE,IEEE Conferences
Enhanced Text Extraction from Arabic Degraded Document Images Using EM Algorithm,使用EM算法從阿拉伯語降解文檔圖像中增強文本提取,W. Boussellaa; A. Bougacha; A. Zahour; H. E. Abed; A. Alimi,"ENIS, Univ. of Sfax, Sfax, Tunisia; ENIS, Univ. of Sfax, Sfax, Tunisia; IUT, Univ. du Havre, Le Havre, France; Inst. for Commun. Technol. (IfN), Tech. Univ. Braunschweig, Braunschweig, Germany; ENIS, Univ. of Sfax, Sfax, Tunisia",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,743,747,"This paper presents a new enhanced text extraction algorithm from degraded document images on the basis of the probabilistic models. The observed document image is considered as a mixture of Gaussian densities which represents the foreground and background document image components. The EM algorithm is introduced in order to estimate and improve the parameters of the mixtures of densities recursively. The initial parameters of the EM algorithm are estimated by the k-means clustering method. After the parameter estimation, the document image is partitioned into text and background classes by the means of ML approach. The performance of the proposed approach is evaluated on a variety of degraded documents comes from the collections of the National library of Tunisia.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.220,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277497,segmentation;k-means clustering;expectation-maximisation algorithm (EM);Maximum likelihood algorithm(ML);Arabic degraded document image,Degradation;Clustering algorithms;Image segmentation;Image enhancement;Algorithm design and analysis;Parameter estimation;Maximum likelihood estimation;Text analysis;Partitioning algorithms;Clustering methods,document image processing;expectation-maximisation algorithm;Gaussian processes;image representation;image segmentation;maximum likelihood detection;natural language processing;parameter estimation;probability;text analysis,enhanced text extraction algorithm;Arabic degraded document image;probabilistic model;Gaussian mixture;EM algorithm;parameter estimation;k-means clustering method;National library of Tunisia;image representation;ML algorithm,,5,,16,,2-Oct-09,,,IEEE,IEEE Conferences
Employing Clustering Techniques for Automatic Information Extraction From HTML Documents,使用聚類技術從HTML文檔中自動提取信息,F. Ashraf; T. ?zyer; R. Alhajj,"Dept. of Comput. Sci., Calgary Univ., Calgary, AB; Dept. of Comput. Sci., Calgary Univ., Calgary, AB; Dept. of Comput. Sci., Calgary Univ., Calgary, AB","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",19-Aug-08,2008,38,5,660,673,"In the past few years, there has been an exponential increase in the amount of information available on the World Wide Web. This plethora of information can be extremely beneficial for users. However, the amount of human intervention that is currently required for this is inconvenient. Information extraction (IE) systems try to solve this problem by making the task as automatic as possible. Most of the existing approaches, however, require user feedback in one form or another during the extraction. This paper proposes a system that employs clustering techniques for automatic IE from HTML documents containing semistructured data. Using domain-specific information provided by the user, the proposed system parses and tokenizes the data from an HTML document, partitions it into clusters containing similar elements, and estimates an extraction rule based on the pattern of occurrence of data tokens. The extraction rule is then used to refine clusters, and finally, the output is reported. We employed a multiobjective genetic-algorithm-based clustering approach in the process; it is capable of finding the number of clusters and the most natural clustering. The proposed approach is tested by conducting experiments on a number of Web sites from different domains. To demonstrate the effectiveness of this approach, the results of the experiments are tested against those reported in the literature, and prove comparable.",1558-2442,,10.1109/TSMCC.2008.923882,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591416,Clustering;Hypertext Markup Language (HTML) documents;information extraction (IE);Web pages;Clustering;Hypertext Markup Language (HTML) documents;information extraction (IE);Web pages,Data mining;HTML;Computer science;Databases;Web sites;Humans;Testing;Web pages;Search engines;Keyword search,genetic algorithms;hypermedia markup languages;information retrieval;pattern clustering;Web sites,automatic information extraction;HTML document;World Wide Web;user feedback;pattern clustering technique;multiobjective genetic-algorithm;Web sites,,16,,35,,8-Aug-08,,,IEEE,IEEE Journals
Search result clustering studies in Turkish,土耳其語中的搜索結果聚類研究,B. Dural; B. Diri,"Bilgisayar M羹hendisli?i B繹l羹m羹, Y覺ld覺z Teknik ?niversitesi, 襤stanbul, T羹rkiye; Bilgisayar M羹hendisli?i B繹l羹m羹, Y覺ld覺z Teknik ?niversitesi, 襤stanbul, T羹rkiye",2013 21st Signal Processing and Communications Applications Conference (SIU),13-Jun-13,2013,,,1,4,"Contents of the internet is rapidly growing day by day. To reach the information we search on the internet, we're using internet search engines, which can return millions of search results depends on the query. Finding the information needed on the search results can be a massive problem. Search result clustering is a useful operation to solve this problem. One of the most popular search result clustering algorithm is Suffix Tree Clustering can achieve successful results. In this work, we try to improve Suffix Tree Clustering to get more successful results on clustering Turkish web search results.",,978-1-4673-5563-6,10.1109/SIU.2013.6531602,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531602,Suffix tree clustering;search result clustering;document clustering,Internet;Encyclopedias;Electronic publishing;TV;Clustering algorithms;Web search,Internet;pattern clustering;search engines,Internet search engines;search result clustering algorithm;suffix tree clustering;Turkish Web search results,,,,6,,13-Jun-13,,,IEEE,IEEE Conferences
Document Clustering Using Message Passing between Data Points,使用數據點之間的消息傳遞進行文檔聚類,N. Sahu; K. K. Mohbey; G. S. Thakur,"Singhania Univ., Pacheri, India; MANIT, Bhopal, India; MANIT, Bhopal, India",2013 International Conference on Communication Systems and Network Technologies,10-Jun-13,2013,,,591,596,"This paper presents an efficient approach for Document Classification based on FDCKE. The paper introduces a new Framework for Document Classification and Knowledge Extraction (FDCKE). The FDCKE approach is an integration of document classification phases like document collection from heterogeneous sources, Text Pre-processing of the documents, Feature Selection, Indexing, Classification Process, Results Analysis and Performance Measures. The proposed FDCKE is a unified interface that can be used for Classification, Association and Clustering. Twenty News group data sets [23] are used in the Experiments. The performance evaluation of Experimental Results done by SAS Software. The Experimental Results show that the proposed approach out performs.",,978-1-4673-5603-9,10.1109/CSNT.2013.127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6524470,Classification;Centroid;FDCKE;BMM;Indexing methods;Clustering;association;Text mining;Web mining and Semistructured Data,Availability;Clustering methods;Indexing;Accuracy;Semantics;Classification algorithms;Time series analysis,document handling;message passing;pattern clustering,document clustering;message passing;data points;framework for document classification and knowledge extraction;FDCKE;document collection,,,,24,,10-Jun-13,,,IEEE,IEEE Conferences
High Speed Document Clustering in Reconfigurable Hardware,可重構硬件中的高速文檔聚類,G. A. Covington; C. L. G. Comstock; A. A. Levine; J. W. Lockwood; Y. H. Cho,"Applied Research Laboratory, Washington University One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899 USA, gac1@arl.wustl.edu, http://www.arl.wustl.edu/projects/fpx/reconfig.htm; Applied Research Laboratory, Washington University One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899 USA, cc1@cse.wustl.edu, http://www.arl.wustl.edu/projects/fpx/reconfig.htm; Applied Research Laboratory, Washington University One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899 USA, aal1@arl.wustl.edu, http://www.arl.wustl.edu/projects/fpx/reconfig.htm; Applied Research Laboratory, Washington University One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899 USA, lockwood@arl.wustl.edu, http://www.arl.wustl.edu/projects/fpx/reconfig.htm; Applied Research Laboratory, Washington University One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899 USA, young@arl.wustl.edu, http://www.arl.wustl.edu/projects/fpx/reconfig.htm",2006 International Conference on Field Programmable Logic and Applications,16-Apr-07,2006,,,1,7,"High-performance document clustering systems enable similar documents to be automatically organized into groups. In the past, the large amount of computational time needed to cluster documents prevented practical use of such systems with a large number of documents. A full hardware implementation of the K-means clustering algorithm has been designed and implemented in reconfigurable hardware that clusters 512k documents rapidly. This implementation, uses four parallel cosine distance metrics to cluster document vectors that each have 4000 dimensions. The synthesized hardware runs on the field programmable port extender (FPX) platform at a clock rate of 80 MHz. Although the clock rate on the Xilinx VirtexE 2000 is slower than a CPU, the implementation runs 26 times faster than an algorithmically equivalent software algorithm running on an Intel 3.60 GHz Xeon. The same architecture was used to synthesize a faster and larger design for the Xilinx Virtex4 LX200. This larger implementation can contain up to 25 parallel cosine distance metrics. The implementation synthesized with a clock rate of 250 MHz and outperforms the equivalent software by a factor of 328",1946-1488,1-4244-0312-X,10.1109/FPL.2006.311245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4101007,,Hardware;Clustering algorithms;Clocks;Field programmable gate arrays;Software algorithms;Application specific integrated circuits;Laboratories;Algorithm design and analysis;Central Processing Unit;Computer architecture,document handling;field programmable gate arrays;indexing;microprocessor chips;parallel algorithms;pattern clustering;reconfigurable architectures,reconfigurable hardware;document clustering systems;K-means clustering algorithm;parallel cosine distance metrics;document vectors;field programmable port extender platform;Xilinx VirtexE 2000;CPU;software algorithm;Intel Xeon;Xilinx Virtex4 LX200;80 MHz;250 MHz,,6,,11,,16-Apr-07,,,IEEE,IEEE Conferences
A New Web Search Result Clustering based on True Common Phrase Label Discovery,基於真實通用短語標籤發現的新Web搜索結果聚類,J. Janruang; W. Kreesuradej,"King Mongkut's Institute of Technology Ladkrabang Bankok, Thailand; King Mongkut's Institute of Technology Ladkrabang Bankok, Thailand",2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),8-Jan-07,2006,,,242,242,"Web search results clustering are navigator for users to search results. Therefore the correct cluster label is important which has been index the set of web document. Suffix tree clustering (STC) is fast automatically clustering and labeling. However, STC is inadequate since they generate interrupted cluster label due to using n-gram technique. In this paper, we propose an approach for web search results clustering and labeling based on a new suffix tree data structure, a new base cluster combining algorithm with a new partial phase join operation. The algorithm for constructing the data structure is an incremental and a linear time algorithm. Thus, the proposed approach is suitable for on-the-fly the web search results clustering and labeling cluster. The proposed approach provides more readable and true common phrase of web document cluster than conventional web search result clustering. Experimental results also show that the proposed approach has better performance than that of conventional web search result clustering.",,0-7695-2731-0,10.1109/CIMCA.2006.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052852,,Web search;Clustering algorithms;Labeling;Tree data structures;Data structures;Search engines;Computational intelligence;Information technology;Navigation;Feeds,document handling;information retrieval;pattern clustering;search engines;tree searching,Web search;result clustering;common phrase label discovery;Web document;suffix tree clustering;n-gram technique,,14,,14,,8-Jan-07,,,IEEE,IEEE Conferences
Text clustering using fuzzy neighborhood and evaluation of clusters,使用模糊鄰域的文本聚類和聚類評估,Z. Canlun; S. Miyamoto,"Master Program in Risk Engineering, University of Tsukuba, Ibaraki 305-8573, Japan; Department of Risk Engineering, University of Tsukuba, Ibaraki 305-8573, Japan",2014 IEEE International Conference on Granular Computing (GrC),15-Dec-14,2014,,,19,24,"This study is concerned with text clustering and evaluating the clusters. Fuzzy neighborhood, a method of text mining, is used for this purpose. Five different clustering algorithms are used, they are kernel affinity propagation, kernel hard c-means, kernel fuzzy c-means, kernel hard k-means++ and kernel variable size hard c-means. These algorithms are applied to the clustering of nouns and adjectives in Chinese documents and SNS. The results from the previous four algorithms except kernel affinity propagation are evaluated by Rand index.",,978-1-4799-5464-3,10.1109/GRC.2014.6982800,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982800,clustering;fuzzy neighborhood;text mining;evaluation,Kernel;Indexes;Clustering methods;Clustering algorithms;Companies;Text mining;Linear programming,data mining;fuzzy set theory;natural language processing;pattern clustering;text analysis,text clustering;fuzzy neighborhood;cluster evaluation;text mining;kernel affinity propagation;kernel hard c-means;kernel fuzzy c-means;kernel hard k-means++;kernel variable size hard c-means;Chinese documents;SNS,,2,,12,,15-Dec-14,,,IEEE,IEEE Conferences
A hyper-heuristic approach to design and tuning heuristic methods for web document clustering,一種用於設計和調整Web文檔集群的啟發式方法的超啟發式方法,C. Cobos; M. Mendoza; E. Le籀n,"Computer Science Department, Universidad del Cauca, Popay獺n, Colombia; Computer Science Department, Universidad del Cauca, Popay獺n, Colombia; Systems and Industrial Department Universidad Nacional de Colombia, Bogot獺, Colombia",2011 IEEE Congress of Evolutionary Computation (CEC),14-Jul-11,2011,,,1350,1358,"This paper introduces a new description-centric algorithm for web document clustering called HHWDC. The HHWDC algorithm has been designed from a hyper-heuristic approach and allows defining the best algorithm for web document clustering. HHWDC uses as heuristic selection methodology two options, namely: random selection and roulette wheel selection based on performance of low-level heuristics (harmony search, an improved harmony search, a novel global harmony search, global-best harmony search, restrictive mating, roulette wheel selection, and particle swarm optimization). HHWDC uses the k-means algorithm for local solution improvement strategy, and based on the Bayesian Information Criteria is able to automatically define the number of clusters. HHWDC uses two acceptance/replace strategies, namely: Replace the worst and Restricted Competition Replacement. HHWDC was tested with data sets based on Reuters-21578 and DMOZ, obtaining promising results (better precision results than a Singular Value Decomposition algorithm).",1941-0026,978-1-4244-7835-4,10.1109/CEC.2011.5949773,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949773,web document clustering;hyper-heuristic;genetic algorithm;memetic algorithm;harmony search;particle swarm,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Heuristic algorithms;Bandwidth;Particle swarm optimization;Time division multiplexing,Bayes methods;document handling;Internet;particle swarm optimisation;pattern clustering,hyper-heuristic approach;heuristic method tuning;heuristic method design;Web document clustering;description-centric algorithm;HHWDC algorithm;k-means algorithm;local solution improvement strategy;Bayesian information criteria;restricted competition replacement;data set;DMOZ,,8,,35,,14-Jul-11,,,IEEE,IEEE Conferences
An evolutionary algorithm for Feature Selective Double Clustering of text documents,文本文檔特徵選擇性雙聚類的進化算法,S. N. Nourashrafeddin; E. Milios; D. V. Arnold,"Faculty of Computer Science, Dalhousie University, Halifax, Nova Scotia, Canada B3H 4R2; Faculty of Computer Science, Dalhousie University, Halifax, Nova Scotia, Canada B3H 4R2; Faculty of Computer Science, Dalhousie University, Halifax, Nova Scotia, Canada B3H 4R2",2013 IEEE Congress on Evolutionary Computation,15-Jul-13,2013,,,446,453,"We propose FSDC, an evolutionary algorithm for Feature Selective Double Clustering of text documents. We first cluster the terms existing in the document corpus. The term clusters are then fed into multiobjective genetic algorithms to prune non-informative terms and form sets of keyterms representing topics. Based on the topic keyterms found, representative documents for each topic are extracted. These documents are then used as seeds to cluster all documents in the dataset. FSDC is compared to some well-known co-clusterers on real text datasets. The experimental results show that our algorithm can outperform the competitors.",1941-0026,978-1-4799-0454-9,10.1109/CEC.2013.6557603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557603,Genetic algorithm;co-clustering;multiobjective optimization;text clustering,Clustering algorithms;Genetic algorithms;Vectors;Evolutionary computation;Mutual information;Approximation algorithms;Computer science,feature extraction;genetic algorithms;pattern clustering;text analysis,FSDC;evolutionary algorithm;feature selective double clustering;text documents;document corpus;multiobjective genetic algorithms;noninformative term pruning;topic keyterm set;representative document extraction;real text datasets,,1,,19,,15-Jul-13,,,IEEE,IEEE Conferences
Hybrid approach for visualization of documents clusters using GHSOM and sammon projection,使用GHSOM和sammon投影的文檔簇可視化混合方法,P. Butka; J. P籀csov獺,"Technical University of Ko禳ice, Faculty of Electrical Engineering and Informatics, Department of Cybemetics and Artificial Intelligence, Letn獺 9, 040 01, Kosice, Slovakia; Technical University of Ko禳ice, BERG Faculty, Institute of Control and Informatization of Production Processes",2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI),26-Sep-13,2013,,,337,342,"This paper presents the hybrid approach for visualization of documents sets by the combination of hierarchical clustering method, based on the Growing Hierarchical Self-Organizing Maps algorithm, and Sammon projection. Algorithms based on the self-organizing maps provide robust clustering method suitable for visualization of larger number of documents into the grid-based 2D maps. Sammon projection is nonlinear projection method suitable mostly to visualization of smaller sets of object on (usually 2D) maps based on the projections. Here we have implemented and tested combination of these approaches, where starting set of documents is organized using GHSOM to subsets of similar documents, then for clusters at the end of clustering phase, with smaller number of inputs, Sammon maps are created in order to provide distinction also for documents in these clusters. The method for extraction of characteristic terms based on the information gain analysis was used for description of clusters. Existing library JBOWL was used for implementation of the hybrid algorithm. For testing purposes, the documents in English language were used.",,978-1-4673-6400-3,10.1109/SACI.2013.6608994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608994,,Vectors;Clustering algorithms;Stress;Visualization;Data mining;Algorithm design and analysis;Adaptation models,data visualisation;document handling;information analysis;natural language processing;pattern clustering;self-organising feature maps,English language;hybrid algorithm;library JBOWL;information gain analysis;clustering phase;nonlinear projection method;grid-based 2D maps;robust clustering method;growing hierarchical self-organizing maps algorithm;hierarchical clustering method;Sammon projection;GHSOM;documents cluster visualization,,1,,16,,26-Sep-13,,,IEEE,IEEE Conferences
Semantic-Based Text Document Clustering Using Cognitive Semantic Learning and Graph Theory,基於認知語義學習和圖論的基於語義的文本文檔聚類,I. Ali; A. Melton,"Dept. of Comput. Sci., Kent State Univ., Kent, OH, USA; Dept. of Comput. Sci., Kent State Univ., Kent, OH, USA",2018 IEEE 12th International Conference on Semantic Computing (ICSC),12-Apr-18,2018,,,243,247,"Semantic-based text document clustering aims to group documents into a set of topic clusters. We propose a new approach for semantically clustering of text documents based on cognitive science and graph theory. We apply a computational cognitive model of semantic association for human semantic memory, known as Incremental Construction of an Associative Network (ICAN). The vector-based model of Latent Semantic Analysis (LSA), has been a leading computational cognitive model for semantic learning and topic modeling, but it has well-known limitations including not considering the original word-order and doing semantic reduction with neither a linguistic nor cognitive basis. These issues have been overcome by the ICAN model. ICAN model is used to generate document-level semantic-graphs. Cognitive and graph-based topic and context identification methods are used for semantic reduction of ICAN graphs. A corpus-graph is generated from ICAN graphs, and then a community-detection graph algorithm is applied for the final step of document clustering. Experiments are conducted on three commonly used datasets. Using the purity and entropy criteria for clustering quality, our results show a notable outperformance over the LSA-based approach.",,978-1-5386-4408-9,10.1109/ICSC.2018.00042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334465,document clustering;semantic learning;semantic representation;cognitive semantics,Semantics;Computational modeling;Task analysis;Matrix decomposition;Context modeling;Clustering algorithms;Linguistics,graph theory;learning (artificial intelligence);pattern clustering;text analysis,semantic-based text document clustering;cognitive semantic learning;semantic reduction;community-detection graph algorithm;corpus-graph;ICAN graphs;context identification methods;document-level semantic-graphs;ICAN model;original word-order;topic modeling;leading computational cognitive model;Latent Semantic Analysis;Associative Network;human semantic memory;semantic association;cognitive science;graph theory,,3,,34,,12-Apr-18,,,IEEE,IEEE Conferences
Co-Clustering with Side Information for Text mining,與輔助信息共同集群以進行文本挖掘,R. E. Thomas; S. S. Khan,"Department of Computer Engineering, St.Francis Institute of Technology, Mumbai, India; Department of Computer Engineering, St.Francis Institute of Technology, Mumbai, India",2016 International Conference on Data Mining and Advanced Computing (SAPIENCE),12-Dec-16,2016,,,105,108,"Many of the text mining applications contain a huge amount of information from document in the form of text. This text can be very helpful for Text Clustering. This text also includes various kind of other information known as Side Information or Metadata. Examples of this side information include links to other web pages, title of the document, author name or date of Publication which are present in the text document. Such metadata may possess a lot of information for the clustering purposes. But this Side information may be sometimes noisy. Using such Side Information for producing clusters without filtering it, can result to bad quality of Clusters. So we use an efficient Feature Selection method to perform the mining process to select that Side Information which is useful for Clustering so as to maximize the advantages from using it. The proposed technique, CCSI (Co-Clustering with Side Information) system makes use of the process of Co-Clustering or Two-mode clustering which is a data mining technique that allows concurrently clustering of the rows and columns of a matrix.",,978-1-4673-8594-7,10.1109/SAPIENCE.2016.7684152,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7684152,Co-Clustering;Data Mining;Text Mining;Side Information,Clustering algorithms;Indexes;Partitioning algorithms;Data mining;Spatial databases;Noise measurement,data mining;Internet;matrix algebra;meta data;optimisation;pattern clustering;text analysis,text mining applications;text clustering;Metadata;Web pages;document title;author name;publication date;efficient feature selection method;CCSI;co-clustering with side information;data mining technique;matrix column clustering;matrix row clustering,,3,,9,,12-Dec-16,,,IEEE,IEEE Conferences
Hierarchical Clustering using Reconfigurable Devices,使用可重配置設備的層次集群,S. Padmanabhan; M. Looks; D. Legorreta; Y. Cho; J. Lockwood,Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis,2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines,11-Dec-06,2006,,,327,328,"Non-hierarchical k-means algorithms have been implemented in hardware, most frequently for image clustering. Here, we focus on hierarchical clustering of text documents based on document similarity. To our knowledge, this is the first work to present a hierarchical clustering algorithm designed for hardware implementation and ours is the first hardware-accelerated implementation",,0-7695-2661-6,10.1109/FCCM.2006.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020943,,Clustering algorithms;Motorcycles;Hardware;Partitioning algorithms;Runtime;Computer science;Algorithm design and analysis;Frequency;Binary trees;Embedded system,document handling;pattern clustering,reconfigurable devices;nonhierarchical k-means algorithms;image clustering;text documents;document similarity;hierarchical clustering algorithm;hardware implementation,,,,5,,11-Dec-06,,,IEEE,IEEE Conferences
Fuzzy multisets and fuzzy clustering of documents,文檔的模糊多集和模糊聚類,S. Miyamoto,"Inst. of Eng. Mech. & Syst., Tsukuba Univ., Ibaraki, Japan",10th IEEE International Conference on Fuzzy Systems. (Cat. No.01CH37297),7-Aug-02,2001,3,,1539,1542 vol.2,Aims at developing a method of fuzzy clustering based on fuzzy multisets. Data clustering has been discussed in relation to information retrieval models and fuzzy multisets provide an appropriate model of information retrieval on the WWW. Fuzzy clustering of fuzzy multisets is thus necessary for application to an information retrieval model. A term-document matrix in which entries are multisets are considered. Two dissimilarity measures on fuzzy multisets are proposed. Two methods of fuzzy c-means using these measures are studied in which calculation of cluster centers is focused upon.,,0-7803-7293-X,10.1109/FUZZ.2001.1008956,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008956,,Information retrieval;Fuzzy systems;World Wide Web;Relational databases;Systems engineering and theory;Web sites;Search engines;Mathematical model;Spatial databases;Database languages,information retrieval;fuzzy set theory;pattern clustering;information resources,fuzzy multisets;fuzzy clustering;documents clustering;data clustering;information retrieval models;World Wide Web;term-document matrix;dissimilarity measures;fuzzy c-means;cluster centers,,6,,15,,7-Aug-02,,,IEEE,IEEE Conferences
Finding Hotspots in Document Collection,在文檔收集中查找熱點,W. Peng; C. Ding; T. Li; T. Sun,"Florida Int. Univ., Miami; NA; NA; NA",19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007),4-Jan-08,2007,1,,313,320,"Given a document collection, it is often desirable to find the core subset of documents focusing on a specific topic. We propose a new algorithm for this task. Document clustering aims at partitioning the document-term datasets into different groups by optimizing certain objective functions. However, they are not suitable for finding hotspots that are described by a small set of documents with few tightly coupled terms. In this paper we propose a novel hot spot finding algorithm, DCC (Dense Concept Clustering) in document collections. DCC can extract distinct small topics with most representative documents and words simultaneously. The hotspots are dense bicliques in binary document-word matrices and they can be discovered sequentially one at a time using the generalized Motzkin-Straus formalism. The representative documents and words are tightly correlated for concept descriptions. Experiments on real document datasets show the effectiveness of the proposed algorithm.",2375-0197,978-0-7695-3015-4,10.1109/ICTAI.2007.173,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410300,,Clustering algorithms;Partitioning algorithms;USA Councils;Computer science;Artificial intelligence;Sun;Technological innovation;Concrete;Clustering methods;Bioinformatics,document handling;pattern clustering,document collection;document clustering;document-term datasets;hot spot finding algorithm;dense concept clustering;dense bicliques;binary document-word matrices;generalized Motzkin-Straus formalism,,1,,19,,4-Jan-08,,,IEEE,IEEE Conferences
Semi-supervised PLSA for Document Clustering,用於文檔聚類的半監督PLSA,L. Niu; Y. Shi,"Res. Center on Fictitious Econ. & Data Sci., Chinese Acad. of Sci., Beijing, China; Res. Center on Fictitious Econ. & Data Sci., Chinese Acad. of Sci., Beijing, China",2010 IEEE International Conference on Data Mining Workshops,20-Jan-11,2010,,,1196,1203,"By utilizing the must-link or cannot-link pair wise constraints in data, semi-supervised clustering improves the performance of unsupervised clustering significantly. A number of semi-supervised clustering algorithms have been proposed to consider such pair wise constraints. However, most of them assign a hard label to each data item and produce little information about the cluster itself. In this work, we propose a Probabilistic Latent Semantic Analysis(PLSA) based semi-supervised algorithm for documents clustering by employing the must-link supervision between two documents, which is available in many real world data. The new algorithm can produce the soft cluster label assignment for each document as well as the probabilistic representation of latent topics in the cluster. No additional parameters need to be estimated besides the parameters in standard PLSA. This reduces the risk of over-fitting especially when the data is sparse. We provide the Expectation Maximization(EM) procedure for semi-supervised PLSA to determine the local optimal parameters that maximize the likelihood. To utilize multiple computation nodes for large scale data set, we also propose a distributed implementation of the EM procedure based on the MapReduce framework. Experimental results on public data set validate the effectiveness and efficiency of the new method.",2375-9259,978-1-4244-9244-2,10.1109/ICDMW.2010.85,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693430,Semi-supervised Clustering;PLSA;Topic Model;Distributed Algorithm,Clustering algorithms;Estimation;Probabilistic logic;Joints;Algorithm design and analysis;Data models;Distributed databases,distributed algorithms;document handling;expectation-maximisation algorithm;pattern clustering;probability;unsupervised learning,semisupervised PLSA;document clustering;pairwise constraints;probabilistic latent semantic analysis;soft cluster label assignment;expectation maximization;distributed implementation;MapReduce,,6,,22,,20-Jan-11,,,IEEE,IEEE Conferences
Identifying Similar Cases in Document Networks Using Cross-Reference Structures,使用交叉引用結構識別文檔網絡中的相似案例,T. Botsis; J. Scott; E. J. Woo; R. Ball,"Office of Biostatistics and Epidemiology, Center for Biologics Evaluation and Research, Food and Drug Administration, MD, USA; Office of Biostatistics and Epidemiology, Center for Biologics Evaluation and Research, MD, USA; Office of Biostatistics and Epidemiology, Center for Biologics Evaluation and Research, MD, USA; Office of Biostatistics and Epidemiology, Center for Biologics Evaluation and Research",IEEE Journal of Biomedical and Health Informatics,3-Nov-15,2015,19,6,1906,1917,"Our objective was to explore the creation of document networks based on different thresholds of shared information and different clustering algorithms on those networks to identify document clusters describing similar clinical cases. We created networks from vaccine adverse event report sets using seven approaches for linking reports. We then applied three clustering algorithms [visualization of similarities (VOS), Louvain, k-means] to these networks and evaluated their ability to identify known clusters. The report sets included one simulated set and three sets from the Vaccine Adverse Event Reporting System; each was split into training and testing subsets. Training subsets were used to estimate parameter values for the clustering algorithms and testing subsets to evaluate clusters. We created the networks by linking reports based on shared information in the form either of individual Medical Dictionary for Regulatory Activities Preferred Terms (PTs) or of dyads, triplets, quadruplets, quintuplets, and sextuplets of PTs; we created another network by weighting the single PT network connections by Lin's information theoretic approach to similarity. We then repeated this entire process using networks based on text mining output rather than structured data. We evaluated report clustering using recall, precision, and f-measure. The VOS algorithm outperformed Louvain and k-means in general. The best weighting scheme appeared to be related to the complexity of the known cluster. For example, singleton weighting performed best for an intussusception cluster driven by a single PT. We observed marginal differences between the code- and textual-based clustering. In conclusion, our approach supported identification of similar nodes in a document network.",2168-2208,,10.1109/JBHI.2014.2345873,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6873230,Classification;clustering;document networks;postmarketing product surveillance;similarity,Clustering algorithms;Training;Algorithm design and analysis;Information retrieval;Network topology,data mining;data structures;electronic health records;parameter estimation;pattern classification;pattern clustering,document networks;cross-reference structures;clustering algorithms;vaccine adverse event report sets;visualization-of-similarities;Louvain method;k-means method;testing subsets;training subsets;parameter estimation values;individual medical dictionary-for-regulatory activities preferred terms;Lin's information theoretic approach;text mining output;f-measure;VOS algorithm;intussusception cluster;textual-based clustering,"Algorithms;Biomedical Research;Cluster Analysis;Data Mining;Databases, Factual;Humans;Medical Informatics Computing;Pattern Recognition, Automated",4,,38,,7-Aug-14,,,IEEE,IEEE Journals
Web Document Clustering Using Document Index Graph,使用文檔索引圖的Web文檔聚類,B. F. Momin; P. J. Kulkarni; A. Chaudhari,"Student Member IEEE, Dept. Of Computer Science & Engineering, Walchand College Of Engineering, SANGLI, INDIA. bfmomin@rediffmail.com; Dept. Of Computer Science & Engineering, Walchand College Of Engineering, SANGLI, INDIA. pjk_walchand@rediffmail.com; Dept. Of Computer Science & Engineering, Walchand College Of Engineering, SANGLI, INDIA. amolac_02@yahoo.com",2006 International Conference on Advanced Computing and Communications,13-Aug-07,2006,,,32,37,"Document Clustering is an important tool for many Information Retrieval (IR) tasks. The huge increase in amount of information present on Web poses new challenges in clustering regarding to underlying data model and nature of clustering algorithm. Document clustering techniques mostly rely on single term analysis of document data set. To achieve more accurate document clustering, more informative feature such as phrases are important in this scenario. Hence first part of the paper presents phrase-based model, Document Index Graph (DIG), which allows incremental phrase-based encoding of documents and efficient phrase matching. It emphasizes on effectiveness of phrase-based similarity measure over traditional single term based similarities. In the second part, a Document Index Graph based Clustering (DIGBC) algorithm is proposed to enhance the DIG model for incremental and soft clustering. This algorithm incrementally clusters documents based on proposed cluster-document similarity measure. It allows assignment of a document to more than one cluster. The DIGBC algorithm is more efficient as compared to existing clustering algorithms such as single pass, K-NN and Hierarchical Agglomerative Clustering (HAC) algorithm.",,1-4244-0715-X,10.1109/ADCOM.2006.4289851,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4289851,,Clustering algorithms;HTML;Text analysis;Computer science;Educational institutions;Information retrieval;Data models;Data analysis;Encoding;Web sites,encoding;information retrieval;Internet,Web document clustering;document index graph;information retrieval;phrase-based encoding;phrase matching;phrase-based similarity;soft clustering,,5,,8,,13-Aug-07,,,IEEE,IEEE Conferences
A similarity-based soft clustering algorithm for documents,基於相似度的文檔軟聚類算法,King-Ip Lin; R. Kondadadi,"Dept. of Math. Sci., Memphis Univ., Memphis, TN, USA; NA",Proceedings Seventh International Conference on Database Systems for Advanced Applications. DASFAA 2001,7-Aug-02,2001,,,40,47,"Document clustering is an important tool for applications such as Web search engines. Clustering documents enables the user to have a good overall view of the information contained in the documents that he has. However, existing algorithms suffer from various aspects, hard clustering algorithms (where each document belongs to exactly one cluster) cannot detect the multiple themes of a document, while soft clustering algorithms (where each document can belong to multiple clusters) are usually inefficient. We propose SISC (similarity-based soft clustering), an efficient soft clustering algorithm based on a given similarity measure. SISC requires only a similarity measure for clustering and uses randomization to help make the clustering efficient. Comparison with existing hard clustering algorithms like K-means and its variants shows that SISC is both effective and efficient.",,0-7695-0996-7,10.1109/DASFAA.2001.916362,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916362,,Clustering algorithms;Search engines;Web sites;Keyword search;Web pages;Web search;Data mining;Animals;Parameter estimation,very large databases;data mining;document handling;pattern clustering,similarity-based soft clustering;document clustering;Web search engines;SISC;similarity measure;randomization;K-means clustering;data mining;very large databases,,19,,22,,7-Aug-02,,,IEEE,IEEE Conferences
CSIM: a document clustering algorithm based on swarm intelligence,CSIM：基於群體智能的文檔聚類算法,Wu Bin; Zheng Yi; Liu Shaohui; Shi Zhongzhi,"Key Lab. of Intelligent Inf. Process., Chinese Acad. of Sci., China; Key Lab. of Intelligent Inf. Process., Chinese Acad. of Sci., China; Key Lab. of Intelligent Inf. Process., Chinese Acad. of Sci., China; Key Lab. of Intelligent Inf. Process., Chinese Acad. of Sci., China",Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),7-Aug-02,2002,1,,477,482 vol.1,"This paper presents a document clustering algorithm based on swarm intelligence and k-means: CSIM. First, a document clustering algorithm based on swarm intelligence is employed. It is derived from a basic model interpreting ant colony organization of cemeteries. Swarm intelligence for flexibility, self-organization and robustness has been applied in a variety of areas. Taking advantage of these traits, good initial clusters are obtained in the first phase in CSIM. We then combine it with the classical k-means clustering method by using the clusters as initial centers. CSIM inherits the prominent properties of both swarm intelligence and k-means. It also offsets the weakness of those two techniques. Experimental results show the good performance of the hybrid document clustering algorithm.",,0-7803-7282-4,10.1109/CEC.2002.1006281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006281,,Clustering algorithms;Particle swarm optimization;Clustering methods;Insects;Animals;Laboratories;Information processing;Computers;Robustness;Web sites,pattern clustering;document handling;information resources;self-adjusting systems;information retrieval,document clustering algorithm;swarm intelligence;k-means;CSIM;ant colony organization;cemeteries;robustness;self-organization;flexibility;k-means clustering method,,8,,22,,7-Aug-02,,,IEEE,IEEE Conferences
A Novice Approach for Web Document Clustering Using FP Growth Based Fuzzy Particle Swarm Optimization,基於FP增長的模糊粒子群優化的Web文檔聚類新手方法。,R. V. Pamba; E. Sherly,"Sch. of Comput. Sci., M.G. Univ., Kottayam, India; VRCLC, IIITMK, Trivandrum, India",2016 3rd International Conference on Soft Computing & Machine Intelligence (ISCMI),5-Oct-17,2016,,,90,93,"The success of any Information Retrieval system relies upon extracting relevant pages of similar knowledge matching the requirements of the user. The traditional best of all statistical methodologies fails in conquering the issues of relevancy and redundancy of web pages retrieved. In this paper we propose a novel architecture, FP Growth based Fuzzy Particle swarm optimization which captures the dynamicity and fuzziness of web documents. With FPGrowth we attain a much lesser but frequent sets recurring repeatedly. Indirectly the FPGrowth reduce the redundancy of the search space. These reduced frequent sets are optimized efficiently with evolutionary nature inspired PSO algorithm. This scenario of divide and conquer strategy of FP Growth to reduce the list of transactions to frequent items and being optimised using FuzzyPSO is extended to web document clustering. The major contribution in this paper is the generation of number of clusters and frequent item sets(particles) achieved via FP Growth which in rest of all algorithms are user given and better optimized accuracy in retrieval using FuzzyPSO avoiding the limitation of local minima of FCM completely with the global and local search mechanism. The evaluation reveals an optimised results for the proposed hybrid approach.",,978-1-5090-3696-7,10.1109/ISCMI.2016.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057445,frequent pattern;swarm;fuzzy particle swarm optimisation,Clustering algorithms;Particle swarm optimization;Algorithm design and analysis;Optimization;Information retrieval;Data mining;Conferences,fuzzy set theory;information retrieval;Internet;particle swarm optimisation;pattern clustering;search problems;statistical analysis,web document clustering;frequent item sets;FP Growth;Fuzzy Particle swarm optimization;Information Retrieval system;web pages;statistical methodologies;FuzzyPSO,,1,,12,,5-Oct-17,,,IEEE,IEEE Conferences
Stop word detection in compressed textual images: An experiment on indic script documents,在壓縮的文本圖像中停止單詞檢測：印度腳本文檔的實驗,U. Garain; A. K. Das,"CVPR Unit, ISI, Kolkata, India; CST Dept., BESU, Shibpur, India",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"Stop word detection is attempted in this work in the context of retrieval of document images in the compressed domain. Algorithms are presented to identify text lines and words and to cluster similar words to count word occurrence frequencies. A list of words with their occurrence frequencies is generated from a corpus of textual images. As stop words in any language show high occurrence frequencies, such words occupy the upper positions in the sorted word list. Experiments have been carried out on two major indic scripts (Devanagari (Hindi) and Bangla). Test results using 150 document images consisting of about 12 K words in each script show the promising potential of the proposed approach.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761529,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761529,,Image coding;Prototypes;Frequency;Streaming media;Dictionaries;Image retrieval;Labeling;Scattering;Intersymbol interference;Clustering algorithms,data compression;document image processing;image retrieval;image texture;natural languages;pattern clustering;text analysis,stop word detection;compressed textual image;indic script document;document image retrieval;word cluster;word occurrence frequency;text word identification,,2,,12,,23-Jan-09,,,IEEE,IEEE Conferences
An Exploratory Study of Enhancing Text Clustering with Auto-Generated Semantic Tags,利用自動生成的語義標記增強文本聚類的探索性研究,X. Tang; J. Dang,"Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA; Corp. Res. & Technol., Siemens Corp., Princeton, NJ, USA","2012 Eighth International Conference on Semantics, Knowledge and Grids",24-Dec-12,2012,,,104,111,"With the exponentially growing volume of digital documents and internet content, it becomes very challenging to locate right information when desired. We heavily rely on search engines but most existing search tools are key-word based and they often return search results with low precision and recall. The emerging semantic tagging technology provides an automatic way to generate semantic tags from text. It has drawn more and more interest from text mining research communities. It is critical to study how to utilize semantic tags to improve text mining including clustering, which helps users to enhance their experience of searching and browsing documents. Unfortunately, most previous works on text clustering merely based on content information. A few recent researches take user-generated tags into account, however user generated tags are often noisy, inconsistent, redundant and lack of semantic information and hierarchical structure. In this work, we propose a Semantic Text Mining (STeM) framework to generate semantic tags for given documents and then utilize the semantic tags to improve text clustering. Different from the previous works, we represent a document by a combination of domains and high quality noun phrases. We investigate the performance of our methods in two different datasets and the results are evaluated by normalized mutual information. Experiment results demonstrated that our proposed method substantially outperformed the traditional Term Frequency-Inverse Document Frequency (TF-IDF) term vector based clustering. We find that incorporating semantic information into document representation is critical to improve the performance of text clustering.",,978-1-4673-2561-5,10.1109/SKG.2012.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391817,semantic tags;clustering;document;SteM,Semantics;Vectors;Tagging;Ontologies;Clustering algorithms;Frequency domain analysis;Knowledge based systems,data mining;information retrieval;pattern clustering;semantic Web;text analysis,text clustering enhancement;autogenerated semantic tag;digital documents;Internet content;information location;search engine;keyword based search tool;semantic tagging technology;document searching;document browsing;content information;user-generated tags;semantic information;hierarchical structure;semantic text mining;STeM;document representation;high quality noun phrase;normalized mutual information;term frequency-inverse document frequency term vector;TF-IDF term vector based clustering,,1,,17,,24-Dec-12,,,IEEE,IEEE Conferences
VisIRR: Visual analytics for information retrieval and recommendation with large-scale document data,VisIRR：可視化分析，用於通過大型文檔數據進行信息檢索和推薦,J. Choo; C. Lee; H. Kim; H. Lee; Z. Liu; R. Kannan; C. D. Stolper; J. Stasko; B. L. Drake; H. Park,Georgia Institute of Technology; Google Inc.; Georgia Institute of Technology; University of Maryland; Adobe Research; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Tech Research Institute; Georgia Institute of Technology,2014 IEEE Conference on Visual Analytics Science and Technology (VAST),16-Feb-15,2014,,,243,244,"We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.",,978-1-4799-6227-3,10.1109/VAST.2014.7042511,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042511,Recommendation;document analysis;dimension reduction;clustering;information retrieval;scatter plot,Data visualization;Alzheimer's disease;Information retrieval;Visual analytics;Support vector machines,data visualisation;document handling;interactive systems;query processing;recommender systems,visual analytics;large-scale document data;VisIRR;interactive visual information retrieval and recommendation system;query;retrieved documents visualization;topic summary;interactive personalized preference feedback;recommended documents,,5,,4,,16-Feb-15,,,IEEE,IEEE Conferences
Image Analytics for Legal Document Review : A Transfer Learning Approach,用於法律文件審閱的圖像分析：一種轉移學習方法,N. Huber-fliflet; F. Wei; H. Zhao; H. Qin; S. Ye; A. Tsang,"Data & Technology Ankura,Washington DC,USA; Data & Technology Ankura,Washington DC,USA; Data & Technology Ankura,Washington DC,USA; Data & Technology Ankura,Washington DC,USA; Data & Technology Ankura,Washington DC,USA; Data & Technology Ankura,New York, NY,USA",2019 IEEE International Conference on Big Data (Big Data),24-Feb-20,2019,,,4325,4328,"Though technology assisted review in electronic discovery has been focusing on text data, the need of advanced analytics to facilitate reviewing multimedia content is on the rise. In this paper, we present several applications of deep learning in computer vision to Technology Assisted Review of image data in legal industry. These applications include image classification, image clustering, and object detection. We use transfer learning techniques to leverage established pretrained models for feature extraction and fine tuning. These applications are first of their kind in the legal industry for image document review. We demonstrate effectiveness of these applications with solving real world business challenges.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006551,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006551,predictive coding;legal document review;deep learning;image classification;image clustering;object detection,Feature extraction;Training;Law;Image classification;Machine learning;Object detection,computer vision;data handling;document image processing;feature extraction;image classification;learning (artificial intelligence);object detection;text analysis,multimedia content;deep learning;image data;legal industry;image classification;image clustering;transfer learning techniques;image document review;image analytics;legal document review;transfer learning;electronic discovery;text data,,,,8,,24-Feb-20,,,IEEE,IEEE Conferences
Text Clustering Algorithm Based on the Graph Structures of Semantic Word Co-occurrence,基於語義詞共現圖結構的文本聚類算法,C. Jin; Q. Bai,"Fac. of Comput. & Software Eng., Huaiyin Inst. of Technol., Huaian, China; Fac. of Autom., Huaiyin Inst. of Technol., Huaian, China",2016 International Conference on Information System and Artificial Intelligence (ISAI),16-Jan-17,2016,,,497,502,"Text theme is the key of text clustering, while the co-occurrence words can be very stronger to express text theme in document. This paper proposes a text clustering algorithm based on the text semantic representation and the graph structure of word co-occurrence on the basis of in-depth studying text theme mining and word co-occurrence. First, the algorithm constructs the text graph-structure according to the co-occurrence of feature words. In other words, it uses the graph structure to represent all texts. Then, it adopts the maximum common sub-graph between two texts to calculate their similarity and combines with K-means clustering algorithm to realize the document clustering. The compared experimental results with hierarchical clustering algorithm show the K-means clustering algorithm based on the graph structures of word co-occurrence greatly reduce the high dimension of text vector and the algorithm complexity, significantly improves the efficiency and accuracy of text clustering, and it can also produce the clustering effect of good quality.",,978-1-5090-1585-6,10.1109/ISAI.2016.0112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816765,co-occurrence unit;graph structure;maximum common sub-graph;text clustering,Semantics;Clustering algorithms;Feature extraction;Algorithm design and analysis;Vocabulary;Software algorithms;Complexity theory,pattern clustering;text analysis;word processing,text clustering algorithm;text graph structures;semantic word cooccurrence;text theme;cooccurrence words;text semantic representation;theme mining;K-means clustering algorithm;document clustering;hierarchical clustering algorithm;text vector;algorithm complexity,,1,,13,,16-Jan-17,,,IEEE,IEEE Conferences
Vector based privacy-preserving document similarity with LSA,與LSA基於向量的隱私保護文檔相似度,X. Yu; X. Chen; J. Shi,"Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),21-Dec-17,2017,,,1383,1387,"Document similarity is the foundation of many intelligent data processing systems, such as information retrieval, text classification and clustering. However, traditional document similarity algorithms are challenged by the privacy-preserving problem. Recently, privacy-preserving document similarity approaches are provided to solve this problem and there are two kinds of approaches which are vector based protocols and set based protocols. Existing vector based protocols mainly use vector space model for document similarity computation. But vector space model has deficiencies to compute document similarity effectively and efficiently. In this paper, we focus on vector based document similarity and present a novel protocol with latent semantic analysis. Experimental evaluation shows that our protocol has better accuracy and performance than existing protocols.",2472-8489,978-1-5090-3822-0,10.1109/ICCSN.2017.8230336,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8230336,privacy-preserving;document similarity;latent semantic analysis,Protocols;Computational modeling;Encryption;Classification algorithms;Semantics,data privacy;document handling;information retrieval;pattern clustering;vectors,traditional document similarity algorithms;intelligent data processing systems;vector based privacy-preserving document similarity;existing protocols;vector space model;vector based protocols,,1,,14,,21-Dec-17,,,IEEE,IEEE Conferences
Hindi Multi-document Word Cloud based Summarization through Unsupervised Learning,通過無監督學習的基於印地語多文檔詞云的摘要,P. B. Bafna; J. R. Saini,"Symbiosis International Deemed University,Symbiosis Institute of Computer Studies and Research,Pune; Symbiosis International (Deemed) University,Symbiosis Institute of Computer Studies and Research,Maharashtra,India",2019 9th International Conference on Emerging Trends in Engineering and Technology - Signal and Information Processing (ICETET-SIP-19),14-May-20,2019,,,1,7,"Managing documents is a critical and significant task and supports many applications ranging from information retrieval to clustering search engine results. The multilinguistic facility provided by websites makes Hindi as a major language in the digital domain of information technology today. This work focuses on document management and summarization of Hindi corpus. The objective is to manage the documents and summarize Hindi corpus by applying extracting tokens and document clustering. The work is better in terms of scalability and supports consistent quality of cluster for incremental data set. Most of the past and contemporary research works have targeted English corpus document management. Hindi corpus has been mostly exploited by the researchers for exploring stemming, single- document summarization and classifier design on Hindi corpus. Implementing unsupervised learning on Hindi corpus for summarization of multiple documents through Word Cloud is still an untouched area. Technically speaking, the current work is an application of TF-IDF, cosine-based document similarity measures and cluster dendrograms, in addition to various other Natural Language Processing (NLP) activities. Entropy and precision are used to evaluate the experiments carried on different live and available/tested datasets and results.",2157-0485,978-1-7281-3506-9,10.1109/ICETET-SIP-1946815.2019.9092259,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092259,Classification;Clustering;Document Management;Hindi;Summarization;Word,Tag clouds;Entropy;Natural language processing;Market research;Information processing;Unsupervised learning,cloud computing;entropy;information retrieval;natural language processing;pattern classification;pattern clustering;search engines;text analysis;unsupervised learning;Web sites,cosine-based document similarity measures;Hindi multidocument Word Cloud;unsupervised learning;Hindi corpus;document clustering;English corpus document management;single-document summarization;multiple documents;classifier design;TF-IDF;cluster dendrograms;natural language processing;entropy;precision;document management;token extraction;information retrieval;search engine;multilinguistic facility;Web site,,2,,15,,14-May-20,,,IEEE,IEEE Conferences
Automated Graph Regularized Projective Nonnegative Matrix Factorization for Document Clustering,用於文檔聚類的自動圖正則化正投影非負矩陣分解,X. Pei; T. Wu; C. Chen,"School of Software, HuaZhong University of Science and Technology, Wuhan, Hubei, China; School of Software, HuaZhong University of Science and Technology, Wuhan, Hubei, China; School of Software, HuaZhong University of Science and Technology, Wuhan, Hubei, China",IEEE Transactions on Cybernetics,20-May-17,2014,44,10,1821,1831,"In this paper, a novel projective nonnegative matrix factorization (PNMF) method for enhancing the clustering performance is presented, called automated graph regularized projective nonnegative matrix factorization (AGPNMF). The idea of AGPNMF is to extend the original PNMF by incorporating the automated graph regularized constraint into the PNMF decomposition. The key advantage of this approach is that AGPNMF simultaneously finds graph weights matrix and dimensionality reduction of data. AGPNMF seeks to extract the data representation space that preserves the local geometry structure. This character makes AGPNMF more intuitive and more powerful than the original method for clustering tasks. The kernel trick is used to extend AGPNMF model related to the input space by some nonlinear map. The proposed method has been applied to the problem of document clustering using the well-known Reuters-21578, TDT2, and SECTOR data sets. Our experimental evaluations show that the proposed method enhances the performance of PNMF for document clustering.",2168-2275,,10.1109/TCYB.2013.2296117,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6712130,Clustering;projective nonnegative matrix factorization;Clustering;projective nonnegative matrix factorization,Matrix decomposition;Kernel;Clustering algorithms;Optimization;Linear programming;Cybernetics;Geometry,data reduction;data structures;document handling;graph theory;matrix decomposition;pattern clustering,SECTOR data sets;TDT2;Reuters-21578;local geometry structure;data representation space extraction;data dimensionality reduction;graph weights matrix;PNMF decomposition;AGPNMF;automated graph regularized projective nonnegative matrix factorization;document clustering,,21,,41,,14-Jan-14,,,IEEE,IEEE Journals
Native XML Document Fragmentation Model,本機XML文檔碎片模型,L. Birhanu; S. Atnafu; F. Getahun,"Dept. of Comput. Sci., AAU, Addis Ababa, Ethiopia; Dept. of Comput. Sci., AAU, Addis Ababa, Ethiopia; Dept. of Comput. Sci., AAU, Addis Ababa, Ethiopia",2010 Sixth International Conference on Signal-Image Technology and Internet Based Systems,17-Feb-11,2010,,,233,240,"As XML document is distributed across the web, it can be considered like a distributed repository of XML documents and is subjected to distribution design. However, there is no adequate works on XML document distribution design. To address the shortcomings in XML document fragmentation design, in this work, we have focused on the vertical fragmentation design of the XML documents. Two fragmentation models have been proposed: query based fragmentation and structure-and size-based fragmentation. For the query based fragmentation model, vertical fragmentation techniques are proposed using the bond energy algorithm and graphical based algorithm. We have implemented both algorithms and evaluated their performance. The performance of our fragmentation algorithms are compared with centralized and fully replicated XML documents, where better results are obtained. The Structure- and size-based Fragmentation model and its implementation algorithms are also evaluated and encouraging results are achieved.",,978-1-4244-9527-6,10.1109/SITIS.2010.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5714557,XML Document;XML Document Distribution;Vertical Fragmentation of XML Document;XML Document Grouping Heuristic;XML Document Splitting Heuristic,XML;Clustering algorithms;Partitioning algorithms;Object oriented modeling;Symmetric matrices;Query processing,document handling;replicated databases;XML,native XML document fragmentation model;distributed repository;XML document distribution design;structure based fragmentation;size based fragmentation;vertical fragmentation techniques;bond energy algorithm;graphical based algorithm;replicated XML documents,,1,,23,,17-Feb-11,,,IEEE,IEEE Conferences
Document Classification Based on the Topic Evaluation and Its Usage in Data Compression,基於主題評估的文檔分類及其在數據壓縮中的應用,J. Martinovic; J. Dvorsky,"VSB - Tech. Univ. of Ostrava, Ostrava; VSB - Tech. Univ. of Ostrava, Ostrava",2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops,14-Jan-08,2007,,,204,207,Several actions are usually performed when document is appended to textual database in information retrieval system. The most frequent actions are compression of the document and cluster analysis of the textual database to improve quality of answers to users' queries. The information retrieved from the clustering can be very helpful in compression. Word-based compression using information about cluster hierarchy is presented in this paper. Some experimental results are provided at the end of the paper.,,0-7695-3028-1,10.1109/WI-IATW.2007.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427572,text compressiondata clustering,Data compression;Search engines;Information retrieval;Web search;Intelligent agent;Spatial databases;Clustering algorithms;Clustering methods;Extraterrestrial measurements;Conferences,classification;data compression;document handling;information retrieval systems,document classification;topic evaluation;data compression;textual database;information retrieval system;cluster analysis;word-based compression,,1,,15,,14-Jan-08,,,IEEE,IEEE Conferences
Using Swarm intelligence for XML Clustering,使用Swarm Intelligence進行XML集群,Tong Wang; Daxin Liu; Xuanzuo Lin; Xiaohua Sun,"Harbin Engineering University, Harbin, 150001. E-mail: wangtong@hrbeu.edu.cn; Harbin Eng. Univ.; NA; NA",2006 6th World Congress on Intelligent Control and Automation,23-Oct-06,2006,2,,6000,6004,"Data mining in large-scale XML documents set can facilitate to query and manage XML documents. This paper proposes a novel XML document clustering method based on swarm intelligence. Firstly, the approach extracts path sequences from documents, and then the documents are transformed to vectors in a high-dimensional Euclidean space. Finally, CSX clustering method is applied to with high performance. The advantage of the approach is that swarm intelligence can help skip out of the local optima of the search space. Data sets are obtained from DBLP, and the experiment results show that the performance of the proposed techniques outperformed the standard C-means method in clustering compact and accuracy",,1-4244-0332-4,10.1109/WCICA.2006.1714231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1714231,clustering;swarm intelligence;data mining;XML,Particle swarm optimization;XML;Electronic mail;Clustering methods;Data mining;Books;Sun;Agricultural engineering;Agriculture;Large-scale systems,data mining;optimisation;pattern clustering;search problems;XML,swarm intelligence;XML clustering;data mining;large-scale XML documents;path sequences extraction;high-dimensional Euclidean space;CSX clustering method;DBLP;c-means method,,,,,,23-Oct-06,,,IEEE,IEEE Conferences
An intelligent similarity measure for effective text document clustering,用於有效文本文檔聚類的智能相似性度量,M. L. Aishwarya; K. Selvi,"Department of Computer Science, RMK Engineering College, Chennai, India; Department of Computer Science, RMK Engineering College, Chennai, India",2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16),31-Oct-16,2016,,,1,5,"In many applications, continuous use of multiple queries has become the focal point of many researches. Various evaluations on document clustering methods reveal the need for effective methodology to capture the actual requirement of the user from the web browsers. An intelligent similarity measure with the concept of Neural Network algorithm has been proposed. Experiments show that application of Echo State Neural Network and Radial Basis Function to the training data set gives better clustering of text documents based on the stored weights in order to avoiding retrieval of irrelevant documents.",,978-1-4673-8437-7,10.1109/ICCTIDE.2016.7725342,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725342,Document Clustering;Artificial Neural Network;Similarity Measure;Text Mining;Machine Learning;Web Document Analysis,Training;Clustering algorithms;Neural networks;Indexes;Matrix converters;Search engines;Semantics,pattern clustering;query processing;radial basis function networks;text analysis,intelligent similarity measure;text document clustering;multiple queries;Web browsers;echo state neural network;radial basis function,,1,,28,,31-Oct-16,,,IEEE,IEEE Conferences
An Efficient Semantic based Clustering Algorithm for Textual Documents,一種基於語義的高效文本文檔聚類算法,R. Karthika; L. J. Deborah,"Department of Computer Science Engineering, University College of Engineering Tindivanam, Tindivanam, Tamilnadu, India; Department of Computer Science Engineering, University College of Engineering Tindivanam, Tindivanam, Tamilnadu, India",2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET),2-Sep-19,2018,,,1,6,"Documents that are classified into different categories gets flooded in the internet every day. These documents have many links or associations with the other documents in the web. The terms in the document are open to multiple interpretations which are vague and unclear. Hence there is a need to find the semantic understanding of the terms. One of the major application in identifying and applying such semantic measure lies in clustering the related textual documents. However, the traditional clustering algorithms may exhibit reduced performances due to the existence of irrelevant terms in the raw documents. Hence, the proposed algorithm in this paper exploits the use of a feature selection algorithm in order to increase the performance of the clustering algorithm. In this paper, a feature selection algorithm with booster technique is used. Moreover, clustering algorithm based on a fuzzy linguistic variable measure that uses separation and dominance value is used in this paper for precise clustering. Experimental analysis shows that the three performance measures that evaluates the clustering algorithm increases, in comparison to the other existing algorithms.",,978-1-5386-0576-9,10.1109/ICCSDET.2018.8821148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821148,fuzzy clustering;fuzzy linguistic measure;feature selection algorithm;booster,Clustering algorithms;Semantics;Feature extraction;Linguistics;Partitioning algorithms;Indexes;Search engines,feature selection;fuzzy set theory;Internet;pattern classification;pattern clustering;text analysis,feature selection algorithm;textual documents;clustering algorithm;document classification;Internet documents;Web documents;booster technique;fuzzy linguistic variable measure,,,,20,,2-Sep-19,,,IEEE,IEEE Conferences
An Efficient Algorithm for Clustering Search Engine Results,一種高效的搜索引擎結果聚類算法,H. Zhang; B. Pang; K. Xie; H. Wu,"National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. hzhang@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. pangbin@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. xieke@nlsde.buaa.edu.cn; National Laboratory of Software Development Environment, Beihang University, Beijing 100083, China. wuhui@nlsde.buaa.edu.cn",2006 International Conference on Computational Intelligence and Security,29-Jan-07,2006,2,,1429,1434,"With the increasing number of Web documents in the Internet, the most popular keyword-matching-based search engines, such as Google, often return a long list of search results ranked based on their relevance and importance to the query. To cluster the search engine results can help users find the results in several clustered collections, so it is easy to locate the valuable search results that the users really needed. In this paper, we propose a new key-feature clustering (KFC) algorithm which firstly extracts the significant keywords from the results as key features and cluster them, then clusters the documents based on these clustered key features. At last, the paper presents and analyzes the results from experiments we conducted to test and validate the algorithm",,1-4244-0604-8,10.1109/ICCIAS.2006.295296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076202,,Clustering algorithms;Search engines;Frequency;Algorithm design and analysis;Internet;Flowcharts;Software algorithms;Programming;Testing;Web pages,document handling;Internet;pattern clustering;relevance feedback;search engines,search engine result clustering;Web documents;Internet;keyword matching;Google;key-feature clustering,,3,,11,,29-Jan-07,,,IEEE,IEEE Conferences
Partition document clustering using ontology approach,使用本體論方法的分區文檔聚類,S. C. Punitha; R. Jayasree; M. Punithavalli,"HOD, Department of Computer Science, P.S.G.R. KrishnammalCollege for Women, Coimbatore, India; MPhil Scholar, P.S.G.R. Krishnammal College for Women, Coimbatore, India; Director, MCA, Sri Ramakrishna College of Engineering and Technology, Coimbatore, India",2013 International Conference on Computer Communication and Informatics,21-Feb-13,2013,,,1,5,"Data mining is the extraction of hidden predictive information from large databases and it is a powerful new technology with great potential to help companies focus on the most important information in their data warehouses. Data mining tools predict future trends and behaviors, allowing businesses to make proactive, knowledge-driven decisions. In data mining there are two activities such as Classification and clustering [5]. Text clustering typically involves clustering in a high dimensional space, which appears difficult with regard to virtually all practical settings. The creation and deployment of knowledge repositories for managing, sharing, and reusing tacit knowledge within an organization has emerged as a prevalent approach in current knowledge management practices.",,978-1-4673-2907-1,10.1109/ICCCI.2013.6466246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466246,,Clustering algorithms;Algorithm design and analysis;Ontologies;Data mining;Partitioning algorithms;Computers;Educational institutions,data mining;data warehouses;knowledge management;ontologies (artificial intelligence);pattern clustering;text analysis,partition document clustering;ontology approach;data mining;hidden predictive information;large databases;data warehouses;text clustering;knowledge repositories;tacit knowledge;knowledge management,,2,,17,,21-Feb-13,,,IEEE,IEEE Conferences
Wikipedia and How to Use It for Semantic Document Representation,維基百科及其如何用於語義文檔表示,I. Witten,NA,2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,1-Nov-10,2010,2,,1,1,"Summary form only given. Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. This talk focuses on the process of ""wikification""; that is, automatically and judiciously augmenting a plain-text document with pertinent hyperlinks to Wikipedia articles as though the document were itself a Wikipedia article. I first describe how Wikipedia can be used to determine semantic relatedness between concepts. Then I explain how to wikify documents by exploiting Wikipedia's internal hyperlinks for relational information and their anchor texts as lexical information. Data mining techniques are used throughout to optimize the models involved. I will discuss applications to knowledge-based information retrieval, topic indexing, document tagging, and document clustering. Some of these perform at human levels. For example, on CiteULike data, automatically extracted tags are competitive with tag sets assigned by the best human taggers, according to a measure of consistency with other human taggers. All this work uses English, but involves no syntactic parsing, so the techniques are language independent.",,978-1-4244-8482-9,10.1109/WI-IAT.2010.304,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615840,,,data mining;document handling;information retrieval;knowledge based systems;pattern clustering;search engines,semantic document representation;Wikipedia internal hyperlinks;relational information;lexical information;data mining techniques;knowledge-based information retrieval;topic indexing;document tagging;document clustering;CiteULike data,,,,,,1-Nov-10,,,IEEE,IEEE Conferences
A Multiple-Label Guided Clustering Algorithm for Historical Document Dating and Localization,用於歷史文檔約會和本地化的多標籤引導聚類算法,S. He; P. Samara; J. Burgers; L. Schomaker,"Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, Groningen, The Netherlands; Huygens Institute for the History of the Netherlands, The Hague, The Netherlands; Huygens Institute for the History of the Netherlands, The Hague, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, Groningen, The Netherlands",IEEE Transactions on Image Processing,16-Sep-16,2016,25,11,5252,5265,"It is of essential importance for historians to know the date and place of origin of the documents they study. It would be a huge advancement for historical scholars if it would be possible to automatically estimate the geographical and temporal provenance of a handwritten document by inferring them from the handwriting style of such a document. We propose a multiple-label guided clustering algorithm to discover the correlations between the concrete low-level visual elements in historical documents and abstract labels, such as date and location. First, a novel descriptor, called histogram of orientations of handwritten strokes, is proposed to extract and describe the visual elements, which is built on a scale-invariant polar-feature space. In addition, the multi-label self-organizing map (MLSOM) is proposed to discover the correlations between the low-level visual elements and their labels in a single framework. Our proposed MLSOM can be used to predict the labels directly. Moreover, the MLSOM can also be considered as a pre-structured clustering method to build a codebook, which contains more discriminative information on date and geography. The experimental results on the medieval paleographic scale data set demonstrate that our method achieves state-of-the-art results.",1941-0042,,10.1109/TIP.2016.2602078,Dutch Organization for Scientific Research NWO; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551181,Historical document dating;historical document localization;histogram of orientations of handwritten stroke;multi-label self-organizing map,Visualization;Urban areas;Histograms;Water;Writing;Correlation;Self-organizing feature maps,correlation methods;document image processing;history;text analysis,multiple-label guided clustering algorithm;historical document dating;historical scholars;geographical provenance;temporal provenance;handwritten document;handwriting style;low-level visual elements;abstract labels;handwritten strokes;scale-invariant polar-feature space;multilabel self-organizing map;MLSOM;prestructured clustering method;medieval paleographic scale data set,,9,,46,,24-Aug-16,,,IEEE,IEEE Journals
Leveraging Global and Local Topic Popularities for LDA-Based Document Clustering,利用基於LDA的文檔聚類的全球和本地主題流行度,P. Yang; Y. Yao; H. Zhou,"Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China",IEEE Access,10-Feb-20,2020,8,,24734,24745,"Document clustering is of high importance for many natural language technologies. A wide range of computational traditional topic models, such as LDA (Latent Dirichlet Allocation) and its variants, have made great progress. However, traditional LDA-based clustering algorithms might not give good results due to such probabilistic models require prior distributions which are always difficult to define. In this paper, we propose a probabilistic model named tpLDA, which incorporates different levels of topic popularity information to determine the prior LDA distribution, discover the latent topics and achieve better clustering. Specifically, global topic popularity is introduced to reduce the potential distraction in local cluster popularity and the local cluster popularity draws more attention on certain parts of the global topic popularity. The two popularities contribute complementary information and their integration can dynamically adjust statistical parameters of the model. Experimental evaluations on real data sets show that, compared with state-of-the-art approaches, our proposed framework dramatically improves the accuracy of documents clustering.",2169-3536,,10.1109/ACCESS.2020.2969525,Consulting Project of Chinese Academy of Engineering; National Natural Science Foundation of China; Collaborative Innovation Center of Novel Software Technology and Industrialization; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970318,Document clustering;latent Dirichlet allocation;machine learning;topic modeling,Clustering algorithms;Probabilistic logic;Semantics;Resource management;Data mining;Probability distribution;Computational modeling,natural language processing;pattern clustering;statistical analysis;statistical distributions;text analysis;unsupervised learning,LDA-based clustering algorithms;local topic popularity;real data sets;statistical parameters;complementary information;potential distraction reduction;tpLDA;prior LDA distribution;topic popularity information;topic probabilistic model;Latent Dirichlet Allocation;computational traditional topic models;natural language technologies;LDA-based document clustering;local cluster popularity;global topic popularity;latent topics,,1,,42,CCBY,28-Jan-20,,,IEEE,IEEE Journals
Mining similar radiology reports using BoW and Fuzzy C-means clustering,使用BoW和Fuzzy C均值聚類挖掘相似的放射學報告,S. Turkeli; B. S. Akkoca Gazioglu; K. K. Kurt; H. T. Atay; Y. Gorur,"Control and Automation Department, Istanbul Technical University, Istanbul, Turkey; Computer Engineering Department, Istanbul Technical University, Istanbul, Turkey; Biomedical Engineering Graduate Program, Bogazici University, Istanbul, Turkey; Biomedical Engineering Graduate Program, Istanbul Technical University, Istanbul, Turkey; Electronics and Communication Engineering, Istanbul Technical University, Istanbul, Turkey",2017 International Artificial Intelligence and Data Processing Symposium (IDAP),2-Nov-17,2017,,,1,5,"Finding similar diagnoses for the same region are vital for patients. In this paper, we aim to find the similarity radiology reports based on bag-of-words (BoW) and Fuzzy C-Means Clustering methods. A double-layer structure is applied. Firstly, extracting features from data BoW method is applied and then Fuzzy C-Means algorithm is performed to cluster the blocks into the similar cluster and the non-similar cluster. 457 radiology reports were examined which were collected from a research and education hospital in Istanbul. Data were tested according to the 23 regions and 137 diagnosis. By the opinion of the radiologist a vocabulary consists of these regions and diagnosis were created. Experimental results on data sets have shown that for the standard documents BoW and Fuzzy C-Means Clustering can be used to find similarity.",,978-1-5386-1880-6,10.1109/IDAP.2017.8090213,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090213,Bag of words;Fuzzy C Means clustering;Radiology reports;Text mining,Clustering algorithms;Feature extraction;Radiology;Text mining;Medical services;Signal processing algorithms,data analysis;data mining;document handling;feature extraction;fuzzy set theory;medical computing;patient diagnosis;pattern clustering;radiology,data BoW method;education hospital;patient diagnosis;block clustering;similarity radiology report mining;Fuzzy C-Means Clustering method;Fuzzy C-Means algorithm;Istanbul;vocabulary;feature extraction;double-layer structure;bag-of-words;similar diagnoses,,,,16,,2-Nov-17,,,IEEE,IEEE Conferences
Detecting fraudulent words: Using PFCM clustering,檢測欺詐性單詞：使用PFCM聚類,R. Singhal; N. Deepika,"Computer Science &Engineering, New Horizon College of Engineering, Bangalore, India; Computer Science &Engineering, New Horizon College of Engineering, Bangalore, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",9-Jan-17,2016,,,2015,2017,"Word Classification involves grouping the words in a document into clusters. Clustering data sets is a much researched problem. In 2005, Nikhil R. Pal, Kuhu Pal, James M. Keller, and James C. Bezdek proposed A Possibilistic Fuzzy cMeans (PFCM) Clustering Algorithm. The PFCM model gives the membership values and the typicality values, along with the cluster centers. It is a hybrid algorithm of possibilistic c-means (PCM) and fuzzy c-means (FCM) algorithm. In this paper, we propose that PFCM can be used well to classify the words in text files also.For using PFCM on the words, the model proposed also gives a strategy to convert textual words to data points. The proposed model is highly user friendly and it takes into account how the user wants the clustering to be done. This approach can also be used for detecting any frauds in the text messages. Our examples show that the clustering of text is quite efficient usingAbstract-Word Classification involves grouping the words a document into clusters. Clustering data sets is a much researched problem. In 2005, Nikhil R. Pal, Kuhu Pal, James M. Keller, and James C. Bezdek proposed A Possibilistic Fuzzy Means (PFCM) Clustering Algorithm. The PFCM model gives the membership values and the typicality values, along with the cluster centers. It is a hybrid algorithm of possibilistic c-means (PCM) and fuzzy c-means (FCM) algorithm. In this paper, we propose that PFCM can be used well to classify the words in text files also.For using PFCM on the words, the model proposed also gives a strategy to convert textual words to data points. The proposed model is highly user friendly and it takes into account how the user wants the clustering to be done. This approach can also be used for detecting any frauds in the text messages. Our examples show that the clustering of text is quite efficient using PFCM, and it can be used with WhatsApp emailed chats also.",,978-1-5090-0774-5,10.1109/RTEICT.2016.7808192,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808192,possibilistic clustering;fuzzy clustering;criticality factors;membership matrix;typicality matrix,Phase change materials;Clustering algorithms;Mathematical model;Market research;Classification algorithms;Conferences;Communications technology,fraud;fuzzy set theory;natural language processing;pattern classification;pattern clustering;possibility theory;text analysis,fraudulent word detection;word classification;document word grouping;possibilistic fuzzy c-means clustering algorithm;PFCM clustering algorithm;membership values;typicality values;PCM;text files;textual words;data points;text message;WhatsApp emailed chat,,,,5,,9-Jan-17,,,IEEE,IEEE Conferences
Layout Analysis on Challenging Historical Arabic Manuscripts using Siamese Network,暹羅網絡對具有挑戰性的阿拉伯歷史手稿的佈局分析,R. Alaasam; B. Kurar; J. El-Sana,Ben-Gurion University of the Negev; Ben-Gurion University of the Negev; Ben-Gurion University of the Negev,2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,738,742,"This paper presents layout analysis for historical Arabic documents using siamese network. Given pages from different documents, we divide them into patches of similar sizes. We train a siamese network model that takes as an input a pair of patches and gives as an output a distance that corresponds to the similarity between the two patches. We used the trained model to calculate a distance matrix which in turn is used to cluster the patches of a page as either main text, side text or a background patch. We evaluate our method on challenging historical Arabic manuscripts dataset and report the F-measure. We show the effectiveness of our method by comparing with other works that use deep learning approaches, and show that we have state of art results.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978059,"Layout Analysis, Siamese Network, Historical Arabic Documents, Clustering",Feature extraction;Layout;Clustering algorithms;Image segmentation;Classification algorithms;Training;Computer science,document image processing;history;learning (artificial intelligence);natural language processing;neural nets,layout analysis;historical Arabic documents;Siamese network model;distance matrix;historical Arabic manuscripts dataset,,1,,17,,3-Feb-20,,,IEEE,IEEE Conferences
"Mask-edge connectivity: Theory, computation, and application to historical document analysis",遮罩邊緣連接：理論，計算及其在歷史文檔分析中的應用,M. H. F. Wilkinson; J. Oosterbroek,"Johann Bernoulli Institute, University of Groningen, P.O. Box 407, 9700 AK Groningen, The Netherlands; Johann Bernoulli Institute, University of Groningen, P.O. Box 407, 9700 AK Groningen, The Netherlands",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,1334,1337,"In this paper a new connectivity model is introduced which allows combined clustering and partitioning of structures without distortion, in contrast to mask connectivity. An algorithm to compute morphological attribute filters based on Max-Trees for this new form of connectivity is presented. It is shown that the new form of connectivity is effective in clustering diacritics together with the appropriate letters in historical documents, whilst separating letters from different lines in a single Max-Tree algorithm.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460386,,Image edge detection;Clustering algorithms;Signal processing algorithms;Morphology;Image segmentation;Joining processes;Pattern recognition,document handling;filtering theory;history;mathematical morphology;pattern clustering;trees (mathematics),mask-edge connectivity;historical document analysis;connectivity model;structure partitioning;structure clustering;morphological attribute filters;diacritics clustering;Max-Tree algorithm,,1,,10,,14-Feb-13,,,IEEE,IEEE Conferences
"High Quality, Efficient Hierarchical Document Clustering Using Closed Interesting Itemsets",使用封閉有趣項目集的高質量，高效的分層文檔聚類,H. H. Malik; J. R. Kender,Columbia University; Columbia University,Sixth International Conference on Data Mining (ICDM'06),8-Jan-07,2006,,,991,996,"High dimensionality remains a significant challenge for document clustering. Recent approaches used frequent itemsets and closed frequent itemsets to reduce dimensionality, and to improve the efficiency of hierarchical document clustering. In this paper, we introduce the notion of ""closed interesting"" itemsets (i.e. closed itemsets with high interestingness). We provide heuristics such as ""super item"" to efficiently mine these itemsets and show that they provide significant dimensionality reduction over closed frequent itemsets. Using ""closed interesting"" itemsets, we propose a new, sub-linearly scalable, hierarchical document clustering method that outperforms state of the art agglomerative, partitioning and frequent-itemset based methods both in terms of clustering quality and runtime performance, without requiring dataset specific parameter tuning. We evaluate twenty interestingness measures and show that when used to generate ""closed interesting"" itemsets, and to select parent nodes, mutual information, added value, Yule's Q and Chi- Square offer best clustering performance.",2374-8486,978-0-7695-2701-7,10.1109/ICDM.2006.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053141,,Itemsets;Data mining;Association rules;Clustering algorithms;Scalability;Frequency;Merging;Computer science;Clustering methods;Runtime,data mining;document handling;pattern clustering,hierarchical document clustering;closed interesting itemsets;closed frequent itemsets;dimensionality reduction;itemset mining;clustering quality;runtime performance;interestingness measure;parent nodes;mutual information;added value,,22,,13,,8-Jan-07,,,IEEE,IEEE Conferences
Single Document Summarization Based on Triangle Analysis of Dependency Graphs,基於依賴圖三角分析的單文檔摘要,K. Cheng; Y. Li; X. Wang,"Grad. Sch. of Inf. Sci., Kyushu Sangyo Univ., Fukuoka, Japan; Grad. Sch. of Inf. Sci., Kyushu Sangyo Univ., Fukuoka, Japan; Grad. Sch. of Inf. Sci., Kyushu Sangyo Univ., Fukuoka, Japan",2013 16th International Conference on Network-Based Information Systems,19-Dec-13,2013,,,38,43,"Extractive document summarization is a fundamental technique for document summarization. Most well-known approaches to extractive document summarization utilize supervised learning where algorithms are trained on collections of ""ground truth"" summaries built for a relatively large number of documents. In this paper, we propose a novel algorithm, called Triangle Sum for key sentence extraction from single document based on graph theory. The algorithm builds a dependency graph for the underlying document based on co-occurrence relation as well as syntactic dependency relations. In such a dependency graph, nodes represent words or phrases of high frequency, and edges represent dependency-co-occurrence relations between them. The clustering coefficient is computed from each node to measure the strength of connection between a node and its neighbors in a dependency graph. By identifying triangles of nodes in the graph, a part of the dependency graph can be extracted as marks of key sentences. At last, a set of key sentences that represent the main document information can be extracted.",2157-0426,978-1-4799-2510-0,10.1109/NBiS.2013.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685374,document summarization;dependency structure analysis;key sentence;clustering coefficient;triangle finding,Algorithm design and analysis;Clustering algorithms;Syntactics;Educational institutions;Pragmatics;Feature extraction;Semantics,document handling;graph theory;learning (artificial intelligence);pattern clustering;word processing,triangle analysis;extractive document summarization technique;supervised learning;ground truth summaries;TriangleSum algorithm;key-sentence extraction;dependency graph theory;syntactic dependency relations;dependency graph nodes;word frequency;phrase frequency;dependency graph edges;dependency-co-occurrence relations;clustering coefficient;document information representation;single-document summarization,,2,,20,,19-Dec-13,,,IEEE,IEEE Conferences
Simultaneous categorization of text documents and identification of cluster-dependent keywords,文本文檔的同時分類和與簇相關的關鍵字的標識,H. Frigui; O. Nasraoui,"Dept. of Electr. & Comput. Eng., Univ. of Memphis, TN, USA; Dept. of Electr. & Comput. Eng., Univ. of Memphis, TN, USA",2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),7-Aug-02,2002,2,,1108,1113 vol.2,"We propose an approach to clustering text documents based on a coupled process of clustering and cluster-dependent keyword weighting. The proposed approach is based on the the fuzzy c-means clustering algorithm. Hence it is computationally and implementationally simple. Moreover, it learns a different set of keyword weights for each cluster. This means that, as a by-product of the clustering process, each document cluster will be characterized by a possibly different set of keywords. The cluster dependent keyword weights help in partitioning the document collection into more meaningful categories. They can also be used to automatically generate a brief summary of each cluster in terms of not only the attribute values, but also their relevance. For the case of text data, this approach can be used to automatically annotate the documents. We illustrate the performance of the proposed algorithm by using it to cluster a real collection of text documents.",,0-7803-7280-8,10.1109/FUZZ.2002.1006659,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1006659,,Clustering algorithms;Search engines;Frequency;Degradation;Text mining;Information retrieval;Nearest neighbor searches;Web sites;Pattern recognition;Data mining,text analysis;fuzzy set theory;pattern clustering;unsupervised learning,text documents categorization;cluster-dependent keywords identification;clustering;fuzzy c-means clustering algorithm;document cluster;document collection,,5,,21,,7-Aug-02,,,IEEE,IEEE Conferences
Clustering web search results using conceptual grouping,使用概念性分組對網絡搜索結果進行聚類,Hong-Mei Li; Chen-Xia Sun; Ke-Jian Wang,"College of Information Science and Technology, Agricultural University of Hebei, Baoding 071001, China; College of Information Science and Technology, Agricultural University of Hebei, Baoding 071001, China; College of Information Science and Technology, Agricultural University of Hebei, Baoding 071001, China",2009 International Conference on Machine Learning and Cybernetics,25-Aug-09,2009,3,,1499,1503,"Clustering Web search results facilitates users' quick browsing through the information returned and locating interested results. This paper introduces a semantic, online clustering algorithm to automatically organize Web search results into groups. The semantic relationships among index terms are mined via the conceptual grouping and these terms are grouped to form candidate clusters related to the query topic by their semantic coherence. Then the documents are assigned to relevant clusters. The cluster labels are selected according to the importance of the terms in the search results and the clusters. Experimental results show that the proposed algorithm performs better than k-means.",2160-1348,978-1-4244-3702-3,10.1109/ICMLC.2009.5212322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212322,Information retrieval;Search engine;Clustering;Conceptual grouping,Web search;Clustering algorithms;Search engines;Machine learning;Cybernetics;Information retrieval;Web pages;Clustering methods;Frequency;Sun,pattern clustering;query processing;search engines;semantic Web,Web search clustering;conceptual grouping;online clustering algorithm;semantic coherence;query topic;documents cluster labels;information network;search engines,,,,10,,25-Aug-09,,,IEEE,IEEE Conferences
A Robust Clustering Method for XML Documents,XML文檔的魯棒聚類方法,B. Zhao; Y. Zhang; H. Zhang,"Coll. of Inf. Sci. & Eng., Shandong Normal Univ., Jinan, China; Coll. of Inf. Sci. & Eng., Shandong Normal Univ., Jinan, China; Coll. of Inf. Sci. & Eng., Shandong Normal Univ., Jinan, China","2008 International Conference on Information Management, Innovation Management and Industrial Engineering",6-Jan-09,2008,1,,19,23,"With the increase of XML data over the Internet, managing and analyzing huge amount of XML documents has played an important role for information management. This paper addresses the problem of clustering XML documents. Borrowing the idea of semi-clustering, it proposes a robust clustering method through a combination of single partitional and hierarchical clustering algorithms, which can eliminate the defects of single clustering algorithms. Experiments on real XML documents collection show that our method can group large collection of XML documents into appropriate clusters efficiently without fixed number of clusters. Moreover, our method is less sensitive to noises than single clustering algorithm.",2155-1472,978-0-7695-3435-0,10.1109/ICIII.2008.181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737488,,Robustness;Clustering methods;XML;Clustering algorithms;Partitioning algorithms;Information management;Innovation management;Internet;Data mining;Industrial engineering,information management;Internet;XML,robust clustering method;XML documents;XML data;Internet;information management;single clustering algorithm,,1,,11,,6-Jan-09,,,IEEE,IEEE Conferences
Parallel Spectral Clustering in Distributed Systems,分佈式系統中的並行光譜聚類,W. Chen; Y. Song; H. Bai; C. Lin; E. Y. Chang,"Yahoo! Inc, Sunnyvale; Microsoft Research Asia, Beijing; Google Information Technology (China) Co, Ltd., Beijing; National Taiwan University, Taipei; Google Research, Palo Alto",IEEE Transactions on Pattern Analysis and Machine Intelligence,17-Jan-11,2011,33,3,568,586,"Spectral clustering algorithms have been shown to be more effective in finding clusters than some traditional algorithms, such as k-means. However, spectral clustering suffers from a scalability problem in both memory use and computational time when the size of a data set is large. To perform clustering on large data sets, we investigate two representative ways of approximating the dense similarity matrix. We compare one approach by sparsifying the matrix with another by the Nystr繹m method. We then pick the strategy of sparsifying the matrix via retaining nearest neighbors and investigate its parallelization. We parallelize both memory use and computation on distributed computers. Through an empirical study on a document data set of 193,844 instances and a photo data set of 2,121,863, we show that our parallel algorithm can effectively handle large problems.",1939-3539,,10.1109/TPAMI.2010.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444877,Parallel spectral clustering;distributed computing;normalized cuts;nearest neighbors;Nystr繹m approximation.,Sparse matrices;Clustering algorithms;Nearest neighbor searches;Distributed computing;Concurrent computing;Laplace equations;Computer science;USA Councils;Scalability;Parallel algorithms,parallel algorithms;pattern clustering;sparse matrices,Nystrom method;sparse matrix;parallel spectral clustering;distributed systems;distributed computers;dense similarity matrix approximation;k-mean clusters;parallel algorithm,"Algorithms;Artificial Intelligence;Cluster Analysis;Computer Communication Networks;Computer Simulation;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Systems Integration",287,,54,,8-Apr-10,,,IEEE,IEEE Journals
Evaluating the Performance of Similarity Measures Used in Document Clustering and Information Retrieval,評估用於文檔聚類和信息檢索的相似性度量的性能,R. Subhashini; V. J. S. Kumar,"Sathyabama Univ., Chennai, India; Anna Univ., Chennai, India",2010 First International Conference on Integrated Intelligent Computing,16-Sep-10,2010,,,27,31,"This paper presents the results of an experimental study of some similarity measures used for both Information Retrieval and Document Clustering. Our results indicate that the cosine similarity measure is superior than the other measures such as Jaccard measure, Euclidean measure that we tested. Cosine Similarity measure is particularly better for text documents. Previously these measures are compared with the conventional text datasets but the proposed system collects the datasets with the help of API and it returns the collection of XML pages. These XML pages are parsed and filtered to get the web document datasets. In this paper, we compare and analyze the effectiveness of these measures for these web document datasets.",,978-1-4244-7963-4,10.1109/ICIIC.2010.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571521,Document clustering;Web mining;similarity measure;Information Retrieval,Information retrieval;Euclidean distance;XML;Clustering algorithms;Context;Internet,application program interfaces;information retrieval;Internet;pattern clustering;text analysis;XML,performance evaluation;similarity measures;document clustering;information retrieval;cosine similarity measure;text documents;API;XML page parsing;XML page filtering;Web document datasets,,23,,18,,16-Sep-10,,,IEEE,IEEE Conferences
A modified Support Vector Clustering method for document categorization,一種改進的支持向量聚類的文檔分類方法,B. S. Harish; M. B. Revanasiddappa; S. V. Aruna Kumar,"Department of Information Science & Engineering, JSS Science & Technology University, Mysuru, Karnataka, India; Department of Information Science & Engineering, JSS Science & Technology University, Mysuru, Karnataka, India; Department of Information Science & Engineering, JSS Science & Technology University, Mysuru, Karnataka, India",2016 IEEE International Conference on Knowledge Engineering and Applications (ICKEA),2-Jan-17,2016,,,1,5,"In this paper, we propose a novel text categorization method based on modified Support Vector Clustering (SVC). SVC is a density based clustering approach, which handles the arbitrary shape clusters effectively. The main drawback of traditional SVC is that it treats unclassified documents as outliers. To overcome this problem, we employed Fuzzy C-Means (FCM) to cluster unclassified documents. The modified SVC (SVC-FCM) is applied to categorize text documents. The proposed method consists of three steps: In the first step, Regularized Locality Preserving Indexing (RLPI) is applied on Term Document Matrix (TDM) to reduce dimensionality of features. In second step, we use SVC to find base-cluster centers of documents. Finally, we use FCM to cluster unclassified documents. To evaluate the performance of the proposed method, we conducted experiments on standard 20-NewsGroup dataset.",,978-1-5090-3471-0,10.1109/ICKEA.2016.7802982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802982,text categorization;support vector clustering;fuzzy C-Means;term document matrix,Static VAr compensators;Standards;Computational complexity;Support vector machines,category theory;fuzzy set theory;indexing;information resources;matrix algebra;pattern clustering;support vector machines;text analysis,support vector clustering;document categorization;novel text categorization;SVC;unclassified documents;fuzzy c-means;FCM;regularized locality preserving indexing;RLPI;term document matrix;TDM;NewsGroup dataset,,3,,24,,2-Jan-17,,,IEEE,IEEE Conferences
Comparison of Two Document Clustering Techniques which use Neural Networks,兩種使用神經網絡的文檔聚類技術的比較,I. Mokris; L. Skovajsova,"Slovak Academy of Sciences, Bratislava, Slovakia, mokris@valm.sk; Slovak Academy of Sciences, Bratislava, Slovakia, upsylesk@savba.sk",2008 IEEE International Conference on Computational Cybernetics,22-Dec-08,2008,,,75,78,"This paper presents text document space dimension reduction in text document retrieval by two different neural networks and their comparison. First neural network is Hebbian-type neural network, and second neural network is autoassociative neural network which uses backpropagation learning rule. Both neural networks reduce document space to two dimensions so each document is represented as a point in the reduced document space. Moreover, the clusters are formed in reduced document space. Both neural networks give promising results.",,978-1-4244-2874-8,10.1109/ICCCYB.2008.4721382,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721382,,Neural networks;Matrix decomposition;Information retrieval;Principal component analysis;Eigenvalues and eigenfunctions;Singular value decomposition;Internet;Computer networks;Backpropagation;Functional analysis,backpropagation;data reduction;Hebbian learning;information retrieval;neural nets;pattern clustering;text analysis,document clustering;text document space dimension reduction;text document retrieval;Hebbian neural network;autoassociative neural network;backpropagation learning rule,,1,,19,,22-Dec-08,,,IEEE,IEEE Conferences
A Distributing Strategy Based on Web Log Mining for Clustered Web Servers,基於Web日誌挖掘的群集Web服務器分佈策略,Q. Wang; X. He; X. Bi,NA; NA; NA,2011 International Conference on Internet Technology and Applications,29-Aug-11,2011,,,1,4,"In order to distribute web documents reasonably and improve the performance of clustered web servers, a distributing strategy based on web log mining is presented on this paper. The strategy is established in web users' accessing mode, which obtained from web log. Web documents are divided into groups by using data mining methods as clustering, which is based on the distribution is carried out, along with discovered hot documents and accessing frequency. Then the holistic analysis and design of document distributing system with specific application that takes campus network of military academy as an example is proposed in the paper.",,978-1-4244-7255-0,10.1109/ITAP.2011.6006225,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006225,,Data mining;Force;Clustering algorithms;Web servers;Computers;Bismuth,data mining;document handling;file servers;Internet;workstation clusters,distributing strategy;Web log mining;clustered Web servers;Web documents;data mining;document distributing system;campus network;military academy,,,,6,,29-Aug-11,,,IEEE,IEEE Conferences
Facilitating Understanding of Large Document Collections,促進對大型文獻收藏的理解,J. H. Bae; W. Xu; M. Esteva,"Adv. Comput. Center, Univ. of Texas at Austin, Austin, TX, USA; Adv. Comput. Center, Univ. of Texas at Austin, Austin, TX, USA; Adv. Comput. Center, Univ. of Texas at Austin, Austin, TX, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1334,1338,"Large document collections containing multiple topics can be overwhelming to understand, requiring librarians and archivists significant time and efforts to develop access points. Efficient computational methods can aid this process by uncovering groups of documents that can be described for access. We investigate the use of density based clustering with document segmentation to identify points of access as dense clusters of information. The method returns stories and classes of cohesive clusters that can be described as precise points of access. We found that our method performs more efficiently than K-means clustering and topic model using Latent Dirichlet Allocation (LDA). We use Hadoop to process a large document collection.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065527,density based clustering;information retrieval;distributed processing;Hadoop/MapReduce;digital archives,Electronic mail;Noise;Clustering algorithms;Vectors;Resource management;Clustering methods;Educational institutions,information retrieval;pattern clustering;text analysis,large document collection;density based clustering;document segmentation;Hadoop,,,,20,,3-Nov-11,,,IEEE,IEEE Conferences
A Method for Clustering E-business Contents,一種電子商務內容的聚類方法,L. Ronghui; Z. Jianguo; W. Xiang,"Sch. of Manage., Donghua Univ., Shanghai, China; Sch. of Manage., Donghua Univ., Shanghai, China; Sch. of Manage., Donghua Univ., Shanghai, China",2010 WASE International Conference on Information Engineering,16-Sep-10,2010,2,,43,46,"With the rapid development of deep web, high quality data pre-processing and extraction are extremely essential from these web data sources. The clustering is a crucial step for the data processing. This paper presents a unified solution to tackle the issue of clustering e-business web contents. Firstly, the vocabulary are segmented based on the obtained web contents, and then perform statistically analysis on the segmentation results to tune the document frequency (DF) so that the dimensionality of feature vector representing the web contents is under control. Next, term frequency (TF) and inverse document frequency (IDF) are used to form a weighted vector matrix, which is utilized to cluster the obtained web contents. Experiments show that this approach is capable to cluster e-business web contents with reasonable recall rate and precision.",,978-1-4244-7507-0,10.1109/ICIE.2010.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571224,Deep Web;Data extraction;Words segmentation;TF. IDF;Clustering,,document handling;electronic commerce;Internet;matrix algebra;pattern clustering;statistical analysis,e-business contents;content clustering;deep Web;Web data sources;statistical analysis;document frequency;term frequency;inverse document frequency;weighted vector matrix,,1,,11,,16-Sep-10,,,IEEE,IEEE Conferences
Basic Consideration of Online and Mini-Batch Algorithms for MMMs-induced Fuzzy Co-clustering,MMM引起的模糊共聚的在線和小批量算法的基本考慮,S. Ubukata; A. Notsu; K. Kida; K. Honda,"Graduate School of Engineering, Osaka Prefecture University, Sakai, Osaka, 599-8531, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, Osaka, 599-8531, Japan; Graduate School of Humanities and Sustainable System Sciences, Osaka Prefecture University, Sakai, Osaka, 599-8531, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, Osaka, 599-8531, Japan",2018 International Conference on Fuzzy Theory and Its Applications (iFUZZY),1-Jul-19,2018,,,85,90,"Fuzzy co-clustering schemes including Fuzzy Co-Clustering induced by Multinomial Mixture models (FCCMM) are promising approaches for analyzing object-item cooccurrence information such as document-keyword frequencies and customer-product purchase history transactions. However, such cooccurrence datasets are generally maintained as very large matrices and cannot be dealt with conventional batch algorithms. Online algorithms that load sequentially a single object for adjusting parameters are effective approaches for big data analysis. Mini-batch algorithms that load sequentially a small chunk (mini-batch) of objects for adjusting parameters are also effective. In this paper, we propose an online algorithm for FCCMM clustering and a mini-batch algorithm for FCCMM clustering and observe their characteristics and performance through numerical experiments.",2377-5831,978-1-5386-7411-6,10.1109/iFUZZY.2018.8751686,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8751686,Fuzzy clustering;Fuzzy co-clustering;FCCMM;Online algorithm;Mini-batch algorithm,Clustering algorithms;Machine learning algorithms;Linear programming;Mixture models;History;Clustering methods;Text analysis,data analysis;fuzzy set theory;mixture models;pattern clustering,mini-batch algorithm;object-item cooccurrence information;document-keyword frequencies;customer-product purchase history transactions;online algorithm;load sequentially;FCCMM clustering;multinomial mixture models;MMM-induced fuzzy co-clustering,,1,,7,,1-Jul-19,,,IEEE,IEEE Conferences
Clustering web search results using semantic information,使用語義信息對網絡搜索結果進行聚類,Han Wen; Guo-Shun Huang; Zhao Li,"School of Science, FOSHAN University, 528000, China; School of Science, FOSHAN University, 528000, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou 510641, China",2009 International Conference on Machine Learning and Cybernetics,25-Aug-09,2009,3,,1504,1509,Clustering Web search results will help users finding relevant information quickly. Suffix tree clustering (STC) algorithm is well fit for clustering Web documents. This paper puts forward an improved Web search results clustering algorithm based on STC. It uses latent semantic indexing method to assist finding common descriptive and meaningful topic phrases for the final document clusters. Using semantic information for clustering web snippets is able to make search engine results easy to browse and help users quickly find Web information interested. Evaluation of experiment results demonstrates that clustering Web search results based on the improved suffix tree algorithm gets better performance in cluster label quality and snippets assignment precision.,2160-1348,978-1-4244-3702-3,10.1109/ICMLC.2009.5212332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212332,Latent semantic indexing;Singular value decomposition;Suffix tree clustering,Web search;Clustering algorithms;Search engines;Indexing;Frequency;Machine learning;Cybernetics;Singular value decomposition;Internet;Machine learning algorithms,document handling;indexing;information retrieval;online front-ends;pattern clustering;search engines;semantic Web;trees (mathematics),Web search results clustering;semantic information;suffix tree clustering algorithm;clustering Web documents;latent semantic indexing method;document clusters;Web snippets;search engine;Web browser;cluster label quality;snippets assignment precision,,1,,15,,25-Aug-09,,,IEEE,IEEE Conferences
Semantically Rich Spaces for Document Clustering,用於文檔聚類的語義豐富空間,R. Basili; P. Marocco; D. Milizia,"Dept. of Comput. Sci., Rome Univ., Rome; Dept. of Comput. Sci., Rome Univ., Rome; Dept. of Comput. Sci., Rome Univ., Rome",2008 19th International Workshop on Database and Expert Systems Applications,12-Sep-08,2008,,,43,47,"Dimensionality reduction techniques address a relevant problem of vector space models that is the size of involved dictionaries. Certain geometrical transformations applied over the original feature space, like the latent semantic analysis (LSA), aim at preserving and discovering semantic relations between documents within small dimensional spaces. In this paper, a linear transformation method, named locality preserving projections (LPP), is evaluated with respect to a document clustering task and results are compared with LSA. LPP is here applied directly on the original space, through an efficient C-based implementation, and different parameterizations are investigated. Experimental results suggest that LPP is an effective technique able to account for the availability of a priori knowledge within an unsupervised learning framework.",2378-3915,978-0-7695-3299-8,10.1109/DEXA.2008.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624689,Linear embedding;Locality Preserving Projection;Latent Semantic Analysis;Document clustering,Vectors;Accuracy;Availability;Clustering algorithms;Matrix decomposition;Computational modeling;Linear approximation,data reduction;document handling;information retrieval;pattern clustering;unsupervised learning,semantic relation discovery;document clustering;dimensionality reduction technique;vector space model problem;geometrical transformation;latent semantic analysis;linear transformation method;locality preserving projection;unsupervised learning framework;information retrieval,,,,10,,12-Sep-08,,,IEEE,IEEE Conferences
Self-organising map for document categorization using latent semantic analysis,使用潛在語義分析的文檔自組織圖分類,B. Mahalakshmi; K. Duraiswamy,"Department of CSE, K.S.Rangasamy College of Technology, Anna University Coimbatore, Tiruchengode 637215, India; Department of CSE, K.S.Rangasamy College of Technology, Anna University Coimbatore, Tiruchengode 637215, India",2010 International Conference on Innovative Computing Technologies (ICICT),29-Mar-10,2010,,,1,6,"With the increasing amount of unstructured content available electronically on the web, content categorization becomes very important for efficient information retrieval. The basic approaches for information retrieval in text documents are searching using keywords, categorization of the documents and filtering out the stream. To extract information from raw data, its complexity needed to be first reduced. Clustering methods and Projection methods are aimed at reducing the amount of data and dimensionality of data respectively. SOM is a special case in that it can be used at the same time for both clustering and projection. It projects onto a 2D-grid. Various methods were developed for the automatic clustering of worldwide webdocuments according to the user requirements. The objective of this paper is to reduce the time and effort the user has to find the information sought after. The method termed topological organization of content can generate classified topics from a set of unstructured documents. The TOC is a set of hierarchically organized 1D-growing SOMs. In TOC, vector space model is used for indexing of 1D-SOM. In the proposed approach, latent semantic indexing of 1D-SOM can be used to enhance the association between terms. Latent semantic analysis is a technique that projects the original high dimensional document vector into a space with latent semantic dimensions. A term-by-document matrix is constructed for the information retrieval. A brief review is given on existing methods for documents clustering and organization. The proposed method which can use LSI will be efficient in terms of computational cost, accuracy and visualization. It can be easily adapted for large data set. The proposed method will provide feature for retrieving meaningful related topics.",,978-1-4244-6491-3,10.1109/ICINNOVCT.2010.5440089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440089,Document categorization;information retrieval;one-dimensional self-organizing map (1D-SOM);latent semantic analysis(LSA),Information retrieval;Indexing;Content based retrieval;Information filtering;Information filters;Data mining;Clustering methods;Functional analysis;Large scale integration;Computational efficiency,document handling;information retrieval;pattern clustering;self-organising feature maps,self organising map;document categorization;latent semantic analysis;unstructured content;information retrieval;text documents;clustering methods;projection methods;2D grid;worldwide Web documents;topological content organization;high dimensional document vector;LSI,,1,,21,,29-Mar-10,,,IEEE,IEEE Conferences
Concept Extraction and Clustering for Topic Digital Library Construction,主題數字圖書館建設的概念提取與聚類,C. Zhang; D. Wu,"Dept. of Inf. Manage., Nanjing Univ. of Sci. & Technol., Nanjing; Sch. of Inf. Manage., Wuhan Univ., Wuhan",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,3,,299,302,"This paper is to introduce a new approach to build topic digital library using concept extraction and document clustering. Firstly, documents in a special domain are automatically produced by document classification approach. Then, the keywords of each document are extracted using the machine learning approach. The keywords are used to cluster the documents subset. The clustered result is the taxonomy of the subset. Lastly, the taxonomy is modified to the hierarchical structure for user navigation by manual adjustments. The topic digital library is constructed after combining the full-text retrieval and hierarchical navigation function.",,978-0-7695-3496-1,10.1109/WIIAT.2008.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740784,concept extraction;document clustering;topic digital library,Software libraries;Data mining;Statistics;Frequency;Intelligent agent;Information management;Taxonomy;Navigation;Labeling;Web and internet services,digital libraries;information retrieval;learning (artificial intelligence);pattern classification;pattern clustering;text analysis,concept extraction;document clustering;topic digital library construction;document classification approach;machine learning approach;taxonomy;hierarchical navigation function;full-text retrieval,,1,,11,,6-Jan-09,,,IEEE,IEEE Conferences
XML Documents Clustering Using Tensor Space Model -- A Preliminary Study,使用Tensor空間模型的XML文檔聚類-初步研究,S. Kutty; R. Nayak; Y. Li,"Sch. of Comput. Sci., Queensland Univ. of Technol. (QUT), Brisbane, QLD, Australia; Sch. of Comput. Sci., Queensland Univ. of Technol. (QUT), Brisbane, QLD, Australia; Sch. of Comput. Sci., Queensland Univ. of Technol. (QUT), Brisbane, QLD, Australia",2010 IEEE International Conference on Data Mining Workshops,20-Jan-11,2010,,,1167,1173,"A hierarchical structure is used to represent the content of the semi-structured documents such as XML and XHTML. The traditional Vector Space Model (VSM) is not sufficient to represent both the structure and the content of such web documents. Hence in this paper, we introduce a novel method of representing the XML documents in Tensor Space Model (TSM) and then utilize it for clustering. Empirical analysis shows that the proposed method is scalable for a real-life dataset as well as the factorized matrices produced from the proposed method helps to improve the quality of clusters due to the enriched document representation with both the structure and the content information.",2375-9259,978-1-4244-9244-2,10.1109/ICDMW.2010.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693426,XML documents;Clustering;Tensor;Structure and Content;Decomposition,XML;Tensile stress;Clustering algorithms;Books;Matrix decomposition;Data mining;Clustering methods,matrix decomposition;pattern clustering;tensors;vectors;XML,XML documents clustering;tensor space model;vector space model;matrix factorisation,,2,,18,,20-Jan-11,,,IEEE,IEEE Conferences
Cluster-Based Sample Selection for Document Image Binarization,基於聚類的文檔圖像二值化樣本選擇,A. Krantz; F. Westphal,Blekinge Institute of Technology; Blekinge Institute of Technology,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),7-Nov-19,2019,5,,47,52,"The current state-of-the-art, in terms of performance, for solving document image binarization is training artificial neural networks on pre-labelled ground truth data. As such, it faces the same issues as other, more conventional, classification problems; requiring a large amount of training data. However, unlike those conventional classification problems, document image binarization involves having to either manually craft or estimate the binarized ground truth data, which can be error-prone and time-consuming. This is where sample selection, the act of selecting training samples based on some method or metric, might help. By reducing the size of the training dataset in such a way that the binarization performance is not impacted, the required time spent creating the ground truth is also reduced. This paper proposes a cluster-based sample selection method that uses image similarity metrics and the relative neighbourhood graph to reduce the underlying redundancy of the dataset. The method, implemented with affinity propagation and the structural similarity index, reduces the training dataset on average by 49.57% while reducing the binarization performance only by 0.55%.",,978-1-7281-5054-3,10.1109/ICDARW.2019.40080,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892888,document image binarization;sample selection;neural networks;computer vision,Training;Measurement;Bridges;Neural networks;Training data;Indexes;Testing,document image processing;graph theory;learning (artificial intelligence);neural nets;pattern classification;pattern clustering,document image binarization;pre-labelled ground truth data;classification problems;binarized ground truth data;cluster-based sample selection method;image similarity metrics;artificial neural networks;relative neighbourhood graph;affinity propagation;structural similarity index,,,,26,,7-Nov-19,,,IEEE,IEEE Conferences
Document clustering using a graph covering with pseudostable sets,使用覆蓋偽穩定集的圖進行文檔聚類,J. D繹rpinghaus; S. Schaaf; J. Fluck; M. Jacobs,"Fraunhofer Institute for Algorithms and Scientific Computing, Schloss Birlinghoven, Sankt Augustin, Germany; Fraunhofer Institute for Algorithms and Scientific Computing, Schloss Birlinghoven, Sankt Augustin, Germany; Fraunhofer Institute for Algorithms and Scientific Computing, Schloss Birlinghoven, Sankt Augustin, Germany; Fraunhofer Institute for Algorithms and Scientific Computing, Schloss Birlinghoven, Sankt Augustin, Germany",2017 Federated Conference on Computer Science and Information Systems (FedCSIS),13-Nov-17,2017,,,329,338,"In text mining, document clustering describes the efforts to assign unstructured documents to clusters, which in turn usually refer to topics. Clustering is widely used in science for data retrieval and organisation. In this paper we present a new graph theoretical approach to document clustering and its application on a real-world data set. We will show that the well-known graph partition to stable sets or cliques can be generalized to pseudostable sets or pseudocliques. This allows to make a soft clustering as well as a hard clustering. We will present an integer linear programming and a greedy approach for this NP-complete problem and discuss some results on random instances and some real world data for different similarity measures.",,978-83-946253-7-5,10.15439/2017F84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104561,,Color;Minimization;Clustering algorithms;Integer linear programming;Graph theory;Optimization;Manganese,document handling;graph theory;information retrieval;integer programming;pattern clustering,NP-complete problem;greedy approach;integer linear programming;hard clustering;real-world data set;graph theoretical approach;data retrieval;unstructured documents;text mining;pseudostable sets;graph covering;document clustering,,1,,22,,13-Nov-17,,,IEEE,IEEE Conferences
An efficient k-means algorithm integrated with Jaccard distance measure for document clustering,結合Jaccard測距的高效k均值算法用於文檔聚類,M. Shameem; R. Ferdous,"Dept. of Computer Science and Engineering, University of Development Alternative (UODA), Dhaka, Bangladesh; Master in Computer Science, University of Trento, Trento, Italy",2009 First Asian Himalayas International Conference on Internet,24-Nov-09,2009,,,1,6,"Document Clustering is a widely studied problem in Text Categorization. It is the process of partitioning or grouping a given set of documents into disjoint clusters where documents in the same cluster are similar. K-means, one of the simplest unsupervised learning algorithms, solves the well known clustering problem following a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centroids, one for each cluster. This clustering algorithm uses an iterative procedure which converges to one of numerous local minima. We have found that these iterative techniques are especially sensitive to initial starting conditions of the centroid of each cluster and the more the distance among the cluster centroid the better the clustering performance. In simple K-means algorithm the way to initialize the centroid is not specified and one popular way to start is to randomly choose k points of the samples as k centroids but this process does not guarantee to choose the maximum dissimilar documents as the centroid point for k-cluster. In this paper we proposed a modified k-means algorithm which uses Jaccard distance measure for computing the most dissimilar k documents as centroids for k clusters. Our experimental results demonstrate that our proposed K-means algorithm with Jaccard distance measure for computing the centroid improves the clustering performance of the simple K-means algorithm.",1941-0131,978-1-4244-4569-1,10.1109/AHICI.2009.5340335,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340335,Document Clustering;K-Means algorithm;Precission;Recall;F1-Measure;Entropy,Clustering algorithms;Iterative algorithms;Partitioning algorithms;Computer science;Text categorization;Information retrieval;Unsupervised learning;Entropy;Iterative methods;Measurement techniques,pattern clustering;statistics;text analysis;unsupervised learning,k-means algorithm;Jaccard distance measure;document clustering;text categorization;unsupervised learning algorithms,,16,,7,,24-Nov-09,,,IEEE,IEEE Conferences
Web News Summarization via Soft Clustering Algorithm,通過軟聚類算法總結網絡新聞,J. Wu,"Sch. of Politics & Law & Public Adm., Hubei Univ., Wuhan, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,7,,618,621,"As the information available on the internet is growing explosively, this paper proposed a new method of web news summarization via sentence clustering algorithm. It adopted cluster algorithm to cluster all the sentences. Feature fusion will be used to extract summary sentences. Experimental result shows that the proposed summarization method can improve the performance of summary.",,978-0-7695-3735-1,10.1109/FSKD.2009.838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360083,Clustering algorithm;Web news summary;Sentences;Feature fusion,Clustering algorithms;Clustering methods;Internet;Search engines;Iterative algorithms;Matrix decomposition;Labeling;Fuzzy systems;Data mining;Statistics,document handling;information retrieval;Internet;pattern clustering,Web news summarization;soft clustering algorithm;sentence clustering algorithm;feature fusion;Internet,,,,12,,28-Dec-09,,,IEEE,IEEE Conferences
Incorporating User Provided Constraints into Document Clustering,將用戶提供的約束納入文檔聚類,Y. Chen; M. Rege; M. Dong; J. Hua,"Wayne State Univ., Detroit; Wayne State Univ., Detroit; Wayne State Univ., Detroit; NA",Seventh IEEE International Conference on Data Mining (ICDM 2007),12-Mar-08,2007,,,103,112,"Document clustering without any prior knowledge or background information is a challenging problem. In this paper, we propose SS-NMF: a semi-supervised non- negative matrix factorization framework for document clustering. In SS-NMF, users are able to provide supervision for document clustering in terms of pairwise constraints on a few documents specifying whether they ""must"" or ""cannot"" be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the document- document similarity matrix to infer the document clusters. Theoretically, we show that SS-NMF provides a general framework for semi-supervised clustering and that existing approaches can be considered as special cases of SS-NMF. Through extensive experiments conducted on publicly available data sets, we demonstrate the superior performance of SS-NMF for clustering documents.",2374-8486,978-0-7695-3018-5,10.1109/ICDM.2007.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4470234,,Iterative algorithms;Clustering algorithms;Symmetric matrices;Data mining;Databases;Clustering methods;Couplings;Machine vision;Pattern recognition;Computer graphics,document handling;matrix decomposition;pattern clustering,user provided constraints;document clustering;semisupervised nonnegative matrix factorization;pairwise constraints,,22,,38,,12-Mar-08,,,IEEE,IEEE Conferences
A Fuzzy-Cluster based Semantic Information Retrieval System,基於模糊聚類的語義信息檢索系統,D. Mahapatra; C. Maharana; S. P. Panda; J. P. Mohanty; A. Talib; A. Mangaraj,"Silicon Institute of Technology,Department of CSE,Bhubaneswar; Silicon Institute of Technology,Department of CSE,Bhubaneswar; Silicon Institute of Technology,Department of CSE,Bhubaneswar; Silicon Institute of Technology,Department of CSE,Bhubaneswar; Silicon Institute of Technology,Department of CSE,Bhubaneswar; Silicon Institute of Technology,Department of CSE,Bhubaneswar",2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC),23-Apr-20,2020,,,675,678,"Due to the increasing number of digital document repositories there is a heavy demand for information retrieval systems and therefore, information retrieval is still appearing as an emerging area of research. The information retrieval technology these days focuses on achieving better performance under different context by extracting documents most appropriate to the user's query. Majority of the classical keyword based retrieval techniques does not focus on semantic meanings and therefore, are found to be less effective in reconstructing the actual information conveyed in the context. Also, retrieval of the relevant documents depends on appropriate analysis of the query terms. As words are polysemic, their actual meanings are influenced by their relationships with other words and their syntactic roles in the sentence. This work presents a fuzzy-cluster based semantic information retrieval model that considers these relationships to determine the exact meaning of the user query and extracts relevant documents as per their relevance scores.",,978-1-7281-4889-2,10.1109/ICCMC48092.2020.ICCMC-000125,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076534,Information Retrieval;Semantic Information Retrieval System;fuzzy-cluster model;Boolean model,,document handling;fuzzy set theory;pattern clustering;query processing,digital document repositories;user query;document extraction;fuzzy-cluster;semantic information retrieval system;keyword based retrieval,,,,12,,23-Apr-20,,,IEEE,IEEE Conferences
Clustering Scientific Document Based on an Extended Citation Model,基於擴展引文模型的科學文獻聚類,S. Zhang; Y. Xu; W. Zhang,"School of Information, Zhejiang University of Finance and Economics, Hangzhou, China; School of Information, Zhejiang University of Finance and Economics, Hangzhou, China; School of Information, Zhejiang University of Finance and Economics, Hangzhou, China",IEEE Access,10-May-19,2019,7,,57037,57046,"With the number of published scientific paper increasing exponentially, scientific document clustering is becoming a challenging task. Therefore, a scientific document clustering model with high quality is needed. In this paper, we propose an extended citation model for scientific document clustering. On the one hand, the proposed model considers that 1) the high frequency and the wide distribution of a scientific document cited in other documents will result in the high similarity between the citing and the cited documents; and 2) the close location of two scientific documents cited in a scientific document will also result in the high similarity between these two documents. On the other hand, the proposed model combines a citation networks and textual similarity network to enhance the performance of scientific document clustering. To evaluate the performance of our proposed model, we collect scientific documents from PMC and PubMed databases in the field of oncology as a case study. It is proved that our proposed model can obtain reasonably clustering results by comparing it with traditional scientific documents clustering models, such as traditional bibliographic coupling model and textual similarity model, according to the indices of precision, recall, and F1-score.",2169-3536,,10.1109/ACCESS.2019.2913995,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703770,Scientific document clustering;citation frequency analysis;citation distribution analysis;citation proximity analysis;textual similarity;random walk algorithm,Couplings;High frequency;Clustering algorithms;Citation analysis;Weight measurement;Text analysis;Frequency measurement,citation analysis;pattern clustering;text analysis,extended citation model;scientific document clustering model;bibliographic coupling model;oncology;PubMed database;PMC database;textual similarity network,,1,,30,,1-May-19,,,IEEE,IEEE Journals
Fuzzy clustering approach for star-structured multi-type relational data,星型多類型關係數據的模糊聚類方法,J. Mei; L. Chen,"Division of Information Engineering, School of EEE, Nanyang Technological University, 50 Nanyang Avenue, Singapore; Division of Information Engineering, School of EEE, Nanyang Technological University, 50 Nanyang Avenue, Singapore",2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011),1-Sep-11,2011,,,2500,2506,"Recently, mining interrelated data among multiple types of objects attracts a lot of attention due to its importance in many real-world applications. Despite of extensive study on fuzzy clustering of vector space data and homogeneous relational data, very limited exploration has been made on fuzzy clustering of relational data involving several object types. In this paper, we propose FC-SMR, a fuzzy approach for clustering star-structured multi-type relational data, where the central type is related to multiple attribute types. In FC-SMR, objects of the central type are clustered based on the rankings of objects of different attribute types. We formulate the clustering problem as a constrained maximization problem and give an efficient algorithm for finding local solutions of the defined objective function. Experimental studies conducted on real-world document data show the effectiveness of the new approach.",1098-7584,978-1-4244-7317-5,10.1109/FUZZY.2011.6007422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007422,Fuzzy clustering;co-clustering;multi-type;heterogeneous relational data;document categorization,Clustering algorithms;Data mining;Sports equipment;Distributed databases;Weapons;Partitioning algorithms;Motorcycles,data mining;document handling;fuzzy reasoning;optimisation;pattern clustering;relational databases,fuzzy clustering approach;star structured multitype relational data clustering problem;interrelated data mining;real-world applications;vector space data;homogeneous relational data;FC-SMR;multiple attribute type;constrained maximization problem;local solution finding;real-world document data,,2,,7,,1-Sep-11,,,IEEE,IEEE Conferences
Robust Recognition of Documents by Fusing Results of Word Clusters,通過融合詞簇的結果對文檔進行穩健的識別,V. Rasagna; A. Kumar; C. V. Jawahar; R. Manmatha,"Center for Visual Inf. Technol., HIT, Hyderabad, India; Center for Visual Inf. Technol., HIT, Hyderabad, India; Center for Visual Inf. Technol., HIT, Hyderabad, India; Dept. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,566,570,"The word error rate of any optical character recognition system (OCR) is usually substantially below its component or character error rate. This is especially true of Indic languages in which a word consists of many components. Current OCRs recognize each character or word separately and do not take advantage of document level constraints. We propose a document level OCR which incorporates information from the entire document to reduce word error rates. Word images are first clustered using a locality sensitive hashing technique. Individual words are then recognized using a (regular) OCR. The OCR outputs of word images in a cluster are then corrected probabilistically by comparing with the OCR outputs of other members of the same cluster. The approach may be applied to improve the accuracy of any OCR run on documents in any language. In particular, we demonstrate it for Telugu, where the use of language models for post-processing is not promising. We show a relative improvement of 28% for long words and 12% for all words which appear at least twice in the corpus.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277591,locality sensitive hashing;OCR,Robustness;Optical character recognition software;Character recognition;Error analysis;Optical sensors;Error correction;Optical materials;Image segmentation;Clustering algorithms;Text analysis,cryptography;document image processing;error statistics;file organisation;optical character recognition,robust document recognition;word clusters;word error rate;optical character recognition system;character error rate;Indic languages;document level constraints;hashing technique,,6,,12,,2-Oct-09,,,IEEE,IEEE Conferences
Small-worlds clustering applied to documents re-ranking,小世界聚類應用於文檔重新排序,M. Khazri; M. Tmar; M. Abid,"National Ingeneering School, 4 Road of Soukra, 3038 Sfax, Tunisia; Informatique and Multimedia Institut, 10 Road of tunis, 3018 Sfax, Tunisia; National Ingeneering School, 4 Road of Soukra, 3038 Sfax, Tunisia",ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010,27-Sep-10,2010,,,1,6,"We propose in this paper an approach for document clustering. It consists on representing the corpus as a document graph, where links are defined by some criteria. These links are quantified by similarity measures. We aim to join this context with clustering to build small-world networks of homogeneous documents. The homogeneity of the clusters is measured according to the properties of small-worlds networks. The clusters, as well as their properties, allow to re-rank search results. Some experiments have been undertaken into a corpus provided by TREC and the obtained results show the contribution of small-worlds properties and analysis in information retrieval and document re-ranking.",2161-5330,978-1-4244-7717-3,10.1109/AICCSA.2010.5586938,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586938,,,document handling;graph theory;information retrieval;pattern clustering,small-worlds clustering;document re-ranking;document clustering;document graph;homogeneous document;re-rank search;information retrieval,,,,12,,27-Sep-10,,,IEEE,IEEE Conferences
Clustering WSDL Documents to Bootstrap the Discovery of Web Services,集群WSDL文檔以引導發現Web服務,K. Elgazzar; A. E. Hassan; P. Martin,"Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada",2010 IEEE International Conference on Web Services,23-Aug-10,2010,,,147,154,"The increasing use of the Web for everyday tasks is making Web services an essential part of the Internet customer's daily life. Users query the Internet for a required Web service and get back a set of Web services that may or may not satisfy their request. To get the most relevant Web services that fulfill the user's request, the user has to construct the request using the keywords that best describe the user's objective and match correctly with the Web Service name or location. Clustering Web services based on function similarities would greatly boost the ability of Web services search engines to retrieve the most relevant Web services. This paper proposes a novel technique to mine Web Service Description Language (WSDL) documents and cluster them into functionally similar Web service groups. The application of our approach to real Web services description files has shown good performance for clustering Web services based on function similarity, as a predecessor step to retrieving the relevant Web services for a user request by search engines.",,978-1-4244-8146-0,10.1109/ICWS.2010.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552792,clustering WSDL documents;Web service;feature extraction;Web service clustering,Web services;Feature extraction;Search engines;Meteorology;XML;Clustering algorithms;Frequency estimation,computer bootstrapping;document handling;high level languages;search engines;Web services,WSDL documents clustering;bootstrap;Web services;Internet;Web service description language;search engines,,119,,22,,23-Aug-10,,,IEEE,IEEE Conferences
An encoding technique based on word importance for the clustering of Web documents,基於單詞重要性的Web文檔聚類編碼技術,J. Zakos; B. Verma,"Sch. of Inf. Technol., Griffith Univ., Australia; Sch. of Inf. Technol., Griffith Univ., Australia","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",5-Jun-03,2002,5,,2207,2211 vol.5,"We present a word encoding and clustering technique that groups Web documents based on the importance of the words that appear in the documents. We use a two level self-organizing map architecture to generate clusters of words and documents. We propose that by capturing word importance information of words, similar documents can be then clustered to assist in Web document retrieval. A Web document retrieval system is presented to demonstrate how this approach could. be integrated into Web search.",,981-04-7524-1,10.1109/ICONIP.2002.1201885,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1201885,,Encoding;Histograms;Information retrieval;Gold;Web pages;Internet;Self organizing feature maps;Search engines;Information processing;Information technology,word processing;encoding;Internet;information retrieval;pattern clustering;self-organising feature maps;search engines,encoding technique;word importance;Web document clustering;word encoding;two level self-organizing map architecture;word importance information;Web document retrieval system,,,,20,,5-Jun-03,,,IEEE,IEEE Conferences
Using Mahout for Clustering Wikipedia's Latest Articles: A Comparison between K-means and Fuzzy C-means in the Cloud,使用Mahout聚類維基百科的最新文章：雲中K均值和模糊C均值的比較,R. M. Esteves; C. Rong,"Dept. of Electr. & Comput. Eng., Univ. of Stavanger, Stavanger, Norway; Dept. of Electr. & Comput. Eng., Univ. of Stavanger, Stavanger, Norway",2011 IEEE Third International Conference on Cloud Computing Technology and Science,19-Jan-12,2011,,,565,569,"This paper compares k-means and fuzzy c-means for clustering a noisy realistic and big dataset. We made the comparison using a free cloud computing solution Apache Mahout/ Hadoop and Wikipedia's latest articles. In the past the usage of these two algorithms was restricted to small datasets. As so, studies were based on artificial datasets that do not represent a real document clustering situation. With this ongoing research we found that in a noisy dataset, fuzzy c-means can lead to worse cluster quality than k-means. The convergence speed of k-means is not always faster. We found as well that Mahout is a promise clustering technology but the preprocessing tools are not developed enough for an efficient dimensionality reduction. From our experience the use of the Apache Mahout is premature.",,978-1-4673-0090-2,10.1109/CloudCom.2011.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133195,Mahout;document clustering;k-means;fuzzy c-means,Vectors;Clustering algorithms;Internet;Encyclopedias;Electronic publishing;Convergence,cloud computing;document handling;fuzzy set theory;pattern clustering;Web sites,Wikipedia latest article clustering;k-means clustering;fuzzy c-means clustering;noisy realistic dataset;free cloud computing solution;Apache Mahout;Hadoop;real document clustering;artificial datasets;cluster quality,,27,,24,,19-Jan-12,,,IEEE,IEEE Conferences
Integrating phrases to enhance HSOMART-based document clustering,集成短語以增強基於HSOMART的文檔聚類,M. F. Hussin; M. S. Kamel,"Dept. of Comput. Sci. & Autom. Control, Alexandria Univ., Egypt; NA",2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),17-Jan-05,2004,3,,2347,2352 vol.3,"Document clustering is one of the popular techniques that assist users in organizing collections of documents. Two successful models of unsupervised neural networks, self-organizing map (SOM) and adaptive resonance theory (ART), have shown promising results in this task. Most of the existing neural network based document clustering techniques rely on a ""bag of words"" document representation. Each word in the document is considered as a separate feature, ignoring the word order. We investigate the use of phrases rather than words as document features applied to our proposed document clustering technique, called hierarchical SOMART (HSOMART), which is a hierarchical network built up from independent SOM and ART neural networks. We describe a phrase grammar extraction technique, and the proposed HSOMART. The experimental results of clustering documents from the REUTERS corpus using the extracted phrases as features show an improvement in the clustering performance evaluated using the entropy and F-measure.",1098-7576,0-7803-8359-1,10.1109/IJCNN.2004.1380993,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1380993,,Neural networks;Clustering algorithms;Subspace constraints;Tree graphs;Text categorization;Computer science;Automatic control;Organizing;Resonance;Feature extraction,self-organising feature maps;ART neural nets;pattern clustering;tree data structures;document handling;unsupervised learning;entropy,phrase integration;document clustering techniques;unsupervised neural networks;self organizing map;ART;adaptive resonance theory;bag of words;document representation;hierarchical network;phrase grammar extraction technique;feature extraction;REUTERS corpus;entropy;F-measure,,1,,20,,17-Jan-05,,,IEEE,IEEE Conferences
Using SOM based graph clustering for extracting main ideas from documents,使用基於SOM的圖聚類從文檔中提取主要思想,D. Phuc; M. X. Hung,"University of Information Technology , VNU-HCM, HoChiMinh City, VietNam; University of Information Technology , VNU-HCM, HoChiMinh City, VietNam","2008 IEEE International Conference on Research, Innovation and Vision for the Future in Computing and Communication Technologies",8-Aug-08,2008,,,209,214,"In this paper, we would like to present a graph clustering system for grouping the similar documents and extracting the main ideas in documents. To cluster the documents, we need a model for representing the documents. The traditional approaches used a word set based model or a vector based model for representing the documents. These models discard the important structural information of documents such as word position, the semantic relations of words in document... Recently, some research works using the graph for representing the documents have been appeared. We use the graph to be created by analyzing the co-occurrence and position of two words in a section of document. After representing the documents by using graph, we used self organizing map (SOM) with two-dimensional output layer for grouping the graphs. One of the advantages of SOM is to cluster the data without specifying the number of clusters. Besides, two-dimensional SOM output layer can be put on the computer display and it can help to access the similar documents on the computer display. We use the graph distance based on the maximum common sub-graph (mcs) which is discovered by maximal frequent sub-graph algorithm and the updated operation of neurons on SOM ouput layer based on the weighted means graphs and the genetic algorithm.",,978-1-4244-2379-8,10.1109/RIVF.2008.4586357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4586357,Weighted means graphs;Graph clustering;Graph Distance;Genetic algorithm;Kohonen neural network,,genetic algorithms;graph theory;pattern clustering;self-organising feature maps;text analysis,SOM based graph clustering;document main idea extraction;graph clustering system;document clustering;document structural information;word position;word semantic relation;word cooccurrence;self organizing map;maximum common subgraph;neuron operation;weighted mean graph;genetic algorithm,,5,,10,,8-Aug-08,,,IEEE,IEEE Conferences
Rough clustering of Korean foods based on adjectives for taste evaluation,基於形容詞的韓國食品的粗略分類，以評估口味,Joonwhoan Lee; D. Ghimire; Jeong-Ok Rho,"Dept. of Computer Engineering, Jeonbuk National University, Jeonju-si, Jeollabuk-do 561-756, Rep. of Korea; Dept. of Computer Engineering, Jeonbuk National University, Jeonju-si, Jeollabuk-do 561-756, Rep. of Korea; Dept. of Food Science and Human Nutrition, Jeonbuk National University, Jeonju-si, Jeollabuk-do 561-756, Rep. of Korea",2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),19-May-14,2013,,,472,475,"There are many adjectives to express the taste of Korean foods. In the paper, those adjectives are used to categorize the foods based on rough tolerant relation as the same way as document clustering. Before clustering, we selected 87 adjectives that are frequently used for taste evaluation and obtained the sets of adjectives that can be used for the taste of 51 kinds of Korean cuisine and refreshments. For clustering non-hierarchical algorithm was used. The foods with similar ingredients, state of cuisine for serving, cooking methods and taking methods share the same cluster as we expected. The set of adjectives corresponding cluster representative can be used as linguistic scales for evaluating taste of foods in the category.",,978-1-4673-5253-6,10.1109/FSKD.2013.6816243,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816243,food;rough tolerant clustering;adjectives;taste evaluation;Korean Food,Clustering algorithms;Approximation methods;Educational institutions;Pragmatics;Taxonomy;Vectors;Green products,chemioception;food products;pattern clustering,rough clustering;Korean foods;food categorization;rough tolerant relation;document clustering;taste evaluation;adjectives;Korean cuisine;Korean refreshments;nonhierarchical clustering algorithm;ingredients;cuisine state;serving methods;cooking methods;taking methods;cluster representative;linguistic scales,,,,6,,19-May-14,,,IEEE,IEEE Conferences
Script and Language Identification in Noisy and Degraded Document Images,嘈雜和降級的文檔圖像中的腳本和語言識別,L. Shijian; C. Lim Tan,"Nat. Univ. of Singapore, Singapore; Nat. Univ. of Singapore, Singapore",IEEE Transactions on Pattern Analysis and Machine Intelligence,21-Nov-07,2008,30,1,14,24,"This paper reports an identification technique that detects scripts and languages of noisy and degraded document images. In the proposed technique, scripts and languages are identified through the document vectorization, which converts each document image into a document vector that characterizes the shape and frequency of the contained character or word images. Document images are vectorized by using vertical component cuts and character extremum points, which are both tolerant to the variation in text fonts and styles, noise, and various types of document degradation. For each script or language under study, a script or language template is first constructed through a training process. Scripts and languages of document images are then determined according to the distances between converted document vectors and the preconstructed script and language templates. Experimental results show that the proposed technique is accurate, easy for extension, and tolerant to noise and various types of document degradation.",1939-3539,,10.1109/TPAMI.2007.1158,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359308,Document analysis;shape;script identification;language identification;clustering;classification;association rules;Document analysis;shape;script identification;language identification;clustering;classification;association rules,Degradation;Image converters;Optical character recognition software;Noise shaping;Shape;Engines;Frequency conversion;Association rules;Text analysis;Performance analysis,document image processing;text analysis,script-language identification;document images;document vectorization;character images;word images;vertical component cuts;character extremum point;document degradation;document text,"Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Simulation;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Language;Models, Statistical;Natural Language Processing;Pattern Recognition, Automated;User-Computer Interface",40,,20,,21-Nov-07,,,IEEE,IEEE Journals
A Document Clustering Approach for Automatic Building of Ontologies,自動構建本體的文檔聚類方法,S. Sellah; V. Hilaire,"LE2I EA 7508, UTBM, Univ. Bourgogne Franche-Comte, Belfort, 90010, FRANCE; LE2I EA 7508, UTBM, Univ. Bourgogne Franche-Comte, Belfort, 90010, FRANCE","2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)",2-Dec-18,2018,,,220,225,"In the context of globalization, companies need to capitalize on their knowledge. The knowledge of a company is present in two forms tacit and explicit. Explicit knowledge represents all formalized information i.e all documents (pdf, words ...). Tacit knowledge is present in documents and mind of employees, this kind of knowledge is not formalized, it needs a reasoning process to discover it. In this paper, we propose a novel approach for documents clustering that is based on word clusters automatically extracted from the documents. The word concepts are considered as concepts candidates. In a second step, word clusters are used to ease the automatic building of ontologies from a given corpus. Some experiments allows for a validation of the whole approach. The chosen corpus is Reuters-21578 that is among the most used for text categorization research.",,978-1-5386-9588-3,10.1109/SNAMS.2018.8554486,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554486,Knowledge management;Ontologies;Document Clustering,Ontologies;Semantics;Buildings;Clustering algorithms;Social network services;Security;Companies,ontologies (artificial intelligence);pattern clustering;text analysis,document clustering approach;explicit knowledge;tacit knowledge;word clusters;word concepts;formalized information;automatic ontology building;text categorization research,,,,16,,2-Dec-18,,,IEEE,IEEE Conferences
Document clustering: TF-IDF approach,文件叢集：TF-IDF方法,P. Bafna; D. Pramod; A. Vaidya,"SICSR, Symbiosis International, University Pune, Maharastra, India; SICSR, Symbiosis International, University Pune, Maharastra, India; SICSR, Symbiosis International University, Pune, Maharastra, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)",24-Nov-16,2016,,,61,66,"Recent advances in computer and technology resulted into ever increasing set of documents. The need is to classify the set of documents according to the type. Laying related documents together is expedient for decision making. Researchers who perform interdisciplinary research acquire repositories on different topics. Classifying the repositories according to the topic is a real need to analyze the research papers. Experiments are tried on different real and artificial datasets such as NEWS 20, Reuters, emails, research papers on different topics. Term Frequency-Inverse Document Frequency algorithm is used along with fuzzy K-means and hierarchical algorithm. Initially experiment is being carried out on small dataset and performed cluster analysis. The best algorithm is applied on the extended dataset. Along with different clusters of the related documents the resulted silhouette coefficient, entropy and F-measure trend are presented to show algorithm behavior for each data set.",,978-1-4673-9939-5,10.1109/ICEEOT.2016.7754750,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754750,TF-IDF;entropy;silhouette coefficient;hierarchical;fuzzy k-means;clustering,Clustering algorithms;Entropy;Electronic mail;Classification algorithms;Algorithm design and analysis;Feature extraction;Symbiosis,fuzzy set theory;pattern classification;pattern clustering,document clustering;TF-IDF approach;document classification;decision making;repositories classification;real datasets;artificial datasets;NEWS 20 datasets;Reuters datasets;term frequency-inverse document frequency algorithm;fuzzy k-means;hierarchical algorithm;cluster analysis;silhouette coefficient;entropy;F-measure,,29,,34,,24-Nov-16,,,IEEE,IEEE Conferences
Optimized Query Terms Creation Based on Meta-Search and Clustering,基於元搜索和聚類的優化查詢詞創建,H. Yang; J. Chen; Y. Zhang; X. Meng,"Beijing Univ. of Posts & Telecommun., Beijing; Beijing Univ. of Posts & Telecommun., Beijing; Beijing Univ. of Posts & Telecommun., Beijing; Beijing Univ. of Posts & Telecommun., Beijing",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,2,,38,42,"Meta-search is a search engine that sends users' requests to several other search engines and/or databases and returns the results from each one. Concepts are defined in a semantic network that contains links to the original terms. This article provides a framework of three layers, meta-search layer, concept layer, concept clustering and sorting layer to guide user to refine the query terms. The first layer, meta-search layer provides the ability to get the relevant document fetched by traditional search engines, including title, brief, URL, and document. Concept layer derives concepts from retrieved documents by a concept dictionary. The last layer clusters the concepts, and provides the refined query terms for individual semantic domain of the origin query term. Then the system guides user to choose the query terms which describe options most precisely and get better results from search engines. At last the testing results prove that refined query terms can be developed and costing time is acceptable.",,978-0-7695-3305-6,10.1109/FSKD.2008.513,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666076,,Metasearch;Search engines;Web pages;Uniform resource locators;Information retrieval;Internet;Web server;Feedback;Clustering algorithms;Fuzzy systems,document handling;meta data;pattern clustering;query processing;relevance feedback;search engines;semantic networks;sorting,optimized query terms creation;meta clustering;search engine;semantic network;meta-search layer;concept layer;concept clustering;sorting layer;relevant document;semantic domain,,1,,13,,5-Nov-08,,,IEEE,IEEE Conferences
Clustering XML Documents by Combining Content and Structure,通過組合內容和結構來聚類XML文檔,G. Yongming; C. Dehua; L. Jiajin,"Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai",2008 International Symposium on Information Science and Engineering,30-Dec-08,2008,1,,583,587,"XML has become a de facto standard for data representation and exchange over the Internet. With the emergence of more and more XML documents, the clustering of XML documents has become an active research area. XML documents lie between structured data and unstructured data which describe both content and structure, so how to effectively cluster XML documents is a huge challenge. However, most of existing clustering algorithms are based on the structural similarities between XML documents and not or less take into account content of the XML documents. In this paper, we develop a novel method for measuring similarities between XML documents, which effectively combines structure and contents of the XML documents. Based on this similarity model, we adopt hierarchy clustering algorithm to cluster XML documents. The experiments show that this method gains better clustering quality.",2160-1291,978-0-7695-3494-7,10.1109/ISISE.2008.301,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732285,Clusering;XML;Extended Vector Space Model,XML;Clustering algorithms;Internet;Extraterrestrial measurements;Space technology;Books;Information science;Computer science;Data mining;Feature extraction,data structures;document handling;electronic data interchange;Internet;pattern clustering;XML,XML document clustering;de facto standard;data representation;data exchange;Internet;data structure;structural similarities;hierarchy clustering algorithm,,9,,12,,30-Dec-08,,,IEEE,IEEE Conferences
Hypergraph based clustering for document similarity using FP growth algorithm,使用FP增長算法的基於超圖的文檔相似性聚類。,N. Ramakrishnan; M. Nair J.; D. Jayaprakash; H. Ananthakrishnan; S. Rani S.,"Amrita Vishwa Vidyapeetham,Department of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Engineering,Amritapuri,India",2019 International Conference on Intelligent Computing and Control Systems (ICCS),16-Apr-20,2019,,,332,336,"Modelling multiple documents for different applications is a major field of research due to the tremendous growth in Web data. To find the document similarity, we require clustering to determine the grouping of unlabelled data. Graph models have the capability or knowledge of capturing the structural information in texts. It organizes high dimensional data in such a way that the user can effortlessly access the desired information. In this paper, we use a hypergraph with the help of an association rule mining to model a collection of text documents and find similarity between them using a hypergraph partitioning algorithm. Here we use FP-Growth algorithm to find the association relationship which is a recursive elimination scheme. We then uses a spectral clustering algorithm which uses eigenvalues and vectors which is found out from the matrices to find similar documents. Experiment shows that this algorithm gave better clusters compared to others which commonly take higher eigenvectors.",,978-1-5386-8113-8,10.1109/ICCS45141.2019.9065630,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065630,Hypergraph;Clustering;FP-Growth;Similarity,Eigenvalues and eigenfunctions;Clustering algorithms;Partitioning algorithms;Computational modeling;Data mining;Data models;Numerical models,data mining;eigenvalues and eigenfunctions;graph theory;pattern clustering;text analysis;vectors,document similarity;FP growth algorithm;Web data;graph models;high dimensional data;association rule mining;text documents;hypergraph partitioning algorithm;FP-growth algorithm;spectral clustering algorithm;hypergraph based clustering;multiple document modelling,,,,15,,16-Apr-20,,,IEEE,IEEE Conferences
Document clustering around weighted-medoids,圍繞加權中心的文檔聚類,J. Mei; L. Chen,"Division of Information Engineering, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore 637789, Republic of Singapore; Division of Information Engineering, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore 637789, Republic of Singapore","2011 8th International Conference on Information, Communications & Signal Processing",3-Apr-12,2011,,,1,5,"In this paper, we propose a new similarity-based k-partitions clustering approach, called CAWP. Given the similarities of pairs of objects in the dataset, CAWP groups these objects into K non-overlaped clusters. Each cluster is represented by multiple objects with different weights, called prototype weight. The more representative an object is with respect to a cluster, the larger prototype weight is assigned to that object in the corresponding cluster. Compared with the traditional k-medoids approach, where each cluster is represented by a single medoid or representative object, the way of using prototype weights to allow multiple objects together to describe a cluster is more appropriate in our view. Experimental study using large document datasets show that CAWP is more favorable than other existing similarity-based clustering approaches as it achieves both good effectiveness and efficiency.",,978-1-4577-0031-6,10.1109/ICICS.2011.6173606,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6173606,,Prototypes;Clustering algorithms;Wireless application protocol;Kernel;Vectors;Data mining;Approximation algorithms,document handling;pattern clustering,document clustering;weighted medoids;similarity-based k-partitions clustering;CAWP;prototype weight;traditional k-medoids approach;representative object;document datasets,,,,24,,3-Apr-12,,,IEEE,IEEE Conferences
Search tools through the glass: a story of clustering search results according to document attributes with a glance on the web,一目了然的搜索工具：有關根據文檔屬性將搜索結果歸類到網絡上的故事,S. Arastoopoor,"Ferdowsi University of Mashhad, Iran",2009 Second International Conference on the Applications of Digital Information and Web Technologies,2-Oct-09,2009,,,823,825,"Structuring search results in information retrieval, especially when we have bulky results, helps IS users finding relevant results sooner. Due to the fact that clustering is a well known way of organizing search results this paper aims at mapping different clustering methods on different types of attributes, the last part of this paper deals with current situation of search technologies on the Web, testing if they have the potential of organizing the results according to previously defined attributes. The results of this minor study shows that although clustering search results is not something new on the Web and although most of the attributes are being indexed currently, the search engines do not take the whole advantage.",,978-1-4244-4456-4,10.1109/ICADIWT.2009.5273958,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5273958,,Glass;Information retrieval;Organizing;Clustering methods;Libraries;Keyword search;Vocabulary;Performance analysis;Testing;Search engines,information retrieval;Internet;pattern clustering,clustering search results;document attributes;Web search technologies;information retrieval,,,,19,,2-Oct-09,,,IEEE,IEEE Conferences
Automatic Assignment of Topical Icons to Documents for Faster File Navigation,自動將主題圖標分配給文檔，以實現更快的文件導航,R. S. Roy; A. Singh; P. Chawla; S. Saxena; A. R. Sinha,"Saarland Inf. Campus, Max Planck Inst. for Inf., Saarbrucken, Germany; Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Adobe Syst. India, India; BlackRock Services India, India; Big Data Experience Lab., Adobe Res., India",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,1338,1345,"Several computer users neither assign names to their documents systematically nor organize them into suitable folders, making it difficult to search for relevant files when needed. While this problem can be addressed in several ways, we explore the novel approach of automated assignment of topical icons to documents in order to cue memory for faster navigation. Specifically, we overlay the currently available generic software-oriented file association icons like Acrobat, Word or Powerpoint on documents with algorithmically assigned icons that are specific to the topical content of the documents. Our pipeline method uses document clustering, significant-phrase extraction, phrase generalization and phrase vector matching for assigning icons to documents. Experimental results show that topical iconification significantly speeds up document navigation time vis-獺-vis content-based file naming, in both a controlled laboratory setup as well as in a crowdsourced study. Icons assigned by our algorithm are observed to have satisfactory inter-annotator agreement with respect to their meanings.",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.220,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270151,Document iconification;File search;User studies,Navigation;Taxonomy;Software;Electronic mail;Clustering algorithms;Task analysis;Finance,document handling;file organisation;pattern clustering,topical icons;file association icons;document clustering;phrase vector;automatic assignment;faster file navigation;content-based file naming,,,,33,,29-Jan-18,,,IEEE,IEEE Conferences
A Binarization-Free Clustering Approach to Segment Curved Text Lines in Historical Manuscripts,歷史文獻中彎曲文本行的無二值化聚類方法,A. Garz; A. Fischer; H. Bunke; R. Ingold,"DIVA, Univ. of Fribourg, Fribourg, Switzerland; CENPARMI, Concordia Univ., Montreal, QC, Canada; IAM, Univ. of Bern, Bern, Switzerland; DIVA, Univ. of Fribourg, Fribourg, Switzerland",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,1290,1294,"Text line segmentation is one of the main parts of document image analysis, it provides crucial information for automated reading, word spotting, alignment between image and transcription, or indexing of documents. Yet it remains an open problem for handwritten historical documents because of complex layouts on the one hand, such as curved and touching text lines, and binarization problems on the other hand, caused by ornaments, wrinkles, stains, holes, etc. In this paper, we propose a binarization-free clustering method for text line segmentation that is not only able to cope with touching text lines, but also with complex baseline curvature. Avoiding the assumption of straight baselines, small interest point clusters are grouped into text lines based on their local orientation. Experiments conducted on artificially distorted images of the Saint Gall database show promising results.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.261,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628822,historical documents;text line segmentation;local features;curved lines,Image segmentation;Text analysis;Databases;Context;Layout;Noise;Accuracy,document image processing;image segmentation;pattern clustering,binarization-free clustering approach;curved text line segmentation;document image analysis;historical manuscripts;automated reading;word spotting;indexing;handwritten historical documents;complex baseline curvature;straight baselines;small interest point clusters;local orientation;saint gall database,,11,,20,,15-Oct-13,,,IEEE,IEEE Conferences
Structure in on-line documents,在線文檔的結構,K. Jain; A. M. Namboodiri; J. Subrahmonia,"Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,844,848,"We present a hierarchical approach for extracting homogeneous regions in on-line documents. The problem of identifying and processing ruled and unruled tables, text and drawings is addressed. The on-line document is first segmented into regions with only text strokes and regions with both text and non-text strokes. The text region is further classified as unruled table or plain text. Stroke clustering is used to segment the non-text regions. Each nontext segment is then classified as drawing, ruled table or underlined keyword using stroke properties. The individual regions are processed and the results are assembled to identify the structure of the on-line document.",,0-7695-1263-1,10.1109/ICDAR.2001.953906,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953906,,Text recognition;Text analysis;Assembly;Handwriting recognition;Data preprocessing;Graphics;Focusing;Image segmentation;Color;Web pages,document image processing;feature extraction;image classification;image segmentation,on-line documents;extracting homogeneous regions;segmented;tables;text;drawings;text strokes;table identification;segmenting document pages;document understanding;document analysis;text recognition,,38,,15,,7-Aug-02,,,IEEE,IEEE Conferences
Fuzzy co-clustering of documents and keywords,文檔和關鍵字的模糊聚類,K. Kummamuru; A. Dhawale; R. Krishnapuram,"IBM India Res. Lab., IIT, New Delhi, India; IBM India Res. Lab., IIT, New Delhi, India; IBM India Res. Lab., IIT, New Delhi, India","The 12th IEEE International Conference on Fuzzy Systems, 2003. FUZZ '03.",25-Jun-03,2003,2,,772,777 vol.2,"Conventional clustering algorithms such as K-means and SAHN (also known as AHC) have been well studied and used in the information retrieval community for clustering text documents. More recently, efforts have been made to cluster documents and words simultaneously. The FCCM algorithm due to Oh et al. is a fuzzy clustering algorithm that maximizes the co-occurrence of categorical attributes (keywords) and the individual patterns (documents) in clusters. However, this algorithm poses certain problems when the number of documents or the number of words is very large. In this paper, we modify the FCCM algorithm so that it can be used to cluster large text corpora. Our experiments show that the modified algorithm is scalable and produces meaningful clusters. We also show the relation between FCCM and the Spherical K-Means (SKM) algorithm and introduce the Spherical Fuzzy c-Means (SFCM) algorithm.",,0-7803-7810-5,10.1109/FUZZ.2003.1206527,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206527,,Clustering algorithms;Frequency shift keying;Partitioning algorithms;Information retrieval;Bipartite graph;Text mining,information retrieval systems;information retrieval;fuzzy set theory;pattern clustering;text analysis,fuzzy clustering algorithms;document co-clustering;keyword co-clustering;conventional clustering algorithms;spherical k-means algorithm;sequential agglomerative hierarchial nonoverlapping;information retrieval community;clustering text documents;categorical multivariate data;categorical attributes;individual patterns;spherical fuzzy c-means algorithm,,80,,11,,25-Jun-03,,,IEEE,IEEE Conferences
Fuzzy clustering and relevance ranking of web search results with differentiating cluster label generation,Web搜索結果的模糊聚類和相關性排序，並區分聚類標籤生成,T. Matsumoto; E. Hung,"Department of Computing, Hong Kong Polytechnic University, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hong Kong",International Conference on Fuzzy Systems,23-Sep-10,2010,,,1,8,"This paper introduces a prototype web search results clustering engine that enhances search results by performing fuzzy clustering on web documents returned by conventional search engines, as well as ranking the results and labeling the resulting clusters. This is done using a fuzzy transduction-based clustering algorithm (FTCA), which employs a transduction-based relevance model (TRM) to generate document relevance values. These relevance values are used to cluster similar documents, rank them, and facilitate a term frequency based label generator. The membership degrees of documents to fuzzy clusters also facilitates effective detection and removal of overly similar clusters. FTCA is compared against two other established web document clustering algorithms: Suffix Tree Clustering (STC) and Lingo, which are provided by the free open source Carrot2 Document Clustering Workbench. To measure cluster quality, an extended version of the classic precision measurement is used to take into account relevance and fuzzy clustering, along with recall and F1 score. Results from testing on five different datasets show a considerable clustering quality and performance advantage over STC and Lingo in most cases.",1098-7584,978-1-4244-6921-5,10.1109/FUZZY.2010.5584771,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5584771,,Clustering algorithms;Search engines;Web search;Prototypes;Transmission line measurements;Labeling;Testing,fuzzy set theory;Internet;pattern clustering;public domain software;search engines;trees (mathematics),relevance ranking;differentiating cluster label generation;Web search results clustering engine;search engines;fuzzy transduction-based clustering algorithm;transduction-based relevance model;term frequency;Web document clustering algorithms;suffix tree clustering;Lingo;open source carrot2 document clustering workbench;precision measurement,,6,,16,,23-Sep-10,,,IEEE,IEEE Conferences
High-level parallelisation in a database cluster: a feasibility study using document services,數據庫集群中的高級並行化：使用文檔服務的可行性研究,T. Grabs; K. Bohm; H. -. Schek,"Swiss Federal Inst. of Technol., Zurich, Switzerland; NA; NA",Proceedings 17th International Conference on Data Engineering,7-Aug-02,2001,,,121,130,"Our concern is the design of a scalable infrastructure for complex application services. We want to find out if a cluster of commodity database systems is well-suited as such an infrastructure. To this end, we have carried out a feasibility study based on document services, e.g. document insertion and retrieval. We decompose a service request into short parallel database transactions. Our system, implemented as an extension of a transaction processing monitor, routes the short transactions to the appropriate database systems in the cluster. Routing depends on the data distribution that we have chosen. To avoid bottlenecks, we distribute document functionality, such as term extraction, over the cluster. Extensive experiments show the following. (1) A relatively small number of components - for example eight components $already suffices to cope with high workloads of more than 100 concurrently active clients. (2) Speedup and throughput increase linearly for insertion operations when increasing the cluster size. These observations also hold when bundling service invocations into transactions at the semantic layer. A specialized coordinator component then implements semantic serializability and atomicity. Our experiments show that such a coordinator has minimal impact on CPU resource consumption and on response times.",1063-6382,0-7695-1001-9,10.1109/ICDE.2001.914820,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=914820,,Transaction databases;Database systems;Interconnected systems;Scalability;Routing;Data mining;Throughput;Atomic layer deposition;Delay;Data structures,transaction processing;workstation clusters;parallel databases;document handling,high-level parallelisation;database cluster;feasibility study;document services;scalable infrastructure;complex application services;commodity database systems;document insertion;document retrieval;service request decomposition;short parallel database transactions;transaction processing monitor;transaction routing;data distribution;bottlenecks;distributed document functionality;term extraction;workloads;concurrently active clients;speedup;throughput;insertion operations;cluster size;service invocation bundling;semantic layer;specialized coordinator component;semantic serializability;atomicity;CPU resource consumption;response times,,6,,28,,7-Aug-02,,,IEEE,IEEE Conferences
A Hierarchical Text Clustering Algorithm with Cognitive Situation Dimensions,具有認知情況維度的層次文本聚類算法,Y. Guo; Z. Shao; N. Hua,"Dept. of Comput. Sci. & Eng., East China Univ. of Sci. & Technol., Shanghai; Dept. of Comput. Sci. & Eng., East China Univ. of Sci. & Technol., Shanghai; Telecommun. Eng. Inst., Air Force Eng. Univ., Xi'an",2009 Second International Workshop on Knowledge Discovery and Data Mining,2-Feb-09,2009,,,11,14,"Text clustering is an important task of text mining. The purpose of text clustering is grouping similar text documents together efficiently to meet human interests in information searching and understanding. The procedure of clustering should involve a cognitive process of text understanding or comprehension.This paper introduces an innovative research effort, CogHTC, a hierarchical text clustering algorithm, inspired by cognitive situation models. CogHTC extracts representative features from four elaborately selected cognitive situation dimensions with consideration of the clustering efficiency. The experimental results testified good performance of CogHTC, and revealed that the clustering results of CogHTC are class or domain sensitive, and CogHTC performed better on cross-class clustering than inner- class clustering.",,978-0-7695-3543-2,10.1109/WKDD.2009.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4771866,Hierarchical;Text Clustering;Cognitive;Situation Dimensions,Clustering algorithms;Data mining;Humans;Data engineering;Knowledge engineering;Frequency;Feature extraction;Computer science;Text mining;Testing,cognitive systems;data mining;pattern clustering;query formulation;text analysis,hierarchical text clustering;cognitive situation dimensions;text mining;text documents grouping;information searching;information understanding;cross-class clustering;inner-class clustering,,2,,18,,2-Feb-09,,,IEEE,IEEE Conferences
Multitype features coselection for Web document clustering,Web文檔集群的多類型功能共選,Shen Huang; Zheng Chen; Yong Yu; Wei-Ying Ma,"Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., China; NA; NA; NA",IEEE Transactions on Knowledge and Data Engineering,27-Feb-06,2006,18,4,448,459,"Feature selection has been widely applied in text categorization and clustering. Compared to unsupervised selection, supervised feature selection is more successful in filtering out noise in most cases. However, due to a lack of label information, clustering can hardly exploit supervised selection. Some studies have proposed to solve this problem by ""pseudoclass."" As empirical results show, this method is sensitive to selection criteria and data sets. In this paper, we propose a novel feature coselection for Web document clustering, which is called multitype features coselection for clustering (MFCC). MFCC uses intermediate clustering results in one type of feature space to help the selection in other types of feature spaces. Our experiments show that for most selection criteria, MFCC reduces effectively the noise introduced by ""pseudoclass,"" and further improves clustering performance.",1558-2191,,10.1109/TKDE.2006.1599384,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1599384,Web mining;clustering;feature evaluation and selection.,Mel frequency cepstral coefficient;Text categorization;Noise reduction;Uniform resource locators;Filtering;Machine learning;Data mining;Information theory;Clustering algorithms;Text mining,document handling;Internet;classification;text analysis;data mining;learning (artificial intelligence);feature extraction;pattern clustering,multitype features coselection;Web document clustering;text categorization;text clustering;unsupervised feature selection;supervised feature selection;Web mining,,18,,23,,27-Feb-06,,,IEEE,IEEE Journals
Research on incremental clustering,增量聚類研究,Y. Liu; Q. Guo; L. Yang; Y. Li,"School of computer science and technology, Henan polytechnic university, Jiaozuo, Henan Province, China 454000; Academic publishing center, Henan polytechnic university, Jiaozuo, Henan Province, China 454000; School of computer science and technology, Henan polytechnic university, Jiaozuo, Henan Province, China 454000; School of computer science and technology, Henan polytechnic university, Jiaozuo, Henan Province, China 454000","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)",17-May-12,2012,,,2803,2806,"Currently, incremental document clustering is one the most effective techniques to organize documents in an unsupervised manner for many Web applications. This paper summarizes the research actuality and new progress in incremental clustering algorithm in recent years. First, some representative algorithms are analyzed and generalized from such aspects as algorithm thinking, key technique, advantage and disadvantage. Secondly, we select four typical clustering algorithms and carry out simulation experiments to compare their clustering quality from both accuracy and efficiency. The work in this paper can give a valuable reference for incremental clustering research.",,978-1-4577-1415-3,10.1109/CECNet.2012.6202079,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202079,incremental clustering;Web mining;algorithms;experiments,Clustering algorithms;Algorithm design and analysis;Clustering methods;Entropy;Internet;Classification algorithms;Databases,data mining;document handling;Internet;pattern clustering,incremental document clustering;documents organization;Web applications;representative algorithms;algorithm thinking;key technique;simulation experiments;clustering quality,,3,,20,,17-May-12,,,IEEE,IEEE Conferences
Word spotting using clustering on extracted DCT and DWT features,在提取的DCT和DWT功能上使用聚類進行單詞發現,H. A. Niaz; U. Akram; U. Akbar,"Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences and Technology, Islamabad, Pakistan; Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences and Technology, Islamabad, Pakistan; Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences and Technology, Islamabad, Pakistan",2018 International Conference on Engineering and Emerging Technologies (ICEET),19-Apr-18,2018,,,1,4,"Digitization of documents and books is only effective if it is complemented by a search mechanism allowing users retrieve the desired content. This led to a tremendous research in Optical Character Recognition (OCR) systems which convert document images into text allowing search and retrieval facility. In some cases, recognition of text is very challenging due to complexity of script on which OCR systems are fail. This work present a indexing and retrieval based word spotting system for digitized English documents. The document image with English text is segmented into ligatures and each ligature is represented by a set of features. Features are extracted using DCT and DWT. Clustering of ligature is then carried out to group ligature into cluster. An index file is maintained for each cluster which stores all occurrences (locations) of the ligature in the given document.",,978-1-5386-3205-5,10.1109/ICEET1.2018.8338629,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338629,Optical Character Recognition;Discrete Cosine Transform DCT;Discrete Wavelet Transform DWT;Document image;Segmentation,,discrete cosine transforms;discrete wavelet transforms;document image processing;feature extraction;image retrieval;image segmentation;optical character recognition;pattern clustering;search problems,group ligature;search mechanism;document image;retrieval facility;OCR systems;retrieval based word spotting system;digitized English documents;English text;optical character recognition systems;DCT feature extraction;DWT feature extraction,,,,22,,19-Apr-18,,,IEEE,IEEE Conferences
Page Object Detection from PDF Document Images by Deep Structured Prediction and Supervised Clustering,通過深度結構化預測和監督聚類從PDF文檔圖像中檢測頁面對象,X. Li; F. Yin; C. Liu,"National Laboratory of Pattern Recognition, Institute of Automation of Chinese Academy of Sciences, Beijing, 100190, P.R. China; National Laboratory of Pattern Recognition, Institute of Automation of Chinese Academy of Sciences, Beijing, 100190, P.R. China; National Laboratory of Pattern Recognition, Institute of Automation of Chinese Academy of Sciences, Beijing, 100190, P.R. China",2018 24th International Conference on Pattern Recognition (ICPR),29-Nov-18,2018,,,3627,3632,"Page object detection in document images remains a challenge because the page objects are diverse in scale and aspect ratio, and an object may contain largely apart components. In this paper, we propose a hybrid method combining deep structured prediction and supervised clustering to detect formulas, tables and figures in PDF document images within a unified framework. The primitive region proposals extracted from each column region are classified and clustered with conditional random field (CRF) based graphical models which can integrate both local and contextual information. Both the unary and pairwise potentials of CRFs are formulated as convolutional neural networks (CNNs) to better exploit spatial contextual information. The CRF for clustering predicts the linked/cut label of between-region links. After CRF inference, the line regions of same class within a cluster are grouped into a page object. The state-of-the-art performance obtained on the public available ICDAR2017 POD competition dataset demonstrates the effectiveness and superiority of the nronosed method.",1051-4651,978-1-5386-3788-3,10.1109/ICPR.2018.8546073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546073,page object detection;supervised clustering;deep learning;structured prediction,Image segmentation;Object detection;Proposals;Portable document format;Convolutional neural networks;Task analysis,document image processing;feature extraction;image classification;neural nets;object detection;pattern clustering,CRF inference;spatial contextual information;PDF document imaging;conditional random field;graphical models;CNN;convolutional neural networks;public available ICDAR2017 POD competition dataset;between-region links;primitive region proposals;supervised clustering;deep structured prediction;page object detection,,4,,22,,29-Nov-18,,,IEEE,IEEE Conferences
Discriminative clustering of text documents,文本文檔的歧視性聚類,J. Peltonen; J. Sinkkonen; S. Kaski,"Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland; Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland; Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland","Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",21-May-03,2002,4,,1956,1960 vol.4,"Vector-space and distributional methods for text document clustering are discussed. Discriminative clustering, a recently proposed method, uses external data to find task-relevant characteristics of the documents, yet the clustering is defined even with no external data. We introduce a distributional version of discriminative clustering that represents text documents as probability distributions. The methods are tested in the task of clustering scientific document abstracts, and the ability of the methods to predict an independent topical classification of the abstracts is compared. The discriminative methods found topically more meaningful clusters than the vector space and distributional clustering models.",,981-04-7524-1,10.1109/ICONIP.2002.1199015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1199015,,Clustering methods;Testing;Large scale integration;Kernel;Neural networks;Probability distribution;Abstracts;Indexing;Extraterrestrial measurements;Clustering algorithms,text analysis;data mining;pattern clustering;probability;classification;information retrieval;bibliographic systems,discriminative text document clustering;vector space methods;distributional methods;task-relevant characteristics;probability distributions;scientific document abstract clustering;data mining;information retrieval,,3,,9,,21-May-03,,,IEEE,IEEE Conferences
Enhancing Digital Forensic Analysis Using Memetic Algorithm Feature Selection Method for Document Clustering,使用模因算法特徵選擇方法增強數字取證分析的文檔聚類,I. Al-Jadir; K. W. Wong; C. C. Fung; H. Xie,"Sch. of Eng. & Inf. Technol., Murdoch Univ., Perth, WA, Australia; Sch. of Eng. & Inf. Technol., Murdoch Univ., Perth, WA, Australia; Sch. of Eng. & Inf. Technol., Murdoch Univ., Perth, WA, Australia; Sch. of Eng. & Inf. Technol., Murdoch Univ., Perth, WA, Australia","2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-Jan-19,2018,,,3673,3678,Text clustering is an effective way that helps crime investigation through grouping of crime-related documents. This paper proposes a Memetic Algorithm Feature Selection (MAFS) approach to enhance the performance of document clustering algorithms used to partition crime reports and criminal news as well as some benchmark text datasets. Two clustering algorithms have been selected to demonstrate the effectiveness of the proposed MAFS method; they are the k-means and Spherical k-means (Spk). The reason behind using these clustering methods is to observe the performance of these algorithms before and after applying a hybrid FS that uses a Memetic scheme. The proposed MAFS method combines a Genetic Algorithm-based wrapper FS with the Relief-F filter. The performance evaluation was based on the clustering outcomes before and after applying the proposed MAFS method. The test results showed that the performance of both k-means and spk improved after the MAFS.,2577-1655,978-1-5386-6650-0,10.1109/SMC.2018.00621,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616618,clustering;feature;Genetic;Memetic,Clustering algorithms;Feature extraction;Memetics;Partitioning algorithms;Australia;Benchmark testing;Digital forensics,data mining;digital forensics;feature selection;genetic algorithms;pattern clustering;search problems;text analysis,crime related documents;genetic algorithm;spherical k-means;performance evaluation;clustering methods;MAFS method;benchmark text datasets;criminal news;crime reports;document clustering algorithms;crime investigation;text clustering;memetic algorithm feature selection method;digital forensic analysis,,,,28,,17-Jan-19,,,IEEE,IEEE Conferences
Development and study of clustering algorithms for large sets of data,大數據集聚類算法的研究與開發,Y. Stekh; M. Lobur; F. M. E. Sardieh; M. Dombrova; V. Artsibasov,NA; NA; NA; NA; NA,2011 11th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM),7-Apr-11,2011,,,202,204,This paper focuses on document clustering algorithms that build hierarchical solutions. In this paper is evaluate the performance of different criterion functions for the problem of clustering documents.,,978-966-2191-17-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5744433,,Clustering algorithms;Partitioning algorithms;Data mining;Algorithm design and analysis;Length measurement;Merging;Equations,document handling;pattern clustering,data clustering algorithm;document clustering algorithm,,,,14,,7-Apr-11,,,IEEE,IEEE Conferences
A social spider optimization approach for clustering text documents,用於文本文檔聚類的社交蜘蛛優化方法,T. R. Chandran; A. V. Reddy; B. Janet,"Dept. of Computer Applications, NIT, Trichy, India; Dept. of Computer Applications, NIT, Trichy, India; Dept. of Computer Applications, NIT, Trichy, India","2016 2nd International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB)",11-Aug-16,2016,,,22,26,"Recently, as clustering problem can be mapped to optimization problem, evolutionary optimization techniques have been used by researchers to improve accuracy and efficiency. Evolutionary techniques are stochastic general purpose methods for solving optimization problems. Swarm Intelligence is one such technique that deals with aggregative behavior of swarms and their complex interactions without any supervision. But, because of its robustness, Swarm intelligence paradigm seems to be even more attractive. We proposed a swarm intelligence algorithm called social spider optimization for text document clustering. This algorithm uses cooperative intelligent behavior of social spiders. Depending on its gender, each spider tends to reproduce a specialized behavior. It also helpsin reducing premature convergence and local minima problems considerably in text document clustering. It is compared with K-means clustering technique and found to give better results.",,978-1-4673-9745-2,10.1109/AEEICB.2016.7538275,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538275,Evolutionary optimization techniques;K-means;Text Document Clustering;Swarm Intelligence;Social Spider Optimization,Clustering algorithms;Mathematical model;Optimization;Vibrations;Sociology;Statistics;Particle swarm optimization,evolutionary computation;pattern clustering;swarm intelligence;text analysis,social spider optimization approach;text document clustering;evolutionary optimization techniques;stochastic general purpose methods;swarm intelligence;swarm behavior;cooperative intelligent behavior;convergence;local minima problems;K-means clustering technique,,3,,18,,11-Aug-16,,,IEEE,IEEE Conferences
Hough technique for bar charts detection and recognition in document images,用於在文檔圖像中檢測和識別條形圖的霍夫技術,Yan Ping Zhou; Chew Lim Tan,"Sch. of Comput., Nat. Univ. of Singapore, Singapore; NA",Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101),6-Aug-02,2000,2,,605,608 vol.2,"Charts are common graphic representation for scientific data in technical and business papers. We present a robust system for detecting and recognizing bar charts. The system includes three stages, preprocessing, detection and recognition. The kernel algorithm in detection is newly developed modified probabilistic Hough transform algorithm for parallel lines clusters detection. The main algorithms in recognition are bar pattern reconstruction and text primitives grouping in the Hough space which are also original. The experiments show the system can also recognize slant bar charts, or even hand-drawn charts.",1522-4880,0-7803-6297-7,10.1109/ICIP.2000.899506,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=899506,,Image recognition;Graphics;Clustering algorithms;Business;Robustness;Image reconstruction;Layout;Filters;Kernel;Pattern recognition,Hough transforms;image recognition;image reconstruction;document image processing;probability;pattern clustering,Hough technique;bar charts detection;bar charts recognition;document images;graphic representation;scientific data;technical papers;business papers;robust system;preprocessing;kernel algorithm;modified probabilistic Hough transform algorithm;parallel lines clusters detection;bar pattern reconstruction;text primitives grouping;Hough space;slant bar charts;hand-drawn charts;document image processing,,8,,11,,6-Aug-02,,,IEEE,IEEE Conferences
"Web document clustering based on a new niching Memetic Algorithm, Term-Document Matrix and Bayesian Information Criterion",基於一種新的小模數算法，術語文檔矩陣和貝葉斯信息準則的Web文檔聚類,C. Cobos; C. Montealegre; M. Mej穩a; M. Mendoza; E. Le籀n,University of Cauca; University of Cauca; University of Cauca; University of Cauca; National University of Colombia,IEEE Congress on Evolutionary Computation,27-Sep-10,2010,,,1,8,"This paper introduces a new description-centric algorithm for web document clustering based on Memetic Algorithms with Niching Methods, Term-Document Matrix and Bayesian Information Criterion. The algorithm defines the number of clusters automatically. The Memetic Algorithm provides a combined global and local strategy for a search in the solution space and the Niching methods to promote diversity in the population and prevent the population from converging too quickly (based on restricted competition replacement and restrictive mating). The Memetic Algorithm uses the K-means algorithm to find the optimum value in a local search space. Bayesian Information Criterion is used as a fitness function, while FP-Growth is used to reduce the high dimensionality in the vocabulary. This resulting algorithm, called WDC-NMA, was tested with data sets based on Reuters-21578 and DMOZ, obtaining promising results (better precision results than a Singular Value Decomposition algorithm). Also, it was also then initially evaluated by a group of users.",1941-0026,978-1-4244-6911-6,10.1109/CEC.2010.5586016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586016,,Clustering algorithms;Partitioning algorithms;Memetics;Classification algorithms;Heuristic algorithms;Algorithm design and analysis;Complexity theory,Bayes methods;document handling;genetic algorithms;information retrieval;Internet;matrix algebra;singular value decomposition;vocabulary,Web document clustering;niching memetic algorithm;term-document matrix;Bayesian information criterion;description-centric algorithm;niching methods;k-means algorithm;search space;fitness function;FP-growth;vocabulary;WDC-NMA;Reuters-21578;DMOZ;singular value decomposition algorithm,,10,,35,,27-Sep-10,,,IEEE,IEEE Conferences
Label-based semi-supervised fuzzy co-clustering for document categoraization,基於標籤的半監督模糊共聚文檔分類,Y. Yan; L. Chen,"School of Electric and Electronic Engineering, Nanyang Technological University, Singapore, 639798; School of Electric and Electronic Engineering, Nanyang Technological University, Singapore, 639798","2011 8th International Conference on Information, Communications & Signal Processing",3-Apr-12,2011,,,1,5,"Semi-supervised clustering uses a small amount of labeled data to aid and bias the clustering of unlabeled data. In this paper the use of labeled data at the initial state, as well as the use of the constraints generated from the labels during the clustering process is explored. We formulate the clustering process as a constrained optimization problem, and propose a novel semi-supervised fuzzy co-clustering algorithm which incorporated with a few category labels to handle large overlapping text corpus. Simulations on a few large benchmark datasets demonstrate the strength and potentials of this new approach in terms of accuracy, stability and efficiency with limited labels, compared with some existing label-based semi-supervised clustering algorithms.",,978-1-4577-0031-6,10.1109/ICICS.2011.6173605,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6173605,semi-supervised clustering;class labels;fuzzy co-clustering;document categorization,Clustering algorithms;Accuracy;Complexity theory;Benchmark testing;Partitioning algorithms;Algorithm design and analysis;Optimization,data analysis;fuzzy set theory;learning (artificial intelligence);optimisation;pattern clustering;text analysis,label-based semi-supervised fuzzy co-clustering;document categorization;labeled data;unlabeled data;constrained optimization problem;large overlapping text corpus;machine learning technique,,2,,12,,3-Apr-12,,,IEEE,IEEE Conferences
Evaluation of Gist Operator for Document Image Retrieval,用於文檔圖像檢索的Gist運算符評估,F. Alaei; A. Alaei; U. Pal; M. Blumenstein,"Sch. of ICT, Griffith Univ., Griffith, NSW, Australia; Sch. of ICT, Griffith Univ., Griffith, NSW, Australia; CVPR Unit, Indian Stat. Inst., Kolkata, India; Univ. of Technol., Sydney, NSW, Australia",2018 13th IAPR International Workshop on Document Analysis Systems (DAS),25-Jun-18,2018,,,369,374,"As digitised documents normally contain a large variety of structures, a page segmentation- and layout-free method for document image retrieval is preferable. In this research work, therefore, wavelet transform as a transform-based approach is initially used to provide different under-sampled images from the original image. Then, Gist operator, as a feature extraction technique, is employed to extract a set of global features from the original image as well as the sub-images obtained from the wavelet transform. Moreover, the column-wise variances of the values in each sub-image are computed and they are then concatenated to obtain another set of features. Considering each feature set, locality-sensitive hashing is employed to compute similarity distances between a query and the document images in the database. Finally, a classifier fusion technique using the mean function is taken into account to provide a document image retrieval result. The combination of these features and a clustering score fusion strategy provides higher document image retrieval accuracy. Two different databases of the document image are considered for experimentation. The results obtained from the experimental study are detailed and the results are encouraging.",,978-1-5386-3346-5,10.1109/DAS.2018.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395224,document image retrieval;wavelet transform;Gist operator;locality-sensitive hashing,Feature extraction;Image retrieval;Discrete wavelet transforms;Knowledge based systems,document image processing;feature extraction;image retrieval;pattern clustering;wavelet transforms,Gist operator;digitised documents;transform-based approach;feature extraction technique;feature set;document image retrieval;page segmentation- and layout-free method;wavelet transform;column-wise variances;locality-sensitive hashing;classifier fusion technique;clustering score fusion strategy,,1,,33,,25-Jun-18,,,IEEE,IEEE Conferences
Clustering XML Search Results Based on Content and Structure Similarity,基於內容和結構相似性的XML搜索結果聚類,Z. Min-Juan; W. Chang-Xuan; L. De-Xi; J. Xian-Pei,"Sch. of Inf. & Technol., Jiangxi Univ. of Finance & Econ., Nanchang, China; Sch. of Inf. & Technol., Jiangxi Univ. of Finance & Econ., Nanchang, China; Sch. of Inf. & Technol., Jiangxi Univ. of Finance & Econ., Nanchang, China; Sch. of Inf. & Technol., Jiangxi Univ. of Finance & Econ., Nanchang, China",2011 Fifth International Conference on Management of e-Commerce and e-Government,1-Dec-11,2011,,,10,14,"Clustering XML search results is an effective way to improve performance. However, the key problem is how to measure similarity between XML documents. In this paper, we propose a semantic similarity measure method combining content with structure, in which a variety of XML document features, including term element frequency, term inverse element frequency, semantic weight of tag label and level information of the term, are analyzed and applied for computing the similarity between XML documents. In addition, two new performance evaluation methodology, namely ClusterRatio_Relevant and DocuRatio_Relevant, for clustering quality are introduced motivated by the observations of relevant documents distribution and the fact that collection has no classification information. Experiment results show that proposed similarity method(CAS measure)outperforms traditional document clustering(CO measure) in ClusterRatio_Relevant and DocuRatio_Relevant and produces better clustering quality.",,978-1-4577-1659-1,10.1109/ICMeCG.2011.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092622,XML Clustering;tag weight;node level;relevant cluster ratio;relevant document distribution ratio,XML;Semantics;Frequency measurement;Educational institutions;Weight measurement;Performance evaluation;Multimedia systems,document handling;pattern classification;pattern clustering;XML,XML search results clustering;content similarity;structure similarity;XML documents;ClusterRatio_Relevant;DocuRatio_Relevant;classification information;documents distribution,,,,16,,1-Dec-11,,,IEEE,IEEE Conferences
A Novel Text Clustering Method Based on TGSOM and Fuzzy K-Means,基於TGSOM和模糊K-Means的文本聚類新方法,J. Hu; C. Xiong; J. Shu; X. Zhou; J. Zhu,"Dept. of Comput. Sci., HuaZhong Normal Univ., Wuhan; NA; NA; NA; NA",2009 First International Workshop on Education Technology and Computer Science,26-May-09,2009,1,,26,30,"According to the high-dimensional sparse features of the storage of the textual document, and defects existing in the clustering methods which have already studied by now and some other problems, an effective text clustering approach (short for TGSOM-FS-FKM) based on tree-structured growing self-organizing maps (TGSOM) and fuzzy k-means (FKM) is proposed. It firstly makes preprocess of texts, and filter the majority of noisy words by using unsupervised feature selection method. Then it used TGSOM to execute the first clustering to get the rough classification of texts, and to get the initial clustering number and each textpsilas category. And then introduced LSA theory to improve the precision of clustering and reduce the dimension of feature vector. After that it used TGSOM to execute the second clustering to get the moreprecise clustering result, and used supervised feature selection method to select feature items. Finally, it used FKM to cluster the result set. In the experiment, it remained the same number of feature items.Experimental results indicate that TGSOM-FS-FKM clustering excels to other clustering method such as DSOM-FS-FCM, and the precision is better than DSOM-FCM, DFKCN and FDMFC clustering.",,978-1-4244-3581-4,10.1109/ETCS.2009.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4958717,-tree-structured growing self-organizing maps;Fuzzy K-Means;text clustering;text clustering flow model,Clustering methods;Self organizing feature maps;Computer science;Computer science education;Educational technology;Filters;Text categorization;Neurons;Text processing;Functional analysis,fuzzy set theory;pattern clustering;self-organising feature maps;text analysis,text clustering method;fuzzy k-means;TGSOM;textual document;tree-structured growing self-organizing maps;unsupervised feature selection method;DSOM-FS-FCM;DSOM-FCM;DFKCN;FDMFC,,,,8,,26-May-09,,,IEEE,IEEE Conferences
"libcrn, an Open-Source Document Image Processing Library",libcrn，一個開源文檔圖像處理庫,Y. Leydier; J. Duong; S. Br矇s; V. ?glin; F. Lebourgeois; M. Tola,"CoReNum, France; CoReNum, France; LIRIS, Univ. de Lyon, Lyon, France; LIRIS, Univ. de Lyon, Lyon, France; LIRIS, Univ. de Lyon, Lyon, France; LIRIS, Univ. de Lyon, Lyon, France",2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR),16-Jan-17,2016,,,211,215,"In this paper we introduce libcrn, a multiplatform open-source document image processing library aimed at researchers and companies. It is written in C++11 and has a non-contaminating license that makes it available for use in any project without legal constraints. The features include low-level image processing (color format conversion, binarization, convolution, PDE...), document images specific tools (connected components extraction, recursive block description, PDF export...), maths (matrix arithmetics, linear algebra, GMMs, equation solvers...), classification and clustering (kNN, k-means, HMMs...). The API is comprehensively documented and libcrn's architecture follows modern C++ guidelines to facilitate the handling of the library and enforce its safe usage. A sample OCR, which is only 30 lines long, is described to illustrate libcrn's scope of possibilities.",2167-6445,978-1-5090-0981-7,10.1109/ICFHR.2016.0049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814065,document image processing;open source;library;toolbox,Feature extraction;Libraries;Shape;Tensile stress;Document image processing;C++ languages,application program interfaces;C++ language;document image processing;libraries;public domain software,libcrn;open-source document image processing library;C++ guideline;application program interface;API,,,,18,,16-Jan-17,,,IEEE,IEEE Conferences
Technique of cluster validity for Text Mining,文本挖掘的聚類有效性技術,G. Chernyshova; G. Smorodin; A. Ovchinnikov,"Dept. of Information System in Economics, Saratov Socio-Economic Institute, Russia; EMC Academic Alliance, St. Petersburg, Russia; Dept. of Information System in Economics, Saratov Socio-Economic Institute, Russia",2016 6th International Conference - Cloud System and Big Data Engineering (Confluence),9-Jul-16,2016,,,337,340,The purpose of this article is to present the approach to Text Mining using special software. Authors consider possibility of using of k-means with different similarity measures and cluster validity evaluation. The authors have suggested k-means method with Bregman divergence for text documents clustering. Results show it is efficient in comparison of similar methods.,,978-1-4673-8203-8,10.1109/CONFLUENCE.2016.7508139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508139,Text Mining;k-means clustering;cluster validity,Indexes;Clustering algorithms;Measurement;Algorithm design and analysis;Numerical models;Text mining;Software,data mining;pattern clustering;text analysis,cluster validity technique;text mining;similarity measures;cluster validity evaluation;k-means method;Bregman divergence;text document clustering,,,,8,,9-Jul-16,,,IEEE,IEEE Conferences
Speaker Clustering by Co-optimizing Deep Representation Learning and Cluster Estimation,通過共同優化深度表示學習和聚類估計實現說話人聚類,Y. Li; W. Wang; M. Liu; Z. Jiang; Q. He,"School of Electronic and Information Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, 510640 (e-mail: eeyxli@scut.edu.cn); School of Electronic and Information Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: wcwang123@163.com); School of Electronic and Information Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: mlliu123@163.com); School of Electronic and Information Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: zjjiang123@163.com); School of Electronic and Information Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: qhhe123@163.com)",IEEE Transactions on Multimedia,,2020,PP,99,1,1,"Speaker clustering is a task to merge speech segments uttered by the same speaker into a single cluster, which is an effective tool for alleviating the management of massive amount of audio documents. In this paper, we present a work for co-optimizing the two main steps of speaker clustering, namely, feature learning and cluster estimation. In our method, the deep representation feature is learned by a deep convolutional autoencoder network (DCAN), while the cluster estimation is realized by a softmax layer that is combined with the DCAN. We devise an integrated loss function to simultaneously minimize the reconstruction loss (for deep representation learning) and the clustering loss (for cluster estimation). Many state-of-the-art audio features and clustering methods are evaluated on experimental datasets selected from two publicly available speech corpora (the AISHELL-2 and the VoxCeleb1). The results show that the proposed method exceeds other speaker clustering methods in regard to the normalized mutual information (NMI) and the clustering accuracy (CA). Additionally, the proposed deep representation feature outperforms other features that were widely used in previous works, in terms of both NMI and CA.",1941-0077,,10.1109/TMM.2020.3024667,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201020,Speaker clustering;deep representation;deep convolutional autoencoder network;audio document analysis,Estimation;Feature extraction;Clustering methods;Clustering algorithms;Decoding;Neural networks;Speaker recognition,,,,,,,,21-Sep-20,,,IEEE,IEEE Early Access Articles
An extractive text summarization technique for Bengali document(s) using K-means clustering algorithm,使用K-means聚類算法的孟加拉文文檔提取文本摘要技術,S. Akter; A. S. Asa; M. P. Uddin; M. D. Hossain; S. K. Roy; M. I. Afjal,"Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh; Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh; Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh; Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh; Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh; Faculty of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University (HSTU), Dinajpur-5200, Bangladesh","2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",3-Apr-17,2017,,,1,6,"Text summarization, a field of data mining, is very important for developing various real-life applications. Many techniques have been developed for summarizing English text(s). But, a few attempts have been made for Bengali text because of its some multifaceted structure. This paper presents a method for text summarization which extracts important sentences from a single or multiple Bengali documents. The input document(s) should be pre-processed by tokenization, stemming operation etc. Then, word score is calculated by Term-Frequency/Inverse Document Frequency (TF/IDF) and sentence score is determined by summing up its constituent words' scores with its position. Cue and skeleton words have also been considered to calculate the sentence score. For single or multiple documents, K-means clustering algorithm has been applied to produce the final summary. The experimental result shows satisfactory outputs in comparison to the existing approaches possessing linear run time complexity.",,978-1-5090-6004-7,10.1109/ICIVPR.2017.7890883,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890883,data mining;text summarization;extractive summarization;bengali document(s) summarization;TF*IDF;K-means clustering algorithm,Data mining;Clustering algorithms;Feature extraction;Skeleton;Databases;Big Data;Natural languages,data mining;pattern clustering;text analysis,extractive text summarization;Bengali documents;k-means clustering algorithm;data mining;term frequency inverse document frequency;TF-IDF,,7,,25,,3-Apr-17,,,IEEE,IEEE Conferences
A comparison of binarization methods for historical archive documents,歷史檔案文件二值化方法的比較,J. He; Q. D. M. Do; A. C. Downton; J. H. Kim,"Dept. of Electron. Syst. Eng., Essex Univ., Colchester, UK; Dept. of Electron. Syst. Eng., Essex Univ., Colchester, UK; NA; NA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,538,542 Vol. 1,"This paper compares several alternative binarization algorithms for historical archive documents, by evaluating their effect on end-to-end word recognition performance in a complete archive document recognition system utilising a commercial OCR engine. The algorithms evaluated are: global thresholding; Niblack's and Sauvola's algorithms; adaptive versions of Niblack's and Sauvola's algorithms; and Niblack's and Sauvola's algorithms applied to background removed images. We found that, for our archive documents, Niblack's algorithm can achieve better performance than Sauvola's (which has been claimed as an evolution of Niblack's algorithm), and that it also achieved better performance than the internal binarization provided as part of the commercial OCR engine.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575603,,Optical character recognition software;Clustering algorithms;Engines;Image segmentation;Pixel;Text analysis;Image color analysis;Image recognition;Pursuit algorithms;Image converters,document image processing;character recognition;word processing;history,binarization methods;historical archive documents;end-to-end word recognition;commercial OCR engine;global thresholding;Niblack algorithm;Sauvola algorithm,,56,,5,,16-Jan-06,,,IEEE,IEEE Conferences
Checking the Statutes in Chinese Judgment Document Based on Editing Distance Algorithm,基於編輯距離算法的中國裁判文書法規檢查,Y. Yang; Y. Feng; J. Ge; Y. Zhou; J. Zeng; C. Li; B. Luo,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",2017 14th Web Information Systems and Applications Conference (WISA),9-Apr-18,2017,,,197,200,"With the continuous advancement of the informatization of the Chinese People's Court, the court's view on the extraction and application of information has not only been on the structured data, but also for the semi-structured and unstructured data. In the process of in-depth study of the judgment document, many cases require the collection of the document result as an important data dimension, and the key is that the statute is the core of the whole result, so the integrity and correctness of the statute obtained has played a key role for the process of the judgment document processing. However, in the process of writing a specific judgment document, the same statute has different string forms due to the diversity of writing, which leads directly to the error data source. Comparing the editing distance between the strings can judge the similarity of them to a certain extent. Therefore, an automatic method based on the editing distance algorithm is devised, which constructs the disparity model between different statutes strings, to obtain the standardized writing of the same type data. Using this method can remove the non-standard writing of statutes, and ultimately access to the standard statutes collection. This method has a higher efficiency than the method of enumerating all the writing circumstances, which needs the manual participation, additional data storage and update.",,978-1-5386-4806-3,10.1109/WISA.2017.1,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8332614,statute;standardization;checking;editing distance;Chinese judgment document,Writing;Databases;Clustering algorithms;Data mining;Standardization;Heuristic algorithms;Length measurement,document handling;information retrieval;law administration,type data;nonstandard writing;standard statutes collection;Chinese judgment document;editing distance algorithm;Chinese People's Court;structured data;unstructured data;document result;judgment document processing;error data source;standardized writing;data storage;data dimension,,,,16,,9-Apr-18,,,IEEE,IEEE Conferences
A novel method to summarize and retrieve text documents using text feature extraction based on ontology,基於本體的文本特徵提取總結和檢索文本文檔的新方法,A. R. Patil; A. A. Manjrekar,"Computer science and technology department, Department of technology, Kolhapur, India; Computer science and technology department, Department of technology, Kolhapur, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",9-Jan-17,2016,,,1256,1260,"Data retrieval is a key process of acquiring information as per requirement. Now days, the necessity of proper information has increased. The most basic tools which provide this service are browser. It traverses the data as per user's query and gives the search results of all related information. Hence, it becomes a time consuming process to find required information. In this paper, the focus is done over content based data mining using ontology and text feature extraction. Content based data mining process focuses on domain of the data. Ontology, itself is a domain based data set information system that will help to achieve required data retrieval in a more appropriate way. The proposed systemuses k means clustering algorithm for creation of flat clusters. Flat clusters are the primary classification or clusters of data that are used for various further processing. For more appropriate data retrieval, this system uses text feature extraction algorithm. This algorithm will help to reduce the noisy data from data sets. A noise free data will help to perform better data retrieval process.",,978-1-5090-0774-5,10.1109/RTEICT.2016.7808033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808033,Domain ontology;Text data;Feature extraction;Flat clusters;information retrieval,Feature extraction;Ontologies;Clustering algorithms;Data mining;Classification algorithms;Noise measurement;Algorithm design and analysis,data mining;feature extraction;information systems;online front-ends;ontologies (artificial intelligence);pattern classification;pattern clustering;query processing;text analysis,text document summarization;text document retrieval;text feature extraction;ontology;data retrieval;information acquisition;browser;user query;content based data mining process;domain based data set information system;k means clustering algorithm;flat clusters;primary classification,,2,,20,,9-Jan-17,,,IEEE,IEEE Conferences
Spot It! Finding Words and Patterns in Historical Documents,發現它！在歷史文獻中尋找詞語和模式,V. Dovgalecs; A. Burnett; P. Tranouez; S. Nicolas; L. Heutte,"LITIS, Univ. de Rouen, St. ?tienne du Rouvray, France; LITIS, Univ. de Rouen, St. ?tienne du Rouvray, France; LITIS, Univ. de Rouen, St. ?tienne du Rouvray, France; LITIS, Univ. de Rouen, St. ?tienne du Rouvray, France; LITIS, Univ. de Rouen, St. ?tienne du Rouvray, France",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,1039,1043,"We propose a system designed to spot either words or patterns, based on a user made query. Employing a two stage approach, it takes advantage of the descriptive power of the Bag of Visual Words (BOVW) representation and the discriminative power of the proposed Longest Weighted Profile (LWP) algorithm. First, we try to identify the zones of images that share common characteristics with the query as summed up in a BOVW. Then, we filter these zones using the LWP introducing spatial constraints extracted from the query. We have validated our system on the George Washington handwritten document database for word spotting, and medieval manuscripts from the DocExplore project for pattern spotting.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.208,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628773,word spotting;pattern spotting;historical documents;segmentation free;document image analysis;document understanding,Visualization;Feature extraction;Hidden Markov models;Databases;Clustering algorithms;Training;Robustness,document image processing;filtering theory;image retrieval,historical documents;user made query;bag of visual words representation;BOVW representation;longest weighted profile algorithm;LWP algorithm;George Washington handwritten document database;DocExplore project;word spotting;pattern spotting;medieval manuscripts,,15,,14,,15-Oct-13,,,IEEE,IEEE Conferences
HFRECCA for clustering of text data from travel guide articles,HFRECCA用於對旅行指南文章中的文本數據進行聚類,S. V. Wazarkar; A. A. Manjrekar,"Department of Technology, Shivaji University, Kolhapur, India; Department of Technology, Shivaji University, Kolhapur, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",1-Dec-14,2014,,,1486,1489,"Text clustering is advantageous for extraction of text data from web applications such as e-news papers, collection of research papers, blogs, news feeds at social networks, etc. This paper presents a text clustering Hierarchical Fuzzy Relational Eigenvector Centrality-based Clustering Algorithm (HFRECCA). The algorithm is a combination of fuzzy clustering, divisive hierarchical clustering and page rank algorithm. Travel guide articles are pre-processed to remove stop words and stemming. Then, similarity matrix is generated using word distance computation. In HFRECCA, divisive hierarchical clustering algorithm is applied where it uses Fuzzy Relational Eigenvector Centrality-based Clustering Algorithm (FRECCA) as sub routine algorithm. FRECCA outputs cluster membership values on the basis of page rank score using page rank algorithm and generate clusters according to it. HFRECCA has features of hierarchical clustering as well as fuzzy clustering as it creates hierarchy of clusters and an object can belong to multiple clusters. Structure of information resides in text documents is hierarchical hence HFRECCA is useful for clustering of data from natural language documents.",,978-1-4799-3080-7,10.1109/ICACCI.2014.6968349,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968349,Text Clustering;Similarity Measure;Hierarchical clustering;Fuzzy clustering,Clustering algorithms;Data mining;Semantics;Computational modeling;Data models;Algorithm design and analysis;Partitioning algorithms,eigenvalues and eigenfunctions;natural language processing;text analysis;text detection,travel guide articles;text clustering;text data extraction;Web applications;e-news papers;social networks;hierarchical fuzzy relational eigenvector centrality-based clustering algorithm;page rank algorithm;hierarchical clustering algorithm;sub routine algorithm;fuzzy clustering;natural language documents,,3,,21,,1-Dec-14,,,IEEE,IEEE Conferences
System for a cluster analysis,聚類分析系統,Y. Stekh; F. M. E. Sardieh; M. Lobur,"CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., Lviv, 79013, Ukraine; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., Lviv, 79013, Ukraine; CAD/CAM Department, Lviv Polytechnic National University, 12, S. Bandera Str., Lviv, 79013, Ukraine","2010 International Conference on Modern Problems of Radio Engineering, Telecommunications and Computer Science (TCSET)",12-Apr-10,2010,,,236,236,Summary form only given. This paper focuses on structure of dialog graphical system for pattern recognition with help of distance function. In this paper is evaluate the performance of different criterion functions and algorithms for the problem of clustering large datasets.,,978-966-553-901-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446092,Clustering algorithm;Criterion function;large datasets;distance function,Partitioning algorithms;Iterative algorithms;Frequency;Pattern recognition;Graphics;Clustering algorithms;Chebyshev approximation;Interactive systems;Extraterrestrial measurements;Data analysis,computer graphics;document handling;graphical user interfaces;pattern clustering,cluster analysis;dialog graphical system;pattern recognition;distance function;different criterion functions;large dataset clustering,,,,5,,12-Apr-10,,,IEEE,IEEE Conferences
Sentiment Clustering By Mahalanobis Distance,馬氏距離的情感聚類,H. M. Abdul Fattah; M. A. Masba; K. M. Azharul Hasan,"Dept. of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Dept. of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Dept. of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh",2018 4th International Conference on Electrical Engineering and Information & Communication Technology (iCEEiCT),31-Jan-19,2018,,,479,482,"Sentiment clustering is the computational study of people's opinions, sentiments, attitudes, and emotions. In this paper, we describe the use of Mahalanobis Distance (MD) to cluster review comments of users. MD is widely used for outlier detection in data mining. We have classified the comments into four clusters namely 'excellent', 'good', 'bad' and 'not recommended' where the training data has two classifications 'positive' and 'negative' only. To use this measure, a Representative Term Document Matrix (RTDM) and Inverse Co-Variance Matrix (C-1) has been computed from training data. Using the RTDM and C-1, MD has been calculated and clustering thresholds have been fixed based on MD of training data. Using these thresholds, final outcome are determined. We have used the Amazon watch reviews consisting of 62485 reviews, we received good accuracy based on MD based clustering approach. We also applied the approach collecting comments from street people to measure the accuracy and found over 90% of accuracy.",,978-1-5386-8279-1,10.1109/CEEICT.2018.8628142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628142,Sentiment Clustering;Mahalanobis distance;Opinion mining;Text classifier;Supervised learning,Covariance matrices;Sentiment analysis;Training;Anomaly detection;Training data;Euclidean distance;Thumb,data mining;document handling;matrix algebra;pattern classification;pattern clustering;sentiment analysis,sentiment clustering;MD;outlier detection;data mining;training data;Amazon watch reviews;Mahalanobis distance;representative term document matrix;RTDM;inverse co-variance matrix,,,,19,,31-Jan-19,,,IEEE,IEEE Conferences
Event Clusters Detection on Flickr Images Using a Suffix-Tree Structure,使用後綴樹結構對Flickr圖像進行事件聚類檢測,M. Ruocco; H. Ramampiaro,"Dept. of Comput. & Inf. Sci., Norwegian Univ. of Sci. & Technol., Trondheim, Norway; Dept. of Comput. & Inf. Sci., Norwegian Univ. of Sci. & Technol., Trondheim, Norway",2010 IEEE International Symposium on Multimedia,20-Jan-11,2010,,,41,48,"Image clustering is a problem that has been treated extensively in both Content-based (CBIR) and Text-Based (TBIR) Image Retrieval Systems. In this paper, we propose a new image clustering approach that takes both annotation, time and geographical position into account. Our goal is to develop a clustering method that allows an image to be part of an event cluster. We extend a well-known clustering algorithm called Suffix Tree Clustering (STC), which was originally developed to cluster text documents using a document snippet. To be able to use this algorithm, we consider an image with annotation as a document. Then, we extend it to also include time and geographical position. This appears to be particularly useful on the images gathered from online photo-sharing applications such as Flickr. Here image tags are often subjective and incomplete. For this reason, clustering based on textual annotations alone is not enough to capture all context information related to an image. Our approach has been suggested to address this challenge. In addition, we propose a novel algorithm to extract event clusters. The algorithm is evaluated using an annotated dataset from Flickr, and a comparison between different granularity of time and space is provided.",,978-1-4244-8672-4,10.1109/ISM.2010.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693821,Event Detection;Image Clustering;Event Clustering;Suffix Tree;Image Annotation,Clustering algorithms;Semantics;Feature extraction;Event detection;Silicon;Algorithm design and analysis;Visualization,content-based retrieval;document image processing;image retrieval;text analysis,event cluster detection;Flickr image;suffix-tree structure;image clustering;content-based image retrieval system;text-based image retrieval system;suffix tree clustering;text document clustering;document snippet;geographical position;online photo-sharing application;image tags;textual annotation;annotated dataset,,4,,22,,20-Jan-11,,,IEEE,IEEE Conferences
Text Document Clustering Using K-means Algorithm with Dimension Reduction Techniques,基於K均值算法和降維技術的文本文檔聚類,R. Kumbhar; S. Mhamane; H. Patil; S. Patil; S. Kale,"Sch. of Comput. Eng. & Technol., MIT Acad. of Eng., Pune, India; Sch. of Comput. Eng. & Technol., MIT Acad. of Eng., Pune, India; Sch. of Comput. Eng. & Technol., MIT Acad. of Eng., Pune, India; Sch. of Comput. Eng. & Technol., MIT Acad. of Eng., Pune, India; Sch. of Comput. Eng. & Technol., MIT Acad. of Eng., Pune, India",2020 5th International Conference on Communication and Electronics Systems (ICCES),10-Jul-20,2020,,,1222,1228,"With growing technologies commercial sites, social media, organizations generate lots of data. However, this huge amount of information needs to be organized properly. For this, the text mining process used as extracting relevant features and knowledge of the given unstructured text documents. The document clustering method in text mining is used for retrieving interesting features. In which similar documents organize into different groups called clusters. The large dimensions of data become a barrier in the extraction of useful information, so using dimensionality reduction (DR) technique for reducing the dimensions of the data matrices. Further data divide into groups using the k-means clustering algorithm. This study implements TF-IDF, singular value decomposition (SVD), non-negative matrix factorization (NMF), and k-means clustering. Finally, the results of the comparison of scores of kmeans, SVD with kmeans, and NMF with k-means are shown by graphical representation. The system uses 20 newsgroup datasets for simulating results.",,978-1-7281-5371-1,10.1109/ICCES48766.2020.9137928,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137928,TF-IDF;Truncated SVD;NMF;DR K-means clustering,,data mining;pattern classification;pattern clustering;singular value decomposition;text analysis,text document clustering;k-means algorithm;dimension reduction techniques;text mining process;feature extraction;document clustering method;similar documents;dimensionality reduction technique;data matrices;unstructured text documents;TF-IDF;singular value decomposition;SVD;nonnegative matrix factorization;NMF;k-means clustering;newsgroup datasets,,,,12,,10-Jul-20,,,IEEE,IEEE Conferences
Document clustering with evolved search queries,帶有進化搜索查詢的文檔聚類,L. Hirsch; A. Di Nuovo,"Department of Computing, Sheffield Hallam University, United Kingdom; Department of Computing, Sheffield Hallam University, United Kingdom",2017 IEEE Congress on Evolutionary Computation (CEC),7-Jul-17,2017,,,1239,1246,"Search queries define a set of documents located in a collection and can be used to rank the documents by assigning each document a score according to their closeness to the query in the multidimensional space of weighted terms. In this paper, we describe a system whereby an island model genetic algorithm (GA) creates individuals which can generate a set of Apache Lucene search queries for the purpose of text document clustering. A cluster is specified by the documents returned by a single query in the set. Each document that is included in only one of the clusters adds to the fitness of the individual and each document that is included in more than one cluster will reduce the fitness. The method can be refined by using the ranking score of each document in the fitness test. The system has a number of advantages; in particular, the final search queries are easily understood and offer a simple explanation of the clusters, meaning that an extra cluster labelling stage is not required. We describe how the GA can be used to build queries and show results for clustering on various data sets and with different query sizes. Results are also compared with clusters built using the widely used k-means algorithm.",,978-1-5090-4601-0,10.1109/CEC.2017.7969447,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7969447,text clustering;genetic algorithm;text mining,Genetic algorithms;Biological cells;Clustering algorithms;Information retrieval;Indexes;Libraries;Labeling,data mining;genetic algorithms;pattern clustering;query formulation;text analysis,document ranking;island model genetic algorithm;Apache Lucene search queries;text document clustering;text mining,,2,,23,,7-Jul-17,,,IEEE,IEEE Conferences
A novel similarity measure technique for clustering using multiple viewpoint based method,一種基於多視點的聚類新方法,D. S. Potdar; T. M. Pattewar,"Department of Computer Engineering, R. C. Patel Institute of Technology, Shirpur, MS, India; Department of Computer Engineering, R. C. Patel Institute of Technology, Shirpur, MS, India",2016 10th International Conference on Intelligent Systems and Control (ISCO),3-Nov-16,2016,,,1,4,"Data mining is nothing but the process of automatically searching large stores of data to discover patterns and trends that go beyond simple analysis. So it is observed that while doing clustering there may be a chance of occurring dissimilar data object in a cluster. This paper introduces such technology that makes the patterns more accurate, and it helps to search more accurate analysis of data. This System greedily picks the next frequent item set in the next cluster. For this the multiple viewpoints are used to measure the similarity between two different data objects is introduced. We can define similarity between two objects explicitly or implicitly. Cosine similarity measures will resolve this problem. As multiple viewpoints will focuses on similarity measures at multiple levels. These criteria will be used to group the documents based on similarity. The similarity measured between current cluster documents and also other cluster group documents.",,978-1-4673-7807-9,10.1109/ISCO.2016.7727007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727007,Data Mining;Clustering;Multi Viewpoint Based Similarity;Hierarchical Clustering Method;etc,Data mining;Clustering algorithms;Data models;Partitioning algorithms;Algorithm design and analysis;Computational modeling;Databases,data analysis;data mining;pattern clustering;pattern matching,similarity measure technique;multiple viewpoint based method;data mining;automatic data store searching;data object;Cosine similarity measures;cluster group documents,,,,19,,3-Nov-16,,,IEEE,IEEE Conferences
Word segmentation in handwritten Korean text lines based on gap clustering techniques,基於間隙聚類技術的手寫韓文文本行分詞,S. H. Kim; S. Jeong; Guee-Sang Lee; C. Y. Suen,"Centre for Pattern Recognition & Machine Intelligence, Concordia Univ., Montreal, Que., Canada; NA; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,189,193,"We propose a word segmentation method for handwritten Korean text lines. It uses gap information to separate a text line into word units, where the gap is defined as a white-run obtained after a vertical projection of the line image. Each gap is classified into a between-word gap or a within-word gap using a clustering technique. We take up three gap metrics - the bounding box (BB), run-length/Euclidean (RLE) and convex hull (CH) distances - which are known to have superior performance in Roman-style word segmentation, and three clustering techniques - the average linkage method, the modified MAX method and sequential clustering. An experiment with 498 text-line images extracted from live mail pieces has shown that the best performance is obtained by the sequential clustering technique using all three gap metrics.",,0-7695-1263-1,10.1109/ICDAR.2001.953781,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953781,,Image segmentation;Couplings;Handwriting recognition;Text recognition;Pattern recognition;Machine intelligence;Computer science;Postal services;Character recognition;Humans,handwritten character recognition;image segmentation;document image processing;pattern clustering;runlength codes;mailing systems;software performance evaluation,word segmentation;handwritten Korean text lines;gap clustering techniques;word units;white-run;line image vertical projection;between-word gap;within-word gap;bounding box distance;run-length/Euclidean distance;convex hull distance;performance;average linkage method;modified MAX method;sequential clustering technique;mail pieces,,23,,13,,7-Aug-02,,,IEEE,IEEE Conferences
A Fuzzy Ontological Knowledge Document Clustering Methodology,模糊本體知識文檔聚類方法,A. J. C. Trappey; C. V. Trappey; F. Hsu; D. W. Hsiao,"Dept. of Ind. Eng. & Manage., Nat. Taipei Univ. of Technol., Taipei; Dept. of Manage. Sci., Nat. Chiao Tung Univ., Hsinchu; Avectec, Inc., Hsinchu; Dept. of Ind. Eng. & Eng. Manage., Nat. Tsing Hua Univ., Hsinchu","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",7-Apr-09,2009,39,3,806,814,"This correspondence presents a novel hierarchical clustering approach for knowledge document self-organization, particularly for patent analysis. Current keyword-based methodologies for document content management tend to be inconsistent and ineffective when partial meanings of the technical content are used for cluster analysis. Thus, a new methodology to automatically interpret and cluster knowledge documents using an ontology schema is presented. Moreover, a fuzzy logic control approach is used to match suitable document cluster(s) for given patents based on their derived ontological semantic webs. Finally, three case studies are used to test the approach. The first test case analyzed and clustered 100 patents for chemical and mechanical polishing retrieved from the World Intellectual Property Organization (WIPO). The second test case analyzed and clustered 100 patent news articles retrieved from online Web sites. The third case analyzed and clustered 100 patents for radio-frequency identification retrieved from WIPO. The results show that the fuzzy ontology-based document clustering approach outperforms the K-means approach in precision, recall, F-measure, and Shannon's entropy.",1941-0492,,10.1109/TSMCB.2008.2009463,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804715,Fuzzy inference control;hierarchical clustering;ontology schema;patent analysis;text mining,Ontologies;Testing;Chemical analysis;Content management;Fuzzy logic;Automatic control;Intellectual property;Radio frequency;Radiofrequency identification;Entropy,data mining;document handling;fuzzy set theory;ontologies (artificial intelligence);patents;pattern clustering,fuzzy inference control;hierarchical clustering;ontology;text mining;fuzzy ontological knowledge document clustering methodology,,36,,26,,24-Mar-09,,,IEEE,IEEE Journals
An Improved Data Clustering Algorithm for Mining Web Documents,一種改進的Web文檔挖掘數據聚類算法,O. H. Odukoya; G. A. Aderounmu; E. R. Adagunodo,"Comput. Sci. & Eng. Dept., Obafemi Awolowo Univ., Ile-Ife, Nigeria; Comput. Sci. & Eng. Dept., Obafemi Awolowo Univ., Ile-Ife, Nigeria; Comput. Sci. & Eng. Dept., Obafemi Awolowo Univ., Ile-Ife, Nigeria",2010 International Conference on Computational Intelligence and Software Engineering,30-Dec-10,2010,,,1,8,"This paper formulates, simulates and assess an improved data clustering algorithm for mining web documents with a view to preserving their conceptual similarities and eliminating the problem of speed while increasing accuracy. The improved data clustering algorithm was formulated using the concept of K-means algorithm. Real and artificial datasets were used to test the proposed and existing algorithm. The proposed algorithm was simulated using the fuzzy logic and statistical toolbox in Matlab 7.0. The simulated results were compared with the existing data clustering algorithm using accuracy, response time, adjusted rand index and entropy as performance parameters. The results show an improved data clustering algorithm with a new initialization method based on finding a set of medians extracted from a dimension with maximum variances. The results of the simulation showed that the accuracy is at its peak when the number of clusters is 3 and reduces as the number of clusters increases. When compared with existing algorithm, the proposed clustering algorithm showed an accuracy of 89.3% while the existing had an accuracy of 88.9%. The entropy was stable for both algorithms with a value of 0.2485 at k = 3. This also decreases as the number of clusters increase until when the number of clusters reached eight where it increased slightly. The adjusted rand index values varied from 0 to 1 for both clustering algorithms. The existing method achieved a value of 53% as compared with the proposed method which achieved an adjusted rand index value of 63.7%, when the number of clusters was five. In addition, the response time decreased from 0.0451 seconds to 0.0439 seconds when the number of clusters was three. This showed that the proposed data clustering algorithm decreased by 2.7% in response time as compared to the K-means data clustering. This study has shown that the proposed data clustering algorithm could be adapted by web search engine developers for more efficient web search result clustering.",,978-1-4244-5391-7,10.1109/CISE.2010.5676720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676720,,Clustering algorithms;Accuracy;Partitioning algorithms;Algorithm design and analysis;Iris recognition;Data models;Entropy,data mining;document handling;fuzzy logic;Internet;pattern clustering,data clustering algorithm;Web document mining;K-means algorithm;real dataset;artificial dataset;fuzzy logic;statistical toolbox;Matlab 7.0;adjusted rand index value;K-means data clustering;Web search engine developer;Web search result clustering,,8,,17,,30-Dec-10,,,IEEE,IEEE Conferences
A cluster validity index for FCM-type co-clustering,FCM類型共聚的聚類有效性指標,M. Muranishi; K. Honda; A. Notsu; H. Ichihashi,"Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, 599-8531 Japan; Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, 599-8531 Japan; Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, 599-8531 Japan; Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, 599-8531 Japan",2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),7-Oct-13,2013,,,1,6,"Cluster validation is an important issue in FCM-type clustering applications and many validity indices have been proposed for selecting the optimal fuzzy partition. In most of FCM-type cluster validity indices, the number of clusters is evaluated by considering the partition quality and geometrical features. The quality of memberships was evaluated by the degree of overlapping or the boundary uncertainty, while the geometrical quality was measured by cluster compactness or cluster separation. In this paper, a new approach for FCM-type cluster validation in fuzzy co-clustering is proposed. Because fuzzy co-clustering extracts object-item dual clusters without using prototypes, a new concept of cluster separation is constructed without using the distances between cluster prototypes. The applicability of the new validity index is demonstrated in several numerical experiments including a document clustering application.",1098-7584,978-1-4799-0022-0,10.1109/FUZZ-IEEE.2013.6622386,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622386,Fuzzy clustering;Co-clustering;Cluster validity,Indexes;Prototypes;Clustering algorithms;Resource management;Partitioning algorithms;Context;Entropy,fuzzy set theory;pattern clustering,cluster validity index;FCM-type coclustering;cluster validation;optimal fuzzy partition;partition quality;geometrical feature;degree of overlapping;boundary uncertainty;geometrical quality;cluster compactness;cluster separation,,1,,22,,7-Oct-13,,,IEEE,IEEE Conferences
Exploiting the Social Tagging Network for Web Clustering,利用社交標籤網絡進行Web集群,C. Lu; X. Hu; J. Park,"College of Information Science & Technology, Drexel University, Philadelphia, PA, USA; College of Information Science & Technology, Drexel University, Philadelphia, PA, USA; College of Information Science & Technology, Drexel University, Philadelphia, PA, USA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",18-Aug-11,2011,41,5,840,852,"Social tagging is a major characteristic of Web 2.0. A social tagging system can be modeled with a tripartite network of users, resources, and tags. In this paper, we investigate how to enhance Web clustering by leveraging the tripartite network of social tagging systems. We propose a clustering method called ?Tripartite Clustering??which clusters the three types of nodes (resources, users, and tags) simultaneously by only utilizing the links in the social tagging network. We also investigate two other approaches to exploit social tagging for clustering with K-means and Link K-means. All the clustering methods are experimented on a real-world social tagging data set sampled from del.icio.us. The clustering results are evaluated against a human-maintained Web directory. The experimental results show that the social tagging network is a very useful information source for document clustering. All social-annotation-based clustering methods can significantly improve the performance of content-based clustering. Compared to social-annotation-based K-means and Link K-means, Tripartite Clustering achieves equivalent or better performance and produces more useful information.",1558-2426,,10.1109/TSMCA.2011.2157128,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872068,Clustering methods;social annotation;social tagging;tripartite network,Tagging;Clustering methods;Clustering algorithms;Web pages;Measurement;Semantics;Vectors,document handling;information resources;Internet;pattern clustering,Web clustering;Web 2.0;tripartite network;tripartite clustering;social tagging network;Link K-means;social tagging data set;human maintained Web directory;document clustering;information source;social annotation based clustering method;content based clustering;social annotation based K-means,,28,,46,,9-Jun-11,,,IEEE,IEEE Journals
Automatic Text Summarization Using Gensim Word2Vec and K-Means Clustering Algorithm,使用Gensim Word2Vec和K-Means聚類算法自動進行文本摘要,M. M. Haider; M. A. Hossin; H. R. Mahi; H. Arif,"BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh; BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh; BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh; BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh",2020 IEEE Region 10 Symposium (TENSYMP),2-Nov-20,2020,,,283,286,"The significance of text summarization in the Natural Language Processing (NLP) community has now expanded because of the staggering increase in virtual textual materials. Text summary is the process created from one or multiple texts which convey important insight in a little form of the main text. Multiple text summarization technique assists to pick indispensable points of the original texts reducing time and effort require reading the whole document. The question was approached from a different point of view, in a different domain by using different concepts. Extractive and abstractive are the two main methods of summing up text. Though extractive summary is primarily concerned with what summary content the frequency of words, phrases, and sentences from the original document should be used. This research proposes a sentence based clustering algorithm (K-Means) for a single document. For feature extraction, we have used Gensim word2vec which is intended to automatically extract semantic topics from documents in the most efficient way possible.",2642-6102,978-1-7281-7366-5,10.1109/TENSYMP50017.2020.9230670,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230670,Text summarization;Extractive;Single Document;NLP;Gensim;Word2Vec;K-Means,Elbow;Clustering algorithms;Business;Feature extraction;Sports;Computer science;Data models,feature extraction;natural language processing;pattern clustering;text analysis,K-means clustering algorithm;natural language processing community;virtual textual materials;multiple texts;extractive summary;summary content;sentence based clustering algorithm;single document;feature extraction;automatic text summarization;multiple text summarization technique;Gensim Word2Vec;NLP community,,,,16,,2-Nov-20,,,IEEE,IEEE Conferences
Density-Based Clustering of Massive Short Messages Using Domain Ontology,使用領域本體的大規模短信的基於密度的聚類,S. Yang; Y. Wang,"Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; NA",2009 Asia-Pacific Conference on Information Processing,7-Aug-09,2009,2,,505,508,"With the rapid development of information technology, huge data is accumulated. A vast amount of such data appears as short messages such as emails in companies or conversations in open chatting rooms. It is useful to find the themes or exceptional information from the messages by clustering the short documents based on density. However, traditional vector space model based text clustering algorithms can not get acceptable accuracy because the key words appear at low frequency. On the other hand, traditional text clustering algorithms become very inefficient or even unavailable when processing massive data at TB level. In this paper a density-based short message clustering algorithm using domain ontology based is presented. This algorithm uses domain ontology to calculate the semantic similarity between short messages which improves the parallel method is also used to get better scalability.",,978-0-7695-3699-6,10.1109/APCIP.2009.260,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197247,massive;short messages;text clustering;density;domain ontology,Ontologies;Clustering algorithms;Clustering methods;Databases;Partitioning algorithms;Frequency;Scalability;Optical noise;Information processing;Computer science,data mining;ontologies (artificial intelligence);parallel processing;pattern clustering;text analysis,density-based document clustering;massive short message;domain ontology;information technology;vector space model;text clustering;TB level;massive data processing;semantic similarity;parallel mining,,,,11,,7-Aug-09,,,IEEE,IEEE Conferences
Adding Examples into Java Documents,在Java文檔中添加示例,J. Kim; S. Lee; S. Hwang; S. Kim,"Pohang Univ. of Sci. & Technol., Pohang, South Korea; Pohang Univ. of Sci. & Technol., Pohang, South Korea; Pohang Univ. of Sci. & Technol., Pohang, South Korea; Hong Kong Univ. of Sci. & Technol., Hong Kong, China",2009 IEEE/ACM International Conference on Automated Software Engineering,18-Mar-10,2009,,,540,544,"Code examples play an important role to explain the usage of Application Programming Interfaces (APIs), but most API documents do not provide sufficient code examples. For example, for the JDK 5 documents (JavaDocs), only 2% of APIs have code examples. In this paper, we propose a technique that automatically augments API documents with code examples. Our approach finds and embeds code examples for more than 75% of the APIs in JavaDocs 5.",1938-4300,978-1-4244-5259-0,10.1109/ASE.2009.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431739,API Documents;Examples;Structures;Code Clustering;Ranking,Java;Libraries;Humans;Data mining;Software engineering;Application software;Automatic programming;Productivity;Search engines;Feature extraction,application program interfaces;document handling;Java,Java documents;application programming interfaces;API documents;JDK 5 documents;JavaDocs 5,,16,,22,,18-Mar-10,,,IEEE,IEEE Conferences
The improved non-negative Matrix Factorization algorithm for document clustering,改進的文檔聚類非負矩陣分解算法,W. Zhao; H. Ma; Q. He; Z. Shi,"College of Information, Engineering, Xiangtan University, Hunan 411105 China; College of Mathematics and Information, Northwest Normal University, Gansu, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing China",2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-Sep-11,2011,3,,1836,1839,"Non-negative Matrix Factorization (NMF) is one latest presented approach for obtaining document clusters, which aimed to provide a minimum error non-negative representation of the term-document matrix. In this paper, we have extended the classical NMF approach by imposing sparseness constraints explicitly. The new model can learn much sparser matrix factorization. Also, an objective function is defined to impose the sparseness constraint, in addition to the non-negative constraint. Experimental results on real-world document datasets show that the proposed method can treat document clustering effectively and efficiently.",,978-1-61284-181-6,10.1109/FSKD.2011.6019811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019811,,Clustering algorithms;Educational institutions;Encoding;Information retrieval;Text mining;Algorithm design and analysis,document handling;matrix algebra;pattern clustering,nonnegative matrix factorization algorithm;document clustering;minimum error nonnegative representation;term-document matrix;real-world document datasets,,,,18,,15-Sep-11,,,IEEE,IEEE Conferences
Clustering Documents with Active Learning Using Wikipedia,使用Wikipedia通過主動學習將文檔聚類,A. Huang; D. Milne; E. Frank; I. H. Witten,"Dept. of Comput. Sci., Univ. of Waikato, Hamilton; Dept. of Comput. Sci., Univ. of Waikato, Hamilton; Dept. of Comput. Sci., Univ. of Waikato, Hamilton; Dept. of Comput. Sci., Univ. of Waikato, Hamilton",2008 Eighth IEEE International Conference on Data Mining,10-Feb-09,2008,,,839,844,"Wikipedia has been applied as a background knowledge base to various text mining problems, but very few attempts have been made to utilize it for document clustering. In this paper we propose to exploit the semantic knowledge in Wikipedia for clustering, enabling the automatic grouping of documents with similar themes. Although clustering is intrinsically unsupervised, recent research has shown that incorporating supervision improves clustering performance, even when limited supervision is provided. The approach presented in this paper applies supervision using active learning. We first utilize Wikipedia to create a concept-based representation of a text document, with each concept associated to a Wikipedia article. We then exploit the semantic relatedness between Wikipedia concepts to find pair-wise instance-level constraints for supervised clustering, guiding clustering towards the direction indicated by the constraints. We test our approach on three standard text document datasets. Empirical results show that our basic document representation strategy yields comparable performance to previous attempts; and adding constraints improves clustering performance further by up to 20%.",2374-8486,978-0-7695-3502-9,10.1109/ICDM.2008.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781188,document representation;text clustering;active learning;Wikipedia,Wikipedia,data mining;knowledge representation;pattern clustering;text analysis;unsupervised learning;Web sites,document clustering;active learning;Wikipedia;text mining;semantic knowledge based representation;text document dataset;unsupervised learning,,74,,17,,10-Feb-09,,,IEEE,IEEE Conferences
Ontology based clustering algorithm for information retrieval,基於本體的信息檢索聚類算法,T. N. Ravishankar; R. Shriram,"Department of computer science and engineering B.S. Abdur Rahman University, Chennai-48; Department of computer science and engineering B.S. Abdur Rahman University, Chennai-48","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)",30-Jan-14,2013,,,1,4,"The explosive growth of mobile devices has increased the need for information on the move. As the number of web users through mobile devices is increasing day by day, it is very challenging for the search engines to provide the specific result in response to user query. We present a clustering algorithm which is based on decision tree structure in order to improve the clustering results and visualize only the relevant results to the user. In this paper, the objective is to integrate domain ontology as background knowledge and study the clustering performance. We have computed multiple clusters using standard K-means based on various similarity metrics as well. Our results are compared with previous baseline approaches to find the performance and overheads. The experimental results show that more refinement to be done in the clustering technique to retrieve documents with fewer navigations and less data traffic, which is important especially for mobile devices.",,978-1-4799-3926-8,10.1109/ICCCNT.2013.6726605,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726605,ontology;clustering;K means;similarity measures,Ontologies;Clustering algorithms;Decision trees;Vectors;Cities and towns;Computer science;Educational institutions,Internet;mobile computing;ontologies (artificial intelligence);pattern clustering;query processing;search engines,ontology based clustering algorithm;information retrieval;mobile devices;search engines;decision tree structure;domain ontology;clustering performance;standard k-means;similarity metrics;clustering technique;document retrieval;data traffic,,,,19,,30-Jan-14,,,IEEE,IEEE Conferences
Construction of topics and clusters in Topic Detection and Tracking tasks,主題檢測和跟?任務中主題和聚類的構建,M. Mohd; F. Crestani; I. Ruthven,"Faculty of Information Science & Technology, Universiti Kebangsaan Malaysia, 43600 Selangor Malaysia; University of Lugano (USI), via G. Buffi 13, Ch-6900, Lugano, Switzerland; Dept. of Computer & Information Sciences, University of Strathclyde, Glasgow G1 1XH, Scotland, United Kingdom",2011 International Conference on Semantic Technology and Information Retrieval,22-Aug-11,2011,,,171,174,"This paper discussed the construction of topics to be tracked and clusters to be detected in Topic Detection and Tracking (TDT) tasks. Single Pass Clustering was used to cluster the news articles. As a result, the TDT tasks contained a combination of a good and poor clustering performance based on the F1-measure. Therefore, the selection of clusters and topics from the clustering experiment is important in the Tracking and the Detection tasks. It has contributed towards the user experimental design.",2166-0700,978-1-61284-353-7,10.1109/STAIR.2011.5995784,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995784,Topic Detection and Tracking;Single Pass Clustering;user tasks,Information retrieval;Clustering algorithms;Classification algorithms;Monitoring;User interfaces;Text categorization;Event detection,document handling;information retrieval;pattern clustering,topic detection;single pass clustering;topic tracking;Fl-measure,,2,,9,,22-Aug-11,,,IEEE,IEEE Conferences
KnowSim: A Document Similarity Measure on Structured Heterogeneous Information Networks,KnowSim：結構化異構信息網絡上的文檔相似性度量,C. Wang; Y. Song; H. Li; M. Zhang; J. Han,"Sch. of EECS, Peking Univ., Beijing, China; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Sch. of EECS, Peking Univ., Beijing, China; Sch. of EECS, Peking Univ., Beijing, China; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA",2015 IEEE International Conference on Data Mining,7-Jan-16,2015,,,1015,1020,"As a fundamental task, document similarity measure has broad impact to document-based classification, clustering and ranking. Traditional approaches represent documents as bag-of-words and compute document similarities using measures like cosine, Jaccard, and dice. However, entity phrases rather than single words in documents can be critical for evaluating document relatedness. Moreover, types of entities and links between entities/words are also informative. We propose a method to represent a document as a typed heterogeneous information network (HIN), where the entities and relations are annotated with types. Multiple documents can be linked by the words and entities in the HIN. Consequently, we convert the document similarity problem to a graph distance problem. Intuitively, there could be multiple paths between a pair of documents. We propose to use the meta-path defined in HIN to compute distance between documents. Instead of burdening user to define meaningful meta paths, an automatic method is proposed to rank the meta-paths. Given the meta-paths associated with ranking scores, an HIN-based similarity measure, KnowSim, is proposed to compute document similarities. Using Freebase, a well-known world knowledge base, to conduct semantic parsing and construct HIN for documents, our experiments on 20Newsgroups and RCV1 datasets show that KnowSim generates impressive high-quality document clustering.",1550-4786,978-1-4673-9504-5,10.1109/ICDM.2015.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373428,Document similarity;knowledge base;knowledge graph;structured text similarity;heterogeneous information network,Knowledge based systems;Semantics;Approximation algorithms;Organizations;Approximation methods;Laplace equations;Feature extraction,deductive databases;document handling;pattern classification;pattern clustering,document similarity measure;structured heterogeneous information networks;document-based classification;document-based clustering;document-based ranking;document relatedness evaluation;document represent;heterogeneous information network;graph distance problem;meta-paths;ranking scores;HIN-based similarity measure;KnowSim;Freebase;world knowledge base;semantic parsing;20Newsgroups dataset;RCV1 dataset,,19,,39,,7-Jan-16,,,IEEE,IEEE Conferences
Bilingual Printed Document Image Retrieval Based on SIFT Feature,基於SIFT特徵的雙語印刷文檔圖像檢索,E. Firkat; A. Dawut; P. Tuerxun; A. Hamdulla,"Sch. of Software, Xinjiang Univ., Urumqi, China; Sch. of Software, Xinjiang Univ., Urumqi, China; Sch. of Software, Xinjiang Univ., Urumqi, China; Inst. of Inf. Sci. & Eng., Xinjiang Univ. Urumqi, Urumqi, China","2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)",21-Mar-19,2019,,,548,551,"This paper present a printed document retrieval system which can retrieve Chinese and Uyghur keywords from printed document images. In this paper we introduce the framework of the printed document retrieval system and processing step behind it which can be based line for the word spotting based document retrieval system, we also describe the extraction algorithm that use local feature as SIFT to extract the feature from image and use Euclidean based matching algorithm to query the matching word in printed document image. Some novel idea applied in this system might be helpful for some Other Bilingual printed document image retrieve system.",,978-1-7281-1307-4,10.1109/ICITBS.2019.00137,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669601,word spotting;SIFT;document retrieval system,Feature extraction;Image segmentation;Image retrieval;Optical character recognition software;Euclidean distance;Clustering algorithms;Object recognition,document image processing;feature extraction;image matching;image retrieval;natural language processing;transforms,Bilingual printed document image retrieval;SIFT feature;printed document retrieval system;Euclidean based matching algorithm;Chinese keyword retrieval;Uyghur keyword retrieval;feature extraction,,,,7,,21-Mar-19,,,IEEE,IEEE Conferences
Cross-Domain Document Object Detection: Benchmark Suite and Method,跨域文檔對象檢測：基準套件和方法,K. Li; C. Wigington; C. Tensmeyer; H. Zhao; N. Barmpalios; V. I. Morariu; V. Manjunatha; T. Sun; Y. Fu,Northeastern University; Adobe Research; Adobe Research; Adobe Research; Adobe Document Cloud; Adobe Research; Adobe Research; Adobe Research; Northeastern University,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),5-Aug-20,2020,,,12912,12921,"Decomposing images of document pages into high-level semantic regions (e.g., figures, tables, paragraphs), document object detection (DOD) is fundamental for downstream tasks like intelligent document editing and understanding. DOD remains a challenging problem as document objects vary significantly in layout, size, aspect ratio, texture, etc. An additional challenge arises in practice because large labeled training datasets are only available for domains that differ from the target domain. We investigate cross-domain DOD, where the goal is to learn a detector for the target domain using labeled data from the source domain and only unlabeled data from the target domain. Documents from the two domains may vary significantly in layout, language, and genre. We establish a benchmark suite consisting of different types of PDF document datasets that can be utilized for cross-domain DOD model training and evaluation. For each dataset, we provide the page images, bounding box annotations, PDF files, and the rendering layers extracted from the PDF files. Moreover, we propose a novel cross-domain DOD model which builds upon the standard detection model and addresses domain shifts by incorporating three novel alignment modules: Feature Pyramid Alignment (FPA) module, Region Alignment (RA) module and Rendering Layer alignment (RLA) module. Extensive experiments on the benchmark suite substantiate the efficacy of the three proposed modules and the proposed method significantly outperforms the baseline methods. The project page is at \url{https://github.com/kailigo/cddod}.",2575-7075,978-1-7281-7168-5,10.1109/CVPR42600.2020.01293,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156933,,Portable document format;US Department of Defense;Object detection;Benchmark testing;Layout;Rendering (computer graphics);Detectors,document image processing;learning (artificial intelligence);object detection;pattern clustering;text analysis;text detection,page images;PDF files;standard detection model;domain shifts;benchmark suite substantiate;cross-domain document object detection;document pages;high-level semantic regions;intelligent document editing;aspect ratio;labeled training datasets;target domain;source domain;PDF document datasets;cross-domain DOD model;region alignment module,,,,37,,5-Aug-20,,,IEEE,IEEE Conferences
Fuzzy clustering and categorization of text documents,文本文檔的模糊聚類和分類,H. Ayeldeen; A. E. Hassanien; A. A. Fahmy,"Cairo University, Faculty of Computers and Information, Egypt; Cairo University, Faculty of Computers and Information, Egypt; Cairo University, Faculty of Computers and Information, Egypt",13th International Conference on Hybrid Intelligent Systems (HIS 2013),13-Oct-14,2013,,,262,266,"The fuzzy Euclidean distance clustering algorithm has been well studied and used in information retrieval society for clustering documents. However, the fuzzy logic algorithm poses problems in dealing with large amount of data. In this paper we proposed results for clustering theses documents based on Euclidean distances and cluster-dependent keyword weighting. The proposed approach is based on the Fuzzy Euclidean distance clustering algorithm. The cluster dependent keyword weighting help in partitioning and categorizing the theses documents into more meaningful categories.",,978-1-4799-2439-4,10.1109/HIS.2013.6920493,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920493,Lexical similarity;Medical Ontology;MeSH;Fuzzy Euclidean distance Algorithm,Ontologies;Ultrasonic imaging;Biopsy;Decision support systems;Pacemakers;Catheters;Surgery,data mining;document handling;fuzzy logic;medical computing;ontologies (artificial intelligence);pattern clustering,text document categorization;fuzzy Euclidean distance clustering algorithm;information retrieval;document clustering;fuzzy logic algorithm;cluster-dependent keyword weighting,,1,,19,,13-Oct-14,,,IEEE,IEEE Conferences
Semantic feature reduction in chinese document clustering,中文文檔聚類中的語義特徵約簡,Xianjun Meng; Qingcai Chen; Xiaolong Wang,"ICRC Lab, Shenzhen Graduate School, Harbin Institute of Technology, China; ICRC Lab, Shenzhen Graduate School, Harbin Institute of Technology, China; ICRC Lab, Shenzhen Graduate School, Harbin Institute of Technology, China","2008 IEEE International Conference on Systems, Man and Cybernetics",7-Apr-09,2008,,,3721,3726,"Text clustering techniques were usually used to structure the text documents into topic related groups which can facilitate users to get a comprehensive understanding on corpus or results from information retrieval system. Most of existing text clustering algorithm which derived from traditional formatted data clustering heavily rely on term analysis methods and adopted vector space model (VSM) as their document representation. But because of the essential characteristic underlying text such as high dimensionality features vector space, the problem of sparseness has a strong impact on the clustering algorithm. So feature reduction is an important preprocess step for improving the efficiency and accuracy of clustering algorithm by removing redundant and irrelevant terms from corpus. Even the clustering is considered as an unsupervised learning method, but in text, there is still some priori knowledge we can use from NLP analysis based approach. In this paper, we propose a semantic analysis based feature reduction method which used in Chinese text clustering. Our method bases on a dedicated Part-of-Speech tags selection and synonyms consolidation and can reduce the feature space of documents more effectively compared with traditional feature reduction method tfidf and stopwords removal; meanwhile it preserves or sometimes even improves the accuracy of clustering algorithm. In our experiment, we tested our feature reduction method using bisecting k-means algorithm which was proved be efficient in text clustering. The results show that our method can reduce the feature space significantly, and meanwhile have a better clustering accuracy in terms of the purity.",1062-922X,978-1-4244-2383-5,10.1109/ICSMC.2008.4811878,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4811878,text clustering;feature selection;part-of-speech;synonym,Clustering algorithms;Information retrieval;Unsupervised learning;Functional analysis;Algorithm design and analysis;Testing;Text mining;Navigation;Search engines;Clustering methods,information retrieval;natural language processing;text analysis;unsupervised learning,semantic feature reduction;Chinese document clustering;text clustering;information retrieval system;term analysis;vector space model;document representation;unsupervised learning;natural language processing;part-of-speech tags,,2,,27,,7-Apr-09,,,IEEE,IEEE Conferences
Decomposition of term-document matrix representation for clustering analysis,分解術語-文檔矩陣表示形式以進行聚類分析,J. Yang; J. Watada,"Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan; Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan",2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011),1-Sep-11,2011,,,976,983,"Latent Semantic Indexing (LSI) is an information retrieval technique using a low-rank singular value decomposition (SVD) of term-document matrix. The aim of this method is to reduce the matrix dimension by finding a pattern in document collection with concurrently referring terms. The methods are implemented to calculate the weight of term-document in vector space model (VSM) for document clustering using fuzzy clustering algorithm. LSI is an attempt to exploit the underlying semantic structure of word usage in documents. During the query-matching phase of LSI, a user's query is first projected into the term-document space, and then compared to all terms and documents represented in the vector space. Using some similarity measure, the nearest (most relevant) terms and documents are identified and returned to the user. The current LSI query-matching method requires computing the similarity measure about the query of every term and document in the vector space. In this paper, the Maximal Tree Algorithm is used within a recent LSI implementation to mitigate the computational time and computational complexity of query matching. The Maximal Tree data structure stores the term and document vectors in such a way that only those terms and documents are most likely qualified as the nearest neighbor to the query will be examined and retrieved. In a word, this novel algorithm is suitable for improving the accuracy of data miners.",1098-7584,978-1-4244-7317-5,10.1109/FUZZY.2011.6007525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007525,Fuzzy clustering;data mining;LSI;SVD,Large scale integration;Matrix decomposition;Semantics;Accidents;Indexing;Economics;Singular value decomposition,computational complexity;data mining;fuzzy set theory;indexing;pattern clustering;query processing;singular value decomposition;tree data structures;word processing,term-document matrix representation;latent semantic indexing;information retrieval technique;LSI technique;low-rank singular value decomposition;SVD;term-document matrix decomposition;matrix dimension reduction;vector space model;document clustering;fuzzy clustering algorithm;word usage;user query;similarity measures;nearest term identification;LSI query matching method;maximal tree algorithm;computational complexity;maximal tree data structure;clustering analysis,,,,22,,1-Sep-11,,,IEEE,IEEE Conferences
Clustering patent document in the field of ICT (Information & Communication Technology),ICT（信息和通信技術）領域的專利文件的聚類,A. Widodo; I. Budi,"Faculty of Computer Science, University of Indonesia; Faculty of Computer Science, University of Indonesia",2011 International Conference on Semantic Technology and Information Retrieval,22-Aug-11,2011,,,203,208,"The current classification of patent data that refers to the IPC (International Patent Classification) of the WIPO (World Intellectual Property Organization), deemed not reflect the classification of the field of ICT (Information & Communication Technology). ICT applications are usually included in sections G (Physics) and H (Electricity). This paper will evaluate the eight groupings of patents based on the IPC classes (G01, G06, G09, G11, H01, H03, H04, and H06) of patents registered in the Directorate General of Intellectual Property Rights in Indonesia, from the year 1991 to 2000. The algorithm used to grouping is KMeans, KMeans++, Hierchical Clustering, and a combination of these three algorithms with SVD (Singular Value Decomposition). For external validation, Purity and F-Measure are used, whereas Silhouette is used for internal validation. From the experimental results it can be concluded that SVD provides improvements to the clustering results. In addition, the use of abstract does not necessarily improve the performance of clustering, and the use of phrase does not always yield better cluster than the use of the word as index. Moreover, no cluster has purity measure greater than 50%, which means that the existing IPC classification has not been able to accommodate the field of ICT appropriately.",2166-0700,978-1-61284-353-7,10.1109/STAIR.2011.5995789,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995789,Clustering;Patent;Singular Value Decomposition;Kmeans;Information & Communication Technology,Patents;Clustering algorithms;Abstracts;Indexing;Matrix decomposition;Singular value decomposition,document handling;information technology;patents;pattern classification;pattern clustering;singular value decomposition,patent document clustering;ICT field;information and communication technology;patent data classification;international patent classification;WIPO;world intellectual property organization;patent registration;KMeans++ algorithm;hierarchical clustering;SVD;singular value decomposition;F-Measure validation;Silhouette validation,,4,,14,,22-Aug-11,,,IEEE,IEEE Conferences
Effect of Latent Semantic Indexing for Clustering Clinical Documents,潛在語義索引對臨床文件聚類的影響,C. Han; J. Choi,"Interdiscipl. Program of Bioeng., Seoul Nat. Univ., Seoul, South Korea; Dept. of Biomed. Eng., Seoul Nat. Univ., Seoul, South Korea",2010 IEEE/ACIS 9th International Conference on Computer and Information Science,30-Sep-10,2010,,,561,566,"The measurement of similarity between documents is usually influenced by sparseness of term-document matrix. Latent semantic indexing (LSI) is an alternative method to solve the problem, and the dimension reduction by LSI improves the performance of the measurement of the similarity. In this study, LSI is examined as a method to cluster clinical documents containing the same clinical problems or disorders. The similarity of clinical documents was measured effectively with LSI. LSI performed better on clinical documents which can be characterized with medical terms, various expressions for the same concepts, abbreviations and typos, than editorials. Our result showed that LSI is useful for the measurement of the similarity of the clinical documents examined in this study. And the correlation between co-occurrence of terms and similarity is also analyzed as an important aspect of LSI. Not only the co-occurring terms but unshared terms between documents were found as factors influencing the similarity.",,978-1-4244-8198-9,10.1109/ICIS.2010.138,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591005,co-occurrence;latent semantic indexing;clinical document,Large scale integration;Editorials;Correlation;Discharges;Semantics;Biomedical measurements;Indexing,document handling;indexing;medical administrative data processing;pattern clustering;sparse matrices;statistical analysis,latent semantic indexing;clinical document clustering;term document matrix;co-occurrence,,3,,11,,30-Sep-10,,,IEEE,IEEE Conferences
Classifying text documents using unconventional representation,使用非常規表示法對文本文檔進行分類,B. S. Harish; S. V. Aruna Kumar; S. Manjunath,"Dept. of Information Science & Engineering, S J College of Engineering, Mysore, India; Dept. of Information Science & Engineering, S J College of Engineering, Mysore, India; Dept. of Computer Science, Central University of Kerala, India",2014 International Conference on Big Data and Smart Computing (BIGCOMP),17-Feb-14,2014,,,210,216,"Classification of text documents is one of the most common themes in the field of machine learning. Although a text document expresses a wide range of information, but it lacks the imposed structure of tradition database. Thus, unstructured data, particularly free running text data has to be transferred into a structured data. Hence, in this paper we represent the text document unconventionally by making use of symbolic data analysis concepts. We propose a new method of representing documents based on clustering of term frequency vectors. Term frequency vectors of each cluster are used to form a symbolic representation by the use of Mean and Standard Deviation. Further, term frequency vectors are used in the form a interval valued features. To cluster the term frequency vectors, we make use of Single Linkage, Complete Linkage, Average Linkage, K-Means and Fuzzy C-Means clustering algorithms. To corroborate the efficacy of the proposed model we conducted extensive experimentations on standard datasets like 20 Newsgroup Large, 20 Mini Newsgroup, Vehicles Wikipedia datasets and our own created datasets like Google Newsgroup and Research Article Abstracts. Experimental results reveal that the proposed model gives better results when compared to the state of the art techniques. In addition, as the method is based on a simple matching scheme, it requires a negligible time.",2375-9356,978-1-4799-3919-0,10.1109/BIGCOMP.2014.6741438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741438,Classification;Text Documents;Representation;Clustering Algorithms,Accuracy;Couplings;Vectors;Training;Text categorization;Clustering algorithms;Classification algorithms,data analysis;data structures;fuzzy set theory;learning (artificial intelligence);pattern classification;pattern clustering;pattern matching;text analysis,text document classification;unconventional text document representation;machine learning;unstructured data;free running text data;symbolic data analysis;term frequency vector clustering;symbolic representation;mean;standard deviation;interval valued features;single linkage;complete linkage;average linkage;K-means clustering algorithms;fuzzy C-means clustering algorithms;standard datasets;matching scheme,,2,,26,,17-Feb-14,,,IEEE,IEEE Conferences
A Clustering-Based Approach for Integrating Document-Category Hierarchies,基於聚類的文檔類別層次結構集成方法,T. Cheng; C. Wei,"Southern Taiwan Univ., Tainan; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",3-Mar-08,2008,38,2,410,424,"E-commerce applications generate and consume a tremendous amount of online information, which is typically available as textual documents. Conceivably, organizations and individuals generally use category sets or hierarchies to organize, archive, and access their documents. Meanwhile, organizations and individuals constantly acquire relevant documents from various Internet sources, each of which may organize its documents in a category set or hierarchy different from that used by the acquiring organization or individual. Consequently, the integration of source documents organized in a category hierarchy into an existing category hierarchy deployed by the acquiring organization or individual becomes an important issue in the e-commerce era. Existing category-integration techniques are mainly designed to integrate document catalogs, each of which is organized nonhierarchically (i.e., in a flat set). In this paper, we propose a clustering-based category-hierarchy integration (CHI) technique, which is an extension of the clustering-based category-integration (CCI) technique. Our empirical evaluation results show that the proposed CHI technique appears to improve the effectiveness of category-hierarchy integration compared with that attained by nonhierarchical category-integration techniques, particularly in homogeneous and comparable scenarios.",1558-2426,,10.1109/TSMCA.2007.914758,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459764,Category-hierarchy integration;document-category integration;document clustering;document management;taxonomy integration;text mining,Educational institutions;Internet;Catalogs;Technology management;Finance;Radiofrequency interference;Taxonomy;Text mining;Web sites,electronic commerce;pattern clustering;text analysis,textual document category hierarchy integration;e-commerce application;Internet,,10,,37,,3-Mar-08,,,IEEE,IEEE Journals
WordNet-Based and N-Grams-Based Document Clustering: A Comparative Study,基於WordNet和基於N Grams的文檔聚類：比較研究,A. Amine; Z. Elberrichi; M. Simonet; M. Malki,"Dept. of Comput. Sci., UDL Univ., Sidi Belabbes; Dept. of Comput. Sci., UDL Univ., Sidi Belabbes; TIMC-IMAG Lab., Joseph Fourier Univ., Grenoble; Dept. of Comput. Sci., UDL Univ., Sidi Belabbes","2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications",8-Dec-08,2008,,,394,401,"A great number of methods of unsupervised classifications also called clustering were applied to the textual documents. In this paper, we initially propose the method of the self-organizing maps of Kohonen for the clustering of the textual documents based on the n-grams representation. The same method based on the synsets of WordNet as terms for the representation of the textual documents will be studied thereafter. The effects of these methods are examined in several experiments using 4 measurements of similarity: the Cosine distance, the Euclidean distance, the squared Euclidean distance and the Manhattan distance. The reuters-21578 corpus is used for evaluation. The evaluation was done, by using the F-measure and the entropy. The results obtained show that in spite of the good results obtained by the method of the n-grams, the fact of adding lexical knowledge in the representation makes it possible to build a better classification.",,978-1-4244-3281-3,10.1109/BROADCOM.2008.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4696139,Document clustering;self-organizing maps of Kohonen;n-grams;WordNet;similarity;reuters-21578,Laboratories;Computer science;Self organizing feature maps;Euclidean distance;Broadband communication;Information technology;Application software;Biomedical measurements;Entropy;Internet,document handling;pattern classification;pattern clustering;self-organising feature maps,WordNet;document clustering;unsupervised classifications;Kohonen self-organizing maps;n-grams representation;Cosine distance;Euclidean distance;squared Euclidean distance;Manhattan distance;reuters-21578 corpus;F-measure;entropy;lexical knowledge;data representation,,3,,21,,8-Dec-08,,,IEEE,IEEE Conferences
NMF based dimension reduction methods for Turkish text clustering,基於NMF的土耳其文本聚類降維方法,A. G羹ran; M. C. Ganiz; H. S. Naibo?lu; H. O. Kapt覺ka癟t覺,"Computer Engineering Dept., Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey; Computer Engineering Dept., Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey; Computer Engineering Dept., Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey; Computer Engineering Dept., Do?u? University, Ac覺badem, Kad覺k繹y, 34722, Istanbul, Turkey",2013 IEEE INISTA,15-Aug-13,2013,,,1,5,"In this work, we analyze the effects of NMF based dimension reduction methods on clustering of Turkish documents by using k-means clustering algorithm. All experiments are conducted on two different datasets that we call Milliyet4c1k and 1150haber. The NMF based dimension reduction methods have two purposes: to reduce the original vector space by transformation and to reduce size and dimension by summarizing original documents. Experimental results show that NMF transformation yields to better clustering results on both datasets. Using k-means on summarized documents produces almost identical result with k-means on original documents. Although using summaries instead of full documents doesn't improve quality of clustering, we show that it significantly reduces the size of the processed data and execution time of k-means clustering algorithm.",,978-1-4799-0661-1,10.1109/INISTA.2013.6577618,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577618,Turkish text clustering;k-means;dimension reduction;NMF;NMF based text summarization,Clustering algorithms;Measurement;Semantics;Vectors;Clustering methods;Classification algorithms;Information systems,matrix decomposition;natural language processing;pattern clustering;text analysis,Turkish text clustering;NMF-based dimension reduction methods;Turkish document clustering;k-means clustering algorithm;Milliyet4c1k dataset;1150haber dataset;vector space reduction;summarized document size reduction;summarized document dimension reduction;execution time reduction;negative matrix factorization,,2,,24,,15-Aug-13,,,IEEE,IEEE Conferences
Procreant PSO for document clustering,用於文檔聚類的Procreant PSO,K. Premalatha; A. M. Natarajan,"Kongu Engineering College, Erode - 638 052, TN, India; Kongu Engineering College, Erode - 638 052, TN, India","2008 International Conference on Computing, Communication and Networking",24-Feb-09,2008,,,1,2,"Fast and high-quality document clustering algorithms play an important role towards the goal of organizing large amounts of information into a small number of meaningful clusters. Traditional clustering algorithms will search only a small sub-set of all possible clustering and consequently, there is no guarantee that the solution found will be optimal. This paper presents Procreant PSO (PPSO) algorithm for document clustering. In standard PSO the non-oscillatory route can quickly cause a particle to stagnate and also it may prematurely converge on suboptimal solutions that are not even guaranteed to be local optimum. In this paper a modification strategy is proposed for the particle swarm optimization (PSO) algorithm. The strategy adds reproduction when the stagnation in movement of the particle is identified and carries out local search to improve the goodness of fitting. Procreation has the capability to achieve faster convergence and better solution. Experiments results are examined with document corpus. It demonstrates that the PPSO algorithm statistically outperforms the Simple PSO and K-Means.",,978-1-4244-3594-4,10.1109/ICCCNET.2008.4787660,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787660,,Communication system security;Information security;Wireless sensor networks;Knowledge management;Resource management;Protection;Spread spectrum communication;Internet;Distributed computing;Computer science,particle swarm optimisation;pattern clustering,document clustering;procreant PSO;particle swarm optimization;modification strategy;k-means,,1,,,,24-Feb-09,,,IEEE,IEEE Conferences
An approach to visualize unorganized collections of documents,可視化無組織的文檔集合的方法,D. Karina Trejo T.; Ma. Auxilio Medina N.; M. Jorge de la Calleja; ?. A. Mart穩nez M.; J. Alfredo S獺nchez,"Laboratorio de Percepci籀n por Computadora, Universidad Polit矇cnica de Puebla, Tercer Carril del Ejido Serrano S/N, San Mateo Cuanal獺, Juan C. Bonilla, M矇xico; Laboratorio de Percepci籀n por Computadora, Universidad Polit矇cnica de Puebla, Tercer Carril del Ejido Serrano S/N, San Mateo Cuanal獺, Juan C. Bonilla, M矇xico; Laboratorio de Percepci籀n por Computadora, Universidad Polit矇cnica de Puebla, Tercer Carril del Ejido Serrano S/N, San Mateo Cuanal獺, Juan C. Bonilla, M矇xico; Laboratorio de Percepci籀n por Computadora, Universidad Polit矇cnica de Puebla, Tercer Carril del Ejido Serrano S/N, San Mateo Cuanal獺, Juan C. Bonilla, M矇xico; Laboratory of Interactive and Cooperative Technologies, Universidad de las Am矇ricas Puebla, Ex-hacienda Santa Catarina M獺rtir S/N, San Andr矇s Cholula, M矇xico","2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",1-May-14,2014,,,248,255,"In order to support search and retrieval of documents in digital libraries, this paper proposes an approach to visualize unorganized collections based on the results of two clustering algorithms namely k-means and COBWEB. These algorithms are implemented in Weka and they were chosen experimentally to produce a structure that can be explored by users in a 3D interface. Documents need to be processed before Weka can use them. We describe the main components of the proposed interface and show preliminary results of the application of those algorithms.",,978-1-4799-3469-0,10.1109/CONIELECOMP.2014.6808599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808599,Information visualization;clustering;digital libraries,Clustering algorithms;Machine learning algorithms;Vectors;Partitioning algorithms;Organizations;Libraries;Classification algorithms,data visualisation;digital libraries;document handling;information retrieval;pattern clustering,document searching;document retrieval;digital libraries;k-means clustering algorithm;COBWEB clustering algorithm;Weka;3D interface;document processing;unorganized document collection visualization,,,,6,,1-May-14,,,IEEE,IEEE Conferences
Documents clustering based on max-correntropy nonnegative matrix factorization,基於最大熵非負矩陣分解的文檔聚類,L. Li; J. Yang; Y. Xu; Z. Qin; H. Zhang,"David R. Cheriton School of Computer Science, University of Waterloo, ON N2L3G1, Canada; Department of Computer Science, University of North Georgia, Oakwood, 30566, USA; Pattem Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, China; Pattem Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, China; Pattem Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, China",2014 International Conference on Machine Learning and Cybernetics,15-Jan-15,2014,2,,850,855,"Nonnegative matrix factorization (NMF) has been successfully applied to many areas of both classification and clustering. Commonly used NMF algorithms mainly target on minimizing the l2 distance or the Kullback-Leibler (KL) divergence, which may not be suitable for nonlinear cases. In this paper, we propose a new decomposition method by maximizing the correntropy between the original and the product of two low-rank matrices for document clustering. This method also allows us to learn new basis vectors of the semantic feature space from data. To our knowledge, there is no existing work which clusters high dimensional document data by maximizing the correntropy in NMF. Our experimental results show the supremacy of the proposed method over other variants of NMF algorithms on Reuters21578 and TDT2 databasets.",2160-1348,978-1-4799-4215-2,10.1109/ICMLC.2014.7009720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009720,Document clustering;Nonnegative matrix factorization,Abstracts;Facsimile,document handling;matrix decomposition;pattern clustering,document clustering;max-correntropy nonnegative matrix factorization;NMF algorithms;Kullback-Leibler divergence;l2 distance minimization;low-rank matrices;semantic feature space;TDT2 databasets;Reuters 21578,,6,,34,,15-Jan-15,,,IEEE,IEEE Conferences
Automatic Clustering Assessment through a Social Tagging System,通過社交標籤系統自動進行聚類評估,E. Cunha; ?. Figueira,"CRACS, Univ. do Porto, Porto, Portugal; CRACS, Univ. do Porto, Porto, Portugal",2012 IEEE 15th International Conference on Computational Science and Engineering,24-Jan-13,2012,,,74,81,"Assessing the quality of the clustering process is fundamental in unsupervised clustering. In literature we can find three different clustering validity techniques: external criteria, internal criteria and relative criteria. In this paper, we focus on external criteria and present an algorithm that allows the implementation of external measures to assess clustering quality when the structure of the data set is unknown. To obtain an automatic partition of a data set and to reflect how documents must be grouped according to human intuition we use internal information present in data like descriptions provide by the users as tags and the distance between documents. The results show an evident correlation between manual and automatic classes indicating it is acceptable to use an automatic partition. In addition to presenting an alternative to finding the structure of the data set using meta-data such as tags, we also wanted to test the impact of their integration in the k-means++ algorithm and verify how it influences the quality of the formed clusters, suggesting a model of integration based on the occurrence of tags in document content. The experimental results indicate a positive impact when external measures are calculated, although there was no apparent correlation between the weight assigned to the tags and the quality of the obtained clusters.",,978-0-7695-4914-9,10.1109/ICCSE.2012.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6417277,clustering;effectiveness;cluster validity;tagging;quality assessment,Communities;Clustering algorithms;Vectors;Humans;Partitioning algorithms;Tagging;Manuals,document handling;meta data;pattern clustering;social networking (online);statistical analysis,automatic clustering assessment;social tagging system;clustering process quality assessment;unsupervised clustering validity techniques;external criteria;internal criteria;relative criteria;document content grouping;human intuition;internal information;manual classes;automatic classes;automatic partitioning;meta-data;k-means++ algorithm;tag weight assignment,,1,,16,,24-Jan-13,,,IEEE,IEEE Conferences
Multisets and Clustering XML Documents,多集和集群XML文檔,S. Iyer; D. A. Simovici,"Univ. of Massachusetts at Boston, Boston; Univ. of Massachusetts at Boston, Boston",19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007),4-Jan-08,2007,1,,267,274,We propose a novel and efficient solution to the problem of clustering XML documents based on their structure. We use operations on multisets of paths of document trees to define certain metrics on multisets. These metrics are used for clustering real and synthesized XML documents to produce high-quality clusterings.,2375-0197,978-0-7695-3015-4,10.1109/ICTAI.2007.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410294,,XML;Clustering algorithms;Artificial intelligence;Computer science;Engines;Markup languages;Costs;Data mining;Clustering methods;Fourier transforms,document handling;tree data structures;tree searching;XML,multisets metrics;XML document clustering;document tree path;high-quality clustering;eXtensible Markup Language,,,,13,,4-Jan-08,,,IEEE,IEEE Conferences
Pattern and Cluster Mining on Text Data,文本數據的模式和聚類挖掘,D. Agnihotri; K. Verma; P. Tripathi,"Dept. of Comput. Applic., NIT Raipur, Raipur, India; Dept. of Comput. Applic., NIT Raipur, Raipur, India; Dept. of Comput. Eng. & Applic., NITTTR Bhopal, Bhopal, India",2014 Fourth International Conference on Communication Systems and Network Technologies,29-May-14,2014,,,428,432,"Due to heavy use of electronics devices nowadays most of the information is available in electronic format and a substantial portion of information is stored as text such as in news articles, technical papers, books, digital libraries, email messages, blogs, and web pages. Mining the knowledge like pattern finding or clustering of similar kind of words is one of the important issues nowadays. This paper focuses on mining the important information from the text data. This paper uses the stories data set from project Guttenbergs William Shakespeare stories dataset for experimental study. R is used as Text Mining and statistical analysis tool in Ubuntu 12.04 LTS Linux Operating System. Frequent pattern mining is used to find the frequent terms, appeared in the documents and word Association among two or more words is measured at a given threshold value. Our algorithm uses cosine similarity in order to measure the distance between the words before clustering. The algorithm may be use to find the similarity between stories, news, emails. In this paper k-means and hierarchical agglomerative clustering algorithm is used to form the cluster.",,978-1-4799-3070-8,10.1109/CSNT.2014.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821432,text mining;stop words;stemming;TF - IDF;Clustering;Word Association,Text mining;Clustering algorithms;Algorithm design and analysis;Text analysis;Databases;Electronic mail,data mining;Linux;pattern clustering;statistical analysis;text analysis,frequent pattern mining;cluster mining;text data;knowledge mining;pattern finding;information mining;Guttenbergs William Shakespeare stories dataset;text mining;statistical analysis tool;Ubuntu 12.04 LTS Linux Operating System;frequent terms;documents;cosine similarity;k-means clustering algorithm;hierarchical agglomerative clustering algorithm,,13,,21,,29-May-14,,,IEEE,IEEE Conferences
Robust Volume Minimization-Based Matrix Factorization for Remote Sensing and Document Clustering,基於魯棒體積最小化的矩陣分解技術，用於遙感和文檔聚類,X. Fu; K. Huang; B. Yang; W. Ma; N. D. Sidiropoulos,"Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA",IEEE Transactions on Signal Processing,7-Oct-16,2016,64,23,6254,6268,"This paper considers volume minimization (VolMin)-based structured matrix factorization. VolMin is a factorization criterion that decomposes a given data matrix into a basis matrix times a structured coefficient matrix via finding the minimum-volume simplex that encloses all the columns of the data matrix. Recent work showed that VolMin guarantees the identifiability of the factor matrices under mild conditions that are realistic in a wide variety of applications. This paper focuses on both theoretical and practical aspects of VolMin. On the theory side, exact equivalence of two independently developed sufficient conditions for VolMin identifiability is proven here, thereby providing a more comprehensive understanding of this aspect of VolMin. On the algorithm side, computational complexity and sensitivity to outliers are two key challenges associated with real-world applications of VolMin. These are addressed here via a new VolMin algorithm that handles volume regularization in a computationally simple way, and automatically detects and iteratively downweights outliers, simultaneously. Simulations and real-data experiments using a remotely sensed hyperspectral image and the Reuters document corpus are employed to showcase the effectiveness of the proposed algorithm.",1941-0476,,10.1109/TSP.2016.2602800,National Scientific Foundation; Hong Kong Research Grants Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552586,Document clustering;hyperspectral unmixing;identifiability;matrix factorization;robustness against outliers;simplex-volume minimization (VolMin),Signal processing algorithms;Matrix decomposition;Hyperspectral imaging;Load modeling;Minimization,document handling;geophysical image processing;hyperspectral imaging;matrix decomposition;minimisation;pattern clustering;remote sensing,document clustering;robust volume minimization based structured matrix factorization;VolMin-based structured matrix factorization;data matrix decomposition;basis matrix;structured coefficient matrix;minimum-volume simplex;factor matrices;mild conditions;sufficient conditions;VolMin identifiability;computational complexity;outlier sensitivity;volume regularization;remotely sensed hyperspectral image;Reuters document corpus,,40,,45,,25-Aug-16,,,IEEE,IEEE Journals
A Distributed Flocking Approach for Information Stream Clustering Analysis,信息流聚類分析的分佈式植絨方法,Xiaohui Cui; T. E. Potok,"Oak Ridge National Laboratory, TN; Oak Ridge Nat. Lab., TN","Seventh ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD'06)",26-Jun-06,2006,,,97,102,"Intelligence analysts are currently overwhelmed with the amount of information streams generated everyday. There is a lack of comprehensive tool that can real-time analyze the information streams. Document clustering analysis plays an important role in improving the accuracy of information retrieval. However, most clustering technologies can only be applied for analyzing the static document collection because they normally require a large amount of computation resource and long time to get accurate result. It is very difficult to cluster a dynamic changed text information streams on an individual computer. Our early research has resulted in a dynamic reactive flock clustering algorithm which can continually refine the clustering result and quickly react to the change of document contents. This character makes the algorithm suitable for cluster analyzing dynamic changed document information, such as text information stream. Because of the decentralized character of this algorithm, a distributed approach is a very natural way to increase the clustering speed of the algorithm. In this paper, we present a distributed multi-agent flocking approach for the text information stream clustering and discuss the decentralized architectures and communication schemes for load balance and status information synchronization in this approach",,0-7695-2611-X,10.1109/SNPD-SAWN.2006.2,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640673,,Information analysis;Clustering algorithms;Information retrieval;Text analysis;Data mining;Biological system modeling;Birds;Data visualization;Laboratories;Algorithm design and analysis,document handling;information analysis;information retrieval;multi-agent systems;pattern clustering;resource allocation,distributed flocking approach;information stream clustering analysis;document clustering analysis;information retrieval;static document collection;dynamic changed text information streams;dynamic reactive flock clustering;document contents;distributed multiagent flocking approach;text information stream clustering;load balancing;status information synchronization,,,,17,,26-Jun-06,,,IEEE,IEEE Conferences
Document Topic Extraction Based on Wikipedia Category,基於維基百科類別的文檔主題提取,J. Yun; L. Jing; J. Yu; H. Huang; Y. Zhang,"Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China; Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China; Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China; Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China; Sch. of Control & Comput. Eng., North China Electr. Power Univ., Beijing, China",2011 Fourth International Joint Conference on Computational Sciences and Optimization,18-Jul-11,2011,,,852,856,"Document Topic Extraction aims at using several key phrases to describe the topics of documents. It can be applied in web document categorization and tagging, document clusters topic description and information retrieval tasks. In this paper, we propose a Wikipedia category-based document topic extraction method. Document is mapped to a set of Wikipedia categories and is represented as graph structure in order to conserve the relationship between Wikipedia categories. Then, document topic can be extracted by clustering the related Wikipedia categories in the document collection. Experiment in real data shows Wikipedia category-based document topic extraction method achieves the better result than latent topic modeling method, such as LDA.",,978-1-4244-9712-6,10.1109/CSO.2011.119,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957791,Topic Extraction;Document Representation;Wikipedia Category;Semantic Relatedness,Internet;Encyclopedias;Electronic publishing;Semantics;Sports equipment;Data mining,document handling;information retrieval;pattern clustering;Web sites,document topic extraction;document handling;Web document categorization;tagging;information retrieval task;Wikipedia category based document topic extraction method;document clustering,,3,,12,,18-Jul-11,,,IEEE,IEEE Conferences
Semantic Patent Clustering for Biomedical Communities,生物醫學社區的語義專利聚類,K. Khelif; A. Hedhili; M. Collard,"INRIA Sophia Antipolis Mediterranee, Sophia Antipolis; INRIA Sophia Antipolis Mediterranee, Sophia Antipolis; INRIA Sophia Antipolis Mediterranee, Sophia Antipolis",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,419,422,"In this paper, we present the Pat Clust clustering solution for textual documents based on semantic criteria. Our proposition is dedicated to patent documents of the biomedical domain. We present three different approaches and we show that semantic web techniques clearly allow to improve the quality of resulting clusters.",,978-0-7695-3496-1,10.1109/WIIAT.2008.71,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740486,patent clustering;semantic annotations;ontologies,Intelligent agent;Semantic Web;Intellectual property;Ontologies;Communities;Clustering methods;Text mining;Data mining;Frequency;Radio access networks,medical computing;patents;pattern clustering;semantic Web;text analysis,semantic patent clustering;biomedical community;Pat Clust clustering solution;textual documents;semantic criteria;patent documents;biomedical domain;semantic Web techniques,,1,,9,,6-Jan-09,,,IEEE,IEEE Conferences
Locally Consistent Concept Factorization for Document Clustering,用於文檔聚類的局部一致概念分解,D. Cai; X. He; J. Han,"Zhejiang University, Hangzhou; Zhejiang University, Hangzhou; University of Illinois at Urbana Champaign, Urbana",IEEE Transactions on Knowledge and Data Engineering,21-Apr-11,2011,23,6,902,913,"Previous studies have demonstrated that document clustering performance can be improved significantly in lower dimensional linear subspaces. Recently, matrix factorization-based techniques, such as Nonnegative Matrix Factorization (NMF) and Concept Factorization (CF), have yielded impressive results. However, both of them effectively see only the global euclidean geometry, whereas the local manifold geometry is not fully considered. In this paper, we propose a new approach to extract the document concepts which are consistent with the manifold geometry such that each concept corresponds to a connected component. Central to our approach is a graph model which captures the local geometry of the document submanifold. Thus, we call it Locally Consistent Concept Factorization (LCCF). By using the graph Laplacian to smooth the document-to-concept mapping, LCCF can extract concepts with respect to the intrinsic manifold structure and thus documents associated with the same concept can be well clustered. The experimental results on TDT2 and Reuters-21578 have shown that the proposed approach provides a better representation and achieves better clustering results in terms of accuracy and mutual information.",1558-2191,,10.1109/TKDE.2010.165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567104,Nonnegative matrix factorization;concept factorization;graph Laplacian;manifold regularization;clustering.,Manifolds;Geometry;Linear approximation;Laplace equations;Nearest neighbor searches;Vectors,document handling;graph theory;pattern clustering,locally consistent concept factorization;document clustering;matrix factorization based techniques;nonnegative matrix factorization;NMF;concept factorization;CF;manifold geometry;graph model;LCCF;graph Laplacian;document-to-concept mapping;intrinsic manifold structure,,166,,28,,9-Sep-10,,,IEEE,IEEE Journals
Analysis Partition Clustering and Similarity Measure on Al-Quran Verses,古蘭經經文的分析分區聚類和相似性度量,A. F. Huda; M. R. Deyana; Q. U. Safitri; W. Darmalaksana; U. Rahmani; Mahmud,"UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Hadits Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Islamic Religious Department,Bandung,Indonesia",2019 IEEE 5th International Conference on Wireless and Telematics (ICWT),3-Feb-20,2019,,,1,5,"Clustering text is an important application in data mining. This is related by grouping similar text documents together. In this study, several models are builts to classify Qur'anic verses on Surah Al-Baqarah using three clustering technique: k-means, bisecting kmeans, and k-medoid. Every verse in Surah al-Baqarah represented as a document derived from the translation of the Qur'an in English. Three similarity measures are also used: cosine similarity, jaccard similarity, and correlation coefficient. Then, the cluster of each combination of clustering technique with similarity measure is evaluated using average within cluster distance and davies bouldin index. The result show that the best performance is achieved by using the hemodoidal combined with cosine similarity. Finally obtained the category verses in the Surah al-Baqarah that correlate with each other.",,978-1-7281-4796-3,10.1109/ICWT47785.2019.8978215,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978215,Clustering;Partition Clustering;al-Quran Verses;Proximity;k-Means;Bisecting K-Means;K-Medoid,,data mining;natural language processing;pattern clustering;religion;text analysis,analysis partition clustering;similarity measure;text clustering;data mining;text documents;Quranic verses;Surah Al-Baqarah;clustering technique;cosine similarity;Jaccard similarity;cluster distance;Davies Bouldin index;category verses;Al-Quran verses;k-means clustering;k-medoid;correlation coefficient,,,,12,,3-Feb-20,,,IEEE,IEEE Conferences
Analysis of Web Search Engine Clicked Documents,Web搜索引擎單擊的文檔分析,D. F. Nettleton; L. Calderon-Benavides; R. Baeza-Yates,"University Pompeu Fabra, Spain; University Pompeu Fabra, Spain; Yahoo! Research, Spain",2006 Fourth Latin American Web Congress,11-Dec-06,2006,,,209,219,"In this paper we process and analyze Web search engine query and click data from the perspective of the documents (URs) selected. We initially define possible document categories and select descriptive variables to define the documents. The URL dataset is preprocessed and analyzed using some traditional statistical methods, and then processed by the Kohonen (1984) SOM clustering technique, which we use to produce a two level clustering. The clusters are interpreted in terms of the document categories and variables defined initially. Then we apply the C4.5 (Quinlan, 1993) rule induction algorithm to produce a decision tree for the document category. The objective of the paper is to apply a systematic data mining process to click data, contrasting non-supervised (Kohonen) and supervised (C4.5) methods to cluster and model the data, in order to identify document profiles which relate to theoretical user behavior, and document (URL) organization",,0-7695-2693-4,10.1109/LA-WEB.2006.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022112,,Web search;Search engines;Uniform resource locators;Clustering algorithms;Data mining;Partitioning algorithms;Decision trees;Input variables;Predictive models;Data processing,classification;data mining;decision trees;document handling;Internet;pattern clustering;query processing;search engines;self-organising feature maps,document analysis;Web search engine;query analysis;click data analysis;document categorization;statistical methods;Kohonen SOM clustering;two level clustering;C4.5 rule induction;decision tree;data mining;data clustering;document organization,,2,,9,,11-Dec-06,,,IEEE,IEEE Conferences
Measuring Group Cohesion in Document Collections,測量文檔集合中的組內聚力,B. Renoust; G. Melan癟on; M. Viaud,"LaBRI, INRIA Bordeaux Sud-Ouest, Talence, France; LaBRI, INRIA Bordeaux Sud-Ouest, Talence, France; Inst. Nat. de l'Audiovisuel (INA), Paris, France",2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),23-Dec-13,2013,1,,373,380,"Exploring document collections remains a focus of research. This task can be tackled using various techniques, typically ranking documents according to a relevance index or grouping documents based on various clustering algorithms. The task complexity produces results of varying quality that inevitably carry noise. Users must be careful when interpreting document relevance or groupings. We address this problem by computing cohesion measures for a group of documents confirming/infirming whether it can be trusted to form a semantically cohesive unit. The index is inspired from past work in social network analysis (SNA) and illustrates how document exploration can benefit from SNA techniques.",,978-0-7695-5145-6,10.1109/WI-IAT.2013.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690039,document collection exploration;document group cohesion;entanglement index;text analytics,Indexing;Vectors;Road safety;Bipartite graph;Eigenvalues and eigenfunctions;Social network services,document handling;network theory (graphs);social sciences computing,document confirmation;document exploration;SNA techniques;social network analysis;semantically cohesive unit;document information;document relevance index;clustering algorithms;document grouping;document ranking;document collections;group cohesion measurement,,4,,25,,23-Dec-13,,,IEEE,IEEE Conferences
WISE: Hierarchical Soft Clustering of Web Page Search Results Based on Web Content Mining Techniques,WISE：基於Web內容挖掘技術的網頁搜索結果的層次軟聚類,R. Campos; G. Dias; C. Nunes,"University of Beira Interior, Portugal; University of Beira Interior, Portugal; University of Beira Interior, Portugal",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,301,304,"Typically, search engines are low precision in response to a query, retrieving lots of useless Web pages, and missing some other important ones. In this paper, we study the problem of the hierarchical clustering of Web pages search results. In particular, we propose an architecture called WISE, a meta-search engine that automatically builds clusters of related Web pages embodying one meaning of the query. These clusters are then hierarchically organized and labeled with a phrase representing the key concept of the cluster and the corresponding Web documents. The system which is a Web-based interface (soon available at wise.di.ubi.pt), introduces some interesting new ideas, such as the preselection of the retrieved Web pages, the capacity to statistically detect phrases within documents and the representation of documents based on their most relevant key concepts by using Web content mining techniques. The final step of the system is supported by a graph-based overlapping clustering algorithm which groups the selected documents into a hierarchy of clusters",,0-7695-2747-7,10.1109/WI.2006.201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061381,,Web pages;Search engines;Content based retrieval;Humans;Bioinformatics;Metasearch;Clustering algorithms;Service oriented architecture;Information analysis;Information retrieval,data mining;document handling;graph theory;information retrieval;Internet;pattern clustering;search engines,WISE architecture;Web page search result hierarchical soft clustering;Web content mining;meta-search engine;Web document;Web-based interface;Web page retrieval;document phrase statistical detection;document representation;graph-based overlapping clustering algorithm;document clustering,,5,,19,,15-Jan-07,,,IEEE,IEEE Conferences
Multi-skew detection of Indian script documents,多傾斜檢測印度文字文件,U. Pal; M. Mitra; B. B. Chaudhuri,"Comput. Vision & Pattern Recognition Unit, Indian Stat. Inst., Calcutta, India; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,292,296,"There are many documents where text lines are not parallel to each other i.e. these lines have different inclinations with the horizontal lines (multi-skew documents). For the OCR of such a document we have to estimate the skew angle of individual text lines because a single rotation cannot de-skew all text lines of the document. In this paper, we describe a robust technique for multi-skew angle detection from Indian documents containing the most popular Indian scripts Devnagari and Bangla. Most characters in these scripts have horizontal lines at the top, called head-lines. The character head-lines usually connect one another in a word and the word appears as a single component. In the proposed method, the connected components are at first labeled and selected. The upper envelopes of selected components are found by column-wise scanning from the top of the component. Portions of the upper envelope satisfying the properties of a digital straight line are detected. They are then clustered into groups belonging to single text lines. Estimates from these individual clusters give the skew angle of each text line. The proposed multi-skew detection technique has an accuracy about 98.3%.",,0-7695-1263-1,10.1109/ICDAR.2001.953801,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953801,,Strips;Fourier transforms;Optical character recognition software;Robustness;Computer vision;Pattern recognition;Envelope detectors;Humans;Goniometers;Gray-scale,optical character recognition;document image processing;image segmentation;natural languages,OCR;Indian scripts;multi-skew angle detection;skew angle;Devnagari;Indian script documents;Bangla;document images;character segmentation;skew estimation;skew correction,,9,,12,,7-Aug-02,,,IEEE,IEEE Conferences
Extending the Growing Hierarchal SOM for clustering documents in graphs domain,擴展增長的層次結構SOM以在圖域中對文檔進行聚類,M. F. Hussin; M. R. Farra; Y. El-Sonbaty,NA; NA; NA,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),26-Sep-08,2008,,,4028,4035,"The growing hierarchal self-organizing map (GHSOM) is the most efficient model among the variants of SOM. It is used successfully in document clustering and in various pattern recognition applications effectively. The main constraint that limits the implementation of this model and all the other variants of SOM models is that they work only with vector space model (VSM). In this paper, we extend the GHSOM to work in the graph domain to enhance the quality of clusters. Specifically, we represent the documents by graphs and then cluster those documents by using a new algorithm G-GHSOM: graph-based growing merarchal SOM after modifying its operations to work with the graph instead of vector space. We have tested the G-GHSOM on two different document collections using three different measures for evaluating clustering quality. The experimental results of the proposed G-GHSOM show an improvement in terms of clustering quality compared to classical GHSOM.",2161-4407,978-1-4244-1820-6,10.1109/IJCNN.2008.4634377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4634377,,Neurons;Clustering algorithms;Length measurement;Construction industry;Rivers;Artificial neural networks;Image edge detection,document handling;graph theory;self-organising feature maps,growing hierarchal SOM;clustering documents;graphs domain;growing hierarchal self-organizing map;document clustering;pattern recognition;vector space model;VSM;graph-based growing merarchal;document collections,,4,,5,,26-Sep-08,,,IEEE,IEEE Conferences
Text Clustering using Frequent Contextual Termset,使用頻繁上下文術語集的文本聚類,T. M. Akhriza; Y. Ma; J. Li,"Sch. of Commun. & Inf. Syst., Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ., Shanghai, China; Sch. of Inf. Security Eng., Shanghai Jiao Tong Univ., Shanghai, China","2011 International Conference on Information Management, Innovation Management and Industrial Engineering",29-Dec-11,2011,1,,339,342,"We introduce frequent contextual term set (FCT) as an alternative concept of term set construction for text clustering which is produced from the interestingness of documents. Comparing to state-of-art term set, the proposed approach has some advantages: (1) more efficient in term set production (2) more effective in storing the vocabulary amongst documents which express the context amongst documents and (3) more suitable to discover specificity of dataset. To utilize FCT we also introduce frequent contextual term set based hierarchical clustering (FCTHC) which adopts the concept of cancroids in K-means with some main differences. The experiment shows that FCT is the correct pattern to perform text clustering and FCTHC provides flexible approach in clusters construction.",2155-1472,978-1-61284-450-3,10.1109/ICIII.2011.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115455,Frequent Contextual Termset;Frequent Itemset;Text clustering,Itemsets;Clustering algorithms;Context;Equations;Production;Merging;Data mining,pattern clustering;text analysis;vocabulary,text clustering;term set construction;document interestingness;term set production;vocabulary storage;dataset specificity discovery;frequent contextual term set based hierarchical clustering;cancroid concept;k-means,,,,8,,29-Dec-11,,,IEEE,IEEE Conferences
Evolving document features for Web document clustering: a feasibility study,Web文檔集群中不斷發展的文檔功能：可行性研究,M. P. Sinka; D. W. Corne,"Dept. of Comput. Sci., Reading Univ., UK; NA",Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753),3-Sep-04,2004,1,,891,897 Vol.1,"Document analysis and its associated research underpins Web intelligence and the envisaged 'semantic Web'. A key issue is how to encode a document without losing salient information. Current research almost always uses fixed-length vectors based on word (term) frequency (TF) and/or variants thereof. We explore the question of alternative encodings, and we search for such encodings using an evolutionary algorithm (EA). These alternatives consider a variety of other features that can be extracted from a document, and the EA explores the space of weighted combinations of these. Tests on the BankSearch dataset were able to find encodings which outperformed previous results using TF-based encodings. Among several tentative findings it seems clear that the ideal encoding is highly task-dependent, and we can recommend certain features as useful for specific types of document clustering tasks.",,0-7803-8515-2,10.1109/CEC.2004.1330955,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1330955,,Search engines;Web sites;Encoding;Information retrieval;Internet;Taxonomy;Computer science;Space exploration;Semantic Web;Frequency,evolutionary computation;Internet;text analysis;pattern clustering,Web document clustering;document analysis;Web intelligence;semantic Web;document encoding;fixed-length vectors;term frequency;evolutionary algorithm;BankSearch dataset;TF-based encodings;document features;World Wide Web,,2,,15,,3-Sep-04,,,IEEE,IEEE Conferences
Correlation-based document clustering using web logs,使用Web日誌的基於相關性的文檔聚類,Zhong Su; Qiang Yang; Hongjiang Zhang; Xiaowei Xu; Yuhen Hu,"Dept. of Comput. Sci., Tsinghua Univ., Beijing, China; NA; NA; NA; NA",Proceedings of the 34th Annual Hawaii International Conference on System Sciences,7-Aug-02,2001,,,7 pp.,,"A problem facing information retrieval on the web is how to effectively cluster large amounts of web documents. One approach is to cluster the documents based on information provided only by users' usage logs and not by the content of the documents. A major advantage of this approach is that the relevancy information is objectively reflected by the usage logs; frequent simultaneous visits to two seemingly unrelated documents should indicate that they are in fact closely related. In this paper, we present a recursive density based clustering algorithm that can adaptively change its parameters intelligently. Our clustering algorithm RDBC (Recursive Density Based Clustering algorithm) is based on DBSCAN, a density based algorithm that has been proven in its ability in processing very large datasets. The fact that DBSCAN does not require the pre-determination of the number of clusters and is linear in time complexity makes it particularly attractive in web page clustering. It can be shown that RDBC require the same time complexity as that of the DBSCAN algorithm. In addition, we prove both analytically and experimentally that our method yields clustering results that are superior to that of DBSCAN.",,0-7695-0981-9,10.1109/HICSS.2001.926536,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=926536,,Clustering algorithms;Information retrieval;Scattering;Clustering methods,information retrieval;computational complexity;information resources;Internet,correlation-based document clustering;web logs;information retrieval;web documents;relevancy information;recursive density based clustering algorithm;clustering algorithm RDBC;DBSCAN;density based algorithm;very large datasets;time complexity;web page clustering,,4,,14,,7-Aug-02,,,IEEE,IEEE Conferences
Active clustering of document fragments using information derived from both images and catalogs,使用源自圖像和目錄的信息對文檔片段進行主動聚類,L. Wolf; L. Litwak; N. Dershowitz; R. Shweka; Y. Choueka,"The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Friedberg Genizah Project, Jerusalem, Israel; The Friedberg Genizah Project, Jerusalem, Israel",2011 International Conference on Computer Vision,12-Jan-12,2011,,,1661,1667,"Many significant historical corpora contain leaves that are mixed up and no longer bound in their original state as multi-page documents. The reconstruction of old manuscripts from a mix of disjoint leaves can therefore be of paramount importance to historians and literary scholars. Previously, it was shown that visual similarity provides meaningful pair-wise similarities between handwritten leaves. Here, we go a step further and suggest a semiautomatic clustering tool that helps reconstruct the original documents. The proposed solution is based on a graphical model that makes inferences based on catalog information provided for each leaf as well as on the pairwise similarities of handwriting. Several novel active clustering techniques are explored, and the solution is applied to a significant part of the Cairo Genizah, where the problem of joining leaves remains unsolved even after a century of extensive study by hundreds of scholars.",2380-7504,978-1-4577-1102-2,10.1109/ICCV.2011.6126428,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126428,,Catalogs;Computational modeling;Graphical models;Complexity theory;Humans;Data models;Visualization,document image processing;history;pattern clustering,active clustering;document fragments;image information;historical corpora;multipage documents;semiautomatic clustering;original document clustering;graphical model;catalog information;handwriting pairwise similarities,,5,,15,,12-Jan-12,,,IEEE,IEEE Conferences
Hesitant Distance Similarity Measures for Document Clustering,用於文檔聚類的猶豫距離相似性度量,N. Sahu; G. S. Thakur,"Singhania University, Pacheri Bari, India; MANIT, Bhopal (M.P.), India",2011 World Congress on Information and Communication Technologies,30-Jan-12,2011,,,430,438,"This paper presents new approach, Hesitant Distance Similarity Measures for Document Clustering. The proposed Hesitant Distance Similarity Measures approach is based on Fuzzy Hesitant Sets. In this paper we have used fifty Similarity Measures from f1 to f50. The steps, Document collection, Text Pre-processing, Feature Selection, Indexing, Clustering Process and Results Analysis are used. Twenty News group data sets [27] are used in the Experiments. The experimental results are evaluated using the Analytical SAS 9.0 Software. The Experimental Results show the proposed approach out performs.",,978-1-4673-0126-8,10.1109/WICT.2011.6141284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141284,Hesitant fuzzy set;Similarity measure;Clustering;Distance measure,Clustering methods;Euclidean distance;Communications technology;Accuracy;Weight measurement;Hamming distance;Clustering algorithms,fuzzy set theory;indexing;pattern clustering;text analysis,hesitant distance similarity measure;document clustering;fuzzy hesitant set;document collection;text preprocessing;feature selection;indexing;result analysis;analytical SAS 9.0 software,,3,,29,,30-Jan-12,,,IEEE,IEEE Conferences
Symmetric Nonnegative Matrix Factorization: Algorithms and Applications to Probabilistic Clustering,對稱非負矩陣分解：算法和在概率聚類中的應用,Z. He; S. Xie; R. Zdunek; G. Zhou; A. Cichocki,"Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Faculty of Automation, Guangdong University of Technology, Guangzhou, China; Institute of Telecommunications, Teleinformatics, and Acoustics, Wroclaw University of Technology, Wroclaw, Poland; Laboratory for Advanced Brain Signal Processing, RIKEN Brain Science Institute, Saitama, Japan; RIKEN Brain Science Institute, Saitama, Japan",IEEE Transactions on Neural Networks,12-Dec-11,2011,22,12,2117,2131,"Nonnegative matrix factorization (NMF) is an unsupervised learning method useful in various applications including image processing and semantic analysis of documents. This paper focuses on symmetric NMF (SNMF), which is a special case of NMF decomposition. Three parallel multiplicative update algorithms using level 3 basic linear algebra subprograms directly are developed for this problem. First, by minimizing the Euclidean distance, a multiplicative update algorithm is proposed, and its convergence under mild conditions is proved. Based on it, we further propose another two fast parallel methods: 帢-SNMF and 帣 -SNMF algorithms. All of them are easy to implement. These algorithms are applied to probabilistic clustering. We demonstrate their effectiveness for facial image clustering, document categorization, and pattern clustering in gene expression.",1941-0093,,10.1109/TNN.2011.2172457,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061964,Basic linear algebra subprograms;completely positive;coordinate update;multiplicative update;nonnegative matrix factorization;parallel update;probabilistic clustering;symmetric nonnegative matrix factorization,Symmetric matrices;Clustering algorithms;Algorithm design and analysis;Probabilistic logic;Convergence;MATLAB;Linear algebra,document image processing;face recognition;genetic algorithms;linear algebra;matrix decomposition;pattern clustering;probability;unsupervised learning,symmetric nonnegative matrix factorization;probabilistic clustering;unsupervised learning method;document image processing;document semantic analysis;symmetric NMF;NMF decomposition;parallel multiplicative update algorithms;level 3 basic linear algebra subprograms;Euclidean distance;fast parallel methods;帢-SNMF algorithms;帣 -SNMF algorithms;facial image clustering;document categorization;pattern clustering;gene expression,"Algorithms;Artificial Intelligence;Data Interpretation, Statistical;Image Interpretation, Computer-Assisted;Models, Theoretical;Pattern Recognition, Automated",81,,45,,26-Oct-11,,,IEEE,IEEE Journals
Efficiency of unstructured text search improving methods in the electronic archive of computer-aided design systems,計算機輔助設計系統電子檔案中非結構化文本搜索改進方法的效率,A. Y. Alikov; A. V. Kalinichenko,"The North Caucasian Institute of Mining and Metallurgy (the state technological university), Vladikavkaz, Russia; The North Caucasian Institute of Mining and Metallurgy (the state technological university), Vladikavkaz, Russia","2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",23-Oct-17,2017,,,1,5,"This paper proposes an approach for improving the efficiency of the unstructured text documents search in the electronic archive of computer-aided design systems by increasing the pertinence and relevance. The authors propose a mathematical description of the presentation of information based on fuzzy sets, the context-aware search terms. There was developed an algorithm for constructing a search pattern of the document based on the proposed mathematical description. The researchers propose the method and the algorithm for clusters formation from associated search terms in the document that allows one to form a visual representation of the substantive content of the document as a graph. An algorithm for searching for semantically similar documents was developed and implemented. The proposed method of the semantically similar documents search automatization allows one to: clarify the information needs of users and build up a more detailed search query by introduction of a dialogue; take into account the context similarity of the of significant documents terms collection and of the term sample documents context. The conducted experiments demonstrate the effectiveness of the proposed approach.",,978-1-5090-5648-4,10.1109/ICIEAM.2017.8076469,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076469,information retrieval;semantically similar documents search;retrieval efficiency,Industrial engineering;Manufacturing;Fuzzy sets;Search problems;Clustering algorithms;Visualization,CAD;document handling;fuzzy set theory;information retrieval;pattern clustering;query processing;text analysis;ubiquitous computing,fuzzy sets;context-aware search terms;unstructured text documents search;clusters formation;computer-aided design systems;electronic archive;search query,,,,15,,23-Oct-17,,,IEEE,IEEE Conferences
Exploiting User Tagging for Web Service Co-Clustering,利用用戶標記進行Web服務聯合集群,T. Liang; Y. Chen; W. Gao; M. Chen; M. Zheng; J. Wu,"School of Computer Science and Technology, Hangzhou Dianzi University, Hanzhou, China; College of Computer Science and Technology, Zhejiang University, Hanzhou, China; College of Computer Science and Technology, Zhejiang University, Hanzhou, China; Hithink RoyalFlush Information Network Company, Ltd., Hangzhou, China; Zhejiang Hithink RoyalFlush Artificial Intelligence Research Institute, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hanzhou, China",IEEE Access,28-Nov-19,2019,7,,168981,168993,"We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.",2169-3536,,10.1109/ACCESS.2019.2950355,Subject of the Major Commissioned Project??Research on China?s Image in the Big Data of Zhejiang Province?s Social Science Planning Advantage Discipline ?Evaluation and Research on the Present Situation of China?s Image; Ministry of Education of China; Zhejiang University; Major Scientific Project of Zhejiang Laboratory; National Natural Science Foundation of China; State Key Laboratory of Medical Neurobiology; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892516,Web service;WSDL documents clustering;co-clustering;tag recommendation,Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction,data integrity;document handling;pattern clustering;recommender systems;Web services;word processing,tagging data quality;user tagging;word distribution;WSDL documents;tag recommendation approaches;Web service coclustering;Web services clustering framework,,,,42,CCBY,6-Nov-19,,,IEEE,IEEE Journals
Search Results Clustering Based on a Linear Weighting Method of Similarity,基於線性相似度加權的搜索結果聚類,D. Zheng; H. Liu; T. Zhao,"MOE-MS Key Lab. of Natural Language Process. & Speech, Harbin Inst. of Technol., Harbin, China; MOE-MS Key Lab. of Natural Language Process. & Speech, Harbin Inst. of Technol., Harbin, China; MOE-MS Key Lab. of Natural Language Process. & Speech, Harbin Inst. of Technol., Harbin, China",2011 International Conference on Asian Language Processing,2-Jan-12,2011,,,123,126,"The cluster of search results can facilitate users in finding the needed from massive information. But the effect of the traditional text clustering has been verified not good enough. Lingo Algorithm, which adopts LSI for clustering, generates candidate labels first, then distributes the documents, and forms the clusters finally. On the basis of Lingo Algorithm, this paper presents a linear weighted method of Single-Pass improvement, which integrates HowNet semantic similarity and cosine similarity, fuses and rediscovers clusters, and extracting the cluster labels. The experiments have showed that our method it achieves a good results in clusters in the form of purity and F-measure.",,978-1-4577-1733-8,10.1109/IALP.2011.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121485,Text clustering;Information retrieval;Lingo algorithm;Semantic similarity;Cosine similarity,Clustering algorithms;Semantics;Matrix decomposition;Feature extraction;Vectors;Search engines;Fuses,information needs;pattern clustering;search problems;text analysis,linear weighting method;information need;text clustering;Lingo algorithm;LSI;pattern clustering;document handling;F-measure;semantic similarity,,2,,8,,2-Jan-12,,,IEEE,IEEE Conferences
Clustering analysis of EMG cyclic patterns: A validation study across multiple locomotion pathologies,EMG循環模式的聚類分析：多種運動病理學的驗證研究,V. Agostini; S. Rosati; C. Castagneri; G. Balestra; M. Knaflitz,"Department of Electronics and Telecommunications, Politecnico di Torino, Torino, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Torino, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Torino, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Torino, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Torino, Italy",2017 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),7-Jul-17,2017,,,1,5,"Among other applications, electromyography (EMG) is used in the assessment of locomotion pathologies to quantitatively document abnormal muscle activation patterns during walking. However, EMG cyclic patterns are affected by high cycle-to-cycle variability. Previous research introduced a clustering approach (CIMAP) to recognize gait cycles with similar EMG onset/offset timings, reducing variability. To demonstrate the feasibility of the approach, the algorithm was validated on healthy subjects. The aim of this study is to extend the validation of the algorithm to multiple locomotion pathologies (both orthopedic and neurological). Gait data of a total of 50 subjects suffering from 5 different locomotion alterations were analyzed, considering 4 lower limb muscles. For each patient, datasets were built grouping EMG cycles with the same number of muscle activations. Then, hierarchical clustering was applied to each dataset and cycle-to-cycle variability was calculated for each cluster. Our results showed that CIMAP reduced the median variability below 5% of the gait cycle, for all the considered pathologies. Analyzing the number of clusters obtained we found that, in the great majority of cases, gait cycles cannot be bunched into a single group, but rather 2 or more clusters are necessary. As a consequence, the cluster representative elements, calculated by averaging cycles belonging to the same cluster, provide more trustworthy information for the clinician than indiscriminately averaging all cycles from a dataset.",,978-1-5090-3596-0,10.1109/I2MTC.2017.7969746,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7969746,clustering;gait analysis;EMG;human locomotion,Electromyography;Clustering algorithms;Muscles;Pathology;Timing;Legged locomotion;Diseases,electromyography;gait analysis;muscle;pattern clustering,cluster representative element;lower limb muscle analysis;EMG onset-offset timing;gait cycle recognition;CIMAP;cycle-to-cycle variability;walking;quantitatively document abnormal muscle activation patterns;locomotion pathology assessment;electromyography;EMG cyclic pattern;clustering analysis,,3,,18,,7-Jul-17,,,IEEE,IEEE Conferences
Automated Forgery Detection in Multispectral Document Images Using Fuzzy Clustering,使用模糊聚類的多光譜文檔圖像自動偽造檢測,M. J. Khan; A. Yousaf; K. Khurshid; A. Abbas; F. Shafait,"Dept. of Electr. Eng., Inst. of Space Technol., Islamabad, Pakistan; Dept. of Electr. Eng., Inst. of Space Technol., Islamabad, Pakistan; Dept. of Electr. Eng., Inst. of Space Technol., Islamabad, Pakistan; Sch. of Electr. Eng. & Comput., Univ. of Newcastle, Newcastle, NSW, Australia; Sch. of Electr. Eng. & Comput. Sci., Nat. Univ. of Sci. & Technol., Islamabad, Pakistan",2018 13th IAPR International Workshop on Document Analysis Systems (DAS),25-Jun-18,2018,,,393,398,"Multispectral imaging allows for analysis of images in multiple spectral bands. Over the past three decades, airborne and satellite multispectral imaging have been the focus of extensive research in remote sensing. In the recent years, ground based multispectral imaging has gained an immense amount of interest in the fields ranging from computer vision and medical imaging to art, archaeology and computational forensics. The rich information content in multispectral images allows forensic experts to examine the chemical composition of forensic traces. Due to its rapid, non-contact and non-destructive characteristics, multispectral imaging is an effective tool for visualization, age estimation, detection and identification of forensic traces in document images. Ink mismatch is a key indicator of forgery in a document. Inks of different materials exhibit different spectral signature even if they have the same color. Multispectral analysis of questioned documents images allows identification and discrimination of visually similar inks. In this paper, an efficient automatic ink mismatch detection technique is proposed which uses Fuzzy C-Means Clustering to divide the spectral responses of ink pixels in handwritten notes into different clusters which relate to the unique inks used in the document. Sauvola's local thresholding technique is employed to efficiently segment foreground text from the document image. Furthermore, feature selection is used to optimize the performance of the proposed method. The presented method provides better ink discrimination results than state-of-the-art methods when tested on publicly available UWA Writing Inks Dataset.",,978-1-5386-3346-5,10.1109/DAS.2018.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395228,Multispectral Imaging;Forgery Detection;Ink Mismatch Detection;FCM Clustering;Feature Selection,Ink;Multispectral imaging;Forensics;Forgery;Image analysis;Text analysis;Image color analysis,archaeology;document image processing;image segmentation;text analysis,multispectral images;document image;multispectral analysis;multispectral document images;airborne imaging;satellite multispectral imaging;medical imaging;archaeology;computational forensics;automatic ink mismatch detection technique,,6,,45,,25-Jun-18,,,IEEE,IEEE Conferences
Discovering Latent Semantics in Web Documents Using Fuzzy Clustering,使用模糊聚類發現Web文檔中的潛在語義,I. Chiang; C. C. Liu; Y. Tsai; A. Kumar,"Graduate Institute of Biomedical Informatics, Taipei Medical University, Taipei, Taiwan; Institute of Biomedical Engineering, National Taiwan University, Taipei, Taiwan; Institute of Biomedical Engineering, National Taiwan University, Taipei, Taiwan; Goa Institute of Management, Ribandar, India",IEEE Transactions on Fuzzy Systems,25-Nov-15,2015,23,6,2122,2134,"Web documents are heterogeneous and complex. There exists complicated associations within one web document and linking to the others. The high interactions between terms in documents demonstrate vague and ambiguous meanings. Efficient and effective clustering methods to discover latent and coherent meanings in context are necessary. This paper presents a fuzzy linguistic topological space along with a fuzzy clustering algorithm to discover the contextual meaning in the web documents. The proposed algorithm extracts features from the web documents using conditional random field methods and builds a fuzzy linguistic topological space based on the associations of features. The associations of cooccurring features organize a hierarchy of connected semantic complexes called ?CONCEPTS,??wherein a fuzzy linguistic measure is applied on each complex to evaluate 1) the relevance of a document belonging to a topic, and 2) the difference between the other topics. Web contents are able to be clustered into topics in the hierarchy depending on their fuzzy linguistic measures; web users can further explore the CONCEPTS of web contents accordingly. Besides the algorithm applicability in web text domains, it can be extended to other applications, such as data mining, bioinformatics, content-based, or collaborative information filtering, etc.",1941-0034,,10.1109/TFUZZ.2015.2403878,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042824,fuzzy web hierarchical clustering;fuzzy linguistic topological space;named entity recognition;fuzzy semantic topology;fuzzy aggregation algorithm;Fuzzy aggregation algorithm;fuzzy linguistic topological space;fuzzy semantic topology;fuzzy web hierarchical clustering;named entity recognition (NER),Semantics;Pragmatics;Clustering algorithms;Feature extraction;Neural networks;Context;Data mining,computational linguistics;document handling;feature extraction;fuzzy set theory;random processes;semantic Web,latent semantics;Web documents;fuzzy clustering;fuzzy linguistic topological space;feature extraction;conditional random field methods;connected semantic complexes;CONCEPTS,,20,,61,,16-Feb-15,,,IEEE,IEEE Journals
Frequent Itemsets Methods for Text Clustering,文本聚類的頻繁項集方法,C. E. Saili; S. Fatimi; L. Alaoui,"International University of Rabat,TIC Lab,Sala Al Jadida,Morocco; International University of Rabat,TIC Lab,Sala Al Jadida,Morocco; International University of Rabat,TIC Lab,Sala Al Jadida,Morocco",2020 International Conference on Intelligent Systems and Computer Vision (ISCV),23-Sep-20,2020,,,1,5,"Text clustering is a crucial application of data mining. It can be used to structure hypertext documents or large sets of text. Many research works have dived into document clustering as a technique for improving search, information retrieval, document browsing, automatic topic identification, as well as the primitive task of clustering. Major challenges are entangling researchers, especially when working with large scale datasets, such as very high dimensionality and cluster labeling. To tackle these challenges, a number of techniques using frequent itemsets mining methods in text clustering have been proposed. In this paper, we review such techniques while highlighting their strengths and limitations. With the analysis of associated methodologies, we also propose a general framework for the task of text clustering using frequent itemsets mining algorithms.",,978-1-7281-8041-0,10.1109/ISCV49265.2020.9204168,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204168,text clustering;frequent itemsets mining;apriori;dimensionality reduction;k-means;htbdc;mtdc,,data mining;hypermedia;information retrieval;pattern clustering;text analysis,document clustering;document browsing;cluster labeling;text clustering;frequent itemsets mining;hypertext documents;data mining;information retrieval;automatic topic identification,,,,17,,23-Sep-20,,,IEEE,IEEE Conferences
Clustered genetic semantic graph approach for multi-document abstractive summarization,多文檔抽象摘要的聚類遺傳語義圖方法,A. Khan; N. Salim; H. Farman,"Faculty of Computing, Universiti Teknologi Malaysia, Johor, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, Johor, Malaysia; Department of Computer Science, Islamia College Peshawar (Chartered University), Pakistan",2016 International Conference on Intelligent Systems Engineering (ICISE),23-May-16,2016,,,63,70,"Multi-document summarization aims to produce a compressed version of numerous online text documents and preserves the salient information. A particular challenge for multi-document summarization is that there is an inevitable overlap in the information stored in different documents. Thus, effective summarization methods that merge similar information across the documents are desirable. This paper introduces a clustered genetic semantic graph approach for multi-document abstractive summarization. The semantic graph from the document set is constructed in such a way that the graph vertices represent the predicate argument structures (PASs), extracted automatically by employing semantic role labeling (SRL); and the edges of graph correspond to semantic similarity weight determined from PAS-to-PAS semantic similarity, and PAS-to-document relationship. The PAS-to-document relationship is expressed by different features, weighted and optimized by genetic algorithm. The salient graph nodes (PASs) are ranked based on modified weighted graph based ranking algorithm. The clustering algorithm is performed to eliminate redundancy in such a way that representative PAS with the highest salience score from each cluster is chosen, and fed to language generation to generate summary sentences. Experiment of this study is performed using DUC-2002, a standard corpus for text summarization. Experimental results indicate that the proposed approach outperforms other summarization systems.",,978-1-4673-8753-8,10.1109/INTELSE.2016.7475163,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475163,multi-document abstractive summarization;semantic role labeling;semantic graph;genetic algorithm;semantic similarity measure,Semantics;Genetics;Labeling;Clustering algorithms;Pragmatics;Ontologies;Cyclones,abstracting;genetic algorithms;graph theory;pattern clustering;text analysis,clustered genetic semantic graph;multidocument abstractive summarization;predicate argument structures;semantic role labeling;semantic similarity weight;PAS-to-PAS semantic similarity;PAS-to-document relationship;genetic algorithm;modified weighted graph based ranking algorithm;clustering algorithm;SRL,,6,,31,,23-May-16,,,IEEE,IEEE Conferences
Document warehousing: a document-intensive application of a multimedia database,文檔倉庫：多媒體數據庫的文檔密集型應用程序,H. Ishikawa; M. Ohta; K. Kato,"Tokyo Metropolitan Univ., Japan; NA; NA",Proceedings Eleventh International Workshop on Research Issues in Data Engineering. Document Management for Data Intensive Business and Scientific Applications. RIDE 2001,7-Aug-02,2001,,,25,31,"Nowadays, structured data such as sales are stored in data warehouses for decision-making. Less-structured data such as HTML texts, XML data, images, and videos are increasingly accumulated in PC storage due to the spread of the Internet technology such as WWW. Such less-structured data, collectively called multimedia documents, are also precious as corporate assets. So we need to provide a document warehouse to analyze and manage multimedia documents for corporate-wide information mining and reuse like a data warehouse. As a document-intensive application of a multimedia database, we describe a prototype document warehouse system, which supports management of documents, keyword-based and content-based retrieval, rule-based classification, SOM-based clustering and XML active query facility based on ECA rules.",,0-7695-0957-6,10.1109/RIDE.2001.916488,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916488,,Warehousing;Multimedia databases;Data warehouses;XML;Image storage;Marketing and sales;Decision making;HTML;Videos;Internet,multimedia databases;document handling;data warehouses;Internet;visual databases;data mining;content-based retrieval;hypermedia markup languages,multimedia database;structured data;data warehouses;HTML;XML;image database;video database;Internet;World Wide Web;document warehouse;corporate-wide information mining;document management;keyword-based retrieval;content-based retrieval;rule-based classification;SOM-based clustering;active query facility;ECA rules,,3,,13,,7-Aug-02,,,IEEE,IEEE Conferences
Toward Parallel Document Clustering,走向並行文檔聚類,J. A. Mogill; D. J. Haglin,"Pacific Northwest Nat. Lab., Richland, WA, USA; Pacific Northwest Nat. Lab., Richland, WA, USA",2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum,1-Sep-11,2011,,,1700,1709,"A key challenge to automated clustering of documents in large text corpora is the high cost of comparing documents in a multi-million dimensional document space. The Anchors Hierarchy is a fast data structure and algorithm for localizing data based on a triangle inequality obeying distance metric, the algorithm strives to minimize the number of distance calculations needed to cluster the documents into ""anchors'' around reference documents called ""pivots''. We extend the original algorithm to increase the amount of available parallelism and consider two implementations: a complex data structure which affords efficient searching, and a simple data structure which requires repeated sorting. The sorting implementation is integrated with a text corpora ""Bag of Words'' program and initial performance results of end-to-end document processing workflow are reported.",1530-2075,978-1-61284-425-1,10.1109/IPDPS.2011.327,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009036,,Clustering algorithms;Parallel algorithms;Indexes;Data structures;Concurrent computing;Synchronization;Semantics,data structures;document handling;parallel processing,parallel document clustering;automated clustering;large text corpora;multimillion dimensional document space;anchors hierarchy;fast data structure;distance metric;document processing workflow,,2,,10,,1-Sep-11,,,IEEE,IEEE Conferences
Sparse Poisson Latent Block Model for Document Clustering,用於文檔聚類的稀疏Poisson潛在塊模型,M. Ailem; F. Role; M. Nadif,"LIPADE, University of Paris Descartes, France; LIPADE, University of Paris Descartes, France; LIPADE, University of Paris Descartes, France",IEEE Transactions on Knowledge and Data Engineering,5-Jun-17,2017,29,7,1563,1576,"Over the last decades, several studies have demonstrated the importance of co-clustering to simultaneously produce groups of objects and features. Even to obtain object clusters only, using co-clustering is often more effective than one-way clustering, especially when considering sparse high dimensional data. In this paper, we present a novel generative mixture model for co-clustering such data. This model, the Sparse Poisson Latent Block Model (SPLBM), is based on the Poisson distribution, which arises naturally for contingency tables, such as document-term matrices. The advantages of SPLBM are two-fold. First, it is a rigorous statistical model which is also very parsimonious. Second, it has been designed from the ground up to deal with data sparsity problems. As a consequence, in addition to seeking homogeneous blocks, as other available algorithms, it also filters out homogeneous but noisy ones due to the sparsity of the data. Experiments on various datasets of different size and structure show that an algorithm based on SPLBM clearly outperforms state-of-the-art algorithms. Most notably, the SPLBM-based algorithm presented here succeeds in retrieving the natural cluster structure of difficult, unbalanced datasets which other known algorithms are unable to handle effectively.",1558-2191,,10.1109/TKDE.2017.2681669,AAP; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876732,Co-clustering;clustering;poisson distribution;mixture model;latent block model;text mining;data sparsity,Clustering algorithms;Data models;Sparse matrices;Partitioning algorithms;Mixture models;Algorithm design and analysis;Text mining,mixture models;pattern clustering;statistical distributions;stochastic processes;text analysis,sparse Poisson latent block model;document clustering;sparse high dimensional data;generative mixture model;Poisson distribution;contingency tables;statistical model;data sparsity;SPLBM-based algorithm;natural cluster structure;unbalanced datasets,,10,,37,,13-Mar-17,,,IEEE,IEEE Journals
Intelligent Dynamic XML Documents Clustering,智能動態XML文檔集群,L. I. Rusu; W. Rahayu; D. Taniar,"LaTrobe Univ., Melbourne; LaTrobe Univ., Melbourne; NA",22nd International Conference on Advanced Information Networking and Applications (aina 2008),3-Apr-08,2008,,,449,456,"Clustering as an intelligent technique for mining XML documents has been utilised as an excellent way of grouping the documents by their content or structure. A main step in many distance based XML clustering algorithms is to calculate pair-wise distances between documents; naturally, a time-efficient technique requests the pair-wise distances to be determined in a timely manner. In case of dynamic XML documents, the amount of changes between versions cannot be predicted. Therefore, in case of clustered dynamic XML documents, if changes were little or if they affected only some of the clustered documents, recalculating pair-wise distances every time would be highly redundant. In this paper we propose a time-efficient technique to reassess pair- wise distances between clustered dynamic XML documents which change in time, without performing redundant calculations but considering the previously known distances and the set of changes which might have affected the documents versions.",2332-5658,978-0-7695-3095-6,10.1109/AINA.2008.122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4482741,XML;dynamic;clustering;intelligent,XML;Clustering algorithms;Data mining;Intelligent networks;Intelligent structures;Association rules;Information retrieval;Deductive databases;Distance measurement;Extraterrestrial measurements,XML,intelligent dynamic XML documents clustering;mining XML documents;pair-wise distances;clustered documents;time-efficient technique,,1,,16,,3-Apr-08,,,IEEE,IEEE Conferences
Feature overlap-based dynamic self organizing model for hierarchical text clustering,基於特徵重疊的層次文本聚類動態自組織模型,N. Nathawitharana; S. Matharage; D. Alahakoon,"School of Information and Business Analytics, Deakin University, Australia; School of Information and Business Analytics, Deakin University, Australia; School of Information and Business Analytics, Deakin University, Australia",2013 IEEE 8th International Conference on Industrial and Information Systems,6-Feb-14,2013,,,393,398,"In text document clustering documents are represented as feature vectors where features can be either words or phrases. Documents can belong to different topics when categorized by humans; however it is noted that obtaining one to one mapping between the features and the topics is almost impossible since the same features can and will be used in documents in different topics. Such common features results in overlap in text clustering, and as such traditional cluster purity measures may not be practical or meaningful. In this paper new methodology and algorithm is introduced which considers the feature overlap between the clusters when clustering text documents. Hierarchical clustering facilitated by the Growing Self-Organizing Map (GSOM) is used together with the calculated feature overlap to check the possibility of obtaining clusters with minimum feature overlap. We also present the experimental results obtained from following the proposed methodology with the new algorithm.",2164-7011,978-1-4799-0910-0,10.1109/ICIInfS.2013.6732016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732016,Feature overlap;Growing Self-Organizing Map;Hierarchical clustering;Text document clustering,Clustering algorithms;Vectors;Algorithm design and analysis;Indexes;Feature extraction;Conferences,pattern clustering;self-organising feature maps;text analysis,dynamic self organizing model;feature overlap;hierarchical text clustering;text document clustering;feature vectors;growing self-organizing map;GSOM,,,,17,,6-Feb-14,,,IEEE,IEEE Conferences
A Comparison of Clustering Methods for Word Image Indexing,詞圖像索引聚類方法的比較,S. Marinai; E. Marino; G. Soda,NA; NA; NA,2008 The Eighth IAPR International Workshop on Document Analysis Systems,11-Nov-08,2008,,,671,676,"In this paper we compare three clustering methods used to perform word image indexing. The three methods are: the Self-Organizing Map (SOM), the Growing Hierarchical Self-Organizing Map (GHSOM), and the Spectral Clustering. We test these methods on a real data set composed of word images extracted from an encyclopedia of the XIX-th Century. The word images are grouped on the basis of the clustering methods and subsequently retrieved identifying the closest clusters to a query word. The accuracy of the methods is compared evaluating the performance of the word retrieval algorithm. From the experimental results we conclude that methods designed to automatically determine the number and the structure of clusters, such as GHSOM, are particularly suitable in the context represented by our data set.",,978-0-7695-3337-7,10.1109/DAS.2008.85,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670020,Self Organizing Map;clustering;Document Image Retrieval,Clustering methods;Indexing;Image segmentation;Shape;Image analysis;Convergence;Image converters;Error analysis;Feature extraction;Text analysis,,,,1,,,,11-Nov-08,,,IEEE,IEEE Conferences
A novel hybrid algorithm for binarization of badly illuminated document images,一種新穎的混合算法，用於對照明不良的文檔圖像進行二值化,M. Valizadeh; N. Armanfard; M. Komeili; E. Kabir,"Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran",2009 14th International CSI Computer Conference,8-Dec-09,2009,,,121,126,"In this paper, we present a novel hybrid algorithm for binarization of badly illuminated document images. This algorithm locally enhances the document image and makes the gray levels of text and background pixels separable. Afterward a simple global binarization algorithm binarizes the enhanced image. The enhancement process is a novel method that uses a separate transformation function to map the gray level of each pixel into a new domain. For each pixel, the transformation function is determined using its neighboring pixels gray level. The proposed binarization algorithm is robust for wide variety of degraded document images. Evaluation over a set of degraded document images illustrates the effectiveness of our proposed binarization algorithm.",,978-1-4244-4261-4,10.1109/CSICC.2009.5349338,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349338,badly illuminated document;enhancement;transformation function;hybrid binarization,Degradation;Pixel;Robustness;Text analysis;Digital cameras;Clustering algorithms;Image processing;Pattern recognition;Image converters;Image analysis,document image processing;image enhancement,badly illuminated document images;document image enhancement;global binarization algorithm;transformation function;neighboring pixels gray level,,14,,16,,8-Dec-09,,,IEEE,IEEE Conferences
Stemming and similarity measures for Arabic Documents Clustering,阿拉伯文檔聚類的詞乾和相似性度量,H. Froud; R. Benslimane; A. Lachkar; S. A. Ouatik,"L.T.T.I, University Sidi Mohamed Ben, Abdellah (USMBA), Fez, Morocco; L.T.T.I, University Sidi Mohamed Ben, Abdellah (USMBA), Fez, Morocco; E.N.S.A, University Sidi Mohamed Ben, Abdellah (USMBA), Fez, Morocco; L.I.S.Q, Faculty of Science Dhar EL Mahraz, (FSDM), Fez, Morocco",2010 5th International Symposium On I/V Communications and Mobile Network,3-Dec-10,2010,,,1,4,"Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (TR) systems especially with the rapid growth of the number of online documents present in Arabic language. Document clustering aims to automatically group similar documents in one cluster using different similarity/distance measures. In this paper, we evaluate the impact of the stemming on the Arabic Text Document Clustering with five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for the testing dataset. Our experiments on this latter show that the use of the stemming will not yield good results, but makes the representation of the document smaller and the clustering faster.",,978-1-4244-5998-8,10.1109/ISVC.2010.5656417,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5656417,Information Retrieval Systems;Arabic Language;Arabic Text Clustering;Similarity Measures;Stemming,Information retrieval;Entropy;Clustering algorithms;Euclidean distance;Testing;Correlation,information retrieval systems;natural languages;pattern clustering;text analysis,information retrieval system;online document;Arabic language;Arabic text document clustering;Euclidean distance;cosine similarity;Jaccard coefficient;Pearson correlation coefficient;averaged Kullback-Leibler divergence,,16,,12,,3-Dec-10,,,IEEE,IEEE Conferences
Clustering with Multiviewpoint-Based Similarity Measure,基於多視點的相似性度量聚類,D. T. Nguyen; L. Chen; C. K. Chan,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",IEEE Transactions on Knowledge and Data Engineering,20-Apr-12,2012,24,6,988,1001,"All clustering methods have to assume some cluster relationship among the data objects that they are applied on. Similarity between a pair of objects can be defined either explicitly or implicitly. In this paper, we introduce a novel multiviewpoint-based similarity measure and two related clustering methods. The major difference between a traditional dissimilarity/similarity measure and ours is that the former uses only a single viewpoint, which is the origin, while the latter utilizes many different viewpoints, which are objects assumed to not be in the same cluster with the two objects being measured. Using multiple viewpoints, more informative assessment of similarity could be achieved. Theoretical analysis and empirical study are conducted to support this claim. Two criterion functions for document clustering are proposed based on this new measure. We compare them with several well-known clustering algorithms that use other popular similarity measures on various document collections to verify the advantages of our proposal.",1558-2191,,10.1109/TKDE.2011.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740893,Document clustering;text mining;similarity measure.,Clustering algorithms;Strontium;Euclidean distance;Current measurement;Proposals;Partitioning algorithms;Algorithm design and analysis,document handling;pattern clustering,multiviewpoint-based similarity measure;data objects;dissimilarity measure;informative assessment;document clustering;clustering algorithm,,17,,32,,5-Apr-11,,,IEEE,IEEE Journals
Designing a Super-Peer semantic Network based on Hierarchical Clusters,基於層次聚類的超級對等語義網絡設計,Y. Tan; B. Li; X. Li; Y. Lin,"Department of Information and Computing Science, Changsha University, 410003, China; Department of Information and Computing Science, Changsha University, 410003, China; Department of Information and Computing Science, Changsha University, 410003, China; School of Computer and Communication, Hunan University, Changsha, 410082, China","2010 IEEE Youth Conference on Information, Computing and Telecommunications",14-Feb-11,2010,,,194,197,"In super-peer semantic network, when a new peer joins the network, the peer will use the semantic feature of local share documents to select super-peers. In traditionally networks, client-peers use the semantic feature of clusters in peers to select the super-peers to connect. But the clusters are fixed, general, single-level, and not effective to represent the features of all documents. And, while selecting super-peers to connect, the client-peer cannot select some relative clusters in different level accord to the semantic categories of super-peers semantic group. In this paper, a Super-Peer semantic Network based on Hierarchical Clustering Tree (HCTSPN) is presented. First, an improved hierarchical clustering algorithm for organizing the share documents in a client-peer is proposed. Secondly, the method of constructing a super-peer semantic network based on hierarchical clustering tree and search mechanism are proposed. Finally, the experiments are implemented, and the results show that search efficiency and retrieval quality are improved in the network.",,978-1-4244-8886-5,10.1109/YCICT.2010.5713078,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5713078,peer-to-peer network;super-peer;semantic network;hierarchical cluster,Semantics;Peer to peer computing;Indexes;Clustering algorithms;Organizing;Feature extraction;Algorithm design and analysis,information retrieval;pattern clustering;peer-to-peer computing;semantic networks;tree searching,super-peer semantic network;local share documents;client-peers;hierarchical clustering tree;search mechanism;retrieval quality,,2,,9,,14-Feb-11,,,IEEE,IEEE Conferences
Semi-Supervised Heterogeneous Fusion for Multimedia Data Co-Clustering,半監督異構融合，用於多媒體數據共聚,L. Meng; A. Tan; D. Xu,"School of Computer Engineering , Nanyang Technological University, 50 Nanyang Avenue, Singapore; School of Computer Engineering , Nanyang Technological University, 50 Nanyang Avenue, Singapore; School of Computer Engineering , Nanyang Technological University, 50 Nanyang Avenue, Singapore",IEEE Transactions on Knowledge and Data Engineering,5-Aug-14,2014,26,9,2293,2306,"Co-clustering is a commonly used technique for tapping the rich meta-information of multimedia web documents, including category, annotation, and description, for associative discovery. However, most co-clustering methods proposed for heterogeneous data do not consider the representation problem of short and noisy text and their performance is limited by the empirical weighting of the multi-modal features. In this paper, we propose a generalized form of Heterogeneous Fusion Adaptive Resonance Theory, called GHF-ART, for co-clustering of large-scale web multimedia documents. By extending the two-channel Heterogeneous Fusion ART (HF-ART) to multiple channels, GHF-ART is designed to handle multimedia data with an arbitrarily rich level of meta-information. For handling short and noisy text, GHF-ART does not learn directly from the textual features. Instead, it identifies key tags by learning the probabilistic distribution of tag occurrences. More importantly, GHF-ART incorporates an adaptive method for effective fusion of multi-modal features, which weights the features of multiple data sources by incrementally measuring the importance of feature modalities through the intra-cluster scatters. Extensive experiments on two web image data sets and one text document set have shown that GHF-ART achieves significantly better clustering performance and is much faster than many existing state-of-the-art algorithms.",1558-2191,,10.1109/TKDE.2013.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482563,Semi-supervised learning;heterogeneous data co-clustering;multimedia data mining,Subspace constraints;Vectors;Clustering algorithms;Multimedia communication;Visualization;Feature extraction;Pattern matching,ART neural nets;data mining;document handling;Internet;learning (artificial intelligence);multimedia computing;pattern clustering;statistical distributions,semisupervised heterogeneous fusion;multimedia data co-clustering method;rich meta-information;multimedia Web documents;associative discovery;heterogeneous data;multimodal features;heterogeneous fusion adaptive resonance theory;GHF-ART;large-scale Web multimedia documents;two-channel heterogeneous fusion ART;multimedia data handling;noisy text;textual features;probabilistic distribution;tag occurrences;adaptive method;multiple data sources;intra-cluster scatters;Web image data sets;text document set;semisupervised learning;multimedia data mining,,32,,40,,20-Mar-13,,,IEEE,IEEE Journals
Information Retrieval Based on Heuristic Key Words Extraction and Clusterings for Documents,基於啟發式關鍵詞提取和文檔聚類的信息檢索,Y. Shiono; T. Yoshizumi; K. Tsuchida,"Infomation Technol. Service Center, Yokohama Nat. Univ., Yokohama, Japan; Grad. Sch. of Engeneering, Toyo Univ., Kawagoe, Japan; Fac. of Infomation Sci. & Arts, Toyo Univ., Kawagoe, Japan",2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence,30-Nov-15,2015,,,125,126,"Accumulated data has become enormous according to development and spread of Information technology. Generally the data is saved and is organized on the database system with some kinds of keywords and its clustering. In the case of performing the decision for some problems, we often refer to past cases that are similar to the problem. In such a case, if solutions of past cases are kept in the database as documents, it is very useful to solve a problem. We propose a new information retrieval system based on-heuristic key words extracted from documents and a set of clusters for the documents.",,978-1-4673-9642-4,10.1109/ACIT-CSI.2015.32,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336047,information retrieval;fuzzy theory;heuristic key word extraction;fuzzy clustering,Dictionaries;Data mining;Electronic mail;Information technology;Database systems,document handling;information retrieval;pattern clustering,heuristic key words extraction;clusterings;documents;accumulated data;information technology;database system;information retrieval system;on-heuristic key words,,,,3,,30-Nov-15,,,IEEE,IEEE Conferences
Retrieval of degraded Chinese document based on fuzzy coding strategy,基於模糊編碼策略的降級中文文檔檢索,Y. Xia; X. Jia; K. Wang,"School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China",2012 International Conference on Systems and Informatics (ICSAI2012),25-Jun-12,2012,,,261,264,"For the sake of the low recognition rate for degraded Chinese document, the performance of retrieval is not good if directly based on OCR result. This paper presents a new way to improve the performance of retrieval by fuzzy coding strategy. Lots of character classes with similar shapes are clustered and are indexed by pseudo code. For ease of test, this paper also presents a way to generate ground-truth of imaged document and synthesized degraded document image. A true OCR text collection and two synthesized document image collections are used for performance evaluation, and the result confirms the validation of our method.",,978-1-4673-0199-2,10.1109/ICSAI.2012.6223602,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6223602,Retrieval of degraded Chinese document;fuzzy coding strategy;Synthesis of degraded document,Optical character recognition software;Encoding;Indexing;Image retrieval;Performance evaluation;Degradation;Text analysis,document image processing;fuzzy set theory;image coding;image retrieval;optical character recognition,degraded Chinese document retrieval;fuzzy coding strategy;pseudo code;ground-truth generate;imaged document;synthesized degraded document image;OCR text collection,,,,14,,25-Jun-12,,,IEEE,IEEE Conferences
Multi-view clustering of web documents using multi-objective genetic algorithm,使用多目標遺傳算法的Web文檔多視圖聚類,A. Wahid; X. Gao; P. Andreae,"Victoria University of Wellington, New Zealand; Victoria University of Wellington, New Zealand; Victoria University of Wellington, New Zealand",2014 IEEE Congress on Evolutionary Computation (CEC),22-Sep-14,2014,,,2625,2632,"Clustering ensembles are a common approach to clustering problem, which combine a collection of clustering into a superior solution. The key issues are how to generate different candidate solutions and how to combine them. Common approach for generating candidate clustering solutions ignores the multiple representations of the data (i.e., multiple views) and the standard approach of simply selecting the best solution from candidate clustering solutions ignores the fact that there may be a set of clusters from different candidate clustering solutions which can form a better clustering solution. This paper presents a new clustering method that exploits multiple views to generate different clustering solutions and then selects a combination of clusters to form a final clustering solution. Our method is based on Nondominated Sorting Genetic Algorithm (NSGA-II), which is a multi-objective optimization approach. Our new method is compared with five existing algorithms on three data sets that have increasing difficulty. The results show that our method significantly outperforms other methods.",1941-0026,978-1-4799-1488-3,10.1109/CEC.2014.6900586,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900586,,Clustering algorithms;Optimization;Sociology;Statistics;Linear programming;Evolutionary computation;Standards,document handling;genetic algorithms;Internet;pattern clustering;sorting,multiview clustering problem;Web documents;multiobjective genetic algorithm;clustering ensembles;candidate clustering solutions;multiple data representations;nondominated sorting genetic algorithm;NSGA-II;multiobjective optimization approach,,7,,42,,22-Sep-14,,,IEEE,IEEE Conferences
"Robust image document authentication code with autonomous biometrie key generation, selection, and updating in cloud environment",健壯的圖像文檔身份驗證代碼，可在雲環境中自動生成生物特徵密鑰，進行選擇和更新,Z. A. Abduljabbar; H. Jin; Z. A. Hussien; A. A. Yassin; M. A. Hussain; S. H. Abbdal; D. Zou,"Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; University of Basrah, Basrah, Iraq; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China",2015 11th International Conference on Information Assurance and Security (IAS),16-Jun-16,2015,,,61,66,"Recently, security issues are obstructing the development and using of cloud computing services. Authentication and integrity play an important role in the cloud security, and numerous concerns have been raised to recognize any tampering with exchanges of the image document between two entities (sender and receiver) within the cloud environment. However, none of the existing solutions reduce the probability of known attacks by combining cryptographic hash function with a strong factor that should be periodically changed. For this reason, in this paper we propose a robust one-time image document authentication scheme based on combining non-interactive onetime biometric key and a robust wavelet-based cryptographic hashing scheme. The result of the combination is one-time image document authentication code (OMAC). OMAC is hidden in an image document as a cover image through reversible data embedding steganography. The proposed scheme has several important security attributes, such as key agreement, biometric key management, robust OMAC, invulnerability, and efficiency. In biometric key management, key generation, key selection, and key update algorithms are performed autonomously by the sender and the receiver; thus, no interaction between them is needed.",,978-1-4673-8715-6,10.1109/ISIAS.2015.7492746,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492746,Biometrie key management;cloud computing;OMAC;session key agreement,Authentication;Cryptography;Biomedical imaging;Robustness;Receivers;Cloud computing,cloud computing;cryptography;document image processing;steganography,robust image document authentication code;autonomous biometric key generation;autonomous biometric key updating;autonomous biometric key selection;cloud environment;cloud computing services;cloud security;cryptographic hash function;robust one-time image document authentication scheme;noninteractive one-time biometric key;robust wavelet-based cryptographic hashing scheme;OMAC;data embedding steganography;security attributes;biometric key management,,,,15,,16-Jun-16,,,IEEE,IEEE Conferences
Adaptive documents classification system based on ontology constructed by fuzzy function and fuzzy relations,基於本體的模糊函數和模糊關係自適應文件分類系統,Ju-in Youn; He-Jue Eun; Cheol-Jung Yoo; Yong-Sung Kim,"Dept. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea; Dept. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonju, South Korea",2004 International Conference on Cyberworlds,27-Dec-04,2004,,,182,187,"Documents based on Web have been increased very rapidly according to development of Web technology and the documents are very various in document structure and data type. Therefore, many intelligent information systems have applied document categorization method and semantic relation, which they developed to offer fit documents for the user. But information system in present is unable to offer fit information for personal interesting because of the lack of related knowledge which it specify relationship between concepts based on document content. Also there are unable to interpret the content of information resources due to the lack of knowledge. Therefore, in this paper we want to express hierarchical structure in according to concept degree of different structure and data type of document on Web using ontology method, which it is a specification of a conceptualization. Also we propose an algorithm to classify into the same cluster connected documents of similar meaning and reorganization use to fuzzy function and relation.",,0-7695-2140-1,10.1109/CW.2004.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1366172,fuzzy function;fuzzy relation;ontology;Document classification;Adaptive Document,Fuzzy systems;Adaptive systems;Ontologies,document handling;content management;Internet;ontologies (artificial intelligence);classification;information systems;fuzzy set theory,adaptive documents classification system;ontology construction;fuzzy function;fuzzy relation;Internet;intelligent information system;document categorization method,,,,12,,27-Dec-04,,,IEEE,IEEE Conferences
Clustering of text documents by projective dimension of subspaces using part neural network,使用部分神經網絡按子空間的投影維數對文本文檔進行聚類,R. Krakovsky; I. Mokris,"Department of Informatics, Catholic University, Ruzomberok, Slovakia; Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovakia",2012 7th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI),30-Jul-12,2012,,,203,207,"The paper deals with clustering of text documents by neural networks. For representation of text documents is used the Vector Space (VS) model, which describes the text documents by VS matrix X. Multidimensional space of matrix X for text documents clustering requires the high computational complexity therefore it is needed of its reduction. In our approach for reduction of the text document space we used decomposition of multidimensional space of matrix X by projection into subspaces. The presented approach for creation of subspaces of multidimensional spaces uses the Projective Adaptive Resonance Theory (PART) neural network which enables this way of reduction of multidimensional text document space and also the text document clustering. Efficiency of clustering the text documents by subspaces of multidimensional space it is influenced by properties of PART and because of the optimal parameters of PART have to be set. Thanks to exact settings of distance and vigilance parameter of PART it is possible to find the clusters, their centers in the projective dimensions of subspaces and create outlier cluster for noisy data sets. The utilization of PART neural network to the text document clustering can easy discover the intrinsic clusters in used sets of documents.",,978-1-4673-1012-3,10.1109/SACI.2012.6250002,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6250002,,Computational modeling,ART neural nets;computational complexity;data structures;matrix decomposition;pattern clustering;text analysis;vectors,text document clustering;projective subspace dimension;PART neural network;vector space model;VS model;text document representation;VS matrix;computational complexity;text document space;multidimensional matrix space decomposition;projective adaptive resonance theory;multidimensional text document space reduction;optimal PART parameters;distance parameter;vigilance parameter;outlier cluster;noisy data sets,,1,,14,,30-Jul-12,,,IEEE,IEEE Conferences
Duplicate detection for symbolically compressed documents,重複檢測符號壓縮的文檔,Dar-Shyang Lee; J. J. Hull,"Ricoh Silicon Valley Inc., Menlo Park, CA, USA; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,305,308,"A new family of symbolic compression algorithms has recently been developed that includes the ongoing JBIG2 standardization effort as well as related commercial products. These techniques are specifically designed for binary document images. They cluster individual blobs in a document and store the sequence of occurrence of blobs and representative blob templates, hence the name symbolic compression. This paper describes a method for duplicate detection on symbolically compressed document images. It recognizes the text in an image by deciphering the sequence of occurrence of blobs in the compressed representation. We propose a Hidden Markov Model (HMM) method for solving such deciphering problems and suggest applications in multilingual document duplicate detection.",,0-7695-0318-7,10.1109/ICDAR.1999.791785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791785,,Image coding;Hidden Markov models;Compression algorithms;Image storage;Pattern matching;Clustering algorithms;Silicon;Standardization;Image recognition;Text recognition,hidden Markov models;standardisation;visual databases;image matching,duplicate detection;symbolically compressed documents;symbolic compression algorithms;JBIG2 standardization;binary document images;blob templates;symbolically compressed document images;hidden Markov model;deciphering problems;multilingual document duplicate detection,,3,,18,,6-Aug-02,,,IEEE,IEEE Conferences
Hybrid Reduction Dimension on Clustering Text of English Hadith Translation,英語聖訓翻譯中聚類文本的混合歸約維數,N. Priatna; A. F. Huda; Q. U. Safitri; W. Darmalaksana,"UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Hadith Department,Bandung,Indonesia",2019 IEEE 5th International Conference on Wireless and Telematics (ICWT),3-Feb-20,2019,,,1,5,"Clustering results are strongly influenced by the selected technique and data dimensions. Large data dimensions become the main problem that must be considered. Therefore, a dimensional reduction is needed to select sub-feature that provides important information. One of the dimensions reduction methods is the hybrid method. The hybrid method combines the method of feature selection and feature extraction to select informative sub-feature. Furthermore, the simplest clustering technique is the k-means algorithm, which divides n data into k cluster based on the centroid. This study carried out clustering using the k-means algorithm after reducing dimensions on 892 English translation hadith documents. The clustering results are validated using the silhouette coefficient and Davies Bouldin index (DBI). Experimental results show that dimensional reduction can improve the cluster quality.",,978-1-7281-4796-3,10.1109/ICWT47785.2019.8978239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978239,Clustering;k-Means;Feature Selection;Feature Extraction;Principal Component Analysis (PCA),,document handling;feature extraction;indexing;information retrieval systems;natural language processing;pattern clustering,clustering technique;dimensional reduction;cluster quality;hybrid reduction dimension;clustering text;English hadith translation;data dimensions;dimensions reduction methods;hybrid method;feature selection;feature extraction;informative sub-feature;Davies Bouldin index,,,,12,,3-Feb-20,,,IEEE,IEEE Conferences
Text document clustering with distributed noun with its compactness using relevance measure and Heuristic function,關聯度和啟發式函數的緊湊性與分佈式名詞的文本文檔聚類,S. Vijayalakshmi; D. Manimegalai,"Department of Computer Science, Bharathiyar University, Coimbatore, India; Department of Information Technology, National Engineering College, Kovilpatty, India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)",13-Aug-15,2015,,,1,6,"This paper presents and discusses some ensemble attribute selection methods for text document clustering. In this research, attributes are extracted in two levels. The first level document representation based on distribution of compact noun were constructed and relevance measure is applied on the distributed compact noun-document representation. it is used evaluate the importance of the noun. It has been widely studied in supervised learning, whereas it is still relatively rare researched in unsupervised learning. Vector Space Model has been used in many text mining tasks, where it has achieved good results as well as acceptable computational complexity. So the proposed document representations are exploited the nf-idf style equation. In this work, distributional nouns are selected into three different ways. First method, the distributed nouns are integrated with relevance measures, and secondly, relevance based distributional nouns are incorporated with Heuristic function to find importance of attributes. The distributional noun representations are used to discriminate the Nouns and measure the semantic similarity between documents. This proposed feature selection method have been successfully applied for Text clustering with flocking algorithm, HCLK Means Clustering and evaluated the efficiency of our Document representation with both synthetic and a real datasets. It is found that the proposed algorithm identifies better feature sets and improves the clustering quality.",,978-1-4799-6818-3,10.1109/ICIIECS.2015.7193204,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193204,Distributed Noun attributes;Flocking Algorithm;HCLK Mean;RiTa WordNet;Relevance Measure;Heuristic function,Classification algorithms;Clustering algorithms;Pattern matching,learning (artificial intelligence);pattern clustering;text analysis,text document clustering;distributed noun;relevance measure;heuristic function;supervised learning;vector space model;feature selection method;text clustering;flocking algorithm;HCLK means clustering;document representation,,,,24,,13-Aug-15,,,IEEE,IEEE Conferences
Web Document Clustering with Multi-view Information Bottleneck,具有多視圖信息瓶頸的Web文檔聚類,Y. Gao; S. Gu; L. Xia; Y. Fei,Central South University University; Central South University University; Central South University University; Central South University University,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),8-Jan-07,2006,,,148,148,"Clustering is an important way to organize the large amount of information on the Web. In this paper, we study how to incorporate many information of Web document, such as content, anchor, URL etc, to improve the performance of clustering. We propose a novel algorithm: multi-view information bottleneck (MVIB), to cluster Web documents with multi-type features. In this algorithm, the compatible constraint maximizing the agreement between clustering hypotheses on different views is imposed on the individual views to cluster instances. Based on the compatible constraints, the set of clustering hypotheses revealing lots of information about correct one is obtained. The final hypothesis can be deduced from these hypotheses. We study the performance of MVIB in different views setting. Experiments on two real datasets indicate that MVIB with 3-view setting based on content, anchor text and URL can improve the quality of clusters more effectively.",,0-7695-2731-0,10.1109/CIMCA.2006.232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052777,,Clustering algorithms;Data mining;Random variables;Uniform resource locators;Data compression;Computational intelligence;Mutual information;Information science;Natural language processing;Image processing,document handling;Internet;pattern clustering;text analysis,Web document clustering;multiview information bottleneck;multitype features;clustering hypotheses;anchor text,,,,23,,8-Jan-07,,,IEEE,IEEE Conferences
A New Graph-Based Algorithm for Clustering Documents,基於圖的文檔聚類新算法,A. P. Su獺rez; J. F. M. Trinidad; J. A. C. Ochoa; J. E. M. Pagola,"Adv. Technol. Applic. Center, La Habana; INAOE (Nat. Inst. for Astrophys., Opt. & Electron.), Tonantzintla; INAOE (Nat. Inst. for Astrophys., Opt. & Electron.), Tonantzintla; Adv. Technol. Applic. Center, La Habana",2008 IEEE International Conference on Data Mining Workshops,30-Dec-08,2008,,,710,719,"In this paper a new algorithm, called CStar, for document clustering is presented. This algorithm improves recently developed algorithms like generalized star (GStar) and ACONS algorithms, originally proposed for reducing some drawbacks presented in previous Star-like algorithms.The CStar algorithm uses the condensed star-shaped sub-graph concept defined by ACONS, but defines a new heuristic that allows to construct a new cover of the thresholded similarity graph and to reduce the drawbacks presented in GStar and ACONS algorithms. The experimentation over standard document collections shows that our proposal outperforms previously defined algorithms and other related algorithms used to document clustering.",2375-9259,978-0-7695-3503-6,10.1109/ICDMW.2008.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4733997,Data Mining;Text Mining;Clustering,Clustering algorithms;Astrophysics;Optical filters;Gas insulated transmission lines;Data mining;Conferences;Proposals;Information retrieval;Filtering;Parallel algorithms,document handling;graph theory;pattern clustering,graph-based algorithm;clustering documents;CStar algorithm;generalized star algorithm;GStar algorithm;ACONS algorithm;condensed star-shaped subgraph concept,,2,,16,,30-Dec-08,,,IEEE,IEEE Conferences
Web Image Clustering,Web圖像聚類,M. El Choubassi; A. V. Nefian; I. Kozintsev; J. Bouguet; Y. Wu,"University of Illinois at Urbana Champaign, Image Formation and Processing Group, Urbana, IL; Intel Corporation, Application Research Labs, Santa Clara, CA; Intel Corporation, Application Research Labs, Santa Clara, CA; Intel Corporation, Application Research Labs, Santa Clara, CA; Intel Corporation, Application Research Labs, Santa Clara, CA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",4-Jun-07,2007,4,,IV-1221,IV-1224,"While image clustering has many important applications ranging from personal to Web image management, its use is often limited by the difficulty of extracting reliable semantics from low level image features. The image clusters can be improved by using features extracted from image regions rather than the whole image. Region segmentation can be improved in turn, by considering all images within the same cluster rather than segmenting each image independently. This observation leads to the unified Bayesian framework for image clustering and segmentation presented in this paper. The experimental results, reported using several types of visual feature extractors on a database of Web documents containing over 6000 images, illustrates a significant improvement over existing techniques.",2379-190X,1-4244-0727-3,10.1109/ICASSP.2007.367296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218327,clustering;image segmentation,Image segmentation;Image databases;Feature extraction;Histograms;Merging;Clustering algorithms;Visual databases;Dictionaries;Iterative algorithms;Quantization,Bayes methods;document image processing;feature extraction;image segmentation;Internet,Web image clustering;Web image management;reliable semantic extraction;feature extraction;region segmentation;image segmentation;unified Bayesian framework;Web image documents,,2,,14,,4-Jun-07,,,IEEE,IEEE Conferences
Clustering Hyperlinks for Topic Extraction: An Exploratory Analysis,聚類超鏈接以進行主題提取：探索性分析,S. E. G. Villarreal; L. M. Elizalde; A. C. Viveros,"Tecnol. de Monterrey, Monterrey, Mexico; Tecnol. de Monterrey, Monterrey, Mexico; Tecnol. de Monterrey, Monterrey, Mexico",2009 Eighth Mexican International Conference on Artificial Intelligence,17-Feb-10,2009,,,128,133,"In a Web of increasing size and complexity, a key issue is automatic document organization, which includes topic extraction in collections. Since we consider topics as document clusters with semantic properties, we are concerned with exploring suitable clustering techniques for their identification on hyperlinked environments (where we only regard structural information). For this purpose, three algorithms (PDDP, k-means, and graph local clustering) were executed over a document subset of an increasingly popular corpus: Wikipedia. Results were evaluated with unsupervised metrics (cosine similarity, semantic relatedness, Jaccard index) and suggest that promising results can be produced for this particular domain.",,978-0-7695-3933-1,10.1109/MICAI.2009.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372704,k-means;graph local clustering;principal direction divisive partitioning;topic detection;Wikipedia,Data mining;Wikipedia;Clustering algorithms;Testing;Information retrieval;Artificial intelligence;Partitioning algorithms;Semantic Web;Data visualization;Clustering methods,document handling;Web sites,hyperlinks clustering;topic extraction;automatic document organization;document clusters;clustering techniques;hyperlinked environments;PDDP;k means;graph local clustering;Wikipedia;unsupervised metrics,,,,21,,17-Feb-10,,,IEEE,IEEE Conferences
Unsupervised Learning from Linked Documents,鏈接文檔的無監督學習,Z. Guo; S. Zhu; Y. Chi; Z. Zhang; Y. Gong,"Comput. Sci. Dept., SUNY at Binghamton, Binghamton, NY, USA; NEC Labs. America, Inc., Cupertino, CA, USA; NEC Labs. America, Inc., Cupertino, CA, USA; Comput. Sci. Dept., SUNY at Binghamton, Binghamton, NY, USA; NEC Labs. America, Inc., Cupertino, CA, USA",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,730,733,"Documents in many corpora, such as digital libraries and webpages, contain both content and link information. In a traditional topic model which plays an important role in the unsupervised learning, the link information is either totally ignored or treated as a feature similar to content. We believe that neither approach is capable of accurately capturing the relations represented by links. To address the limitation of traditional topic models, in this paper we propose a citation-topic (CT) model that explicitly considers the document relations represented by links. In the CT model, instead of being treated as yet another feature, links are used to form the structure of the generative model. As a result, in the CT model a given document is modeled as a mixture of a set of topic distributions, each of which is borrowed (cited) from a document that is related to the given document. We apply the CT model to several document collections and the experimental comparisons against state-of-the-art approaches demonstrate very promising performances.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.184,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596032,Unsupervised learning;latent topic model;document clustering,Accuracy;Probabilistic logic;Machine learning;IP networks;Unsupervised learning;Indexing;Measurement,digital libraries;document handling;Internet;unsupervised learning,unsupervised learning;linked documents;digital libraries;Web pages;citation-topic model,,,,13,,7-Oct-10,,,IEEE,IEEE Conferences
Clustering and mapping related news about violence events on their time-lines,在時間軸上聚類和映射有關暴力事件的相關新聞,S. Toufeeq Ahmed; S. Tikves; H. Davulcu,"Department of Computer Science and Engineering, Arizona State University, USA; Department of Computer Science and Engineering, Arizona State University, USA; Department of Computer Science and Engineering, Arizona State University, USA",2010 IEEE International Conference on Intelligence and Security Informatics,14-Jun-10,2010,,,175,175,"Keeping track of news stories and events as they progress can be a tedious job, but as every day routine most of the web users read and follow many stories and events in news. If an analyst in her area has to follow and map all these according to the time-line they happen, the task quickly becomes overwhelming. We present an online tool which attempts to ease the analyst's task of finding all news articles about an event, and sorting and mapping them on a time-line. We implemented an incremental clustering algorithm working on real-time incoming news, experimenting with different feature sets, including named entities and sentence overlap methods. We evaluated these approaches using Document Understand Conference (DUC) datasets.",,978-1-4244-6446-3,10.1109/ISI.2010.5484742,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5484742,,Data mining;Event detection;Sorting;Clustering algorithms;Broadcasting;Computer science;Libraries;Visualization;Cost accounting,document handling;information resources;Internet;pattern clustering,violence event news;Web users;incremental clustering algorithm;named entities methods;sentence overlap methods;document understand conference datasets;mapping news,,,,4,,14-Jun-10,,,IEEE,IEEE Conferences
Text Document Latent Subspace Clustering by PLSA Factors,通過PLSA因素進行文本文檔潛在子空間聚類,X. F. Zhou; J. G. Liang; Y. Hu; L. Guo,"Inst. of Inf. Eng., Beijing, China; Inst. of Inf. Eng., Beijing, China; Inst. of Inf. Eng., Beijing, China; Inst. of Inf. Eng., Beijing, China",2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),20-Oct-14,2014,2,,442,448,"Text documents are often high dimensional and sparse, it is a great challenge to discover the clusters among the unlabelled text data, because there are no obvious clusters by common distance measure. In this paper we present a latent subspace clustering method to find text clusters. In our algorithm, we use latent factors extracted by probability latent semantic analysis (PLSA) to generate latent clustering subspaces, and then use the distance between sample and each latent clustering subspace as similarity for text clustering. On some text document datasets our method shows effective implementation for text clustering.",,978-1-4799-4143-8,10.1109/WI-IAT.2014.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927658,Text clustering;text mining;subspace;PLSA,Vectors;Accuracy;Euclidean distance;Clustering algorithms;Resource management;Semantics,data mining;pattern clustering;probability;semantic networks;text analysis,PLSA factors;unlabelled text data clusters;common distance measure;latent subspace clustering method;latent factors;probability latent semantic analysis;text clustering similarity;text document datasets;text mining,,3,,11,,20-Oct-14,,,IEEE,IEEE Conferences
Categorizing Web documents using competitive learning: an ingredient of a personal adaptive agent,使用競爭性學習對Web文檔進行分類：個人自適應代理的組成部分,I. Khan; D. Blight; R. D. McLeod; H. C. Card,"Dept. of Electr. & Comput. Eng., Manitoba Univ., Winnipeg, Man., Canada; NA; NA; NA",Proceedings of International Conference on Neural Networks (ICNN'97),6-Aug-02,1997,1,,96,99 vol.1,"This paper describes the application of competitive learning to categorize Web documents, which is one component of our adaptive Web agent. The agent learns the different categories of Web documents that the user is interested in, then finds and suggests new similar documents. We discuss the framework of the Web agent, the implementation of the document clustering and current results. This work therefore suggests a new application, rather than an improved learning algorithm.",,0-7803-4122-8,10.1109/ICNN.1997.611644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=611644,,World Wide Web;Databases;Fuzzy systems;Application software;Clustering algorithms;Web sites;Publishing;Information resources;Information retrieval;Software agents,Internet;unsupervised learning;neural nets;fuzzy systems;software agents;pattern classification;document handling;information services,Web document categorisation;competitive learning;adaptive Web agent;document clustering;World Wide Web;fuzzy inference system;neural networks;document classifier,,1,,6,,6-Aug-02,,,IEEE,IEEE Conferences
A Method to Segment the Marked Regions in Document Images,一種分割文檔圖像中標記區域的方法,Y. Chen; R. Yang; J. Guo; M. Zhu,"Comput. Center, East China Normal Univ., Shanghai, China; Comput. Center, East China Normal Univ., Shanghai, China; Comput. Center, East China Normal Univ., Shanghai, China; Comput. Center, East China Normal Univ., Shanghai, China",2012 International Conference on Industrial Control and Electronics Engineering,4-Oct-12,2012,,,439,443,"It's very popular to mark the concerned regions on a document using a mark pen. How to detect the marked regions in the scanned document images is a difficult problem. In this paper, an approach to solve the above problem is proposed. First, the color pixels and run-length are detected. And then, the mark-lines of document can be attained by using run-length. Finally, the mark-lines to get the marked regions are analyzed. Experimental results show the proposed method has high accuracy. Furthermore, it can be applied to many practical problems.",,978-1-4673-1450-3,10.1109/ICICEE.2012.123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322412,marked regions;document images;run-length;mark-line,Image color analysis;Image segmentation;Graphics;Computers;Optical character recognition software;Accuracy;Clustering algorithms,document image processing;image colour analysis;image segmentation;object detection,marked region segmentation;mark pen;marked region detection;scanned document images;color pixels detection;run-length detection;document mark-lines,,,,10,,4-Oct-12,,,IEEE,IEEE Conferences
ExtMiner: combining multiple ranking and clustering algorithms for structured document retrieval,ExtMiner：結合多種排名和聚類算法以進行結構化文檔檢索,M. Nurminen; A. Honkaranta; T. Karkkainen,"Fac. of Inf. Technol., Jyvaskyla Univ., Finland; Fac. of Inf. Technol., Jyvaskyla Univ., Finland; Fac. of Inf. Technol., Jyvaskyla Univ., Finland",16th International Workshop on Database and Expert Systems Applications (DEXA'05),19-Sep-05,2005,,,1036,1040,"This paper introduces ExtMiner, a platform and potential tool for information management in SMEs (small & medium-size enterprise), or for organizational workgroups. ExtMiner supports interactive and iterative clustering of documents. It provides users with a visual cluster and list views at the same time, supporting iterative search process. ExtMiner may also be applied as a platform for research on retrieval fusion, since it combines search, clustering and visualization algorithms. ExtMiner was evaluated with three document collections. Although the findings were encouraging the user interface and performance with large document repositories need further development.",2378-3915,0-7695-2424-9,10.1109/DEXA.2005.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1508411,,Clustering algorithms;Information retrieval;XML;Search engines;Information management;Visualization;Text mining;Web search;Information technology;Iterative algorithms,information retrieval;pattern clustering;data mining;user interfaces;small-to-medium enterprises;document handling,ExtMiner;information management;SME;small-medium size enterprise;organizational workgroup;document clustering;iterative search process;retrieval fusion;visualization algorithm;user interface;multiple ranking;structured document retrieval,,1,,18,,19-Sep-05,,,IEEE,IEEE Conferences
Chinese Keywords Clustering Based on SOM,基於SOM的中文關鍵詞聚類,Y. Wang; H. Jin,"ChengDu Univ. of Inf. Technol., Chengdu; ChengDu Univ. of Inf. Technol., Chengdu",2008 Fourth International Conference on Natural Computation,7-Nov-08,2008,2,,325,329,"Keyword clustering is useful for text information retrieval, text document classification and so on. This paper introduces an unsupervised method to cluster Chinese keyword by the artificial neural network of SOM (self-organized map). Keywords are encoded into numeric vectors by the similarities of their contextual word sets, which are composed by their neighbor words in the range of phrases. The experimental result shows that words can be clustered on the map according to both of their syntactic and semantic features.",2157-9563,978-0-7695-3304-9,10.1109/ICNC.2008.927,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4667010,self-organized map;word clustering;unsupervised machine learning,Information retrieval;Neurons;Clustering methods;Information technology;Artificial neural networks;Text categorization;Frequency;Clustering algorithms;Neural networks;Natural languages,information retrieval;natural language processing;pattern clustering;self-organising feature maps;text analysis,Chinese keywords clustering;text information retrieval;document classification;self-organized map,,,,5,,7-Nov-08,,,IEEE,IEEE Conferences
Using clustering technology to improve XML semantic search,使用集群技術改善XML語義搜索,Xin-Ye Li,"Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, Hebei, 071003, China",2008 International Conference on Machine Learning and Cybernetics,5-Sep-08,2008,5,,2635,2639,"To get semantic related searching results based on simple keywords, XML search engine not only need to search the matched nodes but also need to check whether those matched nodes are semantic related nodes in XML tree. Since the judgment on the semantic related nodes might cost much time, we first use mining technology to cluster XML documents and compute the similarity between query and XML clusters so as to filter the unrelated clusters with the query. To get exact clusters, we use all paths less than or equal to length L as feature vectors for XML document. We also use bipartite graph to express feature vector matrix and use adjacency list to store the bipartite graph. Based on this idea, we improved the path-based XML clustering algorithm. We use common paths as the feature of cluster and give the similarity measure between query and clusters.",2160-1348,978-1-4244-2095-7,10.1109/ICMLC.2008.4620853,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620853,XML Clustering;Path Feature;Semantic Search;Adjacency List,XML;Bipartite graph;Clustering algorithms;Search engines;Conferences;Principal component analysis;Algorithm design and analysis,data mining;graph theory;information filters;matrix algebra;pattern clustering;query processing;search engines;tree data structures;XML,XML semantic search engine;XML tree;mining technology;XML document clustering;query processing;bipartite graph;feature vector matrix;adjacency list,,,,10,,5-Sep-08,,,IEEE,IEEE Conferences
Document clustering algorithm using modified k-means,使用改進的k均值的文檔聚類算法,R. Agrawal; M. Phatak,"Dept. of Computer Engineering, MAEER'S MIT, Pune, India; Dept. of Computer Engineering, MAEER'S MIT, Pune, India",Fourth International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom2012),20-Apr-15,2012,,,294,296,"Document clustering is the task of grouping a set of documents into clusters so that the documents in the same cluster are similar to each other than to those in other clusters. One of the applications of document clustering is in web search engine retrieval system to help the users find relevant information quicker, and allow them to focus their search in the appropriate direction. K-means is a commonly used algorithm for document clustering, but it has some disadvantages. The main limitations of K-means are: 1) The number of clusters K has to be given as input and 2) Based on the initializations it converges to different local minima. 3) It is slow and cannot be used for large number of data points.4) It cannot handle empty clusters. In this paper, we have developed a novel algorithm to eliminate all these basic drawbacks of K-means.",,978-1-84919-929-2,10.1049/cp.2012.2553,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087842,Document clustering;K-means;Cosine similarity;Threshold,,,,,,,,,20-Apr-15,,,IET,IET Conferences
An analytical study of clustered file space properties,群集文件空間屬性的分析研究,I. A. R. Moghrabi,"M.I.S Department, College of Business Administration, Gulf University for Science and Technology, Kuwait",2013 Science and Information Conference,14-Nov-13,2013,,,609,612,"Response time of an information system can be improved by reducing the number of buckets accessed when retrieving a document set. One approach is to cluster the document base in such a way to ensure greater probability that identifying records will be physically contiguously located. This paper considers a clustering algorithm that controls the file space density through the use of a user-specified threshold value for which we will show how this influences file density and retrieval performance. The method is particularly well-suited to the reorganization of traditional multi-attribute files. Statistical models are constructed to predict the file performance parameters. Our results reveal those models, before and after clustering, are reasonably accurate.",,978-0-9893193-0-0,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661801,Information Retrieval;large data;clustering,Hamming distance;Clustering algorithms;Vectors;Information retrieval;Indexes;Estimation,file organisation;information retrieval;pattern clustering;statistical analysis,clustered file space properties;statistical models;multiattribute file reorganization;file density;user-specified threshold value;clustering algorithm;document set retrieval;information system,,,,16,,14-Nov-13,,,IEEE,IEEE Conferences
Word Sense Disambiguation of semantic document,語義文檔的詞義消歧,B. Shi; L. Fang; J. Yan; P. Wang,"College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing 100124, China",2010 2nd International Conference on Future Computer and Communication,28-Jun-10,2010,3,,V3-224,V3-228,"A Max-Probability Density based Clustering (MPDC) algorithm is proposed in this paper to resolve the problem of Word Sense Disambiguation in semantic document. MPDC take the context information of a keyword based on WordNet into account and select the max probability sense by measuring the density of the concept. We also do experiment on semantic documents retrieving from Swoogle and Watson, two famous semantic web searching engines. The result shows MPDC get a good efficiency.",,978-1-4244-5824-0,10.1109/ICFCC.2010.5497655,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497655,word sense disambiguation;Density based Clustering;WordNet,Semantic Web;Frequency;Clustering algorithms;Search engines;Educational institutions;Control engineering;Density measurement;Natural language processing;Intelligent systems;Data mining,information retrieval;natural language processing;pattern clustering;search engines;semantic Web,word sense disambiguation;semantic document retrieval;max-probability density based clustering algorithm;WordNet;semantic Web searching engines;Swoogle;Watson,,,,12,,28-Jun-10,,,IEEE,IEEE Conferences
Finding Conceptual Document Clusters with Improved Top-N Formal Concept Search,通過改進的Top-N正式概念搜索查找概念文檔群,Y. Okubo; M. Haraguchi,"Hokkaido University, Japan; Hokkaido University, Japan",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,347,351,"In this paper, we discuss a method for conceptual clustering of documents. Our cluster is defined with the notion of formal concept analysis which can provide a conceptual meaning for each document cluster. Our clustering is formalized as a Top-N delta-valid formal concept problem. We improve our previous clique search-based algorithm for the problem so that it can be applied to larger scale datasets. For more efficient computation, we present some pruning rules based on theoretical properties of formal concepts. A depth-first branch-and-bound algorithm with the prunings is designed. Our experimental results show valuable clusters can be extracted from a collection of Web documents. Moreover, the algorithm outperforms some fast algorithms for mining closed itemsets equivalent to formal concepts",,0-7695-2747-7,10.1109/WI.2006.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061392,,Clustering algorithms;Data mining;Itemsets;Web pages;Algorithm design and analysis;Clustering methods;Computer science;Information science;Web sites;Internet,data analysis;data mining;document handling;tree searching,conceptual document clustering;formal concept search analysis;clique search-based algorithm;depth-first branch-and-bound algorithm;Web document extraction;Web mining,,6,,14,,15-Jan-07,,,IEEE,IEEE Conferences
A novel approach for feature selection method TF-IDF in document clustering,文檔聚類中特徵選擇方法TF-IDF的新方法,L. H. Patil; M. Atique,"Department of Computer Science and Engineering, Sant Gadge Baba Amravati University, India; Department of Computer Science and Engineering, Sant Gadge Baba Amravati University, India",2013 3rd IEEE International Advance Computing Conference (IACC),13-May-13,2013,,,858,862,"Now a day, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship.",,978-1-4673-4529-3,10.1109/IAdCC.2013.6514339,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514339,Introduction;Document Preprocessing;WordNet;Term Selection approach;Experimental Results,Databases;Feature extraction;Frequency conversion;Time-frequency analysis;Text categorization;Marine vehicles,data mining;data structures;pattern clustering;text analysis,TD-IDF feature selection method;text document clustering;Internet;e-mail;electronic mail;Web page;electronic database format;document preprocessing;term selection;attribute reduction;WordNet;background knowledge;data mining;stop word;porter stemmer algorithm;word net thesaurus;frequent word set;data matrix;term selection approach;TF-IDF approach;TF-DF approach;TF2 approach;threshold value;document representation;clustering accuracy;Reuters Transcription Subsets data;wheat data;trade data;money grain data;ship data,,8,,13,,13-May-13,,,IEEE,IEEE Conferences
A two-pass web document allocation method for load balancing in the multiple grouping of a Web cluster system,Web集群系統多重分組中的負載均衡的兩遍Web文檔分配方法,Ying-Wen Bai; Chia-Yu Chen; Yu-Nien Yang,"Dept. of Electron. Eng., Fu Jen Catholic Univ., Taipei, Taiwan; Dept. of Electron. Eng., Fu Jen Catholic Univ., Taipei, Taiwan; Dept. of Electron. Eng., Fu Jen Catholic Univ., Taipei, Taiwan",Proceedings. 2004 12th IEEE International Conference on Networks (ICON 2004) (IEEE Cat. No.04EX955),4-Apr-05,2004,1,,177,181 vol.1,"For a practical operation, multiple groups of Web servers with different performance may coexist in a Web cluster. In this paper, we propose a quantitative method based on a typical M/G/1 queuing model to analyze and equalize the service performance of multiple groups of Web servers. We also propose a two-pass method for providing uniform distribution of Web documents to the multiple grouping of a Web cluster system. In addition to the primary function of maintaining a load balance of Web requests, we introduce the extra pass to distribute the complement parts of the Web documents to the disks of the other servers. Usually, these hidden documents are not used for normal operation. If one of the servers is down, these servers can be replaced by the spare server that has the spare copy of the Web documents by the allocation of the second pass.",1531-2216,0-7803-8783-X,10.1109/ICON.2004.1409118,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1409118,,Load management;Web server;Web services;Delay effects;Queueing analysis;Performance analysis;Web and internet services;Stochastic processes;Routing protocols;Quality of service,Internet;file servers;workstation clusters;queueing theory,two-pass web document allocation;load balancing;multiple grouping;Web cluster system;Web server;M-G-1 queuing model,,,,9,,4-Apr-05,,,IEEE,IEEE Conferences
A Robust and Binarization-Free Approach for Text Line Detection in Historical Documents,一種魯棒且無二值化的歷史文檔文本行檢測方法,T. Gruuening; G. Leifert; T. Strauss; R. Labahn,"Comput. Intell. Technol. Lab., Univ. of Rostock, Rostock, Germany; Comput. Intell. Technol. Lab., Univ. of Rostock, Rostock, Germany; Comput. Intell. Technol. Lab., Univ. of Rostock, Rostock, Germany; Comput. Intell. Technol. Lab., Univ. of Rostock, Rostock, Germany",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,236,241,"Text line extraction from complex handwritten documents, especially for historical collections, is still an unsolved problem. There is a strong demand for reliable and robust approaches since text line extraction is a crucial pre-processing step for modern text recognition and keyword spotting systems. We propose a binarization-free system which employs a newly developed clustering approach based on so-called 'superpixels'. Although multiple ways of generating superpixels were developed in the past, we demonstrate that even a standard method yields impressive results. Our clustering approach is applicable to various scenarios by making use of general characteristics of text lines (e.g., curvilinearity, interline spacings, local homogeneity), and without adapting its parametrization. State-of-the-art results are achieved by the same parametrization for 8 different well-established benchmarking datasets. These datasets cover historical and modern texts as well as images with diverse resolutions and fonts. The system is developed for detecting text lines in complex scenarios. It is not tuned to assign foreground pixels to detected text lines. Thus, superior performance is achieved for the historical datasets for which no pixel hit accuracy of 95% is required. Remarkably, for the dataset of the ICDAR 2015 Competition on Text Line Detection in Historical Documents, the average cost per text line was reduced from 9.77 (winning team) to 8.19.",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8269978,text line detection;binarization-free;robust;handwritten;historical,Robustness;Image segmentation;Standards;Text recognition;Clustering algorithms;State estimation,document image processing;feature extraction;handwritten character recognition;history;image resolution;pattern clustering;text analysis,Text Line Detection;Historical Documents;text line extraction;historical collections;binarization-free system;superpixels;handwritten documents;text recognition;clustering approach;keyword spotting systems,,12,,31,,29-Jan-18,,,IEEE,IEEE Conferences
Multi-document news summarization via paragraph embedding and density peak clustering,通過段落嵌入和密度峰值聚類進行多文檔新聞摘要,B. Wang; J. Zhang; F. Ding; Y. Zou,"ADSPLAB/ELIP, School of ECE, Peking University, China; Dongguan University of Technology, China; Shenzhen Press Group, China; ADSPLAB/ELIP, School of ECE, Peking University, China",2017 International Conference on Asian Language Processing (IALP),22-Feb-18,2017,,,260,263,"Multi-document news summarization (MDNS) aims to create a condensed summary while retaining the main characteristics of the original set of news documents. Research shows that the text representation is one of the keys for MDNS techniques. Without doubt, the bag-of-words (BOW) methods are most widely used. However, BOW methods generate high-dimensional representation vectors which ask for large storage and high computational complexity for MDNS. Besides, the generated representation vectors by BOW lack the semantic information and temporal information of the words, which limits the performance of MDNS. To tackle above issues, this paper introduces a word/paragraph embedding method via neural network modelling to generate lower dimensional word/paragraph representation vectors retaining word order and context information and semantic relationships between words/paragraphs. Besides, for MDNS, relevance and redundancy are both critical issues. Unlike the traditional MDNS methods quantifying the relevance among different sentences followed with a greedy post-processing module to ensure the diversity of summary, in this study, we concurrently take relevance, diversity and length constraint into account by employing density peak clustering (DPC) technique and the integrated sentence scoring method to select the more representative sentences and generate the summary with less redundancy. Experimental results on the DUC2003 and DUC2004 datasets demonstrate the effectiveness of our MDNS method, compared to the state-of-the-art methods.",,978-1-5386-1981-0,10.1109/IALP.2017.8300593,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300593,Multi-Document News Summarization;Text Representation;Integrated Sentence Scoring Method;Density Peak Clustering,Silicon;Artificial neural networks;Measurement,document handling;information retrieval;pattern clustering,multidocument news summarization;news documents;text representation;MDNS techniques;bag-of-words methods;BOW methods;high-dimensional representation vectors;generated representation vectors;temporal information;word/paragraph embedding method;word order;context information;length constraint;density peak clustering technique;MDNS method;computational complexity;semantic information;diversity constraint;word/paragraph representation vectors;MDNS methods;DUC2003 dataset;DUC2004 dataset,,1,,18,,22-Feb-18,,,IEEE,IEEE Conferences
A clustering-based approach to the separation of text strings from mixed text/graphics documents,基於聚類的方法來分離,Shoujie He; N. Abe,"Dept. of Inf. Syst. & Comput. Sci., Nat. Univ. of Singapore, Singapore; NA",Proceedings of 13th International Conference on Pattern Recognition,6-Aug-02,1996,3,,706,710 vol.3,"A clustering-based approach to the separation of text from mixed text/graphics documents is presented. The approach starts from the grouping of connected components. Clustering is employed at three critical stages to improve the efficiency and effectiveness of the grouping, i.e., prior to the grouping, prior to orientation estimation, and posterior to the orientation estimation. Because of the high accuracy of the estimated orientation, not only the overgrouping but also most of undergrouping cases could be successfully handled.",1051-4651,0-8186-7282-X,10.1109/ICPR.1996.547037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547037,,Data mining;Maximum likelihood estimation;Computer graphics;Computer science;Helium;Information systems;Systems engineering and theory;Tree data structures;Testing;Smoothing methods,document image processing,clustering-based approach;text string separation;mixed text/graphics documents;orientation estimation,,5,,12,,6-Aug-02,,,IEEE,IEEE Conferences
A Fuzzy Approach for Multitype Relational Data Clustering,可視化無組織的文檔集合的方法,J. Mei; L. Chen,"Division of Information Engineering, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Division of Information Engineering, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Fuzzy Systems,3-Apr-12,2012,20,2,358,371,"Mining interrelated data among multiple types of objects or entities is important in many real-world applications. Despite extensive study on fuzzy clustering of vector space data, very limited exploration has been made on fuzzy clustering of relational data that involve several object types. In this paper, we propose a new fuzzy clustering approach for multitype relational data (FC-MR). In FC-MR, different types of objects are clustered simultaneously. An object is assigned a large membership with respect to a cluster if its related objects in this cluster have high rankings. In each cluster, an object tends to have a high ranking if its related objects have large memberships in this cluster. The FC-MR approach is formulated to deal with multitype relational data with various structures. The objective function of FC-MR is locally optimized by an efficient iterative algorithm, which updates the fuzzy membership matrix and the ranking matrix of one type at once while keeping those of other types constant. We also discuss the simplified FC-MR for multitype relational data with two special structures, namely, star-structure and extended star-structure. Experimental studies are conducted on benchmark document datasets to illustrate how the proposed approach can be applied flexibly under different scenarios in real-world applications. The experimental results demonstrate the feasibility and effectiveness of the new approach compared with existing ones.",1941-0034,,10.1109/TFUZZ.2011.2174366,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6068241,Document clustering;fuzzy clustering;multitype;multiway clustering;relational data,Vectors;Integrated circuits;Clustering algorithms;Sparse matrices;Computational modeling;Matrix decomposition;Complexity theory,data mining;fuzzy set theory;iterative methods;pattern clustering;relational databases,fuzzy approach;multitype relational data clustering;interrelated data mining;fuzzy clustering;vector space data;object type;object ranking;FC-MR approach;iterative algorithm;fuzzy membership matrix;ranking matrix;extended star-structure;document dataset,,36,,33,,2-Nov-11,,,IEEE,IEEE Journals
Hybrid neural document clustering using guided self-organization and WordNet,基於最大熵非負矩陣分解的文檔聚類,Chihli Hung; S. Wermter; P. Smith,"Hybrid Intelligent Syst., Univ. of Sunderland, UK; Hybrid Intelligent Syst., Univ. of Sunderland, UK; NA",IEEE Intelligent Systems,4-Apr-05,2004,19,2,68,77,"Document clustering is text processing that groups documents with similar concepts. It's usually considered an unsupervised learning approach because there's no teacher to guide the training process, and topical information is often assumed to be unavailable. A guided approach to document clustering that integrates linguistic top-down knowledge from WordNet into text vector representations based on the extended significance vector weighting technique improves both classification accuracy and average quantization error. In our guided self-organization approach we integrate topical and semantic information from WordNet. Because a document-training set with preclassified information implies relationships between a word and its preference class, we propose a novel document vector representation approach to extract these relationships for document clustering. Furthermore, merging statistical methods, competitive neural models, and semantic relationships from symbolic Word-Net, our hybrid learning approach is robust and scales up to a real-world task of clustering 100,000 news documents.",1941-1294,,10.1109/MIS.2004.1274914,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1274914,,Testing;Data mining;Text processing;Unsupervised learning;Supervised learning;Neural networks;Humans;Vector quantization;Entropy;Merging,self-organising feature maps;pattern clustering;document handling;learning (artificial intelligence);Internet,hybrid neural document clustering;guided self-organization;WordNet;unsupervised learning;linguistic top-down knowledge;text vector representation;vector weighting technique;average quantization error;document-training set;preference class;novel document vector representation;competitive neural model,,24,,22,,4-Apr-05,,,IEEE,IEEE Magazines
Multi-Document Summarization as Applied in Information Retrieval,通過社交標籤系統自動進行聚類評估,D. Zhou; L. Li,"Center for Intelligence Science and Technology Research, Beijing University of Posts and Telecommunications, Beijing 10076, China, zhoud@nlu.caai.cn; Center for Intelligence Science and Technology Research, Beijing University of Posts and Telecommunications, Beijing 10076, China, lilei@nlu.caai.cn",2007 International Conference on Natural Language Processing and Knowledge Engineering,29-Oct-07,2007,,,203,208,"In this paper we presented the use of multi-document summarization as postprocessing step in information retrieval (IR). We examined the differences between requirements for general multi-document summarization and requirements when it is applied for IR, and highlighted the requirements for clustering and context information extraction, which is much helpful to the users for browsing and searching relative results. To generate this type of summary, we first cluster the retrieved documents by their topics using a repeated bisection algorithm, and extract the centroid words for each cluster. The final summary is generated on the base of the query words and the cluster centroids, containing query-centered information as well as context information.",,978-1-4244-1610-3,10.1109/NLPKE.2007.4368034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4368034,,Information retrieval;Data mining;Clustering algorithms;Explosions;Information filtering;Information filters;Guidelines,abstracting;document handling;pattern clustering;query processing,multidocument summarization;information retrieval;clustering requirements;context information extraction;repeated bisection algorithm;centroid words;query words,,,,8,,29-Oct-07,,,IEEE,IEEE Conferences
An Improved Partitioning-Based Web Documents Clustering Method Combining GA with ISODATA,多集和集群XML文檔,Z. Zhu; Y. Tian; J. Xu; X. Deng; X. Ren,"Chongqing University, Chongqing 400044, China; Chongqing University, Chongqing 400044, China; Chongqing University, Chongqing 400044, China; Chongqing University, Chongqing 400044, China; Chongqing University, Chongqing 400044, China",Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007),18-Dec-07,2007,2,,208,213,"The existing partitioning-based clustering algorithms, such as k-means, k-medoids and their variations, are simple in theory and fast in convergence speed, but they always just reach local optimum when the iterations terminate and they are not suitable for discovering clusters in the cases when their sizes are very different. This paper proposes an improved Web documents clustering method, using genetic algorithm (GA) which introduces some ideas of ISODATA [6] into the design of its mutation operation. Experiments show that the GA's global search characteristic can avoid local optimum and the ISODATA-based mutation operation makes the improved clustering algorithm have the self-adjusting ability to discover clusters of different sizes.",,978-0-7695-2874-8,10.1109/FSKD.2007.165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406074,,Clustering methods;Clustering algorithms;Genetic algorithms;Genetic mutations;Iterative algorithms;Streaming media;Partitioning algorithms;Merging;Data mining;Educational institutions,data analysis;document handling;genetic algorithms;Internet;iterative methods;pattern clustering,improved partitioning-based Web documents clustering method;genetic algorithms;ISODATA;iterative self-organizing data analysis technique A algorithm,,2,,11,,18-Dec-07,,,IEEE,IEEE Conferences
Text region extraction and text segmentation on camera-captured document style images,文本數據的模式和聚類挖掘,Y. J. Song; K. C. Kim; Y. W. Choi; H. R. Byun; S. H. Kim; S. Y. Chi; D. K. Jang; Y. K. Chung,"Dept. of Comput. Sci., Sookmyung Women's Univ., Korea; NA; NA; NA; NA; NA; NA; NA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,172,176 Vol. 1,"In this paper, we propose a text extraction method from camera-captured document style images and propose a text segmentation method based on a color clustering method. The proposed extraction method detects text regions from the images using two low-level image features and verifies the regions through a high-level text stroke feature. The two level features are combined hierarchically. The low-level features are intensity variation and color variance. And, we use text strokes as a high-level feature using multi-resolution wavelet transforms on local image areas. The stroke feature vector is an input to a SVM (support vector machine) for verification, when needed. The proposed text segmentation method uses color clustering to the extracted text regions. We improved K-means clustering method and it selects K and initial seed values automatically. We tested the proposed methods with various document style images captured by three different cameras. We confirmed that the extraction rates are good enough to be used in real-life applications.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.234,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575532,,Image segmentation;Image edge detection;Computer science;Cameras;Intelligent robots;Text recognition;Clustering methods;Support vector machines;Lighting;Image recognition,document image processing;character recognition;text analysis;feature extraction;image segmentation;image colour analysis;wavelet transforms;support vector machines;pattern clustering,text region extraction;text segmentation;camera-captured document style images;color clustering method;image feature extraction;text stroke feature;multiresolution wavelet transform;support vector machine;K-means clustering method,,11,,9,,16-Jan-06,,,IEEE,IEEE Conferences
Voting affinity propagation algorithm for clustering XML documents,基於魯棒體積最小化的矩陣分解技術，用於遙感和文檔聚類,X. Wang; J. Wei; B. Fan; T. Yang,"College of Information Technical Science, Nankai University, Tianjin, China 300010; College of Information Technical Science, Nankai University, Tianjin, China 300010; College of Information Technical Science, Nankai University, Tianjin, China 300010; College of Information Technical Science, Nankai University, Tianjin, China 300010",Proceedings of 2012 2nd International Conference on Computer Science and Network Technology,10-Jun-13,2012,,,1907,1913,"Affinity propagation algorithm(AP) is a new powerful tool for unsupervised clustering. Yet AP can't work well as expected on the datasets with high-similarity among some points. All these data points are likely to become exemplars so as to generate overmuch clusters. In this paper, we introduce a new algorithm called voting affinity propagation extended from original affinity propagation(OAP) to overcome the problem of generating overmuch clusters when AP is applied to XML document clustering. We propose to select high quality exemplars from candidate exemplars based on a voting mechanism. We adopt both real and synthetic datasets to test the performance of the proposed algorithm. Experiment results show that voting affinity propagation leads to more stable clustering and works better than OAP in clustering data with extremely high similarities.",,978-1-4673-2964-4,10.1109/ICCSNT.2012.6526292,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526292,Affinity Propagation;Documents Clustering;Voting Affinity Propagation,,pattern clustering;XML,voting affinity propagation algorithm;XML documents clustering;unsupervised clustering;data points;original affinity propagation;OAP;voting mechanism;real datasets;synthetic datasets;stable clustering,,1,,20,,10-Jun-13,,,IEEE,IEEE Conferences
Literature Clustering using Citation Semantics,信息流聚類分析的分佈式植絨方法,Tuanjie Tong; D. Dinakarpandian; Yugyung Lee,NA; NA; NA,2009 42nd Hawaii International Conference on System Sciences,20-Jan-09,2009,,,1,10,"Clustering is a common and powerful technique for statistical data analysis, document categorization and topic discovery. The majority of traditional clustering methods, especially for document clustering, are based on the vector space model for distance measure, where the vector is the word profile of a document in the context of the entire corpus. However, algorithms using this measure achieve limited accuracy. In this paper, we propose a semantic measure which incorporates citation semantics (Citonomy) into literature (document) clustering. Our experimental results show that the performance of clustering can be substantially improved by combining Citonomy and vector space measures.",1530-1605,978-0-7695-3450-3,10.1109/HICSS.2009.294,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4755482,,Clustering algorithms;Partitioning algorithms;Frequency;Iterative algorithms;Ontologies;Clustering methods;Data analysis;Text analysis;Context modeling;Extraterrestrial measurements,citation analysis;data analysis;document handling;pattern clustering;statistical analysis,literature clustering;citation semantic;statistical data analysis;document categorization;topic discovery;document clustering;vector space model;distance measurement,,,,28,,20-Jan-09,,,IEEE,IEEE Conferences
Multi document summarization for the Indonesian language based on latent dirichlet allocation and significance sentence,基於維基百科類別的文檔主題提取,A. Widjanarko; R. Kusumaningrum; B. Surarso,"Department of Information System, Diponegoro University, Semarang, Indonesia; Department of Informatics, Diponegoro University, Semarang, Indonesia; Department of Mathematics, Diponegoro University, Semarang, Indonesia",2018 International Conference on Information and Communications Technology (ICOIACT),30-Apr-18,2018,,,520,524,"Automatic Multi-document summarization in Indonesian Language can help people to get more comprehensive online news information. The clustering algorithm which is widely developed over a decade in the text data domains is Latent Dirichlet Allocation (LDA). The LDA method contributes quite well in the field of text classification and information retrieval. One of LDA's usages is a document summarization method, since LDA is able to get the framework in a document. The multi-document summarization in Indonesian language using unsupervised learning approach, especially LDA, is still limited. The LDA and Significance Sentence methods have the advantage of choosing representative sentences from source documents. The testing model was performed using a combination of alpha parameters 0.1 and 0.001 as well as beta 0.001 and 0.1, which is combined with compression rate at 10%, 30% and 50% in the sentence ranking process of each document. Testing results show that the best result was obtained under parameters combination as follows: alpha value is 0.01, beta value is 0.1, compression rate is 50% and cosine similarity value is 0.931.",,978-1-5386-0954-5,10.1109/ICOIACT.2018.8350668,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8350668,extractive multi document summarization;Sentence LDA;Significance Sentence;Topic Model,Resource management;Mathematical model;Information and communication technology;Clustering algorithms;Text categorization;Unsupervised learning;Frequency measurement,information retrieval;natural language processing;pattern clustering;text analysis;unsupervised learning,news information;text data domains;latent dirichlet allocation;text classification;information retrieval;Indonesian language;representative sentences;source documents;sentence ranking process;significance sentence methods;automatic multidocument summarization,,1,,16,,30-Apr-18,,,IEEE,IEEE Conferences
Topic Discovery in Research Literature Based on Non-negative Matrix Factorization and Testor Theory,生物醫學社區的語義專利聚類,F. Li; Q. Zhu; X. Lin,"Sch. of Comput. Sci. & Technol., Beijing Univ. of Chem. Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Univ. of Chem. Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Univ. of Chem. Technol., Beijing, China",2009 Asia-Pacific Conference on Information Processing,7-Aug-09,2009,2,,266,269,"The paper proposes a new way of comprising the Non-negative matrix factorization (NMF) and Testor theory to make topic discovery. NMF method is good at dealing with high dimensional documents and clustering, while Testor theory is used to find the topic of each cluster. By an example of ten abstracts of Chinese science literature from magazines relative to environmental science, the whole process is described in detail. In the end, a case study about automatic classification of a conference proceeding (in Chinese) is given. The result shows the effectiveness of the whole method.",,978-0-7695-3699-6,10.1109/APCIP.2009.202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197187,Document Clustering;NMF;Term-Document Matrix;Testor theory;Topic Discovery,Testing;Clustering methods;Conference proceedings;Clustering algorithms;Chemical technology;Abstracts;Partitioning algorithms;Information processing;Computer science;Paper technology,classification;data mining;data reduction;literature;matrix decomposition;pattern clustering;text analysis,topic discovery;research literature;nonnegative matrix factorization;Testor theory;high dimensional document clustering;Chinese science literature;environmental science;automatic conference proceeding classification;text data dimensionality reduction;text mining,,4,,5,,7-Aug-09,,,IEEE,IEEE Conferences
Graph Based Semi-supervised Non-negative Matrix Factorization for Document Clustering,用於文檔聚類的局部一致概念分解,N. Guan; X. Huang; L. Lan; Z. Luo; X. Zhang,"Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China",2012 11th International Conference on Machine Learning and Applications,10-Jan-13,2012,1,,404,408,"Non-negative matrix factorization (NMF) approximates a non-negative matrix by the product of two low-rank matrices and achieves good performance in clustering. Recently, semi-supervised NMF (SS-NMF) further improves the performance by incorporating part of the labels of few samples into NMF. In this paper, we proposed a novel graph based SS-NMF (GSS-NMF). For each sample, GSS-NMF minimizes its distances to the same labeled samples and maximizes the distances against different labeled samples to incorporate the discriminative information. Since both labeled and unlabeled samples are embedded in the same reduced dimensional space, the discriminative information from the labeled samples is successfully transferred to the unlabeled samples, and thus it greatly improves the clustering performance. Since the traditional multiplicative update rule converges slowly, we applied the well-known projected gradient method to optimizing GSS-NMF and the proposed algorithm can be applied to optimizing other manifold regularized NMF efficiently. Experimental results on two popular document datasets, i.e., Reuters21578 and TDT-2, show that GSS-NMF outperforms the representative SS-NMF algorithms.",,978-1-4673-4651-1,10.1109/ICMLA.2012.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406696,manifold learning;semi-supervised learning;non-negative matrix factorization,Clustering algorithms;Machine learning;Semisupervised learning;Gradient methods;Manifolds;Linear programming;Periodic structures,approximation theory;document handling;gradient methods;graph theory;learning (artificial intelligence);matrix decomposition;minimisation;pattern clustering,graph-based semisupervised nonnegative matrix factorization approximation;document clustering performance improvement;low-rank matrix product;graph based SS-NMF;labeled samples;distance minimization;distance maximization;unlabeled samples;dimensional space reduction;projected gradient method;GSS-NMF optimization;manifold regularized NMF optimization;Reuters21578 document dataset;TDT-2 document dataset,,10,,26,,10-Jan-13,,,IEEE,IEEE Conferences
Clustering articles in bahasa Indonesia using self-organizing map,古蘭經經文的分析分區聚類和相似性度量,D. Gunawan; A. Amalia; I. Charisma,"Department of Information Technology, Universitas Sumatera Utara, Medan, Indonesia; Department of Computer Science, Universitas Sumatera Utara, Medan, Indonesia; Department of Information Technology, Universitas Sumatera Utara, Medan, Indonesia",2017 International Conference on Electrical Engineering and Informatics (ICELTICs),11-Jan-18,2017,,,239,244,"One of the text explosion in the Internet is the availability of any articles published by any institution at any time. As the articles are widely available in the Internet, there are many observations that can be done by utilizing the available articles. One of the basic things to do is categorizing the articles. As the number of the articles are massive, categorizing them manually is not an option. Therefore, we propose the self-organizing map (SOM) algorithm with the combination of Term Frequency - Inverse Document Frequency (TF-IDF) and multiword expressions to categorize articles in Bahasa Indonesia. This research clusters health articles into several topics according to keywords of each articles. The process is divided into three sections, such as text pre-processing, keywords extraction, and the implementation of self-organizing maps. Number of keywords that are used as parameters to self-organizing maps algorithm will influence the number of resulted clusters.",,978-1-5386-2934-5,10.1109/ICELTICS.2017.8253288,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253288,article clustering;topic clustering;self-organizing maps;tf-idf;multiword expressions;health articles,Self-organizing feature maps;Clustering algorithms;Filtering;Thorax;Electrical engineering;Informatics;Internet,information retrieval;Internet;natural language processing;pattern clustering;self-organising feature maps;text analysis,bahasa Indonesia;self-organizing map algorithm;multiword expressions;Bahasa Indonesia;health articles;articles clustering;Internet;Internet;Term Frequency - Inverse Document Frequency;TF-IDF;keywords extraction,,4,,9,,11-Jan-18,,,IEEE,IEEE Conferences
Summarization for Internet News Based on Clustering Algorithm,Web搜索引擎單擊的文檔分析,H. Yu,"Inst. of Comput. & Commun. Eng., China Univ. of Pet., Dongying, China",2009 International Conference on Computational Intelligence and Natural Computing,4-Sep-09,2009,1,,34,37,"For the explosion of information in the World Wide Web, this paper proposed a new method of summarization for Internet news based on clustering algorithm. It used Google search engine to extract relevant documents, and mixed query sentence into sentences set which segmented from relevant document set, then this paper adopted effective cluster algorithm to cluster all the sentences. In the period of summary generation, the summary sentences were extracted by turns from the clusters which query sentence in. Experimental result shows that the proposed summarization method can improve the performance of summary, clustering algorithm is effective.",,978-0-7695-3645-3,10.1109/CINC.2009.194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231631,summarization;clustering;sentence similarity,Internet;Clustering algorithms;Data mining;Explosions;Search engines;Computational intelligence;Petroleum;Electronic mail;Web sites;Text processing,Internet;pattern clustering;query processing;search engines,Internet news summarization;clustering algorithm;World Wide Web;Google search engine;document extraction;query sentences set,,1,,10,,4-Sep-09,,,IEEE,IEEE Conferences
Classification of scientific papers with big data technologies,測量文檔集合中的組內聚力,S. Gurbuz; G. Aydin,"Firat University, Computer Engineering Department, 23100, Elazig, Turkey; Firat University, Computer Engineering Department, 23100, Elazig, Turkey",2017 International Conference on Computer Science and Engineering (UBMK),2-Nov-17,2017,,,697,701,"Data sizes that cannot be processed by conventional data storage and analysis systems are named as Big Data. It also refers to new technologies developed to store, process and analyze large amounts of data. Automatic information retrieval about the contents of a large number of documents produced by different sources, identifying research fields and topics, extraction of the document abstracts, or discovering patterns are some of the topics that have been studied in the field of big data. In this study, the Na簿ve Bayes classification algorithm, which is run on a data set consisting of scientific articles, has been tried to automatically determine the classes to which these documents belong. We have developed an efficient system that can analyze the Turkish scientific documents with the distributed document classification algorithm run on the Cloud Computing infrastructure. The Apache Mahout library is used in the study. The servers required for classifying and clustering distributed documents are.",,978-1-5386-0930-9,10.1109/UBMK.2017.8093504,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093504,Big Data;classification;cloud computing;distributed documents;parallel computing,Google;File systems;Java;Yarn;Reliability engineering;Data processing,Bayes methods;Big Data;cloud computing;data mining;document handling;information retrieval;pattern classification;pattern clustering,conventional data storage;analysis systems;pattern discovery;scientific papers classification;Big Data technologies;distributed documents clustering;cloud computing infrastructure;Apache Mahout library;distributed document classification algorithm;Turkish scientific documents;scientific articles;Na簿ve Bayes classification algorithm;document abstracts;automatic information retrieval,,,,,,2-Nov-17,,,IEEE,IEEE Conferences
Weighted graph model based sentence clustering and ranking for document summarization,WISE：基於Web內容挖掘技術的網頁搜索結果的層次軟聚類,S. S. Ge; Z. Zhang; H. He,"Department of Electrical & Computer Engineering, National University of Singapore, Singapore 117576, Singapore; Department of Electrical & Computer Engineering, National University of Singapore, Singapore 117576, Singapore; Department of Electrical & Computer Engineering, National University of Singapore, Singapore 117576, Singapore",The 4th International Conference on Interaction Sciences,8-Sep-11,2011,,,90,95,"This paper proposes a sentence ranking and clustering based summarization method that extracts essential sentences from a document. To discover central sentences, a weighted undirected graph that takes sentence similarities and the discourse relationship between sentences as the weights of edges is constructed for the given document. A graph-ranking algorithm is implemented to calculate the scores of sentences. We also build a matrix for the document, and an algorithm based on Sparse Non-negative Matrix Factorization is introduced to cluster the sentences in the document. High ranked sentences of each cluster are selected to comprise the summarization of the document. The experimental results on the Document Understanding Conference (DUC) 2001 data set demonstrate the effectiveness of the document summarization algorithm.",,978-89-88678-45-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014538,,Clustering algorithms;Symmetric matrices;Strontium;Sparse matrices;Semantics;Connectors;Feature extraction,document handling;graph theory;matrix decomposition,weighted graph model;sentence clustering;sentence ranking;document summarization;clustering based summarization method;essential sentence extraction;weighted undirected graph;graph-ranking algorithm;sparse nonnegative matrix factorization;document understanding conference,,2,,21,,8-Sep-11,,,IEEE,IEEE Conferences
Flexible Fuzzy Co-clustering with Feature-cluster Weighting,多傾斜檢測印度文字文件,W. Tjhi; L. Chen,"School of Electrical & Electronic Engineering, Nanyang Technological University, Republic of Singapore. pm817561@ntu.edu.sg; School of Electrical & Electronic Engineering, Nanyang Technological University, Republic of Singapore. elhchen@ntu.edu.sg","2006 9th International Conference on Control, Automation, Robotics and Vision",16-Jul-07,2006,,,1,6,"Fuzzy co-clustering is an unsupervised technique that performs simultaneous fuzzy clustering of objects and features. In this paper, we propose a new flexible fuzzy co-clustering algorithm which incorporates feature-cluster weighting in the formulation. We call it Flexible Fuzzy Co-clustering with Feature-cluster Weighting (FFCFW). By flexible we mean the algorithm allows the number of object clusters to be different from the number of feature clusters. There are two motivations behind this work. First, in the fuzzy framework, many co-clustering algorithms still require the number of object clusters to be the same as the number of feature clusters. This is despite the fact that such rigid structure is hardly found in real-world applications. The second motivation is that while there have been numerous attempts for flexible co-clustering, it is common that in such scheme the relationships between object and feature clusters are not clearly represented. For this reason we incorporate a feature-cluster weighting scheme for each object cluster generated by FFCFW so that the relationships between the two types of clusters are manifested in the feature-cluster weights. This enables the new algorithm to generate more accurate representation of fuzzy co-clusters. FFCFW is formulated by fusing together the core components of two existing algorithms. Like its predecessors, FFCFW adopts an iterative optimization procedure. We discuss in details the derivation of the proposed algorithm and the advantages it has over other existing works. Experiments on several large benchmark document datasets reveal the feasibility of our proposed algorithm",,1-4244-0341-3,10.1109/ICARCV.2006.345069,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4150268,Fuzzy Co-clustering;Data Clustering;Fuzzy System;Autonomous Agent;Computational Intelligence,Clustering algorithms;Iterative algorithms;Fuzzy systems;Computational intelligence;Autonomous agents;Data engineering;Humans,artificial intelligence;feature extraction;fuzzy set theory;object recognition;pattern clustering,feature-cluster weighting;fuzzy object clustering;object relationship;data clustering;fuzzy system;autonomous agent;computational intelligence,,2,,15,,16-Jul-07,,,IEEE,IEEE Conferences
Document summarization method based on heterogeneous graph,擴展增長的層次結構SOM以在圖域中對文檔進行聚類,Y. Wei,"Network Information center, Shanxi Normal University, Linfen, 041004. China",2012 9th International Conference on Fuzzy Systems and Knowledge Discovery,9-Jul-12,2012,,,1285,1289,"Document summarization has been widely studied for many years. Existing methods mainly use statistical or linguistic information to extract the most informative sentences from document. However, those methods ignore the relationship between different granularities (i.e., word, sentence, and topic). Actually, the interactions between those granularities can be used in document summarization. In this paper we proposed a document summarization method based on heterogeneous graph. The method is first implemented by constructing a graph which reflect relationship between different size of granularity nodes, and then using ranking algorithm to calculate score of nodes. Finally, highest score of sentences in the document will be chosen as summary. Experimental results show that our approach outperforms baseline methods.",,978-1-4673-0024-7,10.1109/FSKD.2012.6234047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234047,document summarization;heterogeneous graph;similarity measure;ranking algorithm,Computational modeling;Pragmatics;Feature extraction;Clustering algorithms;Guidelines;Hidden Markov models;Data mining,document handling;graph theory;statistical analysis,document summarization method;heterogeneous graph;linguistic information;statistical information;ranking algorithm;granularity nodes;baseline methods,,5,,7,,9-Jul-12,,,IEEE,IEEE Conferences
Summarization of text clustering based vector space model,使用頻繁上下文術語集的文本聚類,Mingzhen Chen; Yu Song,"Department of Compute Science, Zhejiang University, Hangzhou, 310027, China; Department of Compute Science, Zhejiang University, Hangzhou, 310027, China",2009 IEEE 10th International Conference on Computer-Aided Industrial Design & Conceptual Design,8-Jan-10,2009,,,2362,2365,Text clustering is an important task of natural language processing and is widely applicable in areas such as information retrieval and web mining. The representation of document and the clustering algorithm are the key issues of text clustering. This paper discusses vector space model(VSM)-based clustering algorithms. This paper reviewed the text clustering algorithm.,,978-1-4244-5266-8,10.1109/CAIDCD.2009.5375265,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5375265,Vector Space Model (VSM);Text clustering;Document representation;Distance function,Clustering algorithms;Partitioning algorithms;Information retrieval;Natural language processing;Web mining;Frequency measurement;Text mining;Statistics;Ontologies;Databases,natural language processing;text analysis,text clustering summarization;vector space model;natural language processing;information retrieval;web mining;document representation,,2,,7,,8-Jan-10,,,IEEE,IEEE Conferences
A Web document clustering algorithm based on concept of neighbor,Web文檔集群中不斷發展的文檔功能：可行性研究,Jiang-Chun Song; Jun-Yi Shen,"Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China",Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),19-Feb-04,2003,1,,46,50 Vol.1,"As the WWW developed rapidly, it becomes the most important resource gradually that transfers and shares the global information as well as being full of the latent capacity. Recent years, the researches of the Web mining have been concerned broadly and gotten a great deal of achievements simultaneously. The nearest neighbor technique, which is a hierarchical clustering method based on distance has been applied to many cases widely for the efficiency and validity. In this paper, based on the vector space model (VSM) of the Web documents, we improved the nearest neighbor method, put forward a new Web document clustering algorithm, and researched the validity and scalability of the algorithm, the time and space complexity of the algorithm.",,0-7803-7865-2,10.1109/ICMLC.2003.1264440,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264440,,Clustering algorithms;Web mining;Nearest neighbor searches;World Wide Web;Data mining;Computer science;Clustering methods;Scalability;Unsupervised learning;Pattern analysis,Web sites;information retrieval systems;data mining;computational complexity;unsupervised learning,Web document clustering algorithm;World Wide Web;global information;Web mining;nearest neighbor method;vector space model;space complexity;time complexity;unsupervised learning,,1,,19,,19-Feb-04,,,IEEE,IEEE Conferences
On Using Metadata and Compression Algorithms to Cluster Heterogeneous Documents from a Semantic Point of View,使用Web日誌的基於相關性的文檔聚類,A. Cernian; D. Carstoiu; V. Sgarciu,"Fac. of Autom. Control & Comput. Sci., Politeh. Univ. of Bucharest, Bucharest, Romania; Fac. of Autom. Control & Comput. Sci., Politeh. Univ. of Bucharest, Bucharest, Romania; Fac. of Autom. Control & Comput. Sci., Politeh. Univ. of Bucharest, Bucharest, Romania",2010 Fifth International Conference on Software Engineering Advances,1-Nov-10,2010,,,190,195,"Since data is becoming more and more unstructured, clustering heterogeneous data is essential to getting structured information in response to user queries. In this paper, we test and validate the results of a new clustering technique - clustering by compression - when applied to metadata associated with heterogeneous sets of documents. The clustering by compression procedure is based on a parameter-free, universal, similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files (singly and in pair-wise concatenation). Experimental results show that using metadata could improve the average clustering performances with about 10% over clustering the same sample data set without using metadata.",,978-1-4244-7788-3,10.1109/ICSEA.2010.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615737,clustering by compression;normalized compression distance;heterogeneous data;metadata,,document handling;meta data;pattern clustering,metadata;compression algorithms;cluster heterogeneous documents;semantic point of view;structured information;user queries;clustering technique;compression procedure;normalized compression distance;NCD;pairwise concatenation,,,,13,,1-Nov-10,,,IEEE,IEEE Conferences
Web Document Clustering using Semantic Link Analysis,使用源自圖像和目錄的信息對文檔片段進行主動聚類,S. Arch-int,"Khon Kaen University, 40002, Thailand","International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",22-May-06,2005,2,,13,18,"Searching and discovering the relevant information on the Web have always been challenging research areas. Web document clustering is a promising technique in preparing a huge collection of Web documents suitable for Web search engines. This paper proposes a semantic document clustering approach to categorize Web documents in a semantic manner. First, the formal methods and algorithms are introduced as techniques for document extraction and clustering. The approach incorporates WordNet and ontology knowledge as the assistant mechanisms such that the resulting set of concepts are thus utilized as formal representation for extracted documents. As a consequence, the semantic-based clusters are finally determined the cluster scores. Next, the semantic-based link analysis method is also proposed for clustering Web documents into semantic clusters that are scored based on the notion of semantic-based concepts and documents. Finally, these document scores are subsequently used for evaluating the semantic document similarity and document quality. As such, the precision criterion is employed for efficient evaluations by comparing with keywords-based search method. The experimental results reported that the proposed method was able to outperform the TF/IDF method up to 9% on average",,0-7695-2504-0,10.1109/CIMCA.2005.1631438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1631438,,Web pages;Search engines;Clustering algorithms;Ontologies;Internet;Semantic Web;Information analysis;Information technology;Technological innovation;Computer science,document handling;information retrieval;ontologies (artificial intelligence);semantic Web,Web document clustering;semantic link analysis;document extraction;WordNet;ontology,,,,14,,22-May-06,,,IEEE,IEEE Conferences
Arabic text mining based on clustering and coreference resolution,用於文檔聚類的猶豫距離相似性度量,S. Mahmood; F. M. L. Al-Rufaye,"University of Basrah-Iraq; Middle Technical University-Wasit, Iraq",2017 International Conference on Current Research in Computer Science and Information Technology (ICCIT),3-Jul-17,2017,,,140,144,"Text mining discover and extract useful information from documents, whenever increase the size and number documents leads to redouble features. The huge features for the documents adds challenge to text mining called high dimension. The aim of this proposed study is minimize the high dimension of the documents, and improve Arabic text mining using clustering. In order to achieve this goal, we propose to applied coreference resolution technique using the clustering algorithms k-mediods and k-means. This study uses the similarity metrics Euclidean and Cosine. The system implements using a corpus contains on 200 sport news Arabic. Finally, evaluation measures are used including (Precision' Recall and F-measure) to evaluate our system.",,978-1-5386-2955-0,10.1109/CRCSIT.2017.7965549,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965549,Arabic language processing;Text mining;Clustering,Text mining;Clustering algorithms;Feature extraction;Computer science;Information technology;Measurement,data mining;feature selection;pattern clustering;sport;text analysis,Arabic text mining;coreference resolution;information extraction;document high dimension;clustering algorithms;k-mediods;k-means;similarity metrics;Arabic sport news;features selection,,,,12,,3-Jul-17,,,IEEE,IEEE Conferences
Privileged Information for Hierarchical Document Clustering: A Metric Learning Approach,對稱非負矩陣分解：算法和在概率聚類中的應用,R. M. Marcacini; M. A. Domingues; E. R. Hruschka; S. O. Rezende,"Fed. Univ. of Mato Grosso do Sul (UFMS), Tres Lagoas, Brazil; Math. & Comput. Sci. Inst. (ICMC), Sao Carlos, Brazil; Math. & Comput. Sci. Inst. (ICMC), Sao Carlos, Brazil; Math. & Comput. Sci. Inst. (ICMC), Sao Carlos, Brazil",2014 22nd International Conference on Pattern Recognition,6-Dec-14,2014,,,3636,3641,"Traditional hierarchical text clustering methods assume that the documents are represented only by ""technical information"", i.e., keywords, phrases, expressions and named entities that can be directly extracted from the texts. However, in many scenarios there is an additional and valuable information about the documents which is usually disregarded during the clustering task, such as user-validated tags, annotations and comments from experts, dictionaries and domain ontologies. Recently, Vapnik introduced a new learning paradigm, called LUPI - Learning Using Privileged Information, which allows the incorporation of this additional (privileged) information in a supervised learning setting. We investigated the incorporation of privileged information in unsupervised setting. The key idea in our proposed approach is to extract important relationships among documents represented in the privileged information dimensional space to learn a more accurate metric for text clustering in the technical information space. A thorough experimental evaluation indicates that the incorporation of privileged information through metric learning significantly improves the hierarchical clustering accuracy.",1051-4651,978-1-4799-5209-0,10.1109/ICPR.2014.625,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977337,,Measurement;Clustering algorithms;Data mining;Feature extraction;Accuracy;Partitioning algorithms;Clustering methods,learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;text analysis,privileged information;hierarchical document clustering;metric learning approach;hierarchical text clustering methods;technical information;domain ontologies;dictionaries ontologies;learning using privileged information;LUPI;supervised learning setting;text clustering,,2,,13,,6-Dec-14,,,IEEE,IEEE Conferences
Web page clustering using Harmony Search optimization,計算機輔助設計系統電子檔案中非結構化文本搜索改進方法的效率,R. Forsati; M. Mahdavi; M. Kangavari; B. Safarkhani,"Department of computer Engineering, Islamic Azad University, Karaj Branch, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer &IT, Iran University of Science&Technology, Tehran, Iran; Department of Computer Engineering, Tehran Azad University, Iran",2008 Canadian Conference on Electrical and Computer Engineering,15-Jul-08,2008,,,1601,1604,"Clustering has become an increasingly important task in modern application domains. Targeting useful and relevant information on the World Wide Web is a topical and highly complicated research area. Clustering techniques have been applied to categorize documents on Web and extracting knowledge from the Web. In this paper we propose novel clustering algorithms based on harmony search (HS) optimization method that deals with Web document clustering. By modeling clustering as an optimization problem, first, we propose a pure HS based clustering algorithm that finds near global optimal clusters within a reasonable time. Then we hybridize K-means and harmony clustering to achieve better clustering. Experimental results on five different data sets reveal that the proposed algorithms can find better clusters when compared to similar methods and the quality of clusters is comparable. Also proposed algorithms converge to the best known optimum faster than other methods.",0840-7789,978-1-4244-1642-4,10.1109/CCECE.2008.4564812,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564812,clustering web pages;harmony search;global optimization,Web pages;Clustering algorithms;Partitioning algorithms;Optimization methods;Data engineering;Frequency;Clustering methods;Data mining;Convergence;Genetic algorithms,document handling;information analysis;knowledge acquisition;Web sites,Web page clustering;harmony search optimization;World Wide Web;document categorization;Web knowledge extraction;Web document clustering,,24,,15,,15-Jul-08,,,IEEE,IEEE Conferences
Fuzzy pattern cluster scheme for breast cancer datasets,利用用戶標記進行Web服務聯合集群,D. Vanisri; C. Loganathan,"Kongu Engineering College, Erode 638052, India; Maharaja Arts and Science College, Coimbatore, Tamilnadu, India",2010 International Conference on Communication and Computational Intelligence (INCOCCI),24-Mar-11,2010,,,410,414,Data mining techniques are used for the knowledge discovery process under the large data set environment. Clustering techniques are used to group up the relevant data sets. Hierarchical and partitioned clustering techniques are used for the clustering process. The clustering process is the complex task with high process time. The pattern extraction scheme is applied to find frequent item sets. Association rule mining techniques are applied to carry out the pattern extraction process. The pattern extraction scheme and the clustering scheme are integrated in the simultaneous pattern extraction and clustering scheme. The clustering process is improved with pattern comparison and transaction transfer process. The simultaneous clustering scheme is implemented to analyze the cancer patient diagnosis reports. The cluster accuracy is represented using the fitness values. The system is enhanced with the K-means clustering algorithm. Fuzzy clustering used to optimize the system.,,978-81-8371-369-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738766,Data mining;document clustering;pattern cluster;pattern discovery;data cluster,Clustering algorithms;Compounds;Association rules;Breast cancer;Educational institutions,cancer;data mining;feature extraction;fuzzy set theory;medical computing;patient diagnosis;pattern clustering;transaction processing,fuzzy pattern cluster scheme;breast cancer dataset;data mining techniques;knowledge discovery process;large data set environment;partitioned clustering techniques;pattern extraction scheme;association rule mining techniques;pattern extraction process;transaction transfer process;cancer patient diagnosis reports;K-means clustering algorithm;fuzzy clustering,,,,10,,24-Mar-11,,,IEEE,IEEE Conferences
A frequent term based text clustering approach using novel similarity measure,基於線性相似度加權的搜索結果聚類,G. S. Reddy; T. V. Rajinikanth; A. A. Rao,"Dept. of IT, VNRVJIET, Hyderabad, India; Dept. Of CSE, SNIST, Hyderabad, India; Dept. Of CSE, JNTUA, Anantapur, India",2014 IEEE International Advance Computing Conference (IACC),27-Mar-14,2014,,,495,499,"Text clustering is an unsupervised process forming its basis solely on finding the similarity relationship between documents with the output as a set of clusters [14]. In this research, a commonality measure is defined to find commonality between two text files which is used as a similarity measure. The main idea is to apply any existing frequent item finding algorithm such as apriori or fp-tree to the initial set of text files to reduce the dimension of the input text files. A document feature vector is formed for all the documents. Then a vector is formed for all the static text input files. The algorithm outputs a set of clusters from the initial input of text files considered.",,978-1-4799-2572-8,10.1109/IAdCC.2014.6779374,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779374,Commanality measure;frequent item;Clustering;Apriori,Clustering algorithms;Vectors;Itemsets;Algorithm design and analysis;Text categorization;Conferences,pattern clustering;text analysis;unsupervised learning,frequent term based text clustering approach;novel similarity measure;unsupervised learning process;similarity relationship;commonality measure;frequent item finding algorithm;fp-tree;input text file dimension reduction;document feature vector,,7,,18,,27-Mar-14,,,IEEE,IEEE Conferences
Transformation invariant SOM clustering in Document Image Analysis,EMG循環模式的聚類分析：多種運動病理學的驗證研究,S. Marinai; E. Marino; G. Soda,"Universita di Firenze, Italy; Universita di Firenze, Italy; Universita di Firenze, Italy",14th International Conference on Image Analysis and Processing (ICIAP 2007),29-Oct-07,2007,,,185,190,"In this paper, we propose the combination of the self organizing map (SOM) and of the tangent distance for effective clustering in document image analysis. The proposed model (SOM_TD) is used for character and layout clustering, with applications to word retrieval and to page classification. By using the tangent distance it is possible to improve the SOM clustering so as to be more tolerant with respect to small local transformations of the input patterns.",,978-0-7695-2877-9,10.1109/ICIAP.2007.4362777,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362777,,Text analysis;Image analysis;Clustering algorithms;Neurons;Unsupervised learning;Self organizing feature maps;Artificial neural networks;Prototypes;Pattern recognition;Supervised learning,document image processing;pattern clustering;self-organising feature maps,transformation invariant SOM clustering;document image analysis;selforganizing map;SOM_TD model;character clustering;layout clustering;word retrieval;page classification;tangent distance,,1,,10,,29-Oct-07,,,IEEE,IEEE Conferences
Towards a context aware mining of user interests for consumption of multimedia documents,使用模糊聚類的多光譜文檔圖像自動偽造檢測,M. Wallace; G. Stamou,"Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece",Proceedings. IEEE International Conference on Multimedia and Expo,7-Nov-02,2002,1,,733,736 vol.1,"As the annotation of multimedia documents uses multiple descriptors, it is possible to define multiple, semantically meaningful, similarity (or dissimilarity) relations among them. Therefore, for cases such as the mining of user interests for consumption of multimedia documents, based on usage history, where the clustering of documents is necessary, it is important to develop context aware clustering algorithms that are able to handle this type of information. We explain the relation between context, user interest and the multiple relations; furthermore, we present a clustering algorithm that is able to mine user interests from multi-relational data sets.",,0-7803-7304-9,10.1109/ICME.2002.1035886,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1035886,,Context awareness;Clustering algorithms;Computational Intelligence Society;Merging;Partitioning algorithms;Clustering methods,multimedia databases;relational databases;information retrieval;data mining;pattern clustering,context aware mining;user interests;multimedia documents consumption;multimedia documents annotation;similarity relations;dissimilarity relations;usage history;context aware clustering algorithms;multi-relational data sets;information retrieval system,,13,,8,,7-Nov-02,,,IEEE,IEEE Conferences
Visualization of Potential Technical Solutions by Self-Organizing Maps and Co-Cluster Extraction,使用模糊聚類發現Web文檔中的潛在語義,Y. Nishida; K. Honda,Osaka Prefecture University; Osaka Prefecture University,2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS),16-May-19,2018,,,820,825,"This paper proposes a novel approach for supporting inspiration of potential technical solutions through visualization of solving means varied in patent documents. The data sets to be analyzed by SOM are constructed in two different schemes. In the first scheme, representative words are extracted to generate word level co-occurrence probability vectors. Then, in the second scheme, correlation coefficients of the generated co-occurrence probability vectors are merged into correlation coefficient vectors. Comparing the two SOMs derived with the above schemes, the potential of the method is shown in supporting innovation acceleration through extraction of important pairs of related factors in new technology development. Additionally, co-cluster structures are utilized for emphasizing field-related solutions by constructing multiple SOMs after co-clustering, in which document-keyword co-occurrences are partitioned into co-clusters consisting of mutually related pairs in particular fields.",,978-1-5386-2633-7,10.1109/SCIS-ISIS.2018.00135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716077,"Patent documents, Invention, Technical issue, Technical solution, Spectral ordering, Co-clustering",Patents;Correlation;Self-organizing feature maps;Technological innovation;Probability;Data visualization;Visualization,data visualisation;document handling;patents;pattern clustering;probability;self-organising feature maps,co-cluster extraction;patent documents;SOM;representative words;word level co-occurrence probability vectors;correlation coefficients;correlation coefficient vectors;co-cluster structures;document-keyword co-occurrences;multiple SOM,,,,8,,16-May-19,,,IEEE,IEEE Conferences
Research on Text Clustering Algorithms,文本聚類的頻繁項集方法,Q. Li; X. Huang,NA; NA,2010 2nd International Workshop on Database Technology and Applications,6-Dec-10,2010,,,1,3,"Web documents are enormous. Text clustering is to place the documents with the most words in common into the same cluster. Thus the web search engine can structure the large result set for a certain quest. In this article, we study three kinds of clustering algorithms, prototype based, density based and hierarchical clustering algorithms. We compare two typical algorithms, K-medoids and DBSCAN. The results show that the K-medoids is sensitive to the initial center point and the DBSCAN has a better performance.",2167-194X,978-1-4244-6977-2,10.1109/DBTA.2010.5659055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659055,,Clustering algorithms;Algorithm design and analysis;Classification algorithms;Partitioning algorithms;Films;Noise;Forestry,pattern clustering;query processing;search engines;text analysis,text clustering algorithm;Web document;Web search engine;hierarchical clustering algorithms;density based clustering;prototype based clustering;K-medoids;DBSCAN,,1,,6,,6-Dec-10,,,IEEE,IEEE Conferences
Semi-supervised clustering with soft labels,多文檔抽象摘要的聚類遺傳語義圖方法,C. M. Nebu; S. Joseph,"Amal Jyothi College of Engineering, Kerala, India; Amal Jyothi College of Engineering, Kerala, India",2015 International Conference on Control Communication & Computing India (ICCC),14-Mar-16,2015,,,612,616,"This paper devises a semi-supervised learning algorithm to cluster text documents. The proposed algorithm clusters multi-dimensional documents using the k-means algorithm. It initially reduces the dimensionality of the text so that the clustering algorithm can perform well in the low dimensional feature space. It also removes the irrelevant, redundant and noisy features from the corpus which may otherwise mislead the underlying algorithm. The proposed method employs pLSA algorithm to generate soft labels from these reduced feature subset and these labels along with the class labels guide the k-means algorithm. Experiments were conducted on Reuters-21,578 dataset and the results obtained showed that the proposed method outperforms many previous clustering algorithms without supervision.",,978-1-4673-7349-4,10.1109/ICCC.2015.7432969,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432969,Clustering;semi-supervised learning;dimensionality reduction;text mining,Clustering algorithms;Feature extraction;Semisupervised learning;Support vector machines;Noise measurement;Semantics;Algorithm design and analysis,data mining;pattern clustering;text analysis,"semisupervised clustering algorithm;semisupervised learning algorithm;multidimensional text document;k-means algorithm;low dimensional feature space;pLSA algorithm;Reuters-21,578;text mining",,,,12,,14-Mar-16,,,IEEE,IEEE Conferences
Graph Local Clustering for Topic Detection in Web Collections,文檔倉庫：多媒體數據庫的文檔密集型應用程序,S. E. Garza; R. F. Brena,"Center for Intell. Comput. & Robot., Monterrey, Mexico; Center for Intell. Comput. & Robot., Monterrey, Mexico",2009 Latin American Web Congress,1-Dec-09,2009,,,207,213,"In the midst of a developing Web that increases its size with a constant rhythm, automatic document organization becomes important. One way to arrange documents is by categorizing them into topics. Even when there are different forms to consider topics and their extraction, a practical option is to view them as document groups and apply clustering algorithms. An attractive alternative that naturally copes with the Web size and complexity is the one proposed by graph local clustering (GLC) methods. In this paper, we define a formal framework for working with topics in hyperlinked environments and analyze the feasibility of GLC for this task. We performed tests over an important Web collection, namely Wikipedia, and our results, which were validated using various kinds of methods (some of them specific for the information domain), indicate that this approach is suitable for topic discovery.",,978-0-7695-3856-3,10.1109/LA-WEB.2009.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341516,graph clustering;topic detection;Wikipedia;Web hyperlink structure mining,Wikipedia;Intelligent robots;Rhythm;Performance evaluation;Testing;Clustering methods;Robotics and automation;Clustering algorithms;Probability distribution;Vocabulary,document handling;graph theory;Internet;pattern clustering,graph local clustering;topic detection;Web collections;automatic document organization;document groups;Wikipedia;topic discovery;information domain,,1,,26,,1-Dec-09,,,IEEE,IEEE Conferences
Towards Automated Ink Mismatch Detection in Hyperspectral Document Images,走向並行文檔聚類,A. Abbas; K. Khurshid; F. Shafait,"Sch. of Electr. Eng. & Comput., Univ. of Newcastle, Callaghan, NSW, Australia; Dept. of Electr. Eng., Inst. of Space Technol., Islamabad, Pakistan; Sch. of Electr. Eng. & Comput. Sci., NUST, Islamabad, Pakistan",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,1229,1236,"Hyperspectral imaging helps in identifying patterns and objects in an observed hyperspectral scene on the basis of their unique spectral signatures; such identification is otherwise difficult using regular imaging. Recently, ink mismatch detection analysis based on hyperspectral imaging has shown enormous potential in distinguishing visually similar inks. Such analysis provides significant information to forensic document examiners to determine the authenticity of the questioned documents. However, a major challenge still exists in disproportionate ink mismatch detection because it is inherently an unbalanced clustering problem. The presented approach deals with ink mismatch detection in unbalanced clusters by using hyperspectral unmixing scheme. It identifies the spectral signatures (endmembers) of the inks and their corresponding proportions (abundances). Our results show that HySime outperforms other methods in signal subspace estimation. Hyperspectral unmixing is done by using minimum volume enclosing simplex algorithm. Efficacy of the purposed approach is demonstrated by successfully distinguishing varying disproportionate ink datasets generated from UWA database and results are compared with existing state of the art methods in hyperspectral ink mismatch detection field. We expect that these finding will further encourage the use of hyperspectral imaging in document analysis, particularly towards automated questioned document examination.",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.203,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270134,Hyperspectral document images;hyperspectral unmixing;forgery detection;ink mismatch detection,Ink;Hyperspectral imaging;Image color analysis;Estimation;Eigenvalues and eigenfunctions;Databases,document image processing;hyperspectral imaging;spectral analysis,unbalanced clustering problem;hyperspectral unmixing scheme;disproportionate ink datasets;hyperspectral ink mismatch detection field;hyperspectral imaging;document analysis;automated questioned document examination;ink mismatch detection analysis;disproportionate ink mismatch detection;spectral signatures;automated ink mismatch detection;hyperspectral document images,,6,,36,,29-Jan-18,,,IEEE,IEEE Conferences
On the Optimization of a Duplicate Document Detection Algorithm Based on SIMD and Document Statistics,用於文檔聚類的稀疏Poisson潛在塊模型,X. P. Yuan; J. Long; H. Zhang; Z. P. Zhang; W. H. Gui,"Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China; Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China; Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China; Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China; Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China",2010 International Conference on Computational Intelligence and Software Engineering,30-Dec-10,2010,,,1,4,"Although considerable effort has been devoted to duplicate document detection (DDD) and its applications, there is very limited study on the optimization of its time-consuming functions. An experimental analysis which is conducted on a million Grant Proposal documents from the nsfc.gov.cn shows that even by using the clustering and the sampling methods, the speed of DDD is still quite slow. By analyzing the performance of our system with Intel VTune Performance Analyzer, we find out that the shingle comparison is the most time-consuming part in our system, occupying 58% CPU usage. Based on the analysis of the whole algorithm and the data statistics, we propose and implement an optimized shingle comparison algorithm using Intel SIMD technology. Experiments demonstrate that the proposed optimization technique brings 11.6%-38.5% performance gain with various instruction sets and parameters settings. Further performance gain could be achieved base on the accuracy and speed tradeoff.",,978-1-4244-5391-7,10.1109/CISE.2010.5676949,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676949,,Performance gain;Algorithm design and analysis;Accuracy;Optimization;Registers;Clustering algorithms;Plagiarism,document handling;parallel processing;statistics,duplicate document detection algorithm;document statistics;grant proposal documents;Intel VTune performance analyzer;Intel SIMD technology,,,,12,,30-Dec-10,,,IEEE,IEEE Conferences
A study on template extraction,智能動態XML文檔集群,S. Pushpa; D. Kanagalatchumy,"Department of IT, Sri Manakula Vinayagar Engineering College, Puducherry, India; Department of CSE, Sri Manakula Vinayagar Engineering College, Puducherry, India",2013 International Conference on Information Communication and Embedded Systems (ICICES),25-Apr-13,2013,,,109,115,"The World Wide Web is a vast and rapidly growing source of information. Most of the information consumers are based on the World Wide Web for their exploration and the information is in the form of unstructured and so it's tough to handle. Nevertheless, there are many webpages which are having the structured data. Structured data is the format or pattern which is called as template. Some templates are considered destructive, since they worsen the performance and accuracy of the applications due to the irrelevant terms. Thus Template detection techniques are used to raise the working of search engines, clustering and classification of web documents. In this paper we survey some of the algorithms for extracting templates from different web pages in an efficient manner. First, the documents are clustered based on their similarity and thus templates from each cluster are extracted concurrently. Our intent is to present different techniques for the fast and accurate performances in extracting templates.",,978-1-4673-5788-3,10.1109/ICICES.2013.6508206,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508206,Template extraction;clustering techniques;MDL principle,HTML;Data mining;Web pages;Uniform resource locators;Clustering algorithms;Algorithm design and analysis,document handling;information retrieval;Internet;pattern classification;pattern clustering;search engines;Web sites,World Wide Web;information source;information consumers;Web pages;structured data;search engines;Web document classification;Web document clustering;template detection technique;template extraction,,1,,6,,25-Apr-13,,,IEEE,IEEE Conferences
Knowledge Extraction from Web Services Repositories,基於特徵重疊的層次文本聚類動態自組織模型,V. Kiouftis; E. Theodoridis; A. Tsakalidis,"Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece; Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece; Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece",2013 IEEE 25th International Conference on Tools with Artificial Intelligence,10-Feb-14,2013,,,690,697,"With the increasing use of web and Service Oriented Systems, web-services have become a widely adopted technology. Web services repositories are growing fast, creating the need for advanced tools for organizing and indexing them. Clustering web services, usually represented by Web Service Description Language (WSDL) documents, enables the web service search engines and users to organize and process large web service repositories in groups with similar functionality and characteristics. In this paper, we propose a novel technique of clustering WSDL documents. The proposed method considers web services as categorical data and each service is described by a set of values extracted from the content and structure of its description file and as quality measure of clustering is defined the mutual information of the clusters and their values. We describe the way to represent web services as categorical data and how to cluster them by using LIMBO algorithm, minimizing at the same time the information loss in features values. In experimental evaluation, our approach outperforms in terms of F-Measure the approaches which use alternative similarity measures and methods for clustering WSDL documents.",2375-0197,978-1-4799-2972-6,10.1109/ICTAI.2013.107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735318,Web services repositories;Clustering WSDL documents;Knowledge extraction,Web services;Feature extraction;Clustering algorithms;Random variables;Vectors;Mutual information;Ports (Computers),document handling;knowledge acquisition;pattern clustering;search engines;service-oriented architecture;specification languages;Web services,knowledge extraction;Web services repositories;service oriented systems;Web services clustering;Web service description language documents;WSDL document clustering;Web service search engines;cluster mutual information;categorical data;LIMBO algorithm;F-measure,,,,17,,10-Feb-14,,,IEEE,IEEE Conferences
Models for decryption of historical shorthand documents,詞圖像索引聚類方法的比較,A. A. Rogov; M. B. Gippiev; I. A. Shterkel,"Petrozavodsk State University, 33 pr. Lenina, Petrozavodsk, Karelia, 185910, Russia; Petrozavodsk State University, 33 pr. Lenina, Petrozavodsk, Karelia, 185910, Russia; Petrozavodsk State University, 33 pr. Lenina, Petrozavodsk, Karelia, 185910, Russia","2015 International Conference Stability and Control Processes"" in Memory of V.I. Zubov (SCP)""",3-Dec-15,2015,,,665,667,"This article presents methods that are used for historical shorthand documents recognition. We distinguish following tasks: binarization, clusterization, lines recognition and determination of symbols types (main, superscript, subscript). Each method is evaluated in terms of recall, precision and F-measure criteria. The best method for binarization of shorthand documents appeared to be the modified threshold method. We proposed following methods for graphic symbols clustering: the method of segments lengths comparison, the method of projections comparison and the method of baskets. The best result is achieved with the method of baskets. We also present the algorithms of lines recognition and symbols classification. Lines recognition is performed using two methods: nearest neighbour and relations graph construction. Symbols classification is done by single and by double approximation methods and their modification. The best result of lines segmentation is demonstrated by the method of relations graph construction, and the best result of determination of symbols types is demonstrated by the modified double approximation method.",,978-1-4673-7698-3,10.1109/SCP.2015.7342239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342239,,Approximation methods;Approximation algorithms;Image segmentation;Classification algorithms;Clustering methods;Shape,cryptography;document image processing;graph theory;image recognition;image segmentation;pattern classification,decryption models;historical shorthand documents;shorthand documents recognition;F-measure criteria;shorthand documents binarization;modified threshold method;graphic symbols clustering;lines recognition;symbols classification;relations graph construction;lines segmentation;relations graph construction;modified double approximation method,,1,,8,,3-Dec-15,,,IEEE,IEEE Conferences
A Recommendation Algorithm Based On Fuzzy Clustering,一種新穎的混合算法，用於對照明不良的文檔圖像進行二值化,H. Zhan; W. Zhou; X. Hu; Q. Cai; T. Zhang; L. Yang,"South China Normal University, Guangdong, Guangzhou, 510006, China; South China Normal University, Guangdong, Guangzhou, 510006, China; South China Normal University, Guangdong, Guangzhou, 510006, China; South China Normal University, Guangdong, Guangzhou, 510006, China; Guangdong Con-com Technology Co. LTD, Guangdong, Guangzhou, 510006, China; Guangdong Con-com Technology Co. LTD, Guangdong, Guangzhou, 510006, China",2018 International Conference on Machine Learning and Cybernetics (ICMLC),11-Nov-18,2018,1,,230,233,"A recommendation algorithm based on fuzzy clustering is proposed in this paper. The idea of this system is as follows: Firstly, based on examples of hotel users' reviews and movie reviews collected by website crawler, the high-frequency valueless words and the low-frequency unimportant words are removed by means of term frequency-inverse document frequency (TF-IDF) algorithm of textual word segmentation. After then, term vector representation is used for preparation of subsequent related algorithms. Finally, fuzzy clustering is conducted to the review word segmentation dataset. Based on the clustering dataset mentioned above, the terms, sentences and users are separately clustered. The input of recommendation in each stage is used as the output of next recommendation to finally form an organic whole. Fuzzy clustering method adopted by the system greatly reduces the size of the dataset in the iterative process of the recommended algorithm and improves the precision of the recommendation.",2160-1348,978-1-5386-5214-5,10.1109/ICMLC.2018.8527026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527026,Fuzzy Clustering;Term Frequency;Term Vector;TF-IDF;Word Segmentation;word2vec,Clustering algorithms;Information filters;Recommender systems;Collaboration;Feature extraction,fuzzy set theory;iterative methods;pattern clustering;recommender systems;text analysis;word processing,recommendation algorithm;term frequency-inverse document frequency algorithm;textual word segmentation;term vector representation;review word segmentation dataset;fuzzy clustering method;iterative process,,,,9,,11-Nov-18,,,IEEE,IEEE Conferences
Multi-objective clustering ensemble for high-dimensional data based on Strength Pareto Evolutionary Algorithm (SPEA-II),阿拉伯文檔聚類的詞乾和相似性度量,A. Wahid; X. Gao; P. Andreae,"Victoria University of Wellington, New Zealand; Victoria University of Wellington, New Zealand; Victoria University of Wellington, New Zealand",2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),7-Dec-15,2015,,,1,9,"Clustering is one of the fundamental data analysis techniques, which aims to find distinct groups of similar objects and discovers hidden structures in data. A recent clustering approach, clustering ensembles tries to derive an improved clustering solution based on previously generated different candidate clustering solutions. Clustering ensembles have two steps: generating multiple candidate clustering solutions from the data and forming a final clustering solution from previously generated candidate clustering solutions. A problem of the first step is the text representation, where word frequencies are often used as features. Other semantic information of the text such as topics, hypertext, etc are ignored. The problem for the second step is that the current popular median partition approach selects one clustering solution from previously generated candidate clustering solutions. A common clustering ensemble approach uses word frequencies as features to represent text data (documents). However, documents usually contain semantically rich information i.e. words, hypertext, titles, topics etc. The cluster ensemble approach ignores the semantic information of the documents and hence is prone to produce futile groupings of the documents. In this research work, we present a new multi-objective clustering ensemble method based on Strength Pareto Evolutionary Algorithm (SPEA-II). Our method utilizes the semantic information (rich features) to address the first problem of clustering ensembles. The cluster oriented evolutionary approach which derives the final clustering solution by selecting better quality clusters is in the second step of our method to address the second problem. The results show that our new method provides better results than other clustering ensemble methods.",,978-1-4673-8272-4,10.1109/DSAA.2015.7344795,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344795,Clustering Ensemble;Multi-Objective Optimization;Evolutionary Algorithm,Evolutionary computation;Clustering methods;Linear programming;Optimization;Semantics;Clustering algorithms;Sociology,data analysis;data structures;evolutionary computation;hypermedia;Pareto optimisation;pattern clustering,high-dimensional data;strength Pareto evolutionary algorithm;SPEA-II;data analysis technique;hidden data structure;clustering approach;candidate clustering solution;text representation;word frequency;semantic information;hypertext;median partition approach;clustering ensemble approach;feature representation;text data;multiobjective clustering ensemble method;cluster oriented evolutionary approach;quality cluster,,3,,26,,7-Dec-15,,,IEEE,IEEE Conferences
A Clustering Algorithm Using Twitter User Biography,基於多視點的相似性度量聚類,M. Kohana; S. Okamoto; M. Kaneko,"Dept. of Comput. & Inf. Sci., Seikei Univ., Musashino, Japan; Dept. of Comput. & Inf. Sci., Seikei Univ., Musashino, Japan; Dept. of Comput. & Inf. Sci., Seikei Univ., Musashino, Japan",2013 16th International Conference on Network-Based Information Systems,19-Dec-13,2013,,,432,435,"Our previous work proposed a clustering algorithm to cluster research documents automatically. It used Web hit counts of AND-search on two words as a document vector. Target documents are clustered with a result of k-means clustering method, in which cosine similarity is used to calculate a distance. This paper uses this algorithm to cluster twitter users. However, the twitter users have different characteristics from the research documents. Therefore, we investigate problems of the using our algorithm for twitter users and propose some ideas to resolve it.",2157-0426,978-1-4799-2510-0,10.1109/NBiS.2013.70,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685438,twitter;clustering;web search,Clustering algorithms;Vectors;Twitter;Web search;Clustering methods;Google;Internet,biographies;pattern clustering;social networking (online);word processing,Twitter user biography;k-means clustering method;cosine similarity;Twitter user clustering algorithm;word extraction,,1,,15,,19-Dec-13,,,IEEE,IEEE Conferences
Association Link Network Based Core Events Discovery on the Web,基於層次聚類的超級對等語義網絡設計,Y. Liu; N. Borhan; X. Luo; H. Zhang; X. He,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Fac. of Eng. & IT, Univ. of Technol. Sydney, Sydney, NSW, Australia; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Instn. of Public Safety Res., Tsinghua Univ., Beijing, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China",2013 IEEE 16th International Conference on Computational Science and Engineering,6-Mar-14,2013,,,553,560,"As documents are explosively increasing in the era of big data, document clustering has been proven to be useful for organizing online document streams into events. However, extant studies on document clustering still suffer from the problems of high dimensionality, scalability and accuracy. In this paper, we will present a novel association link network (ALN) based document clustering method, which is an adaptive iteration splitting process to discover core events on the web. In the iteration, we first detect community structures from ALN, then, map documents to the associated community based on words relations in ALN, finally rebuild communities using the mapped documents. Compared to existing document clustering methods, the effectiveness of presented clustering method in automatically discovering the web events is proved by the experimental results on real data set.",,978-0-7695-5096-1,10.1109/CSE.2013.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755268,event discovery;document clustering;adaptive splitting;association link network;community detection,Communities;Semantics;Clustering methods;Clustering algorithms;Vectors;Detection algorithms;Accuracy,Big Data;document handling;Internet,association link network;core events discovery;World Wide Web;big data;document clustering;online document streams;adaptive iteration splitting process;map documents;mapped documents;Web events,,,,21,,6-Mar-14,,,IEEE,IEEE Conferences
Random Centroid Selection for K-means Clustering: A Proposed Algorithm for Improving Clustering Results,半監督異構融合，用於多媒體數據共聚,A. Sen; M. Pandey; K. Chakravarty,"KIIT Deemed-to-be University,School of Computer Engineering,Bhubaneswar,India; KIIT Deemed-to-be University,School of Computer Engineering,Bhubaneswar,India; KIIT Deemed-to-be University,School of Computer Engineering,Bhubaneswar,India","2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)",3-Jul-20,2020,,,1,4,"Primary motivation towards this study was to obtain better clustering results from K-means clustering algorithm. Several studies had provided relevant findings showing normal clustering algorithm and scope of improvements regarding clustering accuracy. With the aim to increase clustering accuracy, a conceptual notion of Genetic Algorithm is utilized in K-means clustering algorithm. Depending on the Genetic algorithm concepts, an improved clustering technique is proposed in this study for obtaining more accurate and more precise clustering outcomes. The contribution from this study could be essential in terms of topic-modelled data and clustering text documents.",,978-1-7281-5830-3,10.1109/ICCSEA49143.2020.9132921,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132921,K-means clustering;Genetic Algorithm;Text mining;Topic modelling,,data mining;feature selection;genetic algorithms;pattern clustering;text analysis,genetic algorithm;random centroid selection;text document clustering;topic modelled data;k-means clustering;text mining;text analytics,,,,10,,3-Jul-20,,,IEEE,IEEE Conferences
Near real-time thematic clustering of web documents and other internet contents,基於啟發式關鍵詞提取和文檔聚類的信息檢索,A. Pusztay; J. Sz羹ley; S. Laki,"Department of Physics of Complex Systems, E繹tv繹s Lor獺nd University, Budapest, Hungary; Department of Physics of Complex Systems, E繹tv繹s Lor獺nd University, Budapest, Hungary; Inter-University Centre for Telecommunications and Informatics, Debrecen, Hungary",2013 IEEE 4th International Conference on Cognitive Infocommunications (CogInfoCom),23-Jan-14,2013,,,307,312,"In the past decade, Internet has radically changed our lives, enabling us to obtain information on everything (disasters, political decisions, ordinary events, etc.) we are interested in in almost real-time. Downloading a web page by a browser, instant messaging or file sharing generate huge amount of network traffic that carry valuable information on the most relevant topics that raise interest in individual users, user groups or the entire society. However, the analysis of this huge amount of unstructured textual data poses many challenges, especially if it is not possible to store the data off-line and real-time clustering is needed. In this paper, we propose a framework for real-time textual content clustering of different sources called documents over the Internet, including posts on Twitter and Facebook, blogs, web sites or other textual contents. To support real-time processing, we extend the spherical on-line K-means clustering algorithm with heuristic improvements: an adaptive dimension reduction technique is introduced to keep the dimension of the document space on a reasonable level, and the ability to open new and remove old clusters according to the actual demand is also added. The performance of our improved algorithm called ASKM (Adaptive Streaming K-Means) has been analyzed on a ground truth data set based on the catalog of Open Directory Project. Furthermore, we also consider a much more realistic scenario where only some parts of the Internet documents are available because of practical limitations of traffic capturing, resulting incomplete textual documents to be clustered. We also show that the proposed method can achieve reasonable good accuracy even in this practical case.",,978-1-4799-1546-0,10.1109/CogInfoCom.2013.6719262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719262,,Blogs;Irrigation;Vectors;Encyclopedias;Electronic publishing;Internet,content management;Internet;pattern clustering;social networking (online);text analysis,near real-time thematic clustering;Web documents;Internet contents;Web page downloading;browser;instant messaging;file sharing;network traffic;unstructured textual data;off-line clustering;real-time textual content clustering;Twitter;Facebook;blogs;Web sites;real-time processing;spherical on-line K-means clustering algorithm;adaptive dimension reduction technique;document space dimension;ASKM;adaptive streaming K-means algorithm;Open Directory Project;Internet document;traffic capturing,,2,,14,,23-Jan-14,,,IEEE,IEEE Conferences
Text Classification and Document Layout Analysis of Paper Fragments,基於模糊編碼策略的降級中文文檔檢索,M. Diem; F. Kleber; R. Sablatnig,"Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,854,858,"In general document image analysis methods are pre-processing steps for Optical Character Recognition (OCR) systems. In contrast, the proposed method aims at clustering document snippets, so that an automated clustering of documents can be performed. Therefore, words are classified according to printed text, manuscripts, and noise. Where, the third class corrects falsely segmented background elements. Having classified text elements, a layout analysis is carried out which groups words into text lines and paragraphs. A back propagation of the class weights - assigned to each word in the first step - enables correcting wrong class labels. The proposed method shows promising results on a dataset consisting of document snippets with varying shapes, content writing and layout. In addition, the system is compared to page segmentation methods of the ICDAR 2009 Page Segmentation Competition.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.175,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065432,local features;text classification;layout analysis,Layout;Feature extraction;Noise;Text analysis;Image segmentation;Optical character recognition software;Noise measurement,document image processing;image classification;image segmentation;optical character recognition;pattern clustering;text analysis,document image analysis methods;optical character recognition system;document snippet clustering;printed text classification;manuscripts;back propagation;content writing;document layout analysis;paper fragment,,8,,14,,3-Nov-11,,,IEEE,IEEE Conferences
Density based clustering for Cricket World Cup tweets using Cosine similarity and time parameter,使用多目標遺傳算法的Web文檔多視圖聚類,N. Pandey,"LJ Institute of Engineering and Technology, PG CE Department, Ahmedabad, India",2015 Annual IEEE India Conference (INDICON),31-Mar-16,2015,,,1,6,"The rapid spread of location-based devices and cheap storage mechanisms, as well as fast development of Internet technology, allowed collection and distribution of huge amounts of user-generated data. These user generated data sometimes are known as georeferenced documents, they have their location information and time of posting embedded with them. These parameters help to retrieve the location information and the time of posting. We need to retrieve the topic from those geo-referenced documents and determine the local topics and events for a particular region. All these clusters are geospatial in arbitrary shape hence density based clustering is the most appropriate clustering algorithm. Here we used tweets from Twitter, while the DBSCAN method is used for generating clusters. Here for finding similarity between tweets cosine similarity is used, but because of its low value we increase its value by adding weight to it by matching the keywords in tweets. Also another parameter of time is used for separating clusters temporally. Results have shown that weighted keyword based method gives more specific clusters than DBSCAN method, while using the time parameter in it we get clusters time separated. Hence for purpose of information retrieval or building marketing strategy by tweets we can use this method.",2325-9418,978-1-4673-7399-9,10.1109/INDICON.2015.7443520,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443520,Density based clustering;geo-referenced documents;spatio-temporal clusters;spatial clusters;topic retrieval;location detection,Clustering algorithms;Internet;Shape;Twitter;Clustering methods;Buildings;Mobile handsets,information retrieval;Internet;mobile computing;pattern clustering;pattern matching;social networking (online);sport,density-based clustering;DBSCAN method;Cricket World Cup tweet;cosine similarity;time parameter;location-based device;Internet technology;Twitter;keyword matching;information retrieval,,1,,12,,31-Mar-16,,,IEEE,IEEE Conferences
Sparse Poisson coding for high dimensional document clustering,健壯的圖像文檔身份驗證代碼，可在雲環境中自動生成生物特徵密鑰，進行選擇和更新,C. Wu; H. Yang; J. Zhu; J. Zhang; I. King; M. R. Lyu,"College of Computer Science, Zhejiang University, Hangzhou, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China",2013 IEEE International Conference on Big Data,23-Dec-13,2013,,,512,517,"Document clustering plays an important role in large scale textual data analysis, which generally faces with great challenge of the high dimensional textual data. One remedy is to learn the high-level sparse representation by the sparse coding techniques. In contrast to traditional Gaussian noise-based sparse coding methods, in this paper, we employ a Poisson distribution model to represent the word-count frequency feature of a text for sparse coding. Moreover, a novel sparse-constrained Poisson regression algorithm is proposed to solve the induced optimization problem. Different from previous Poisson regression with the family of ??-regularization to enhance the sparse solution, we introduce a sparsity ratio measure which make use of both ??-norm and ??-norm on the learned weight. An important advantage of the sparsity ratio is that it bounded in the range of 0 and 1. This makes it easy to set for practical applications. To further make the algorithm trackable for the high dimensional textual data, a projected gradient descent algorithm is proposed to solve the regression problem. Extensive experiments have been conducted to show that our proposed approach can achieve effective representation for document clustering compared with state-of-the-art regression methods.",,978-1-4799-1293-3,10.1109/BigData.2013.6691615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691615,document clustering;sparse coding;Poisson regression,Encoding;Vectors;Clustering algorithms;Algorithm design and analysis;Data models;Optimization;Measurement,data analysis;gradient methods;learning (artificial intelligence);optimisation;pattern clustering;Poisson distribution;regression analysis;text analysis,sparse Poisson coding;high dimensional document clustering;large scale textual data analysis;high-level sparse representation learning;sparse coding technique;Gaussian noise-based sparse coding method;Poisson distribution model;word-count frequency feature;sparse-constrained Poisson regression algorithm;optimization problem;??-regularization;sparsity ratio measure;??-norm;??-norm;gradient descent algorithm;regression problem,,4,,26,,23-Dec-13,,,IEEE,IEEE Conferences
A new distributed name disambiguation system based on MapReduce,基於本體的模糊函數和模糊關係自適應文件分類系統,Liu Pengfei; Ge sheng,"School of Computer Science and Engineering, Beihang University, Beijing 100191, China; School of Computer Science and Engineering, Beihang University, Beijing 100191, China",2012 IEEE 14th International Conference on Communication Technology,2-May-13,2012,,,550,554,"Social network search is a kind of vertical search based on the information polymerization of social network data. And name disambiguation is one of the vital issues in social network search. With the explosive growth of the information, effectively deal with name disambiguation in massive data scenario becomes an important issue. To tackle this issue, we combine the MapReduce model and document clustering algorithm to propose a distributed method for name disambiguation. And then present a distributed name disambiguation system. This system runs on the Hadoop platform, and makes the name disambiguation parallelized by dividing the document clustering task to a couple of maps and reduces. In addition, we evaluated our system in terms of expansibility and accuracy.",,978-1-4673-2101-3,10.1109/ICCT.2012.6511416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511416,name disambiguation;vector space model;document cluster;MapReduce,,data mining;document handling;information retrieval;parallel processing;pattern clustering;social networking (online),distributed name disambiguation system;information polymerization;social network data;MapReduce model;document clustering algorithm;Hadoop platform,,,,11,,2-May-13,,,IEEE,IEEE Conferences
Design and Implementation of Chinese Text Clustering System,使用部分神經網絡按子空間的投影維數對文本文檔進行聚類,Y. Tan; L. Huang; H. Qi; Y. Zhai,"Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China","2009 Fifth International Joint Conference on INC, IMS and IDC",13-Nov-09,2009,,,1136,1140,"Clustering technology is the core technology of text mining. Through text clustering, a large number of text messages can be divided into several meaningful classes or clusters. According to the features of Chinese documents, this paper designs and implements the Chinese Text Clustering System to perform automatic clustering of Chinese documents. Firstly, this system will carry out Chinese word automatic segmentation for the input Chinese document sets by using reverse maximum matching method. Secondly, further text preprocessing is performed. Finally the K-means clustering algorithm is used to obtain the clustering results. The prototype system can also be used in clustering Chinese Web pages to search for user's interest model by search engines, which will improve the efficiency of searching the target content.",,978-1-4244-5209-5,10.1109/NCM.2009.234,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331328,text mining;Chinese text clustering;Chinese word segmentation;reverse maximum matching;K-means algorithm,Text mining;Clustering algorithms;Web pages;Data mining;Search engines;Particle separators;Educational institutions;Computer science;Electronic mail;Prototypes,data mining;Internet;pattern clustering;search engines;text analysis,Chinese text clustering system;text mining;Chinese word automatic segmentation;text preprocessing;K-means clustering algorithm;Chinese Web pages clustering;search engines;reverse maximum matching,,2,,10,,13-Nov-09,,,IEEE,IEEE Conferences
TDP-Shell: A Generic Framework to Improve Interoperability between Batch Queue Systems and Monitoring Tools,重複檢測符號壓縮的文檔,V. J. Ivars; M. A. Senar; E. Heymann,"Comput. Archit. & Oper. Syst. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Archit. & Oper. Syst. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Archit. & Oper. Syst. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain",2011 IEEE International Conference on Cluster Computing,27-Oct-11,2011,,,522,526,"Nowadays distributed applications, including MPI implementations, are executed on computer clusters managed by a batch queue system. Users take advantage of monitoring tools to detect run-time problems on their applications running on those environments. But it is a challenge to use monitoring tools on a cluster controlled by a batch queue system. This is due to the fact that batch queue systems and monitoring tools do not coordinate the management of the resources they share, when executing a distributed application. We name this problem lack of interoperability and to solve it we have developed a framework called TDP-Shell. This framework supports different batch queue systems such as Condor and SGE, and different monitoring tools such as Paradyn, Gdb and Total view, without any changes on their source code. In this paper we describe how our basic design of TDP-Shell for sequential applications was re-designed to support the monitoring of MPI applications that are executed on a cluster controlled by a batch queue system.",2168-9253,978-0-7695-4516-5,10.1109/CLUSTER.2011.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061200,batch queue systems;monitoring tools;interoperability,Monitoring;Computer architecture;Manuals;Protocols;Portable document format;Libraries;Resource management,message passing;open systems;queueing theory;resource allocation;system monitoring,interoperability;batch queue systems;monitoring tools;distributed application;computer clusters;run-time problem detection;resource management;resource sharing;TDP-Shell;Condor;SGE;Paradyn;Gdb;Total view;sequential applications;MPI application monitoring,,,,16,,27-Oct-11,,,IEEE,IEEE Conferences
A document warehouse: a multimedia database approach,英語聖訓翻譯中聚類文本的混合歸約維數,H. Ishikawa; K. Kubota; Y. Noguchi; K. Kato; M. Ono; N. Yoshizawa; A. Kanaya,"Software Lab., Fujitsu Labs. Ltd., Kawasaki, Japan; NA; NA; NA; NA; NA; NA",Proceedings Ninth International Workshop on Database and Expert Systems Applications (Cat. No.98EX130),6-Aug-02,1998,,,90,94,"Nowadays, structured data such as sales and business forms are stored in data warehouses for decision makers to use. Further, unstructured data such as emails, html texts, images, videos, and office documents are increasingly accumulated in personal computer storage due to spread of mailing, WWW, and word processing. Such unstructured data, or what we call multimedia documents, are larger in volume than structured data and precious as corporate assets as well. So we need a document warehouse as a software framework where multimedia documents are analyzed and managed for corporate wide information sharing and reuse like a data warehouse for structured data. We describe a prototype document warehouse system, which supports management of simple and compound documents, keyword based and content based retrieval, rule based classification, SOM based clustering, and business rules.",,0-8186-8353-8,10.1109/DEXA.1998.707385,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707385,,Multimedia databases;Data warehouses;Image storage;Marketing and sales;HTML;Videos;Microcomputers;World Wide Web;Text processing;Information analysis,multimedia computing;very large databases;document handling;information retrieval,document warehouse;multimedia database approach;structured data;sales;business forms;data warehouses;decision makers;unstructured data;office documents;personal computer storage;word processing;multimedia documents;corporate assets;software framework;corporate wide information sharing;data warehouse;prototype document warehouse system;compound documents;rule based classification;SOM based clustering;business rules,,3,,6,,6-Aug-02,,,IEEE,IEEE Conferences
Chi-Sim: A New Similarity Measure for the Co-clustering Task,關聯度和啟發式函數的緊湊性與分佈式名詞的文本文檔聚類,G. Bisson; F. Hussain,"Lab. TIMC-IMAG, Univ. de Grenoble, La Tronche; Lab. TIMC-IMAG, Univ. de Grenoble, La Tronche",2008 Seventh International Conference on Machine Learning and Applications,22-Dec-08,2008,,,211,217,"Co-clustering has been widely studied in recent years. Exploiting the duality between objects and features efficiently helps in better clustering both objects and features. In contrast with current co-clustering algorithms that focus on directly finding some patterns in the data matrix, in this paper we define a (co-)similarity measure, named X-Sim, which iteratively computes the similarity between objects and their features. Thus, it becomes possible to use any clustering methods (k-means, ...) to co-cluster data. The experiments show that our algorithm not only outperforms the classical similarity measure but also outperforms some co-clustering algorithms on the document-clustering task.",,978-0-7695-3495-4,10.1109/ICMLA.2008.103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724977,Co-clustering;co-similarity;text mining,Clustering algorithms;Iterative algorithms;Clustering methods;Sparse matrices;Machine learning;Current measurement;Organizing;Spatial databases;Bioinformatics;Gene expression,document handling;iterative methods;matrix algebra;pattern classification;pattern clustering,Chi-Sim similarity measure;co-clustering task;object clustering;data matrix;iterative computation;document classification,,22,,19,,22-Dec-08,,,IEEE,IEEE Conferences
Dengue fever prediction using K-means clustering algorithm,具有多視圖信息瓶頸的Web文檔聚類,P. Manivannan; P. I. Devi,"Department of Computer Science, Ayya Nadar Janaki Ammal College, Sivakasi, Tamil Nadu, India; Department of Computer Applications, Ayya Nadar Janaki Ammal College, Sivakasi, Tamil Nadu, India","2017 IEEE International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",1-Mar-18,2017,,,1,5,"Dengue fever is a virus infection which is transmitted to humans by mosquitoes that living in tropical and subtropical climates and carries the virus. The dengue viruses occur in 4 serotypes (DENV-1 to DENV-4). A dengue disease ranges from mild febrile disease to severe hemorrhagic fever. Predicting the relationship between the dengue serotypes will surely help the biotechnologists and bioinformaticians to move one step forward to discover antibiotic for dengue. This paper has been focused four stages namely preprocessing, attribute selection, clustering and predicting the dengue fever. R 3.3.2 Tool is used for preprocessing the household of dengue dataset. D win's method has been applied to generate filled dataset by substituting all missing values for nominal and numeric attributes with mode and mean value. Dengue virus can be predicted by applying different data mining techniques. The main goal of research work is to predict the people who are affected by dengue depending upon categorization of age group using K-means clustering algorithm has been implemented.",,978-1-5090-4778-9,10.1109/ITCOSP.2017.8303126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8303126,Dengue;Data Mining;Medical Documents;Clustering techniques;K-means algorithm,Signal processing algorithms;Clustering algorithms;Data mining;Diseases;Classification algorithms;Prediction algorithms;Optimization,data mining;diseases;feature selection;medical computing;microorganisms;patient diagnosis;pattern clustering;prediction theory,dengue fever prediction;virus infection;dengue virus;dengue disease;mild febrile disease;severe hemorrhagic fever;dengue serotypes;dengue dataset;K-means clustering algorithm;R 3.3.2 Tool;attribute selection;data mining techniques,,2,,10,,1-Mar-18,,,IEEE,IEEE Conferences
Identifying bursty areas of emergency topics in geotagged tweets using density-based spatiotemporal clustering algorithm,基於圖的文檔聚類新算法,T. Sakai; K. Tamura,"Graduate School of Information Sciences, Hiroshima City University, Japan; Graduate School of Information Sciences, Hiroshima City University, Japan",2014 IEEE 7th International Workshop on Computational Intelligence and Applications (IWCIA),18-Dec-14,2014,,,95,100,"With the increasing popularity of social media, data posted on social media sites are rapidly becoming collective intelligence, which is a term used to refer to new media that is displacing traditional media. In this paper, we focus on geotagged tweets on the Twitter site; such tweets are referred to as georeferenced documents because they include not only a short text message, but also the documents' posting time and location. Geotagged tweets can be used to identify emergency topics such as natural disasters, weather, diseases and other incidents. Therefore, the utilization of geotagged tweets to observe and analyze emergency topics has received much attention recently. In this paper, we propose a new framework for identifying bursty areas of emergency topics using the (庰, ? )-density-based spatiotemporal clustering algorithm. The aim of this study is to develop a new spatiotemporal clustering technique that can extract bursty areas of observed emergency topics such as, natural disasters, weather, and diseases using geotagged tweets. To evaluate the proposed framework, actual crawling geotagged tweets posted on the Twitter site were used. The proposed method could successfully detect bursty areas of an observed an emergency topic that is related to weather in Japan.",1883-3977,978-1-4799-4770-6,10.1109/IWCIA.2014.6988085,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6988085,Spatiotemporal clustering;Emergency topic detection;Density-based clustering;Naive Bayes classifier;Burst detection,Spatiotemporal phenomena;Clustering algorithms;Media;Rain;Twitter;Real-time systems,document handling;emergency management;geographic information systems;pattern clustering;social networking (online);spatiotemporal phenomena,bursty areas;emergency topics;geotagged tweets;density-based spatiotemporal clustering algorithm;social media sites;Twitter;short text message;documents posting time;documents posting location;Japan,,4,,13,,18-Dec-14,,,IEEE,IEEE Conferences
Semantic Models for Style-Based Text Clustering,Web圖像聚類,A. Leoncini; F. Sangiacomo; C. Peretti; S. Argentesi; R. Zunino; E. Cambria,"Dept. of Biophys. & Electron. Eng., Univ. of Genoa, Genoa, Italy; Dept. of Biophys. & Electron. Eng., Univ. of Genoa, Genoa, Italy; Dept. of Biophys. & Electron. Eng., Univ. of Genoa, Genoa, Italy; Dept. of Biophys. & Electron. Eng., Univ. of Genoa, Genoa, Italy; Dept. of Biophys. & Electron. Eng., Univ. of Genoa, Genoa, Italy; Dept. of Comput. Sci. & Math., Univ. of Stirling, Stirling, UK",2011 IEEE Fifth International Conference on Semantic Computing,27-Oct-11,2011,,,75,82,"The paper addresses some roles of concept-based representations in document clustering to support knowledge discovery. Computational Intelligence algorithms can benefit from semantic networks in the definition of similarity between pairs of documents. After analyzing the tuning of semantic networks in a systematic fashion, the research defines and evaluates a novel semantic-based metrics, which integrates both classical and style-related features of texts. Experimental results confirm the effectiveness of the approach, showing that applying a refined semantic representation into a clustering engine yields consistent structures for information retrieval and knowledge acquisition.",,978-1-4577-1648-5,10.1109/ICSC.2011.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061439,Semantic Text Clustering;Multilingual Document Clustering;Text Mining;WordNet;Kernel K-Means,Semantics;Measurement;Clustering algorithms;Text mining;Computational modeling;Ontologies;Engines,data mining;information retrieval;pattern clustering;semantic networks;text analysis,semantic models;style-based text clustering;concept-based representations;document clustering;knowledge discovery;computational intelligence algorithms;semantic networks;semantic-based metrics;refined semantic representation;clustering engine;information retrieval;knowledge acquisition,,2,,31,,27-Oct-11,,,IEEE,IEEE Conferences
Multi-document summarization for Turkish news,聚類超鏈接以進行主題提取：探索性分析,F. Demirci; E. Karabudak; B. 襤lgen,"Department of Computer Engineering Istanbul K羹lt羹r University Turkey, Istanbul; Department of Computer Engineering Istanbul K羹lt羹r University Turkey, Istanbul; Department of Computer Engineering Istanbul K羹lt羹r University Turkey, Istanbul",2017 International Artificial Intelligence and Data Processing Symposium (IDAP),2-Nov-17,2017,,,1,5,"In this paper, we introduce our multi-document summarization system for Turkish news. The aim of the summarization system is to build a single document for multi document news that have been collected previously. The news were collected from several Turkish news sources via Real Simple Syndication (RSS). They were separated into clusters according to their topics. We utilized cosine similarity metric for the clustering process. Latent Semantic Analysis (LSA) has been used in the summarization phase. Multi-Document Summarization (MDS) differs from single document summarization in that the issues of compression, speed, redundancy and passage selection are essential inside the formation of ideal summaries. In this study, we utilized term frequency in document scoring which let us select the sentences with higher importance degree. We use ROUGE technique for evaluation of the system and our results show that the average of recall and precision percentage of this system is 43%. In the manual summarization phase, fifteen volunteers took part. The reason of low percentage is interpreted as getting texts randomly without any edit. It has been observed that the number of sentences and rate of summarization affect the accuracy rate.",,978-1-5386-1880-6,10.1109/IDAP.2017.8090189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090189,RSS;Multi-Document Summarization;Cosine Similarity;LSA;ROUGE;SVD,Matrix decomposition;Semantics;Statistical analysis;Web sites;Dictionaries;XML;Measurement,feature selection;information resources;information retrieval;pattern clustering;text analysis,multidocument summarization system;Turkish news sources;redundancy;passage selection;document scoring;real simple syndication;cosine similarity metric;topic clustering;latent semantic analysis;ROUGE technique,,,,11,,2-Nov-17,,,IEEE,IEEE Conferences
Enhancing GSOM text clustering with Latent Semantic Analysis,鏈接文檔的無監督學習,S. Matharage; D. Alahakoon,"Clayton School of Information Technology, Faculty of Information Technology, Monash University, Clayton VIC 3800, Australia; Clayton School of Information Technology, Faculty of Information Technology, Monash University, Clayton VIC 3800, Australia",2010 Fifth International Conference on Information and Automation for Sustainability,17-Feb-11,2010,,,441,446,"Growing Self Organizing Map (GSOM) has proven benefits in text clustering. Latent Semantic Analysis (LSA) also has been used in text clustering to capture the latent concepts from text. This paper presents a novel combination of GSOM and LSA to improve text clustering results compared to using GSOM on its own. LSA is an inherently global algorithm that looks at trends and patterns globally and GSOM is a nearest neighborhood based algorithm which looks at local patterns. Combination of these two can be used to discover both the global and local patterns. In the proposed model, initial text corpus is converted into its vector space representation using the traditional Term Frequency - Inverse Document Frequency (TF-IDF) technique. Then the Singular Value Decomposition (SVD) followed by Frobenius norm is applied on the resulting high dimensional vector to come up with a new vector with an optimal number of dimensions. Experiments using the proposed model were conducted and compared with the original GSOM under the same conditions. Experiment results demonstrate that the new combination of these well known techniques enhances the accuracy of clustering results and the computational time than the GSOM alone.",2151-1810,978-1-4244-8552-9,10.1109/ICIAFS.2010.5715702,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715702,Growing Self Organizing Map;text clustering;Latent Semantic Analysis,Neurons;Clustering algorithms;Semantics;Matrix decomposition;Organizing;Artificial neural networks;Accuracy,data mining;pattern clustering;self-organising feature maps;singular value decomposition;text analysis,GSOM text clustering;latent semantic analysis;growing self organizing map;text corpus;vector space;term frequency inverse document frequency;singular value decomposition;Frobenius norm;text mining,,2,,16,,17-Feb-11,,,IEEE,IEEE Conferences
Fuzzy co-clustering of Web documents,在時間軸上聚類和映射有關暴力事件的相關新聞,W. -. Tjhi; Lihui Chen,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore",2005 International Conference on Cyberworlds (CW'05),6-Feb-06,2005,,,7 pp.,551,"The Web is the largest information repository in the history of mankind. Due to its huge size however, finding relevant information without any appropriate tool can be virtually impossible. Web document clustering is one possible technique to improve the efficiency in information finding process. In this paper, we are looking into fuzzy co-clustering, which is known to be robust for clustering standard text documents. In our opinion, its robustness can also be extended to Web documents because it can generate descriptive clusters in high dimension and it is able to discover data clusters with overlaps. We consider two existing fuzzy co-clustering algorithms, FCCM and fuzzy Codok. In addition, we propose a new algorithm, FCC-STF, as an alternative to the existing ones. Empirical study of these algorithms on benchmark datasets is presented, together with the performance comparison with a standard fuzzy clustering algorithm HFCM. The results show that fuzzy co-clustering is generally superior to standard fuzzy clustering in the Web environment, making it a technique with great potential to assist Internet user in discovering relevant information effectively",,0-7695-2378-1,10.1109/CW.2005.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587593,,Clustering algorithms;Robustness;History;Internet;Costs;Information retrieval;Databases,document handling;fuzzy set theory;Internet;pattern clustering,fuzzy coclustering with single term fuzzifier;Web document clustering;information finding process;fuzzy Codok;hierarchical fuzzy c-means;Internet,,5,,11,,6-Feb-06,,,IEEE,IEEE Conferences
Animated Geo-temporal Clusters for Exploratory Search in Event Data Document Collections,通過PLSA因素進行文本文檔潛在子空間聚類,P. Craig; N. R. Se簿ler; A. D. Olvera Cervantes,"Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, China; School of Computing, Edinburgh Napier University, Edinburgh, UK; Inst. de Fis. y Mat., Univ. Tecnol. de la Mixteca, Huajuapan de Le籀n, Mexico",2014 18th International Conference on Information Visualisation,22-Sep-14,2014,,,157,163,"This paper presents a novel visual analytics technique developed to support exploratory search tasks for event data document collections. The technique supports discovery and exploration by clustering results and overlaying cluster summaries onto coordinated timeline and map views. Users can also explore and interact with search results by selecting clusters to filter and re-cluster the data with animation used to smooth the transition between views. The technique demonstrates a number of advantages over alternative methods for displaying and exploring geo-referenced search results and spatio-temporal data. Firstly, cluster summaries can be presented in a manner that makes them easy to read and scan. Listing representative events from each cluster also helps the process of discovery by preserving the diversity of results. Also, clicking on visual representations of geo-temporal clusters provides a quick and intuitive way to navigate across space and time simultaneously. This removes the need to overload users with the display of too many event labels at any one time. The technique was evaluated with a group of nineteen users and compared with an equivalent text based exploratory search engine.",2375-0138,978-1-4799-4103-2,10.1109/IV.2014.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902897,information visualisation;visual analytics;human-computer information retrieval,History;Navigation;Encyclopedias;Electronic publishing;Internet;Data visualization,computer animation;data visualisation;document handling;document image processing;information retrieval;pattern clustering,animated geo-temporal clusters;event data document collections;visual analytics technique;coordinated timeline;map views;animation;geo-referenced search results;spatio-temporal data;equivalent text based exploratory search engine,,4,,56,,22-Sep-14,,,IEEE,IEEE Conferences
Bengali word embeddings and it's application in solving document classification problem,使用競爭性學習對Web文檔進行分類：個人自適應代理的組成部分,A. Ahmad; M. R. Amin,"Search Engine Pipilika, Department of Computer Science and Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh; Computer Science Department, Stony Brook University, NY 11790, USA",2016 19th International Conference on Computer and Information Technology (ICCIT),23-Feb-17,2016,,,425,430,"In this paper, we present Bengali word embeddings and it's application in the classification of news documents. Word embeddings are multi-dimensional vectors that can be created by exploiting the linguistic context of the words in large corpus. To generate the embeddings, we collected Bengali news document of last five years from the major daily newspapers. Word embeddings are generated using the Neural Network based language processing model Word2vec. We use the vector representations of the Bengali words to cluster them using K-means algorithm. We show that those clusters can be used directly to perform various natural language processing task by solving the problem of Bengali news document classification. We use the Support Vector Machine (SVM) for the classification task and achieve ~91% F1-score. The accuracy of our method demonstrates that our word embeddings could capture the semantics of word from the respective context correctly.",,978-1-5090-4090-2,10.1109/ICCITECHN.2016.7860236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860236,Bengali;Word Embedding;Word2vec;Document Classification;Word Cluster,Support vector machines;Context;Training;Clustering algorithms;Computational modeling;Neural networks;Classification algorithms,computational linguistics;document handling;information resources;natural language processing;neural nets;pattern classification;pattern clustering;support vector machines;vectors,Bengali word embeddings;news document classification problem;multidimensional vectors;Bengali word linguistic context;neural network based language processing model Word2vec;Bengali word vector representations;K-means clustering algorithm;natural language processing;support vector machine;word semantics,,11,,35,,23-Feb-17,,,IEEE,IEEE Conferences
An Unsupervised Approach to Cluster Web Search Results Based on Word Sense Communities,一種分割文檔圖像中標記區域的方法,J. Chen; O. R. Za簿ane; R. Goebel,"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,725,729,"Effectively organizing Web search results into clusters is important to facilitate quick user navigation to relevant documents. Previous methods may rely on a training process and do not provide a measure for whether page clustering is actually required. In this paper, we reformalize the clustering problem as a word sense discovery problem. Given a query and a list of result pages, our unsupervised method detects word sense communities in the extracted keyword network. The documents are assigned to several refined word sense communities to form clusters. We use the modularity score of the discovered keyword community structure to measure page clustering necessity. Experimental results verify our method's feasibility and effectiveness.",,978-0-7695-3496-1,10.1109/WIIAT.2008.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740536,Document Clustering;Community Mining;Word Sense Community,Web search;Search engines;Intelligent agent;Training data;Web pages;Clustering algorithms;Communities;Organizing;Navigation;Clustering methods,Internet;natural languages;pattern clustering;query processing;relevance feedback;search engines,unsupervised Web search result clustering;word sense community;Web search result organization;user navigation;relevant documents;page clustering;word sense discovery problem;keyword network;modularity score;query sense identification,,6,,17,,6-Jan-09,,,IEEE,IEEE Conferences
Automatic document image mosaicing algorithm with hand-held camera,ExtMiner：結合多種排名和聚類算法以進行結構化文檔檢索,M. Ligang; Y. Yongjuan,"College of computing & communication Engineering, The Graduated School of the Chinese Academy of Sciences, Beijing, 100049 China; Patent Examination Cooperation Center of SIPO, Beijing 100190 China",2011 2nd International Conference on Intelligent Control and Information Processing,1-Sep-11,2011,2,,1094,1097,"This paper presents an image mosaicing method for camera-captured document images, and it can be used to stitch multiple overlapping document images into a large high resolution image. First, we use the nearest-neighbor(NN) clustering technique in document skew rectification to locate the horizontal vanishing point of the text plane. Secondly we partition the image into multiple overlapping blocks centered with the centroid of each connected component(CC), and propose a run-length opening algorithm(RLOA) to compute the local orientation of vertical character stroke(VSB), which is used to locate the document's vertical vanishing point. Thirdly, a three-step hierarchical rectification method is proposed to rectify document images. Finally, it uses local alignment constraints of all the overlapping image pairs to construct global alignment model, thus, to eliminate the error accumulation effectively. This method is unique in not calibrating the internal and external camera parameters in advance and not restricting the camera position, and it can produce a high resolution and accurate full page mosaic from small image patches of a document.",,978-1-4577-0816-9,10.1109/ICICIP.2011.6008422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008422,,Estimation;Pattern recognition;Transforms;Digital cameras;Clustering algorithms;Robustness,calibration;cameras;document image processing;image enhancement;image resolution;image segmentation;pattern clustering;text analysis,automatic document image mosaicing algorithm;handheld camera;camera-captured document image;multiple overlapping document image stitching;high resolution image;nearest neighbor clustering technique;document skew rectification;horizontal vanishing point;text plane;run-length opening algorithm;vertical character stroke;three-step hierarchical rectification method;document image rectification;error accumulation elimination;camera parameter calibration,,6,,12,,1-Sep-11,,,IEEE,IEEE Conferences
New Cluster Detection Based on Multi-Representation Index Tree Text Clustering,基於SOM的中文關鍵詞聚類,H. Song; L. Wang; B. Li; X. Liu,"Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China",2010 2nd International Workshop on Database Technology and Applications,6-Dec-10,2010,,,1,4,"Traditional Clustering is a powerful technique for revealing the ""hot"" topics among documents. However, it's hard to discover the new type events coming out gradually. In this paper, we propose a novel model for detecting new clusters from time-streaming documents. It consists of three parts: the cluster definition based on Multi-Representation Index Tree (MI-Tree), the new cluster detecting process and the metrics for measuring a new cluster. Compared with the traditional method, we process the newly coming data first and merge the old clustering tree into the new one. This algorithm can avoid this effect: the documents enjoying high similarity were assigned to different clusters. We designed and implemented a system for practical application, the experimental results on a variety of domains demonstrate that our algorithm can recognize new valuable clusters during the iteration process, and produce quality clusters.",2167-194X,978-1-4244-6977-2,10.1109/DBTA.2010.5659018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659018,,Clustering algorithms;Indexes;Peer to peer computing;Accuracy;Merging;Measurement;Algorithm design and analysis,pattern clustering;text analysis;trees (mathematics),multirepresentation index tree;text clustering;iteration process;time streaming document,,,,8,,6-Dec-10,,,IEEE,IEEE Conferences
Text document clustering using self organizing map: Theses and dissertations of universitas Indonesia,使用集群技術改善XML語義搜索,Y. A. B. Panjaitan; I. Surjandari; A. Rosyidah,"Department of Industrial Engineering, Universitas Indonesia, Kampus Baru UI Depok, 16424, Indonesia; Department of Industrial Engineering, Universitas Indonesia, Kampus Baru UI Depok, 16424, Indonesia; Department of Industrial Engineering, Universitas Indonesia, Kampus Baru UI Depok, 16424, Indonesia",2017 3rd International Conference on Science in Information Technology (ICSITech),15-Jan-18,2017,,,121,126,"Accessibility is a critical aspect to be considered by college library in order to facilitate users in searching library collections. The Library of Universitas Indonesia, as one of Asia's largest library with more than 1,500,000 book collections, should also concern about accessibility to balance its numerous collections. UI-ana collections or works produced by and associated with Universitas Indonesia; in particular theses (undergraduate and graduate theses) and dissertations are one of the largest numbers of collections in Universitas Indonesia's Library. However, the current collection's management system was still based on the submission of the collection in Universitas Indonesia's Library. Since these collections are arranged with no exact criterion, it is harder for users to find theses and dissertations with the same topic. Therefore, management of these collections based on certain criterion is extremely needed to facilitate users in searching these collections. This research aims to determine the categories that can represent theses and dissertations through abstract text mining of each collection in 2005-2015 with a clustering algorithm, namely Self-organizing Map. This study found 139 categories which will be used to classify theses and dissertations of Universitas Indonesia.",,978-1-5090-5866-2,10.1109/ICSITech.2017.8257096,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257096,document clustering;self-organizing map;library management system;text mining,Libraries;Self-organizing feature maps;Text mining;Clustering algorithms;Classification algorithms;Information technology,academic libraries;data mining;educational institutions;pattern classification;pattern clustering;self-organising feature maps;text analysis,UI-ana collections;undergraduate theses;text document clustering;library collections;Asia's largest library;book collections;self organizing map;university dissertations;college library accessibility;library collection searching;Universitas Indonesia Library;abstract text mining;clustering algorithm;dissertation classification,,2,,16,,15-Jan-18,,,IEEE,IEEE Conferences
Keyword Extraction and Clustering for Document Recommendation in Conversations,使用改進的k均值的文檔聚類算法,M. Habibi; A. Popescu-Belis,"Idiap Research Institute, Centre du Parc, Idiap Research Institute and ?cole Polytechnique F矇d矇rale de Lausanne (EPFL), 1920 Martigny, Switzerland; Idiap Research Institute, Centre du Parc, 1920 Martigny, Switzerland","IEEE/ACM Transactions on Audio, Speech, and Language Processing",6-Mar-15,2015,23,4,746,759,"This paper addresses the problem of keyword extraction from conversations, with the goal of using these keywords to retrieve, for each short conversation fragment, a small number of potentially relevant documents, which can be recommended to participants. However, even a short fragment contains a variety of words, which are potentially related to several topics; moreover, using an automatic speech recognition (ASR) system introduces errors among them. Therefore, it is difficult to infer precisely the information needs of the conversation participants. We first propose an algorithm to extract keywords from the output of an ASR system (or a manual transcript for testing), which makes use of topic modeling techniques and of a submodular reward function which favors diversity in the keyword set, to match the potential diversity of topics and reduce ASR noise. Then, we propose a method to derive multiple topically separated queries from this keyword set, in order to maximize the chances of making at least one relevant recommendation when using these queries to search over the English Wikipedia. The proposed methods are evaluated in terms of relevance with respect to conversation fragments from the Fisher, AMI, and ELEA conversational corpora, rated by several human judges. The scores show that our proposal improves over previous methods that consider only word frequency or topic similarity, and represents a promising solution for a document recommender system to be used in conversations.",2329-9304,,10.1109/TASLP.2015.2405482,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045531,Document recommendation;information retrieval;keyword extraction;meeting analysis;topic modeling,Speech;Speech processing;Data mining;IEEE transactions;Information retrieval;Encyclopedias,document handling;pattern clustering;query processing;recommender systems;speech recognition,keyword extraction;keyword clustering;document recommendation;conversation fragment;automatic speech recognition;ASR system;information needs;topic modeling techniques;submodular reward function;topic diversity;English Wikipedia;Fisher conversational corpora;AMI conversational corpora;ELEA conversational corpora;word frequency;document recommender system,,24,,56,,19-Feb-15,,,IEEE,IEEE Journals
A Framework for Online Hot Event Discovery on the Web,群集文件空間屬性的分析研究,Y. Liu; X. Luo,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China",2013 IEEE 16th International Conference on Computational Science and Engineering,6-Mar-14,2013,,,989,996,"With the coming era of Big Data, online hot event discovery has emerged to mine the social hot spots on the large-scale web resources. Hot events are naturally evolved over time, and in the meantime, their inherent semantic relations are likely to change. As a result, traditional event detection approaches do not perform well on the dynamic web resources. To overcome these bottlenecks, this paper presents a novel hot event discovery framework to detect hot events online, containing three stages: 1) document preprocessing which selects significant features to represent document content, 2) threshold-resilient document classification, which classifies the incoming documents into topically related events considering event evolution, 3) adaptive splitting document clustering, which is used to timely cluster newly happened hot events. Using online data set from Baidu website, the experiments demonstrate the hot events discovery ability with respect to high accuracy, good scalability and short runtime.",,978-0-7695-5096-1,10.1109/CSE.2013.145,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755326,online event detection;event discovery framework;threshold-resilient classification;adaptive splitting clustering,Communities;Semantics;Timing;Event detection;Clustering methods;Clustering algorithms;Accuracy,Big Data;data mining;document handling;feature selection;Internet;pattern classification;pattern clustering;semantic Web;Web sites,online hot event detection;document preprocessing;feature selection;document content representation;threshold-resilient document classification;incoming document classification;adaptive splitting document clustering;Baidu Web site;semantic relations;large-scale Web resources;social hot spot mining;online hot event discovery;Big Data,,1,,27,,6-Mar-14,,,IEEE,IEEE Conferences
A new PSO methodology for web documents retrieval,語義文檔的詞義消歧,C. Ramya; K. S. Shreedhara,"Department of CSE, UBDTCE, Davanagere, Karnataka, India; Department of CSE, UBDTCE, Davanagere, Karnataka, India","2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)",8-Feb-18,2017,,,1,5,"This paper focuses on retrieval of web documents with improved response time and similarity using particle swarm optimization (PSO) technique. Since the nature of the web data is distributed, volatile and uncertain, an accurate and speedy access is required. Hence a novel approach on evolutionary bio-inspired Swarm Intelligence techniques to optimize search process in Web Information Retrieval systems is proposed and developed. Here, a novel algorithm has been proposed using basic PSO technique which works on both small CACM and huge RCV1 collections. This is applied on the pre-processed documents to retrieve most similar documents with a very less response time. This paper also reveals a comparative study with the existing method.",,978-1-5386-2361-9,10.1109/ICEECCOT.2017.8284622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284622,Particle swarm optimization;web information retrieval (WIR);pre-processing of documents;indexing,Information retrieval;Time factors;Particle swarm optimization;Clustering algorithms;Optimization;Indexing;Algorithm design and analysis,document handling;evolutionary computation;information retrieval;Internet;particle swarm optimisation,Web Information Retrieval systems;web documents retrieval;web data;particle swarm optimization;search process optimization;evolutionary bio-inspired Swarm Intelligence;PSO technique,,,,26,,8-Feb-18,,,IEEE,IEEE Conferences
Multi-spectral document image binarization using image fusion and background subtraction techniques,通過改進的Top-N正式概念搜索查找概念文檔群,N. Mitianoudis; N. Papamarkos,"Image Processing and Multimedia Laboratory, Department of Electrical and Computer Engineering, Democritus University of Thrace, 67100 Xanthi, Greece; Image Processing and Multimedia Laboratory, Department of Electrical and Computer Engineering, Democritus University of Thrace, 67100 Xanthi, Greece",2014 IEEE International Conference on Image Processing (ICIP),29-Jan-15,2014,,,5172,5176,"In this paper, the authors exploit a multispectral image representation to perform more accurate document image binarisation compared to previous color representations. In the first stage, image fusion is employed to create a ?document??and a ?background??image. In the second stage, the FastICA algorithm is used to perform background subtraction. In the third stage, a spatial kernel K-harmonic means classifier binarizes the FastICA output. The proposed system outperforms previous efforts on document image binarization.",2381-8549,978-1-4799-5751-4,10.1109/ICIP.2014.7026047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7026047,Document Image Binarization;Image Fusion;Multi-spectral imaging;Independent Component Analysis,Image fusion;Histograms;Clustering algorithms;Noise;Degradation;Kernel;Discrete cosine transforms,document image processing;image classification;image fusion;image representation;independent component analysis,multispectral document image binarization;multispectral image representation;image fusion;FastICA algorithm;background subtraction;spatial kernel K-harmonic means classifier;FastICA output;independent component analysis,,7,,11,,29-Jan-15,,,IEEE,IEEE Conferences
Document Clustering Using Semantic Kernels Based on Term-Term Correlations,文檔聚類中特徵選擇方法TF-IDF的新方法,A. K. Farahat; M. S. Kamel,"Dept. of Electr. & Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Dept. of Electr. & Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada",2009 IEEE International Conference on Data Mining Workshops,28-Dec-09,2009,,,459,464,"Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. In this paper, we propose the use of semantic kernels that are based on term-term correlations for improving the effectiveness of document clustering algorithms. The used kernels measure proximity between documents based on how their terms are statistically correlated. We analyze semantic kernels that capture different aspects of correlations between terms, and evaluate them by conducting experiments on different benchmark data sets. Results show that the proposed method achieves significant improvement in document clustering compared to VSM.",2375-9259,978-1-4244-5384-9,10.1109/ICDMW.2009.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360448,,Kernel;Clustering algorithms;Text mining;Data mining;Conferences;Unsupervised learning;Organizing;Computational complexity;Algorithm design and analysis,document handling;pattern clustering;vectors,document clustering;semantic kernel;term-term correlation;vector space model,,1,,18,,28-Dec-09,,,IEEE,IEEE Conferences
Text Clustering Algorithm Based on Lexical Graph,Web集群系統多重分組中的負載均衡的兩遍Web文檔分配方法,Y. Sha; G. Zhang; H. Jiang,"Beijing Institute of Petrochemical Technology, China; Beijing Institute of Petrochemical Technology, China; Beijing Institute of Petrochemical Technology, China",Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007),18-Dec-07,2007,2,,277,281,"Text clustering methods can group text into thematic clusters, which is an important topic in many fields, such as search engine. The well-known methods of text clustering, however, do not really address the special problems of text clustering because of the very high dimensionality data and understandability of the cluster description. An algorithm for text clustering based on lexical graph is proposed in this paper, which is a kind of term-based cluster method. The lexical graph is build with nodes representing words and edges representing their concurrent in text. The attribute of each node is text which the word occurs in. A cluster center is defined as node (word) with large degree in this graph, the center attributes (text occurs in) and its neighbors' are partitioned to one cluster whose description is the center node. This approach reduces drastically the dimensionality of the data and improves the synonymy extension ability. An experimental evaluation on Web documents as well as classical text documents on demonstrates that the proposed algorithms obtain clustering of comparable quality significantly more efficiently than K-Means and STC algorithms on the search results data set. Furthermore, this method provides an understandable description of the discovered clusters by their center.",,978-0-7695-2874-8,10.1109/FSKD.2007.560,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406087,,Clustering algorithms;Partitioning algorithms;Computer science;Petrochemicals;Cities and towns;Clustering methods;Search engines;Web search;Scattering;Tree data structures,graph theory;pattern clustering;text analysis,text clustering algorithm;lexical graph structure;search engine;term-based cluster method;data dimensionality reduction;Web document,,,,12,,18-Dec-07,,,IEEE,IEEE Conferences
Method for supporting analysis of personal relationships through place names extracted from documents,一種魯棒且無二值化的歷史文檔文本行檢測方法,F. Kimura; A. Maeda,"Kinugasa Research Organization, Ritsumeikan University, 56-1 Toji-in Kita-machi, Kita-ku, Kyoto, 603-8577, Japan; College of Information Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga 525-8577, Japan",IEEE/ACM Joint Conference on Digital Libraries,4-Dec-14,2014,,,253,256,"Visualizing information extracted from text is helpful for intuitively understanding the information. Extracting and visualizing personal relationships from text is one of the promising applications of this approach. Existing methods usually estimate personal relationships from direct co-occurrences of personal names that appear in a text. In our previous work, we proposed a method for extracting personal relationships from indirect co-occurrence relationships obtained through place names. This method can estimate the relationships among persons who do not necessarily have direct relationships. These relationships are visualized in a network graph. However, it becomes difficult to grasp the relationships when the number of persons increases. In this paper, we propose a method that supports analyzing the extracted personal relationships through place names and that is based on our previous work. Our goal is to support analysis by providing the information of the clustering of closely related people and important place names for each cluster. The proposed method was applied to a Japanese historical chronicle written in the 12th century. Experimental results showed a strong correspondence to the known historical facts. The results also indicate that the proposed method might be able to uncover the characteristics of people whose histories are not clearly known yet.",,978-1-4799-5569-5,10.1109/JCDL.2014.6970176,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970176,text mining;personal relationship;place name;clustering,Visualization;Clustering algorithms;Feature extraction;Vectors;Data mining;Accuracy;Indexes,data mining;data visualisation;feature extraction;pattern clustering;text analysis,personal relationships analysis;place names extraction;information visualization;personal relationships extraction;closely related people clustering;Japanese historical chronicle;documents processing;text mining,,,,6,,4-Dec-14,,,IEEE,IEEE Conferences
Affinity-based similarity measure for Web document clustering,通過段落嵌入和密度峰值聚類進行多文檔新聞摘要,M. -. Shyu; S. -. Chen; M. Chen; S. H. Rubin,"Dept. of Electr. & Comput. Eng., Miami Univ., Coral Gables, FL, USA; NA; NA; NA","Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration, 2004. IRI 2004.",23-May-05,2004,,,247,252,"Compared to the regular documents, the major distinguishing characteristics of the Web documents are the dynamic hyper-structure. Thus, in addition to terms or keywords for regular document clustering, Web document clustering can incorporate some dynamic information such as the hyperlinks and the access patterns extracted from the user query logs. In this paper, we extend the concept of document clustering into Web document clustering by introducing the strategy of affinity-based similarity measure, which utilizes the user access patterns in determining the similarities among Web documents via a probabilistic model. Several comparison experiments are conducted using a real data set and the experimental results demonstrate that the proposed similarity measure outperforms the cosine coefficient and the Euclidean distance method under different document clustering algorithms.",,0-7803-8819-4,10.1109/IRI.2004.1431469,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1431469,,Clustering algorithms;World Wide Web;Uniform resource locators;Particle measurements;Multimedia systems;Distributed computing;Military computing;Systems engineering and theory;Information systems;Laboratories,Internet;document handling;data mining;information retrieval,Web document clustering;hyperlinks;user access patterns;user query logs;affinity-based similarity measure;probabilistic model;document retrieval;cosine coefficient;Euclidean distance method,,2,,14,,23-May-05,,,IEEE,IEEE Conferences
Employing Structural and Textual Feature Extraction for Semistructured Document Classification,基於聚類的方法來分離,M. Khabbaz; K. Kianmehr; R. Alhajj,"Department of Computer Science , University of British Colombia, Vancouver, Canada; Department of Electrical and Computer Engineering, Western University, London, Canada; Department of Computer Science , University of Calgary, Calgary, Canada","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",21-Dec-12,2012,42,6,1566,1578,"This paper addresses XML document classification by considering both structural and content-based features of the documents. This approach leads to better constructing a set of informative feature vectors that represents both structural and textual aspects of XML documents. For this purpose, we integrate soft clustering of words and feature reduction into the process. To extract structural information, we employ an existing frequent tree-mining algorithm combined with an information gain filter to retrieve the most informative substructures from XML documents. However, for extracting content information, we propose soft clustering of words using each cluster as a textual feature. We have conducted extensive experiments on a benchmark dataset, namely 20NewsGroups, and an XML documents dataset given in LOGML that describes the web-server logs of user sessions. With regards to the classifier built only using our textual features, the results show that it outperforms a naive support-vector-machine (SVM)-based classifier, as well as an information retrieval classifier (IRC). We further demonstrate the effectiveness of incorporating both structural and content information into the process of learning, by comparing our classifier model and several XML document classifiers. In particular, by applying SVM and decision tree algorithms using our feature vector representation of XML documents dataset, we have achieved 85.79% and 87.04% classification accuracy, respectively, which are higher than accuracy achieved by XRules, a well-known structural-based XML document classifier.",1558-2442,,10.1109/TSMCC.2012.2208102,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392444,Document classification;feature reduction;soft clustering;structural information;XML documents,XML;Vectors;Feature extraction;Indexes;Clustering algorithms;Data mining,data mining;feature extraction;information retrieval;support vector machines;XML,textual feature extraction;structural feature extraction;semistructured document classification;XML document classification;informative feature vectors;soft clustering;feature reduction;words reduction;tree mining algorithm;information gain filter;informative substructures;web server;support vector machine;SVM;information retrieval classifier;IRC;learning process,,5,,54,,21-Dec-12,,,IEEE,IEEE Journals
Document Classification Using Multiple Views,使用多視圖的文檔分類,A. Gordo; F. Perronnin; E. Valveny,"Comput. Vision Center, Univ. Autonoma de Barcelona, Barcelona, Spain; Xerox Res. Centre Eur., France; Comput. Vision Center, Univ. Autonoma de Barcelona, Barcelona, Spain",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,33,37,"The combination of multiple features or views when representing documents or other kinds of objects usually leads to improved results in classification (and retrieval) tasks. Most systems assume that those views will be available both at training and test time. However, some views may be too `expensive' to be available at test time. In this paper, we consider the use of Canonical Correlation Analysis to leverage `expensive' views that are available only at training time. Experimental results show that this information may significantly improve the results in a classification task.",,978-0-7695-4661-2,10.1109/DAS.2012.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195330,Document classification;multiple views;runlengths;CCA,Training;Visualization;Accuracy;Correlation;Vectors;Histograms;Principal component analysis,covariance matrices;document handling;feature extraction;image classification;pattern clustering,document classification;multiple views;multiple features;document representation;canonical correlation analysis;classification task;image clustering;covariance matrices,,2,,16,,7-May-12,,,IEEE,IEEE Conferences
Automatic prototype stroke generation based on stroke clustering for on-line handwritten Japanese character recognition,基於筆劃聚類的自動原型筆劃生成，用於在線手寫日語字符識別,K. Yamasaki,"Res. Lab., IBM Japan Ltd., Tokyo, Japan",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,673,676,"An automatic method for generating prototype strokes is proposed for on-line handwritten character recognition. The method consists of two steps: an intra-category step and an all-category step. In the first step, which has already been proposed by the author, handwritten stroke examples are clustered to obtain prototype strokes for each category. In the second step, these prototypes are merged to obtain common prototype strokes for all categories. This two step approach alleviates the difficulty in determining initial clusters for a large number of examples. To assess the quality of them, recognition experiments are conducted to see the relationship between the number of prototypes and the accuracy. It is found that the two relationships in kanji categories and in non-kanji ones differ from each other. This observation indicates that a set of prototype strokes for Japanese characters can be made of common prototypes for all kanji categories and prototypes for each non-kanji categories.",,0-7695-0318-7,10.1109/ICDAR.1999.791877,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791877,,Prototypes;Character recognition;Hidden Markov models;Writing;Dictionaries;Character generation;Handwriting recognition;Algorithm design and analysis;Shape;Clustering algorithms,handwritten character recognition;document image processing;merging,prototype stroke generation;stroke clustering;online handwritten Japanese character recognition;merging;experiments;kanji,,5,,7,,6-Aug-02,,,IEEE,IEEE Conferences
A document clustering algorithm based on improved landmark semidefinite embedding,基於改進的界標半定嵌入的文檔聚類算法,H. Wang; H. Qin; L. Ding; G. Hui,"School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University, Nanning, China; School of Computer and Electronic Information, Guangxi University Nanning, China",The 2nd International Conference on Information Science and Engineering,17-Jan-11,2010,,,4827,4830,"The document space is generally of high dimensionality, and clustering in such a high dimensional space is often infeasible due to the curse of dimensionality. In this paper, a novel document clustering method which based on improved landmark semidefinite embedding (lSDE) is proposed. Based on the general lSDE, the point selection rules is modified by Max-min distance algorithm, with a view to ensuring the stability of algorithm. By using the improved lSDE, the documents can be projected into a lower dimension kernel space in which redundant information was filtered, and the documents related to the same semantic are close to each other. On this low-dimensional representation, the processed document data was clustered by kernel K-means. Experimental results show that the new clustering algorithm gives better performance than several advanced clustering methods.",2160-1291,978-1-4244-7618-3,10.1109/ICISE.2010.5690075,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690075,text clustering;Max-min distance algorithm;nonlinear dimensionality reduction;kernel K-means,Principal component analysis;Clustering algorithms;Computers;Kernel;Programming;Educational institutions;Semantics,,,,,,,,17-Jan-11,,,IEEE,IEEE Conferences
Document page segmentation using multiscale clustering,使用多尺度聚類的文檔頁面細分,D. P. Mukherjee; S. T. Acton,"Sch. of Electr. & Comput. Eng., Oklahoma Univ., Norman, OK, USA; NA",Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348),6-Aug-02,1999,1,,234,238 vol.1,"The paper details a multiscale clustering technique for document page segmentation. In contrast to existing hierarchical (coarse-to-fine), multi-resolution methods, this image segmentation technique simultaneously uses information from different scaled representations of the original image. The final clustering of image segments is achieved through a fuzzy c-means based similarity measure between vectors in scale space. The segmentation process reduces the effects of insignificant detail and noise. Furthermore, object integrity is preserved in the segmentation process.",,0-7803-5467-2,10.1109/ICIP.1999.821604,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821604,,Image segmentation;Context modeling;Morphological operations;Laboratories;Extraterrestrial measurements;Noise reduction;Content based retrieval;Graphics;Image coding;Clustering algorithms,image segmentation;image classification;data integrity;fuzzy logic,document page segmentation;multiscale clustering;multi-resolution methods;image segmentation;scaled representations;image segments clustering;fuzzy c-means;similarity measure;object integrity,,11,,9,,6-Aug-02,,,IEEE,IEEE Conferences
Research on Fuzzy Clustering of EMR Based on XML,基於XML的EMR模糊聚類研究。,Y. Xu; J. Chen,"Inst. of Comput., Beijing Inf. Sci. & Technol. Univ., Beijing, China; Inst. of Comput., Beijing Inf. Sci. & Technol. Univ., Beijing, China",2010 International Conference on Biomedical Engineering and Computer Science,6-May-10,2010,,,1,4,"With the continuous promotion and gradual adoption of EMR, it must have accumulated a great number of valuable medical data. As there are a large number of unstructured data in the EMR such as text and images, and there is some uncertainty in symptoms and diseases description, these determine the need of using XML database to store EMR, and using fuzzy clustering algorithm to analyze for XML document. First, the characteristics of EMR and the significance of clustering study on EMR are described in this paper. Then, the specific data preparation process on XML EMR is given, the fuzzy C-means clustering algorithm is introduced, and the optimizing selection methods of optimal weighted exponent m* and the optimal clusters number c* are given. Finally, the clustering results aim at different number of samples and the evaluation on clustering method based on experimental results are given.",2165-9249,978-1-4244-5315-3,10.1109/ICBECS.2010.5462395,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5462395,,XML;Clustering algorithms;Biomedical imaging;Uncertainty;Diseases;Image databases;Image analysis;Algorithm design and analysis;Optimization methods;Clustering methods,fuzzy set theory;mathematics computing;medical administrative data processing;pattern clustering;XML,medical data;XML database;fuzzy clustering algorithm;XML document analysis;data preparation process;fuzzy C-mean clustering algorithm;selection method optimisation;optimal weighted exponent m*;optimal cluster number c*;XML EMR,,,,8,,6-May-10,,,IEEE,IEEE Conferences
A Robust Algorithm for Fuzzy Document Clustering,模糊文檔聚類的魯棒算法,L. Chen; S. Wang; Q. Jiang,"Sch. of Math. & Comput. Sci., Fujian Normal Univ., Fuzhou; NA; NA",2009 International Conference on Advanced Information Networking and Applications Workshops,26-Jun-09,2009,,,679,684,"In many applications of document clustering, a document may include multiple topics and thus may relate to multiple categories at the same time. Most of the existing subspace clustering algorithms can only perform hard clustering on document collections. In this paper, a fuzzy algorithm named R-FPC is introduced for document clustering. The algorithm discovers soft partitions of a data set in the soft subspaces of the data space. Using the proposed R-Greedy initialization method, R-FPC can always generate stable clustering results with competitive accuracy. The experiments are conducted on some widely used corpuses and the results have shown effectiveness and robustness of the proposed methods.",,978-1-4244-3999-7,10.1109/WAINA.2009.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5136727,,Robustness;Clustering algorithms;Partitioning algorithms;Flexible printed circuits;Application software;Computer science;Iterative algorithms;Software algorithms;Mathematics;Algorithm design and analysis,data analysis;document handling;fuzzy set theory;greedy algorithms;pattern clustering,robust algorithm;fuzzy document clustering;R-Greedy initialization method,,1,,17,,26-Jun-09,,,IEEE,IEEE Conferences
Two-handed volumetric document corpus management,雙手式體積文檔語料庫管理,D. S. Ebert; A. Zwa; E. L. Miller; C. D. Shaw; D. A. Roberts,"Maryland Univ., Baltimore, MD, USA; NA; NA; NA; NA",IEEE Computer Graphics and Applications,6-Aug-02,1997,17,4,60,62,"To find a document in the sea of information, you must embark on a search process, usually computer-aided. In the traditional information retrieval model, the final goal is to identify and collect a small number of documents to read in detail. In this case, a single query yielding a scalar indication of relevance usually suffices. In contrast, document corpus management seeks to understand what is happening in the collection of documents as a whole (i.e. to find relationships among documents). You may indeed read or skim individual documents, but only to better understand the rest of the document set. Document corpus management seeks to identify trends, discover common links and find clusters of similar documents. The results of many single queries must be combined in various ways so that you can discover trends. We describe a new system called the Stereoscopic Field Analyzer (SFA) that aids in document corpus management by employing 3D volumetric visualization techniques in a minimally immersive real-time interaction style. This interactive information visualization system combines two-handed interaction and stereoscopic viewing with glyph-based rendering of the corpora contents. SFA has a dynamic hypertext environment for text corpora, called Telltale, that provides text indexing, management and retrieval based on n-grams (n character sequences of text). Telltale is a document management and information retrieval engine which provides document similarity measures (n-gram-based m-dimensional vector inner products) visualized by SFA for analyzing patterns and trends within the corpus.",1558-1756,,10.1109/38.595271,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595271,,Visualization;Information retrieval;Real time systems;Indexing;Environmental management;Information management;Engines;Sea measurements;Information analysis;Pattern analysis,document handling;data visualisation;real-time systems;information retrieval systems,two-handed interaction;stereoscopic viewing;document corpus management;3D volumetric visualization techniques;information retrieval;document relationships;trends identification;common links;document clusters;queries;Stereoscopic Field Analyzer;minimally immersive real-time interaction style;interactive information visualization system;glyph-based rendering;dynamic hypertext environment;text corpora;Telltale;text indexing;n-gram-based m-dimensional vector inner products;document management;information retrieval engine,,1,,4,,6-Aug-02,,,IEEE,IEEE Magazines
Ranked Centroid Projection: A Data Visualization Approach With Self-Organizing Maps,排序質心投影：具有自組織圖的數據可視化方法,G. G. Yen; Z. Wu,"Oklahoma State Univ., Stillwater; Oklahoma State Univ., Stillwater",IEEE Transactions on Neural Networks,7-Feb-08,2008,19,2,245,259,"The self-organizing map (SOM) is an efficient tool for visualizing high-dimensional data. In this paper, the clustering and visualization capabilities of the SOM, especially in the analysis of textual data, i.e., document collections, are reviewed and further developed. A novel clustering and visualization approach based on the SOM is proposed for the task of text mining. The proposed approach first transforms the document space into a multidimensional vector space by means of document encoding. Afterwards, a growing hierarchical SOM (GHSOM) is trained and used as a baseline structure to automatically produce maps with various levels of detail. Following the GHSOM training, the new projection method, namely the ranked centroid projection (RCP), is applied to project the input vectors to a hierarchy of 2D output maps. The RCP is used as a data analysis tool as well as a direct interface to the data. In a set of simulations, the proposed approach is applied to an illustrative data set and two real-world scientific document collections to demonstrate its applicability.",1941-0093,,10.1109/TNN.2007.905858,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359218,Data visualization;document clustering;self-organizing map (SOM);Data visualization;document clustering;self-organizing map (SOM),Data visualization;Self organizing feature maps;Multidimensional systems;Data analysis;Data mining;Clustering algorithms;Principal component analysis;Text mining;Encoding;Production,data analysis;data mining;data visualisation;self-organising feature maps;text analysis,ranked centroid projection;data visualization;self-organizing maps;high-dimensional data;data clustering;textual data analysis;text mining;multidimensional vector space;document encoding;growing hierarchical SOM;2D output maps;data analysis tool;scientific document collections,"Artificial Intelligence;Cluster Analysis;Computer Graphics;Computer Simulation;Neural Networks (Computer);Nonlinear Dynamics;Pattern Recognition, Automated",16,,42,,7-Feb-08,,,IEEE,IEEE Journals
A Chinese Character Localization Method Based on Intergrating Structure and CC-Clustering for Advertising Images,基於積分結構和CC聚類的廣告圖像漢字定位方法,J. Liu; S. Zhang; H. Li; W. Liang,"Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1044,1048,"In this paper, a novel Chinese character localization method is proposed for texts in advertising images. To deal with the texts with gradient color, a color clustering method based on edge is introduced to separate the color image into homogeneous color layers. To solve the problem of locating characters varied in size, style and arranged in irregular direction, a novel character localization method is proposed, which integrates structure and CC-clustering to locate characters according to reliable features of characters. Finally, a new noise removal method based on stroke width histogram is employed to remove all non-characters connected components, and then all characters are located. The experimental results show that the proposed method can effectively locate characters in advertising images.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.211,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065469,character localization;color clustering;connected component analysis,Image color analysis;Image edge detection;Merging;Colored noise;Feature extraction;Advertising;Reliability,gradient methods;image colour analysis;natural language processing;pattern clustering;text analysis,Chinese character localization method;intergrating structure;CC clustering;advertising images;color clustering method;gradient color;noise removal method;text analysis,,6,,13,,3-Nov-11,,,IEEE,IEEE Conferences
Clustering based optimal summary generation using Genetic Algorithm,使用遺傳算法的基於聚類的最佳摘要生成,A. Kogilavani; P. Balasubramanie,"Department of Computer Science & Engineering, Kongu Engineering College, Perundurai, Tamilnadu, India; Department of Computer Science & Engineering, Kongu Engineering College, Perundurai, Tamilnadu, India",2010 International Conference on Communication and Computational Intelligence (INCOCCI),24-Mar-11,2010,,,324,329,"This paper presents Genetic Algorithm based sentence extraction strategy and threshold based document clustering algorithm to produce cluster wise optimal summary. Related documents are grouped into same cluster using threshold based document clustering algorithm. From each cluster important sentences are selected using feature profile which is generated by considering sentence specific features like word weight, sentence position, sentence length, sentence centrality, proper nouns in the sentence and numerical data in the sentence. Based on the feature profile sentence score is calculated for each sentence. To produce optimal summary fitness function is employed which is based on summary quality criteria like maximizing length, coverage and informativeness while minimizing the redundancy. Machine generated summaries are compared against human summaries using Precision, Recall, F-measure and ROUGE-1 measure. The experimental results shows that the proposed approach is efficient and outperforms than the existing multi-document summarization system based on genetic algorithm (MSBGA) approach.",,978-81-8371-369-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738751,Feature Profile;Multi-Document Summarization;Sentence Extraction;Document Clustering;Linguistic Analysis;Sentence Specific Features;Sentence Score;Optimal Summary,Feature extraction;Gallium;Biological cells;Clustering algorithms;Humans;Data mining;Redundancy,genetic algorithms;pattern clustering;text analysis,genetic algorithm;sentence extraction strategy;document clustering algorithm;word weight;sentence position;sentence length;sentence centrality;proper nouns;multidocument summarization system,,,,8,,24-Mar-11,,,IEEE,IEEE Conferences
Automatic text summarization based on sentences clustering and extraction,基於句子聚類和提取的自動文本摘要,P. Zhang; C. Li,"College of Computer & Communication Engineering, China University of Petroleum, Dongying, Shandong, China; College of Computer & Communication Engineering, China University of Petroleum, Dongying, Shandong, China",2009 2nd IEEE International Conference on Computer Science and Information Technology,11-Sep-09,2009,,,167,170,"Technology of automatic text summarization plays an important role in information retrieval and text classification, and may provide a solution to the information overload problem. Text summarization is a process of reducing the size of a text while preserving its information content. This paper proposes a sentences clustering based summarization approach. The proposed approach consists of three steps: first clusters the sentences based on the semantic distance among sentences in the document, and then on each cluster calculates the accumulative sentence similarity based on the multi-features combination method, at last chooses the topic sentences by some extraction rules. The purpose of present paper is to show that summarization result is not only depends the sentence features, but also depends on the sentence similarity measure. The experimental result on the DUC 2003 dataset show that our proposed approach can improve the performance compared to other summarization methods.",,978-1-4244-4519-6,10.1109/ICCSIT.2009.5234971,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234971,text summarization;similarity measure;sentences clustering;sentence extractive technique,Data mining;Educational institutions;Petroleum;Information retrieval;Text categorization;Volume measurement;Web sites;Natural language processing;Clustering algorithms,classification;information retrieval;pattern clustering;text analysis,automatic text summarization;sentence clustering;sentence extraction;information retrieval;text classification;document sentence;multifeature combination method;information overload problem,,18,,18,,11-Sep-09,,,IEEE,IEEE Conferences
A Text Clustering Algorithm Combining K-Means and Local Search Mechanism,結合K均值和局部搜索機制的文本聚類算法,L. Cheng; Y. Sun; J. Wei,"Sch. of Comput. Sci. & Inf. Eng., Tianjin Univ. of Sci. & Technol., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China",2009 International Conference on Research Challenges in Computer Science,29-Jan-10,2009,,,53,56,"Text clustering is one of common techniques in mining large scales of document data. The paper presents an improved K-means text clustering algorithm in which a local search mechanism is introduced. By the iteration process of K-means algorithm, our approach can quickly get a local extreme point, and then use the search strategy of local search mechanism to have K-means jump out of that point and get a better solution. The experimental results show that our approach achieves better performance in the terms of entropy than the traditional algorithm while not slowing down the clustering speed.",,978-1-4244-5410-5,10.1109/ICRCCS.2009.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401292,text clustering;K-Means;local search mechanism,Clustering algorithms;Partitioning algorithms;Computer science;Electronic mail;Iterative algorithms;Data engineering;Sun;Data mining;Large-scale systems;Entropy,data mining;entropy;iterative methods;pattern clustering;search problems;text analysis,local search mechanism;document data mining;K-means text clustering algorithm;iteration process;entropy,,,,7,,29-Jan-10,,,IEEE,IEEE Conferences
Query expansion using thesaurus in improving Malay Hadith retrieval system,使用詞庫的查詢擴展在改進馬來聖訓系統中的應用,N. A. Rahman; Z. A. Bakar; T. M. T. Sembok,"Faculty of Computer & Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer & Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; National Defense University of Malaysia, Kuala Lumpur, Malaysia",2010 International Symposium on Information Technology,2-Sep-10,2010,3,,1404,1409,"Thesaurus has become another valuable structure in any Information Retrieval system. It is a list of terms and concepts that provide a controlled vocabulary of words to use in document indexing, clustering, searching and retrieval. This paper present the results of expanding user's query using Malay thesaurus in the process of searching Malay documents from Malay Hadith retrieval system. The results obtained shows that the retrieval effectiveness improves by four percent when thesaurus is employed in the process of retrieving Malay translated Hadith documents, compared to when single term queries are employed.",2155-899X,978-1-4244-6718-1,10.1109/ITSIM.2010.5561518,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561518,Information Retrieval;Thesaurus;Query Expansion;Cluster-based Retrieval,Thesauri;Information retrieval;Couplings;Books;Indexes;Clustering algorithms,document handling;indexing;information retrieval systems;natural language processing;query processing;thesauri,query expansion;Malay Hadith retrieval system;information retrieval system;controlled vocabulary;document indexing;document clustering;document searching;document retrieval;Malay thesaurus;Malay documents;Malay translated Hadith documents,,11,,21,,2-Sep-10,,,IEEE,IEEE Conferences
Hierarchical clustering based Intelligent Information Retrieval approach,基於層次聚類的智能信息檢索方法,P. S. Nishant; S. Mehrotra; P. R. Sree; P. Srikanth,"Koneru Lakshmaiah Education Foundation,Department of Computer Science and Engineering,Vaddeswaram,A.P.,India; Koneru Lakshmaiah Education Foundation,Department of Computer Science and Engineering,Vaddeswaram,A.P.,India; Koneru Lakshmaiah Education Foundation,Department of Computer Science and Engineering,Vaddeswaram,A.P.,India; Koneru Lakshmaiah Education Foundation,Department of Computer Science and Engineering,Vaddeswaram,A.P.,India",2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),6-Oct-20,2020,,,862,866,"In this digitalera, there is an enormous increase in the digital documents due to the use of the internet. Despite the hype, in the processing of digital documents, the problem of polysemy interprets more than one meaning for a single word. Hence the polysemy related queries will adversely affect the relevant result retrieval process. The paper addresses the polysemyissue on information retrieval paradigm and used clustering as a tool for improving the accuracy and appropriateness in the information retrieval process. This paper proposed a novel approach that generates a coherent group for the query topic. The proposed model is a hybrid of the k-means algorithm and the hierarchical approach named as HK clustering model to utilize the efficiency of both the strategies and overcome the drawbacks of each other.",,978-1-7281-5821-1,10.1109/ICSSIT48917.2020.9214231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214231,K-means;hierarchical clustering;text clustering;polysemy,Clustering algorithms;Partitioning algorithms;Machine learning algorithms;Analytical models;Conferences;Information retrieval;Particle swarm optimization,document handling;information retrieval;Internet;pattern clustering;query processing,hierarchical clustering;intelligent information retrieval;digitalera;digital documents;polysemy related queries;query topic;HK clustering model,,,,27,,6-Oct-20,,,IEEE,IEEE Conferences
Text clustering using statistical and semantic data,使用統計和語義數據進行文本聚類,A. Benghabrit; B. Ouhbi; H. Behja; B. Frikh,"LM2I laboratory, ENSAM, Moulay Isma簿l University, Marjane II, B.P. 4024, Mekn癡s, Morocco; LM2I laboratory, ENSAM, Moulay Isma簿l University, Marjane II, B.P. 4024, Mekn癡s, Morocco; LM2I laboratory, ENSAM, Moulay Isma簿l University, Marjane II, B.P. 4024, Mekn癡s, Morocco; LTTI laboraory, EST-F癡s, Moulay Abdellah University, B.P. 1796 Atlas F癡s, Morocco",2013 World Congress on Computer and Information Technology (WCCIT),3-Oct-13,2013,,,1,6,"The explosive growth of information stored in unstructured texts created a great demand for new and powerful tools to acquire useful information, such as text mining. Document clustering is one of its the powerful methods and by which document retrieval, organization and summarization can be achieved. However, it represents a challenge when dealing with a big number of data due to high dimensionality of the feature space and to the semantic correlation between features. In this paper, we propose a new sequential document clustering algorithm that uses a statistical and semantic feature selection methods. The semantic process was proposed to improve the frequency mechanism with the semantic relations of the text documents. The proposed algorithm selects iteratively relevant features and performs clustering until convergence. To evaluate its performance, experiments on two corpora have been conducted. The obtained results show that the performance of our algorithm is superior to that obtained by the existing algorithms.",,978-1-4799-0462-4,10.1109/WCCIT.2013.6618782,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618782,Text mining;clustering;feature selection methods;performance analysis,Clustering algorithms;Semantics;Vectors;Text mining;Mutual information;Algorithm design and analysis;Convergence,data mining;information retrieval;organisational aspects;pattern clustering;statistical analysis;text analysis,text clustering;semantic data;statistical data;unstructured texts;text mining;document retrieval;document organization;document summarization;semantic correlation;sequential document clustering algorithm;semantic feature selection methods;statistical feature selection methods;semantic process;text documents,,4,,27,,3-Oct-13,,,IEEE,IEEE Conferences
Clustering with projection distance and pseudo Bayes discriminant function for handwritten numeral recognition,投影距離聚類和偽貝葉斯判別函數用於手寫數字識別,Meng Shi; Wataru Ohyama; T. Wakabayashi; F. Kimura,"Fac. of Eng., Mie Univ., Tsu, Japan; NA; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,1007,1011,"This paper investigates the usage of the projection distance and the pseudo Bayes discriminant function as the distortion measure for handwritten numeral clustering problem. These distortion measures not only refer to the mean vectors but are also related to the covariance matrixes of subclasses, thus, the distribution of subclasses are reflected on the obtained clusters, and the accuracy of recognition can be improved. A series of evaluation experiments are performed on the handwritten numeral database NIST SD3 and SD7. The experimental results show that the recognition rate has been increased from 97.35% to 98.35%, which is one of the highest rates ever reported for the database.",,0-7695-1263-1,10.1109/ICDAR.2001.953937,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953937,,Clustering algorithms;Covariance matrix;Distortion measurement;Databases;Handwriting recognition;Performance evaluation;NIST;Writing;Character recognition;Algorithm design and analysis,handwritten character recognition;Bayes methods;pattern clustering,handwritten numeral recognition;clustering;projection distance;pseudo Bayes discriminant function;handwritten numeral clustering;distortion measures;recognition;handwritten numeral database;recognition rate,,,,7,,7-Aug-02,,,IEEE,IEEE Conferences
Hybridization of K-Means and Harmony Search Methods for Web Page Clustering,用於網頁聚類的K-Means混合和和諧搜索方法,R. Forsati; M. Meybodi; M. Mahdavi; A. Neiat,"Dept. of Comput. Eng., Islamic Azad Univ., Qazvin; Dept. of Comput. Eng., AmirKabir Univ. of Technol., Tehran; Dept. of Comput. Eng., Sharif Univ. of Technol., Tehran; Dept. of Comput. Eng., Islamic Azad Univ., Tehran",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,329,335,"Clustering is currently one of the most crucial techniques for dealing with massive amount of heterogeneous information on the web, which is beyond human beingpsilas capacity to digest. Recent studies have shown that the most commonly used partitioning-based clustering algorithm, the K-means algorithm, is more suitable for large datasets. However, the K-means algorithm can generate a local optimal solution. In this paper we present novel harmony search clustering algorithms that deal with documents clustering based on harmony search optimization method. By modeling clustering as an optimization problem, first, we propose a pure harmony search based clustering algorithm that finds near global optimal clusters within a reasonable time. Contrary to the localized searching of the K-means algorithm, the harmony search clustering algorithm performs a globalized search in the entire solution space. Then harmony clustering is integrated with the K-means algorithm in three ways to achieve better clustering. The proposed algorithms improve the K-means algorithm by making it less dependent on the initial parameters such as randomly chosen initial cluster centers, hence more stable. In the experiments we conducted, we applied the proposed algorithms, K-means clustering algorithm on five different document datasets. Experimental results reveal that the proposed algorithms can find better clusters when compared to K-means and the quality of clusters is comparable and converge to the best known optimum faster than it.",,978-0-7695-3496-1,10.1109/WIIAT.2008.370,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740468,Clustering;Data Mining;Optimization,Search methods;Web pages;Clustering algorithms;Partitioning algorithms;Optimization methods;Clustering methods;Intelligent agent;Information retrieval;Data mining;Stochastic processes,Internet;optimisation;pattern clustering;search problems;text analysis,K-means algorithm;harmony search clustering algorithm;Web page clustering;partitioning-based clustering algorithm;document clustering;harmony search optimization method;text analysis,,26,,12,,6-Jan-09,,,IEEE,IEEE Conferences
Ontology-based structured cosine similarity in document summarization: with applications to mobile audio-based knowledge management,文檔摘要中基於本體的結構化餘弦相似度：應用於基於移動音頻的知識管理,Soe-Tsyr Yuan; Jerry Sun,"MIS Dept., Nat. Chengchi Univ., Taipei, Taiwan; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",19-Sep-05,2005,35,5,1028,1040,"Development of algorithms for automated text categorization in massive text document sets is an important research area of data mining and knowledge discovery. Most of the text-clustering methods were grounded in the term-based measurement of distance or similarity, ignoring the structure of the documents. In this paper, we present a novel method named structured cosine similarity (SCS) that furnishes document clustering with a new way of modeling on document summarization, considering the structure of the documents so as to improve the performance of document clustering in terms of quality, stability, and efficiency. This study was motivated by the problem of clustering speech documents (of no rich document features) attained from the wireless experience oral sharing conducted by mobile workforce of enterprises, fulfilling audio-based knowledge management. In other words, this problem aims to facilitate knowledge acquisition and sharing by speech. The evaluations also show fairly promising results on our method of structured cosine similarity.",1941-0492,,10.1109/TSMCB.2005.850153,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510776,B2E M-Commerce;spherical K-means clustering;structured cosine similarity (SCS);text categorization;text summarization,Ontologies;Knowledge management;Sun;Speech recognition;Automatic speech recognition;Text categorization;Data mining;Knowledge acquisition;Business;Customer service,ontologies (artificial intelligence);text analysis;classification;data mining;knowledge management;mobile computing;electronic commerce;pattern clustering;speech recognition,ontology-based structured cosine similarity;document summarization;mobile audio-based knowledge management;automated text categorization;data mining;knowledge discovery;document clustering;B2E M-Commerce;spherical K-means clustering;knowledge acquisition;business-to-employee mobile commerce;automatic speech recognition,"Algorithms;Artificial Intelligence;Cluster Analysis;Databases, Factual;Documentation;Information Storage and Retrieval;Natural Language Processing;Online Systems;Pattern Recognition, Automated;Speech Recognition Software;Telecommunications;User-Computer Interface;Vocabulary, Controlled",23,,23,,19-Sep-05,,,IEEE,IEEE Journals
An Efficient Privacy-Preserving Ranked Keyword Search Method,一種高效的隱私保護排名關鍵詞搜索方法,C. Chen; X. Zhu; P. Shen; J. Hu; S. Guo; Z. Tari; A. Y. Zomaya,"State Key Laboratory Of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory Of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory Of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Cyber Security Lab, School of Engineering and IT, University of New South Wales at the Australian Defence Force Academy, Canberra, Australia; School of Computer Science and Engineering, The University of Aizu, Japan; School of Computer Science, RMIT University, Australia; School of Information Technologies, The University of Sydney, Australia",IEEE Transactions on Parallel and Distributed Systems,10-Mar-16,2016,27,4,951,963,"Cloud data owners prefer to outsource documents in an encrypted form for the purpose of privacy preserving. Therefore it is essential to develop efficient and reliable ciphertext search techniques. One challenge is that the relationship between documents will be normally concealed in the process of encryption, which will lead to significant search accuracy performance degradation. Also the volume of data in data centers has experienced a dramatic growth. This will make it even more challenging to design ciphertext search schemes that can provide efficient and reliable online information retrieval on large volume of encrypted data. In this paper, a hierarchical clustering method is proposed to support more search semantics and also to meet the demand for fast ciphertext search within a big data environment. The proposed hierarchical approach clusters the documents based on the minimum relevance threshold, and then partitions the resulting clusters into sub-clusters until the constraint on the maximum size of cluster is reached. In the search phase, this approach can reach a linear computational complexity against an exponential size increase of document collection. In order to verify the authenticity of search results, a structure called minimum hash sub-tree is designed in this paper. Experiments have been conducted using the collection set built from the IEEE Xplore. The results show that with a sharp increase of documents in the dataset the search time of the proposed method increases linearly whereas the search time of the traditional method increases exponentially. Furthermore, the proposed method has an advantage over the traditional method in the rank privacy and relevance of retrieved documents.",1558-2183,,10.1109/TPDS.2015.2425407,Strategic Priority Re-search Program of Chinese Academy of Sciences; Xinjiang Uygur Autonomous Region science and technology plan; Strategic Priority Research Program of Chinese Academy of Sciences; National High Technology Research and Development Program of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091954,Cloud computing;ciphertext search;ranked search;multi-keyword search;hierarchical clustering;big data;security;Cloud computing;ciphertext search;ranked search;multi-keyword search;hierarchical clustering;security,Servers;Indexes;Privacy;Computer architecture;Encryption,Big Data;cloud computing;computational complexity;computer centres;cryptography;data protection;document handling;outsourcing;pattern clustering;search engines;tree data structures,privacy-preserving ranked keyword search method;cloud data owners;document outsourcing;ciphertext search techniques;encryption process;search accuracy performance degradation;data volume;data centers;online information retrieval;hierarchical clustering method;search semantics;ciphertext search;Big Data environment;hierarchical approach;document clustering;minimum relevance threshold;cluster partitioning;linear computational complexity;document collection;minimum hash subtree;IEEE Xplore;document retrieval;privacy ranking,,66,,41,,22-Apr-15,,,IEEE,IEEE Journals
On Effective XML Clustering by Path Commonality: An Efficient and Scalable Algorithm,通過路徑通用性進行有效的XML聚類：一種高效且可擴展的算法,G. Costa; R. Ortale,"ICAR, Rende, Italy; ICAR, Rende, Italy",2012 IEEE 24th International Conference on Tools with Artificial Intelligence,11-Apr-13,2012,1,,389,396,"XML clustering by structure is, in its most general form, the process of partitioning a corpus of XML documents into disjoint clusters, such that intra-cluster structural homogeneity is high and inter-cluster structural homogeneity is low. In this paper, we propose an algorithm that implements a partitioning strategy, in which root-to-leaf paths are used to separate the XML documents. Paths are discriminatory substructures and, thus, the effectiveness of our algorithm is accordingly high. Moreover, a suitable encoding is adopted for representing and testing the occurrence of the individual paths within each XML document independently of the length of such paths. Not only this expedites clustering, but it also makes our algorithm scalable to process large-scale corpora of XML documents. A comparative evaluation over several standard (real-word and synthetic) XML corpora reveals that our algorithm outperforms all of its competitors in efficiency and scalability, while being as effective as the top-notch competitors. One especially appealing property of the proposed algorithm is that it achieves these levels of performance by automatically establishing a natural number of clusters to be discovered in the underlying XML corpus.",2375-0197,978-1-4799-0227-9,10.1109/ICTAI.2012.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495072,Data Mining;XML clustering,XML;Vegetation;Clustering algorithms;Scalability;Complexity theory;Standards;Partitioning algorithms,document handling;pattern clustering;XML,path commonality;XML clustering;XML documents;intra-cluster structural homogeneity;inter-cluster structural homogeneity;root-to-leaf paths;large-scale corpora;XML corpora;top-notch competitors,,8,,25,,11-Apr-13,,,IEEE,IEEE Conferences
Frequent term based text document clustering using similarity measures: A novel approach,使用相似性度量的基於頻繁術語的文本文檔聚類：一種新穎的方法,V. K. Gupta; M. Dutta; M. Kumar,"Deptt. Of I.T., Govt. Girls Polytechnic, Charkhari, Mahoba, India; Deptt. Of CS&E, N.I.T.T.T.R. Chandigarh; Deptt. Of I.T., BBDNITM, Lucknow India",2017 Fourth International Conference on Image Information Processing (ICIIP),12-Mar-18,2017,,,1,6,Clustering is one of the epic and traditional ways to make sure that the documents are retrieved at the right pace and according to the requirement. Clustering leads to keeping the similar kind of documents all together and so that they can be retrieved easily. The measure through which the relation between two documents is measured is called similarity index. There are several kind of similarity index already in the process. The proposed algorithm uses two kind of similarity index and combines them to produce a new similarity index. Similarity index plays a vital role in the clustering and classification procedure. The proposed algorithm also uses Fuzzy logic for the clustering rules and furthermore it is classified by the Support Vector Machine to justify the accuracy of the proposed solution.,,978-1-5090-6734-3,10.1109/ICIIP.2017.8313704,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8313704,Clustering;Data Mining;Cosine Similarity;Similarity Index;Fuzzy Logic;Support Vector Machine,Data mining;Indexes;Clustering algorithms;Fuzzy logic;Neural networks;Support vector machines;Classification algorithms,fuzzy logic;pattern clustering;support vector machines;text analysis,similarity index;clustering rules;frequent term-based text document clustering;fuzzy logic;support vector machine,,,,24,,12-Mar-18,,,IEEE,IEEE Conferences
Document Retrieval from Multiple Collections by Using Lightweight Ontologies,使用輕量級本體從多個集合中檢索文檔,M. A. Medina; J. A. Sanchez; J. A. Paz,"Universidad Tecnologica de la Mixteca, Mexico; Universidad de las Americas-Puebla, Mexico; Universidad Tecnologica de la Mixteca, Mexico",2006 15th International Conference on Computing,11-Dec-06,2006,,,141,146,This paper proposes the use of lightweight ontologies to describe documents collections. A meta harvesting protocol is used to access the documents. Ontologies are constructed semi-automatically by means of data mining techniques and document clustering algorithms. They are represented in Extensible Markup Language. Resource description framework is used to formalize a unique interpretation of the semantic of their elements. We describe a scenario where software agents use the ontologies to support information retrieval tasks from multiple collections and we show some experimental results,,0-7695-2708-6,10.1109/CIC.2006.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4023801,,Ontologies;Software agents;Information retrieval;Access protocols;Data mining;Clustering algorithms;Search engines;Software libraries;Application software;XML,data mining;document handling;information retrieval;ontologies (artificial intelligence);software agents;XML,document retrieval;ontologies;metaharvesting protocol;data mining;document clustering;Extensible Markup Language;resource description framework;software agents,,2,,12,,11-Dec-06,,,IEEE,IEEE Conferences
Labelling Hierarchical Clusters of Scientific Articles,標記科學文章的層次聚類,I. Peganova; A. Rebrova; Y. Nedumov,ISP RAS; ISP RAS; ISP RAS,2019 Ivannikov Memorial Workshop (IVMEM),24-Oct-19,2019,,,26,32,"Exploration of document collections is a complex task. One way to do this is to cluster the initial collection hierarchically and then label each cluster with a set of extracted terms. Good labelling should help exploration. We focus on the scientific domain and particularly on collections of abstracts of articles. Abstract is commonly a brief of a paper that outlines the research area, the challenge, the proposed solution and the results; so it could be used instead of a full article despite the difficulties related to its shortness. In this paper, we propose a new method HCBasic for labelling hierarchical clusters. It is particularly tuned for articles' abstracts and compared to three other methods: MTWL, hierMTWL and ComboBasic. To evaluate the quality of the labelling algorithms we did A/B testing in which eight volunteers searched for the articles that they were familiar with in the labelled cluster tree. We show that there is no single winner in terms of quality, and different methods are preferable in different cases.",,978-1-7281-4623-2,10.1109/IVMEM.2019.00010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880791,hierarchical clusters;cluster labelling;term extraction,Clustering algorithms;Labeling;Feature extraction;Task analysis;Encyclopedias;Electronic publishing,document handling;information retrieval;pattern clustering;scientific information systems;search engines,scientific articles;document collections;complex task;initial collection;extracted terms;scientific domain;hierarchical clusters;labelling algorithms;labelled cluster tree;HCBasic method,,,,20,,24-Oct-19,,,IEEE,IEEE Conferences
An Entropy Weighting k-Means Algorithm for Subspace Clustering of High-Dimensional Sparse Data,高維稀疏數據子空間聚類的熵權k均值算法,L. Jing; M. K. Ng; J. Z. Huang,"Univ. of Hong Kong, Hong Kong; NA; NA",IEEE Transactions on Knowledge and Data Engineering,25-Jun-07,2007,19,8,1026,1041,"This paper presents a new k-means type algorithm for clustering high-dimensional objects in sub-spaces. In high-dimensional data, clusters of objects often exist in subspaces rather than in the entire space. For example, in text clustering, clusters of documents of different topics are categorized by different subsets of terms or keywords. The keywords for one cluster may not occur in the documents of other clusters. This is a data sparsity problem faced in clustering high-dimensional data. In the new algorithm, we extend the k-means clustering process to calculate a weight for each dimension in each cluster and use the weight values to identify the subsets of important dimensions that categorize different clusters. This is achieved by including the weight entropy in the objective function that is minimized in the k-means clustering process. An additional step is added to the k-means clustering process to automatically compute the weights of all dimensions in each cluster. The experiments on both synthetic and real data have shown that the new algorithm can generate better clustering results than other subspace clustering algorithms. The new algorithm is also scalable to large data sets.",1558-2191,,10.1109/TKDE.2007.1048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262534,k{\hbox{-}}{\rm{means}} clustering;variable weighting;subspace clustering;text clustering;high-dimensional data.,Entropy;Clustering algorithms;Clustering methods;Text mining;Frequency measurement;Companies,data handling;pattern clustering,data sparsity problem;high-dimensional object clustering;high-dimensional sparse data;subspace clustering;entropy weighting k-means algorithm,,346,,39,,25-Jun-07,,,IEEE,IEEE Journals
Mathematical Document Retrieval for Problem Solving,解決問題的數學文檔檢索,S. H. Samarasinghe; S. C. Hui,"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore",2009 International Conference on Computer Engineering and Technology,2-Feb-09,2009,1,,583,587,"Solving mathematical problems is both challenging and difficult for many students. This paper proposes a document retrieval approach to help solve mathematical problems. The proposed approach is based on Kohonenpsilas Self-Organizing Maps for data clustering of similar mathematical documents from a mathematical document database. Based on a user query problem, similar mathematical documents with their associated solutions are retrieved in order to provide hints or solutions on solving the user problem. In this paper, we will discuss the proposed mathematical document retrieval approach. The performance of the proposed approach will also be presented in comparison with other clustering techniques.",,978-1-4244-3334-6,10.1109/ICCET.2009.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4769534,Knowledge Data Engineering;Data Mining;Artificial Intelligence,Problem-solving;Databases;Data mining;Search engines;Data engineering;Self organizing feature maps;Artificial intelligence;Clustering algorithms;Partitioning algorithms;Neural networks,computer aided instruction;data mining;mathematics computing;pattern clustering;query processing;self-organising feature maps,mathematical document retrieval;problem solving;Kohonen self-organizing map;data clustering;mathematical document database;query problem;data mining,,3,,16,,2-Feb-09,,,IEEE,IEEE Conferences
Concepts extraction for medical documents using ontology,使用本體提取醫學文檔的概念,V. Mala; D. K. Lobiyal,"School of Computer and Systems Sciences, Jawaharlal Nehru University, New Delhi, India; School of Computer and Systems Sciences, Jawaharlal Nehru University, New Delhi, India",2015 International Conference on Advances in Computer Engineering and Applications,23-Jul-15,2015,,,773,777,"In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems; Semantic text mining is to discover the hidden information from unstructured text and making relationships of the terms occurring in them. In the biomedical text, the text should be in the form of text which can be present in the books, articles, literature abstracts, and so forth. Most of information is stored in the text format, so in this paper we will focus on the role of ontology for semantic text mining by using WordNet. Specifically, we have presented a model for extracting concepts from text documents using linguistic ontology in the domain of medical.",,978-1-4673-6911-4,10.1109/ICACEA.2015.7164807,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164807,Text mining;Information retrieval;Semantic relations;English WordNet;Concept cluster,Ontologies;Text mining;Semantics;Information retrieval;Clustering algorithms;Diseases;Cancer,document handling;information retrieval;medical administrative data processing;medical computing;natural language processing;ontologies (artificial intelligence),medical documents;concept extraction;biomedical domain;text documents;unstructured information;digital text;text mining;medical domain;information retrieval;information extraction;natural language processing;NLP;semantic text mining;hidden information;unstructured text;WordNet;linguistic ontology,,1,,16,,23-Jul-15,,,IEEE,IEEE Conferences
Metrics for information retrieval: A case study,信息檢索指標：案例研究,R. T. Nadana; R. Shriram,"Dept. of CSE, B.S.Abdur Rahman University, Chennai, India; Dept. of CSE, B.S.Abdur Rahman University, Chennai, India",International Conference on Software Engineering and Mobile Application Modelling and Development (ICSEMA 2012),1-Jul-13,2012,,,1,5,"The domain of information retrieval (IR)has used clustering methods in a big way. Clustering is a technique that groups a set of documents into clusters or subsets. How efficiently and effectively the relevant documents are extracted from World Wide Web is a challenging issue. In this work, we compare and analyse the effectiveness of similarity measures such as City Block distance, Cosine similarity, Point symmetry distance and Dicecoefficient to improve document clustering with and without the presence of ontology. This has two objectives: a comparison of metrics in the domain and study the impact of various methods like ontology comparison and clustering on the metrics as a whole. This will lead to further refinement of the metrics for current and future needs in the domain. Earlier works in the domain have highlighted the fact that the results of the similarity measures are more or less the same. However our work shows that the use of ontology based clustering marked changes in the results. The results show the need for more work to be focused on the metrics aspect in information retrieval.",,978-1-84919-736-6,10.1049/ic.2012.0146,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549310,Text clustering;K-means;similarity measures;ontology,,document handling;information retrieval;ontologies (artificial intelligence);pattern clustering,information retrieval metric;IR;clustering method;World Wide Web;document extraction;city block distance measure;cosine similarity measure;point symmetry distance measure;Dice coefficient measure;ontology comparison;ontology clustering,,,,,,1-Jul-13,,,IET,IET Conferences
Exploiting statistical and semantic information for document clustering: An evaluation on feature selection,利用統計和語義信息進行文檔聚類：功能選擇評估,A. Benghabrit; B. Ouhbi; E. M. Zemmouri; B. Frikh; H. Behja,"LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LTTI Laboratory, EST-F癡s, Sidi Mohamed Ben, Abdellah, F癡s, Morocco; Greentic Laobarory, ENSEM, Hassan 2 University, Casablanca, Morocco",2014 Third IEEE International Colloquium in Information Science and Technology (CIST),22-Jan-15,2014,,,96,101,"Feature selection is not only a key to handle the high dimensionality phenomenon caused by the vector space model representation, but mainly an efficient technique to reduce the noise generated by the irrelevant and redundant terms. However, in order to effectively capture the most important features, both the semantic and the statistical information within the feature space should be taken into account. Thereby, we propose a sequential and a hybrid clustering and feature selection approaches that combines statistical and semantic feature weight estimation in order to select the most informative features. We first perform a comparative study on powerful statistical feature selection methods and an analysis was done for the semantic methods. Then, we extract the best combination of statistical and semantic methods for the sequential and hybrid approaches. Detailed experimental results on three different data sets are provided in this paper.",2327-1884,978-1-4799-5979-2,10.1109/CIST.2014.7016601,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016601,Document Clustering;feature selection methods;statistical and semantic analysis;performance analysis,Decision support systems;Semantics;Estimation;Vectors;Extraterrestrial phenomena;Noise;Feature extraction,document handling;feature selection;pattern clustering;statistical distributions;vectors,statistical information;semantic information;document clustering;feature selection;vector space model representation,,1,,42,,22-Jan-15,,,IEEE,IEEE Conferences
A new partitioning based algorithm for document clustering,一種基於分區的文檔聚類新算法,Z. Wang; Z. Liu; D. Chen; K. Tang,"School of Computer Science and Technology, Xidian University, Xi'an China; School of Computer Science and Technology, Xidian University, Xi'an China; School of Computer Science and Technology, Xidian University, Xi'an China; School of Computer Science and Technology, Xidian University, Xi'an China",2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-Sep-11,2011,3,,1741,1745,"Document clustering is one of the key problems in text mining and information retrieval area. It groups text documents in a way that maximizes the similarity within clusters and minimizes the similarity between different clusters. Most partitioning based algorithms are sensitive to the initial centroids, the clustering result greatly depends on the initial centroids. This paper first uses unsupervised feature selection method to reduce the dimension of document feature space and then proposes a novel partitioning based algorithm which select initial cluster centriods in the process of clustering by the size and density of cluster in the datasets. The experiments on several text datasets show that the proposed approach effectively improves the quality of clustering.",,978-1-61284-181-6,10.1109/FSKD.2011.6019857,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019857,clustering;partitioning;centroid;mention;feature,Clustering algorithms;Partitioning algorithms;Heuristic algorithms;Databases;Educational institutions;Algorithm design and analysis;Complexity theory,feature extraction;information retrieval;text analysis,partitioning based algorithm;document clustering;text mining;information retrieval;unsupervised feature selection method;document feature space;text datasets;clustering quality,,3,,17,,15-Sep-11,,,IEEE,IEEE Conferences
Using Topic and Sample Weighting Clustering to Detect Hotspots and Their Trend in a Special Domain,使用主題和?本加權聚類檢測特定領域中的熱點及其趨勢,C. Zhang,"Inst. of Sci. & Tech. Inf. of China, Beijing, China",2009 Fourth International Conference on Cooperation and Promotion of Information Resources in Science and Technology,28-Dec-09,2009,,,304,309,"Detecting hotspots and trend in a special domain based on the traditional method is costly, time-consuming and hysteretic. A method based on topic and sample weighting clustering is proposed to analysis and cluster the academic papers including the publication date. The PageRank value of each document is calculated according to the cited relationship among them, and it is used as the weight in the clustering. The topic and sample weighting clustering method can detect hotspots and trend of a particular domain. Experimental results show that the proposed method can detect hotspots and trend in a special domain quickly and accurately.",,978-0-7695-3898-3,10.1109/COINFO.2009.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5361898,hotspots and trend detecting;topic clustering;sample weighting clustering;document clustering,Libraries;Information analysis;Information science;Bibliometrics;Documentation;Hysteresis;Data mining;Costs;Frequency;Information resources,document handling;information analysis;pattern clustering,topic weighting clustering;sample weighting clustering;hotspots detection;academic papers;publication date;PageRank value,,,,38,,28-Dec-09,,,IEEE,IEEE Conferences
Centroid Based Summarization of Multiple Documents Implemented Using Timestamps,使用時間戳實現的多個文檔的基於質心的匯總,N. Ramanujam,"Dept. of Comput. Sci. & Eng., Sri Venkateswara Coll. of Eng., Pennalur",2008 First International Conference on Emerging Trends in Engineering and Technology,29-Jul-08,2008,,,480,485,"We propose a multiple-document summarization system with user interaction. We introduce a system that would extract a summary from multiple documents based on the document cluster centroids, which is effectively the distribution of terms in the multiple documents in the cluster. This summarization technique is a cluster- based, extractive summarization method, where passages are first clustered based on similarity, prior to the selection of passages that form the extractive summary of the documents. The sentences are then issued a timestamp based on the order of their occurrence in the original document, thereby ensuring the chronological order of sentences. Passage clustering forms a main component in this system that aims to extract the most relevant sentences of the documents at the same time keeping the summary non-redundant. The implementation is based on the MEAD extraction algorithm and redundancy based algorithm. MEAD extraction algorithm uses three features to compute the salience of the sentence. They are centroid value, positional value and first-sentence overlap. Redundancy algorithm checks for overlapping words in sentences and issues a redundancy penalty. Timestamps are issued to sentences to maintain the chronological order of the sentences and hence a coherent and free- flowing summary can be generated.",2157-0485,978-0-7695-3267-7,10.1109/ICETET.2008.122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579948,MEAD;Multi-document summariztion;Centroid and Timestamp,Data mining;Feature extraction;Clustering algorithms;Natural languages;Redundancy;US Department of Energy;Lead,pattern clustering;text analysis,multiple-document summarization system;time stamp;user interaction;document cluster centroid;extractive summarization method;chronological sentence order;passage clustering;MEAD extraction algorithm;redundancy algorithm;first-sentence overlap;Automatic text summarization,,1,,9,,29-Jul-08,,,IEEE,IEEE Conferences
Cluster graph based model for extracting and searching algorithm in bigdata,基於聚類圖的大數據提取與搜索模型,G. Veena; J. Joseph,"Dept. of Computer Science College of Engineering, Kidangoor Kottayam, India; Dept. of Computer Science College of Engineering, Kidangoor Kottayam, India",2017 International Conference on Networks & Advances in Computational Technologies (NetACT),23-Oct-17,2017,,,432,437,"Search systems that is used to search for information. Cite Seer was a search engine to search academic documents. Platforms are not available to discover algorithms in scholarly big data. The limitations of these search engines make the searching more difficult. Hence special purpose systems are used. Here proposes a search system to extract algorithm representations. Algorithms can be represented using pseudocode and algorithm procedures. Two methods are used for this purpose. The ways to extract textual metadata for each algorithm is discussed. Based on graph based clustering, context term is extracted. The metadata extraction is done using document element summarization. The synopsis is generated using topic modeling approach and context terms. Finally a synopsis comparison is done.",,978-1-5090-6590-5,10.1109/NETACT.2017.8076810,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076810,Metadata extraction;Clustering;Context terms,Clustering algorithms;Metadata;Feature extraction;Machine learning algorithms;Context modeling;Classification algorithms;Probabilistic logic,Big Data;graph theory;information retrieval;meta data;pattern clustering;search engines;text analysis,cluster graph;search system;Cite Seer;search engine;algorithm representations;Big Data;topic modeling;textual metadata extraction;pseudocode;document element summarization,,,,7,,23-Oct-17,,,IEEE,IEEE Conferences
A Parallel Comparator of Documents,文檔的並行比較器,S. A. Ksouri; M. S. Hidri; K. Barkaoui,"Ecole Nat. d'Ingonieurs de Tunis, Univ. Tunis El Manar, Tunis, Tunisia; Ecole Nat. d'Ingonieurs de Tunis, Univ. Tunis El Manar, Tunis, Tunisia; CEDRIC-CNAM, Paris, France",2013 24th International Workshop on Database and Expert Systems Applications,7-Oct-13,2013,,,48,52,"Documents, sentences and words clustering are well studied problems. Most existing algorithms cluster documents, sentences and words separately but not simultaneously. However, when analyzing large textual corpuses, the amount of data to be processed in a single machine is usually limited by the main memory available, and the increase of these data to be analyzed leads to increasing computational workload. In this paper we present a parallel fuzzy triadic similarity measure called PFT-Sim, to calculate fuzzy memberships in a context of document co-clustering based on a parallel programming architecture. It allows computing simultaneously fuzzy co-similarity matrices between documents/sentences and sentences/words. Each one is built on the basis of the others. The PFT-SIM model provides a parallel data analysis strategy and divides the similarity computing task into parallel sub-tasks to tackle efficiency and scalability problems.",2378-3915,978-1-4799-2138-6,10.1109/DEXA.2013.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621344,Text Mining;Document co-clustering;Fuzzy sets;multi-thread architecture;Parallel computing;Three-partite graph,Parallel processing;Clustering algorithms;Computational modeling;Computer architecture;Text mining;Complexity theory;Data models,data analysis;fuzzy set theory;matrix algebra;parallel programming;pattern clustering;text analysis,parallel comparator;large textual corpuses;parallel fuzzy triadic similarity measure;PFT-Sim;fuzzy memberships;document co-clustering;parallel programming architecture;fuzzy co-similarity matrices;parallel data analysis strategy;similarity computing task;parallel sub-tasks;words clustering;sentence clustering,,,,16,,7-Oct-13,,,IEEE,IEEE Conferences
Visualizing document space by force-directed dynamic layout,通過強制動態佈局可視化文檔空間,J. Tatemura,"Inst. of Ind. Sci., Tokyo Univ., Japan",Proceedings. 1997 IEEE Symposium on Visual Languages (Cat. No.97TB100180),6-Aug-02,1997,,,119,120,We propose an interactive document keyword layout technique that enables browsing and manipulation of a collection of documents visually. This layout technique applies a force directed graph drawing algorithm and clusters documents and keywords by reacting to a user's interaction dynamically. An example of visual interaction is demonstrated on an experimental system.,1049-2615,0-8186-8144-6,10.1109/VL.1997.626566,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=626566,,Springs;Manipulator dynamics;Java;Clustering algorithms;Information retrieval;Heuristic algorithms;Workstations;Data visualization,graphical user interfaces;online front-ends;information retrieval;interactive systems;visual programming;document handling,document space visualization;force directed dynamic layout;interactive document keyword layout technique;browsing;document manipulation;layout technique;force directed graph drawing algorithm;keywords;user interaction;visual interaction,,5,,4,,6-Aug-02,,,IEEE,IEEE Conferences
A robust technique for text extraction in mixed-type binary documents,混合類型二進製文檔中一種可靠的文本提取技術,C. Strouthopoulos; A. Nikolaidis,"Department of Informatics and Communications, Technological Educational Institute of Serres, Terma Magnisias, Serres 62124, GREECE; Department of Informatics and Communications, Technological Educational Institute of Serres, Terma Magnisias, Serres 62124, GREECE",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"A crucial preprocessing stage in applications such as OCR is text extraction from mixed-type documents. The present work, in contrast to most until now, successfully faces the problem of varying text orientation and size. The technique first identifies marks using a contour following technique, followed by a PCA (principal component analyzer) which determines the direction of the main axis of each mark. Next, a nearest-neighbor technique is employed to find the shortest distances between marks, and a feature vector is formed based on calculated mark dimensions and distances, which is then fed into a SOFM (self organizing feature map) which defines homogeneous mark clusters. Resulting cluster weights and variances are used to form a set of fuzzy rules, and a fuzzy classification scheme identifies marks as characters or non-characters. The technique succeeds in correctly and quickly extracting text areas in a variety of mixed-type documents.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761820,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761820,,Robustness;Optical character recognition software;Principal component analysis;Fuzzy sets;Feature extraction;Nearest neighbor searches;Karhunen-Loeve transforms;Informatics;Communications technology;Educational technology,document image processing;feature extraction;fuzzy reasoning;image classification;image segmentation;pattern clustering;principal component analysis;self-organising feature maps;text analysis,robust technique;text extraction;mixed-type binary document;text orientation;principal component analyzer;nearest-neighbor technique;feature vector;self organizing feature map;homogeneous mark cluster;fuzzy rule set;fuzzy classification scheme,,2,,9,,23-Jan-09,,,IEEE,IEEE Conferences
A clustering-Based KNN improved algorithm CLKNN for text classification,基於聚類的KNN改進算法CLKNN用於文本分類,Lijuan Zhou; Linshuang Wang; Xuebin Ge; Qian Shi,"Information Engineering College, Capital Normal University, Beijing, China; Information Engineering College, Capital Normal University, Beijing, China; Information Engineering College, Capital Normal University, Beijing, China; Information Engineering College, Capital Normal University, Beijing, China","2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)",29-Apr-10,2010,3,,212,215,"As a simple, effective and nonparametric classification method, k Nearest Neighbor (KNN) is widely used in document classification for dealing with the much more difficult problem such as large-scale or many of categories. But KNN classifier may have a problem when training samples are uneven. The problem is that KNN classifier may decrease the precision of classification because of the uneven density of training data. To solve the problem, a new clustering-based KNN method is presented in this paper. It preprocesses training data by using clustering, then classify with a new KNN algorithm, which adopts a dynamic adjustment in each iteration for the neighborhood number K. This method would avoid the uneven classification phenomenon and reduce the misjudgment of the boundary testing samples. We have an experiment in text classification and the result shows that it has good performance.",1948-3422,978-1-4244-5192-0,10.1109/CAR.2010.5456668,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456668,Text Classification;KNN;Clustering,Clustering algorithms;Text categorization;Testing;Classification algorithms;Educational institutions;Nearest neighbor searches;Support vector machines;Support vector machine classification;Robotics and automation;Large-scale systems,pattern classification;pattern clustering;text analysis,text classification;nonparametric classification method;k nearest neighbor;document classification;KNN classifier;clustering-based KNN method;training data;KNN algorithm;dynamic adjustment;boundary testing,,9,,11,,29-Apr-10,,,IEEE,IEEE Conferences
Web Services Classification Based on Intelligent Clustering Techniques,基於智能聚類技術的Web服務分類,H. Gao; W. Stucky; L. Liu,"Sch. of Manage. & Econ., Beijing Inst. of Technol., Beijing, China; Inst. of Appl. Inf. & Formal Description Method (AIFB), Univ. of Karlsruhe, Karlsruhe, Germany; Inst. of Appl. Inf. & Formal Description Method (AIFB), Univ. of Karlsruhe, Karlsruhe, Germany",2009 International Forum on Information Technology and Applications,4-Sep-09,2009,3,,242,245,The better way for Web services discovery is to classify the Web services into different categories when they are published in a registry. The method of intelligent clustering based classification of the Web services is mainly studied. A hybrid solution is proposed for building and populating a taxonomy structure by combining human knowledge with the techniques of artificial intelligence. The Web service is converted into standard vector format through Web service description language (WSDL) document. The self-organizing neural network based learning algorithm and clustering algorithm are designed and implemented to classify the Web services automatically.,,978-0-7695-3600-2,10.1109/IFITA.2009.518,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232105,clustering analysis;artificial intelligence;Web service;neural network;semantic vector space,Web services;Clustering algorithms;Space technology;Humans;Taxonomy;Algorithm design and analysis;XML;Artificial intelligence;Simple object access protocol;Neural networks,learning (artificial intelligence);pattern classification;pattern clustering;self-organising feature maps;Web services,Web services classification;intelligent clustering techniques;Web services discovery;taxonomy structure;human knowledge;artificial intelligence;Web service description language document;self-organizing neural network;learning algorithm,,11,,8,,4-Sep-09,,,IEEE,IEEE Conferences
Tree clustering for layout-based document image retrieval,樹狀聚類，用於基於佈局的文檔圖像檢索,S. Marinai; E. Marino; G. Soda,"Dipt. di Sistemi e Informatica, Univ. di Firenze, Italy; Dipt. di Sistemi e Informatica, Univ. di Firenze, Italy; Dipt. di Sistemi e Informatica, Univ. di Firenze, Italy",Second International Conference on Document Image Analysis for Libraries (DIAL'06),8-May-06,2006,,,9 pp.,253,We describe a system for the retrieval on the basis of layout similarity of document images belonging to collections stored in digital libraries. Layout regions are extracted and represented with the XY tree. The proposed indexing method combines a new tree clustering algorithm (based on self organizing maps) with principal component analysis. The combination of these techniques allows us to retrieve the most similar pages from large collections without the need for a direct comparison of the query page with each indexed document,,0-7695-2531-8,10.1109/DIAL.2006.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612966,,Image retrieval;Indexing;Self organizing feature maps;Principal component analysis;Neurons;Software libraries;Clustering algorithms;Encoding;Tree data structures;Computational efficiency,document image processing;image retrieval;indexing;principal component analysis;trees (mathematics),tree clustering;layout-based document image retrieval;layout similarity;digital libraries;indexing method;self organizing maps;principal component analysis,,9,,15,,8-May-06,,,IEEE,IEEE Conferences
Compression of Chinese Document Images by Complex Shape Matching,基於復雜形狀匹配的中文文檔圖像壓縮,C. Shiah; Y. Yen,NA; NA,The Computer Journal,18-Jan-18,2013,56,11,1292,1304,"In this paper, we present a novel approach for compressing Chinese document images and the procedure consists of four phases. In the first phase, document images are segmented into Chinese characters using connected component analysis. Since the nested property of Chinese characters, disjoint components belonging to a single character will be merged. In the second phase, we extract spatial domain features for each character and locate similar-shaped patterns using shape matching. The shape matching is a modified shape context matching and is suitable for matching complex image patterns. We also include stroke density features for quickly pruning unlikely characters to match each other. In the third phase, similar-shaped characters are grouped into clusters using hierarchical clustering. For each cluster, a representative token image is computed. The number of representative token images can be reduced to the minimum by eliminating redundant characters so that high compression ratio can be achieved. In the last phase, binary and integer arithmetic coding are employed to encode representative token images and their corresponding position data. In our experiments, the results show that (i) The average compression ratio of our proposed method is better than the average compression ratio of the JBIG2 algorithm. (ii) Compared with the JBIG2 algorithm, our method is more suitable for content-based keyword/image searching operations. Both of the above features are the desired attributes for content-based information retrieval in the digital archive of Chinese document images.",1460-2067,,10.1093/comjnl/bxs100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8131373,document image compression;shape matching;symbolic compression;hierarchical clustering,,,,,1,,,,18-Jan-18,,,OUP,OUP Journals
An active learning approach to frequent itemset-based text clustering,一種基於頻繁項集的文本聚類的主動學習方法,R. M. Marcacini; G. N. Corr礙a; S. O. Rezende,"Mathematical and Computer Sciences Institute - ICMC University of S瓊o Paulo - USP - S瓊o Carlos, SP, Brazil; Mathematical and Computer Sciences Institute - ICMC University of S瓊o Paulo - USP - S瓊o Carlos, SP, Brazil; Mathematical and Computer Sciences Institute - ICMC University of S瓊o Paulo - USP - S瓊o Carlos, SP, Brazil",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,3529,3532,"Frequent itemset-based text clustering has emerged as a promising way to automatic organization of text documents, because it allows high clustering accuracy combined with understandable cluster descriptors. However, the clustering results may not be satisfactory because they do not reflect the user's point of view. In this context, active learning is an interesting approach to incorporate the user's knowledge in the text clustering task by querying the users about the data. We introduce an active learning approach to frequent itemset-based text clustering called AL2FIC. In our approach, the users can provide feedback directly on the cluster descriptors without the need to know the document labels. An experimental evaluation on real text collections demonstrated that our AL2FIC approach significantly increases the text clustering performance even when only few descriptors are selected by the users.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460926,,Itemsets;Clustering algorithms;Vectors;Mathematical model;Equations;Data mining;Power capacitors,learning (artificial intelligence);pattern clustering;query processing;text analysis,active learning approach;frequent itemset-based text clustering;automatic organization;text documents;cluster descriptors;user knowledge;AL2FIC;document labels,,1,,18,,14-Feb-13,,,IEEE,IEEE Conferences
MPRK algorithm for clustering the large text datasets,大型文本數據集聚類的MPRK算法,M. Thangarasu; H. H. Inbarani,"Department of Computer Science, Periyar University, Salem; Department of Computer Science, Periyar University, Salem",2016 IEEE International Conference on Advances in Computer Applications (ICACA),30-Mar-17,2016,,,224,229,"Text Document clustering is changing the massive collections of text documents into a lesser amount of suitable clusters. While numerous clustering approaches have been projected in the last few decades, the partitioned clustering algorithms are stated performing well on document clustering based on the reviewed papers. In this research, Modified Parallel Rough K-means (MPRK) algorithm is proposed for clustering the text document and it is evaluated on datasets and the results are compared to benchmark algorithms K-means and DPPSOK-means. The experimental analysis shows the proposed algorithm produces efficient result compared to the existing algorithms.",,978-1-5090-3770-4,10.1109/ICACA.2016.7887955,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887955,Clustering;Text document;Parallel Technique;Rough K-Means;Time complexity,Clustering algorithms;Algorithm design and analysis;Rough sets;Partitioning algorithms;Parallel processing;MATLAB;Program processors,data mining;pattern clustering;text analysis,large text dataset clustering;text document clustering;partitioned clustering algorithms;modified parallel rough k-means;MPRK algorithm;DPPSOK-means;data mining,,,,16,,30-Mar-17,,,IEEE,IEEE Conferences
Bag of Characters and SOM Clustering for Script Recognition and Writer Identification,字符袋和SOM聚類，用於腳本識別和作者識別,S. Marinai; B. Miotti; G. Soda,"Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,2182,2185,"In this paper, we describe a general approach for script (and language) recognition from printed documents and for writer identification in handwritten documents. The method is based on a bag of visual word strategy where the visual words correspond to characters and the clustering is obtained by means of Self Organizing Maps (SOM). Unknown pages (words in the case of script recognition) are classified comparing their vectorial representations with those of one training set using a cosine similarity. The comparison is improved using a similarity score that is obtained taking into account the SOM organization of cluster centroids. Promising results are presented for both printed documents and handwritten musical scores.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.534,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5595942,Self-Organizing Map;Writer Identification;Script Recognition,Visualization;Character recognition;Feature extraction;Indexing;Text analysis;Artificial neural networks,document image processing;handwritten character recognition;pattern clustering;self-organising feature maps,SOM clustering;script recognition;writer identification;handwritten documents;printed documents;bag of visual word strategy;self organizing maps;cosine similarity;handwritten musical scores,,9,,10,,7-Oct-10,,,IEEE,IEEE Conferences
BiblioMapper: a cluster-based information visualization technique,BiblioMapper：基於集群的信息可視化技術,Min Song,"Sch. of Libr. & Inf. Sci., Indiana Univ., Bloomington, IN, USA",Proceedings IEEE Symposium on Information Visualization (Cat. No.98TB100258),6-Aug-02,1998,,,130,136,"The purpose of the paper is to develop a visualization system of a document space, called BiblioMapper, for CISI collections, one of the bibliographic databases available on the Internet. The major function of BiblioMapper is to visualize the document space with a cluster-based visualization technique. The cluster-based visualization technique assembles a set of documents according to semantic similarities. One advantage of this technique is that users are able to focus on and assess each cluster and the documents which the cluster comprises according to their information needs.",,0-8186-9093-3,10.1109/INFVIS.1998.729569,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729569,,Information retrieval;Clustering algorithms;Data visualization;Navigation;Libraries;Visual databases;Testing;Neural networks;Self organizing feature maps;Information science,information needs;Internet;data visualisation;information retrieval;bibliographic systems;document image processing,BiblioMapper;cluster-based information visualization technique;document space;CISI collections;bibliographic databases;Internet;document space visualization;semantic similarities;information needs,,2,,17,,6-Aug-02,,,IEEE,IEEE Conferences
An intelligent project agency for web-3D virtual trading community based on google earth,基於Google Earth的Web-3D虛擬交易社區的智能項目代理,Huaiyu Xu; Hao Zhuang; Xiaoyu Hou; Jun Ge,"Integrated Circuit Applied Software Lab, Software College, Northeastern University, Shenyang, China 110004; Integrated Circuit Applied Software Lab, Software College, Northeastern University, Shenyang, China 110004; Integrated Circuit Applied Software Lab, Software College, Northeastern University, Shenyang, China 110004; Integrated Circuit Applied Software Lab, Software College, Northeastern University, Shenyang, China 110004",2009 2nd IEEE International Conference on Computer Science and Information Technology,11-Sep-09,2009,,,149,152,"With the advent of Google Earth, it is an interesting idea that we build a Web-3D virtual trading community where users can have their own houses or offices and engage in trading activities with each other. Thus, it is necessary to develop an intelligent project agency (IPA) to deal with the huge amount of project information. IPA is an effective way of not only filtering and clustering the users' requirements accurately, but providing interface of high performance for search engine and sending the matching information to the users automatically as well. In order to achieve these targets, IPA is proposed in this paper. The architecture of IPA is described and its detailed implementation is also explained. Additionally, we demonstrate that IPA can provide users with the most serviceable information and shorten the period of trading greatly.",,978-1-4244-4519-6,10.1109/ICCSIT.2009.5234591,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234591,Project Agency;Web-3D Virtual Community;Document Cluster,Earth;Information filtering;Information filters;Matched filters;Search engines;Internet;Educational institutions;Employment;Business;Intellectual property,document handling;information filtering;Internet;pattern clustering;pattern matching;project management;search engines;user interfaces;virtual enterprises,Web-3D virtual trading community;IPA;intelligent project agency;Google earth;user requirement clustering;user requirement filtering;user interface;search engine;information matching;massive documents clustering,,,,10,,11-Sep-09,,,IEEE,IEEE Conferences
Integrating element and term semantics for similarity-based XML document clustering,使用多視圖的文檔分類,Jianwu Yang; W. K. Cheung; Xiaoou Chen,"Inst. of Comput. Sci. & Tech., Peking Univ., Beijing, China; NA; NA",The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),17-Oct-05,2005,,,222,228,"Structured link vector model (SLVM) is a recently proposed document representation that takes into account both structural and semantic information for measuring XML document similarity. Its formulation includes an element similarity matrix for capturing the semantic similarity between XML elements - the structural components of XML documents. In this paper, instead of applying heuristics to define the similarity matrix, we proposed to learn the matrix using pair wise similar training data in an iterative manner. In addition, we extended SLVM to SLVM-LSI by incorporating term semantics into SLVM using latent semantic indexing, with the element similarity related properties of the original SLVM preserved. For performance evaluation, we applied SLVM-LSI to similarity-based clustering of two XML datasets and the proposed SLVM-LSI was found to significantly outperform the conventional vector space model and the edit-distance based methods. The similarity matrix, obtained as a byproduct via the learning, can provide higher level knowledge about the semantic relationship between the XML elements.",,0-7695-2415-X,10.1109/WI.2005.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517846,,XML;Training data;Virtual manufacturing;Indexing;Text analysis;Information analysis;Fourier transforms;Kernel;Tree data structures,XML;document handling;matrix algebra;pattern clustering;indexing,element semantics;term semantics;XML document clustering;structured link vector model;XML document similarity;element similarity matrix;semantic similarity;pair wise similar training data;SLVM-LSI;latent semantic indexing;similarity-based clustering;vector space model;edit-distance method,,6,,16,,17-Oct-05,,,IEEE,IEEE Conferences
Ranking Web Pages Using Cosine Similarity Measure,基於筆劃聚類的自動原型筆劃生成，用於在線手寫日語字符識別,S. Irfan; S. Ghosh,"Galgotias University Greater Noida,Uttar Pradesh,India; Galgotias University Greater Noida,Uttar Pradesh,India","2019 International Conference on Computing, Power and Communication Technologies (GUCON)",27-Dec-19,2019,,,867,870,The work presented in this paper throws light on various models of Information Retrieval and proposed an approach to rank web pages using document clustering on the basis of content similarity. The paper presents a method where the content of the documents are matched on the basis of the query and thereby the most similar documents are clustered together for the ranking process. The given approach overall reduces the computation complexity and help in retrieving and ranking the most relevant pages in less amount of time.,,978-93-5351-098-5,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940454,Information Retrieval;Web Mining;Clustering;Similarity Measure,Web pages;Hypertext systems;Web mining;Task analysis;Clustering algorithms,document handling;information retrieval;Internet;pattern clustering;search engines,Web page ranking;cosine similarity measure;Information Retrieval;document clustering;content similarity;ranking process,,,,14,,27-Dec-19,,,IEEE,IEEE Conferences
Feature Reduction for Text Categorization Using Cluster-Based Discriminant Coefficient,基於改進的界標半定嵌入的文檔聚類算法,L. Gao; B. Chien,"Dept. of Comput. Sci. & Inf. Eng., Nat. Univ. of Tainan, Tainan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Univ. of Tainan, Tainan, Taiwan",2012 Conference on Technologies and Applications of Artificial Intelligence,10-Jan-13,2012,,,137,142,"Text classification is an important research topic for managing numerous electronic documents. Feature reduction is the key issue for text classification with high dimensional keywords. A document analysis method called discriminant coefficient was proposed to reduce features and achieve high precision text classification. However, the main problem of the discriminant based feature reduction method is that the final number of reduced features is exactly equal to the number of document classes. Although the precisions of classification are high in such a method, the recalls are relatively low. In this paper, we propose an improvement on the analyzing method indiscriminant coefficients. We apply a simple clustering method to distinguish the documents in each document class to reserve hidden differences among keywords in the same class. The clustering results can help to adjust the number of reduction features flexibly. The experimental results show that the proposed clustering mechanism supports adaptive features reduction and both of the recall and F1 measurements are improved.",2376-6824,978-1-4673-4976-5,10.1109/TAAI.2012.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395020,classification;feature reduction;feature clustering;discriminant coefficient,Feature extraction;Text categorization;Clustering algorithms;Classification algorithms;Algorithm design and analysis;Clustering methods;Matrix converters,feature extraction;pattern classification;pattern clustering;statistical analysis;text analysis,text categorization;cluster-based discriminant coefficient;electronic documents;high dimensional keywords;document analysis method;discriminant coefficient;high-precision text classification;discriminant based feature reduction method;clustering mechanism;F1 measurements,,,,17,,10-Jan-13,,,IEEE,IEEE Conferences
Indexing and retrieval of words in old documents,使用多尺度聚類的文檔頁面細分,S. Marinai; E. Marino; G. Soda,"Florence Univ., Italy; Florence Univ., Italy; Florence Univ., Italy","Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,223,227 vol.1,"This paper describes a system for efficient indexing and retrieval of words in collections of document images. The proposed method is based on two main principles: unsupervised prototype clustering, and string encoding for efficient string matching. During indexing, a self organizing map (SOM) is trained so as to cluster together similar symbols (character-like objects) in a sub-set of the documents to be stored. By using the trained SOM the words in the whole collection can be stored and represented with a fixed-length description that can be easily compared in order to score most similar words in response to a user query. The system can be automatically adapted to different languages and font styles. The most appropriate applications are for the processing of old documents (18th and 19th Centuries) where current OCRs have more difficulties. Experimental results describe three application scenarios having various levels of difficulty for current OCR systems.",,0-7695-1960-1,10.1109/ICDAR.2003.1227663,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227663,,Indexing;Optical character recognition software;Image retrieval;Encoding;Organizing;Content based retrieval;Data mining;Prototypes;Software libraries;Internet,indexing;information retrieval;image matching;document image processing;feature extraction;character recognition,word indexing;word retrieval;prototype clustering;string encoding;string matching;self organizing map;SOM;OCR;optical character recognition;text recognition,,13,,9,,8-Sep-03,,,IEEE,IEEE Conferences
A topic based indexing approach for searching in documents,基於XML的EMR模糊聚類研究。,D. Osuna-Ontiveros; I. Lopez-Arevalo; V. Sosa-Sosa,"Information Technology Laboratory, CINVESTAV - IPN, Tamaulipas, Mexico; Information Technology Laboratory, CINVESTAV - IPN, Tamaulipas, Mexico; Information Technology Laboratory, CINVESTAV - IPN, Tamaulipas, Mexico","2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control",19-Dec-11,2011,,,1,6,"Nowadays, users of computers store a lot of text documents. This requires fast and precise searches over documents. The goal of Information Retrieval (IR) models is to provide users with those documents that will satisfy their information needs. The core of such models is the document representation used in the indexing of documents. Traditional IR models handle the frequency of query terms. The disadvantage of these models is that they exclusively consider terms in the query and ignore similar terms. This paper proposes a topic based indexing approach to represent topics associated to documents. Documents are modeled by using clustering algorithms based on natural language processing. As result of this proposal is a document-topic matrix representation denoting the importance of topics inside documents. In a similar way, each query over documents is converted into a vector of topics. Thus, a similarity measure can be applied over this vector and the matrix of documents to retrieve the most relevant documents.",,978-1-4577-1013-1,10.1109/ICEEE.2011.6106659,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6106659,Language technologies for IR;Semi-structured information retrieval;Semantic search,Information retrieval;Vectors;Proposals;Semantics;Mathematical model;Computational modeling;Indexing,document handling;indexing;information needs;matrix algebra;natural language processing;pattern clustering;query formulation;vectors,topic based indexing approach;document searching;information retrieval model;information needs;document representation;document indexing;clustering algorithm;natural language processing;document-topic matrix representation;document querying;topics vector;similarity measure;document matrix,,,,18,,19-Dec-11,,,IEEE,IEEE Conferences
An algorithm for conceptual clustering of Chinese text,模糊文檔聚類的魯棒算法,Zhi Cai; Huan-Tong Geng; Xin Zhao; Qing-Sheng Cai,"Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, China; Dept. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, China",Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),24-Jan-05,2004,5,,3035,3039 vol.5,"In this paper, an algorithm for conceptual clustering of Chinese text is presented. Authors adopt ontology - Hownet, use VSM (vector space model) to represent document. Then, the authors cluster the document by a partitioned algorithm. The test results show this algorithm is more efficient than the traditional text clustering method based on the keywords set.",,0-7803-8403-2,10.1109/ICMLC.2004.1378553,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1378553,,Clustering algorithms;Ontologies;Documentation;Partitioning algorithms;Space technology;Taxonomy;Computer aided instruction;Clustering methods;Text mining;Tree data structures,text analysis;pattern clustering;ontologies (artificial intelligence),conceptual clustering;Chinese text clustering;ontology;Hownet;vector space model;document clustering,,2,,9,,24-Jan-05,,,IEEE,IEEE Conferences
Application of Query Sensitive Similarity Measure in IR systems,雙手式體積文檔語料庫管理,S. Hasanzadeh; A. Keshavarzi,"Marvdasht Branch, Islamic Azad Univ., Marvdasht; Marvdasht Branch, Islamic Azad Univ., Marvdasht",2009 Third Asia International Conference on Modelling & Simulation,12-Jun-09,2009,,,73,78,"Document clustering has been widely used in information retrieval systems in order to improve the efficiency and also the effectiveness of ranked output systems using cluster hypothesis. This hypothesis states that relevant documents tend to be more similar to each other than to non-relevant documents, and therefore tend to appear in the same clusters. So far, the effectiveness of cluster hypothesis experimentally has been examined only for static-clustering and query-specific clustering using cosine similarity measure. On the other hand, the effectiveness of document clustering using query-sensitive similarity measure (QSSM) has been studied only with N-nearest neighbor test for very small and topic-specific document collections. In this paper, the cluster hypothesis for query-specific clustering is investigated using a query-sensitive similarity measure and a large document collection in an experimental environment. The results show that the cluster hypothesis holds for query-specific clustering using employed QSSM. And, the effectiveness of query-specific clustering will increase through the use of that QSSM.",2376-1172,978-1-4244-4154-9,10.1109/AMS.2009.116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071961,Query sensitive similarity measure;document clustering;information retrieval,Information retrieval;Clustering methods;Asia;System testing;Scattering;Performance evaluation;Information resources,document handling;information retrieval systems;pattern clustering;query processing,query sensitive similarity measure;IR system;document clustering;information retrieval system;cluster hypothesis,,,,17,,12-Jun-09,,,IEEE,IEEE Conferences
An across-scale fusion approach to segment document image,排序質心投影：具有自組織圖的數據可視化方法,Qingshneg Zhu; Hongfu Wu; Qian Wang; Zhengyu Zhu,"Dept. of Comput. Sci., Chongqing Univ., China; Dept. of Comput. Sci., Chongqing Univ., China; Dept. of Comput. Sci., Chongqing Univ., China; Dept. of Comput. Sci., Chongqing Univ., China","2002 IEEE Region 10 Conference on Computers, Communications, Control and Power Engineering. TENCOM '02. Proceedings.",28-Feb-03,2002,1,,553,556 vol.1,"Segmenting an image into text and picture area is very important for efficiently compressing document images. The paper introduces an across-scale fusion approach to segment document images, which makes use of the multiscale down-sampling bi-level images and a Markov tree model in order to directly calculate the classification based original image to be segmented. The main process of the method is divided into a classification stage and a segmentation stage. In the first stage, we separate picture from text blocks based on multiscale bi-level images. Then we segment the image into the foreground level and the background level in the second stage. The paper describes in detail a multiscale representation of the image, the Markov tree, likelihood computation for classifying image blocks, and a modified bi-color clustering algorithm for segmenting the non-picture area.",,0-7803-7490-8,10.1109/TENCON.2002.1181335,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181335,,Image segmentation;Image coding;Pixel;Classification tree analysis;Software libraries;Wavelet domain;Statistics;Probability density function;Computer science;Clustering algorithms,document image processing;image segmentation;image classification;Markov processes,across-scale fusion approach;document image segmentation;document image compression;multiscale down-sampling bi-level images;Markov tree model;classification based original image;classification stage;segmentation stage;text blocks;foreground level;background level;multiscale image representation;likelihood computation;modified bi-color clustering algorithm,,,,8,,28-Feb-03,,,IEEE,IEEE Conferences
OntoSIR: An OAI service for multi-collection document retrieval based on ontologies of metadata records,基於積分結構和CC聚類的廣告圖像漢字定位方法,M. A. Medina; J. A. Sanchez; Y. Ostrovskaya; N. R. Brisaboa,"Univ. Tecnologica de la Mixteca, Oaxaca, Mexico; NA; NA; NA",Third Latin American Web Congress (LA-WEB'2005),6-Mar-06,2005,,,4 pp.,,"This paper describes the design and implementation of OntoSIR, an asynchronous service for document retrieval from multiple collections. Our approach builds hierarchical structures to support retrieval tasks based on harvesting of metadata records, document clustering techniques and information retrieval methods. This service provides an alternative searching mechanism for the Open Archives Initiative community.",,0-7695-2471-0,10.1109/LAWEB.2005.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1592366,,Ontologies;Information retrieval;Software libraries;Information resources;Access protocols;Frequency;Clustering algorithms;XML,information retrieval;meta data;ontologies (artificial intelligence);document handling,OntoSIR;OAI service;multicollection document retrieval;metadata record;document clustering technique;information retrieval method,,3,,14,,6-Mar-06,,,IEEE,IEEE Conferences
"Web document clustering based on Global-Best Harmony Search, K-means, Frequent Term Sets and Bayesian Information Criterion",使用遺傳算法的基於聚類的最佳摘要生成,C. Cobos; J. Andrade; W. Constain; M. Mendoza; E. Le籀n,University of Cauca; University of Cauca; University of Cauca; University of Cauca; National University of Colombia,IEEE Congress on Evolutionary Computation,27-Sep-10,2010,,,1,8,"This paper introduces a new description-centric algorithm for web document clustering based on the hybridization of the Global-Best Harmony Search with the K-means algorithm, Frequent Term Sets and Bayesian Information Criterion. The new algorithm defines the number of clusters automatically. The Global-Best Harmony Search provides a global strategy for a search in the solution space, based on the Harmony Search and the concept of swarm intelligence. The K-means algorithm is used to find the optimum value in a local search space. Bayesian Information Criterion is used as a fitness function, while FP-Growth is used to reduce the high dimensionality in the vocabulary. This resulting algorithm, called IGBHSK, was tested with data sets based on Reuters-21578 and DMOZ, obtaining promising results (better precision results than a Singular Value Decomposition algorithm). Also, it was also then evaluated by a group of users.",1941-0026,978-1-4244-6911-6,10.1109/CEC.2010.5586109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586109,,Clustering algorithms;Partitioning algorithms;Vocabulary;Classification algorithms;Algorithm design and analysis;Heuristic algorithms;Nickel,Bayes methods;document handling;Internet;particle swarm optimisation;pattern clustering;search problems,Web document clustering;global-best harmony search;k-means clustering;frequent term sets;Bayesian information criterion;description-centric algorithm;swarm intelligence;local search space;vocabulary,,15,,39,,27-Sep-10,,,IEEE,IEEE Conferences
Segmentation and classification for mixed text/image documents using neural network,基於句子聚類和提取的自動文本摘要,S. Imade; S. Tatsuta; T. Wada,NA; NA; NA,Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93),6-Aug-02,1993,,,930,934,"A segmentation and classification method for separating a document image into printed character, handwritten character, photograph, and painted image regions is presented. A document image is segmented into rectangular areas. Each of which contains a cluster of image elements. A layered feed-forward neural network is then used to classify each segmented area using the histograms of gradient vector directions and luminance levels. A high classification performance was obtained, even with a small number of training samples. It is confirmed that the histograms of gradient vector directions and luminance levels are significantly effective features for the classification of the four kinds of image regions. Increasing the number of the discrimination areas improves the classification performance sufficiently even using a small number of training samples for the neural network.<>",,0-8186-4960-7,10.1109/ICDAR.1993.395584,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395584,,Image segmentation;Neural networks;Image coding;Image converters;Histograms;Image storage;Data compression;Pixel;Optical imaging;High speed optical techniques,document image processing;image recognition;handwriting recognition;feedforward neural nets;image classification;image segmentation,mixed text/image documents;segmentation;classification;printed character;handwritten character;photograph;painted image;rectangular areas;image elements;layered feed-forward neural network;histograms;gradient vector directions;luminance levels;classification performance;training samples;discrimination areas,,23,,5,,6-Aug-02,,,IEEE,IEEE Conferences
Automatic and Adaptive Clusters for Information Extraction,結合K均值和局部搜索機制的文本聚類算法,B. S. Charulatha; P. Rodrigues; T. Chitralekha,"JNTUK, Kakinada, India; DMI College of Engineering, Chennai, India; Central Univ., Pondicherry, India",2014 International Conference on Soft Computing and Machine Intelligence,6-Apr-15,2014,,,60,63,"The web pages are heterogeneous and unstructured. The heterogeneity is due to the hybrid nature of the documents. The unstructureness is due to either multilingual or multimedia content in the web page. The mining should be independent of the language and software. The objective is when any data or content mining is done on a set of data is chosen to form the basis as done with keywords. If the base data is chosen arbitrarily, it is automatic, whereas some 'knowledge' or 'background' is put in the choice it is adaptive. Statistical features of the images are extracted from the pixel map of the image. The extracted features are presented to clustering algorithms, Fuzzy C Means and Subtractive clustering algorithm. The algorithm classifies the given image as a text or image representation. The accuracy of classification is compared and presented.",,978-1-4673-6751-6,10.1109/ISCMI.2014.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079355,heterogeneous;unstructured;multimedia;statistical features;clustering;Fuzzy c means;subtractive clustering accuracy,Clustering algorithms;Feature extraction;Web pages;Data mining;Classification algorithms;Image representation;Accuracy,data mining;feature extraction;fuzzy set theory;image classification;image representation;Internet;pattern clustering;statistical analysis,automatic clusters;adaptive clusters;information extraction;Web pages;multilingual content;multimedia content;data mining;content mining;image statistical feature extraction;image pixel map;fuzzy C means algorithm;subtractive clustering algorithm;image classification;image representation,,,,6,,6-Apr-15,,,IEEE,IEEE Conferences
Robust text-line and word segmentation for handwritten documents images,使用詞庫的查詢擴展在改進馬來聖訓系統中的應用,T. Stafylakis; V. Papavassiliou; V. Katsouros; G. Carayannis,"Institute for Language and Speech Processing of Athena - Research and Innovation Center in Information, Communication and Knowledge Technologies, Athens, Greece; Institute for Language and Speech Processing of Athena - Research and Innovation Center in Information, Communication and Knowledge Technologies, Athens, Greece; Institute for Language and Speech Processing of Athena - Research and Innovation Center in Information, Communication and Knowledge Technologies, Athens, Greece; Institute for Language and Speech Processing of Athena - Research and Innovation Center in Information, Communication and Knowledge Technologies, Athens, Greece","2008 IEEE International Conference on Acoustics, Speech and Signal Processing",12-May-08,2008,,,3393,3396,"This paper addresses the problem of automatic text-line and word segmentation in handwritten document images. Two novel approaches are presented, one for each task. In text-line segmentation a Viterbi algorithm is proposed while an SVM-based metric is adopted to locate words in each text-line. The overall algorithm was tested in the ICDAR2007 handwriting segmentation contest and showed highly promising results.",2379-190X,978-1-4244-1483-3,10.1109/ICASSP.2008.4518379,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518379,Document image processing;handwriting recognition;image segmentation;Viterbi estimation;support vector machines,Robustness;Image segmentation;Statistics;Viterbi algorithm;Probability;Handwriting recognition;Clustering algorithms;Carbon capture and storage;Natural languages;Speech processing,document image processing;image segmentation;maximum likelihood estimation;support vector machines,text-line segmentation;word segmentation;handwritten documents images;Viterbi algorithm;SVM-based metric,,18,,8,,12-May-08,,,IEEE,IEEE Conferences
An improved optimized clustering technique for crime detection,基於層次聚類的智能信息檢索方法,N. Tomar; A. K. Manjhvar,"Dept. of CSE/IT, MITS Gwalior, India; Dept. of CSE/IT, MITS, Gwalior, India",2016 Symposium on Colossal Data Analysis and Networking (CDAN),19-Sep-16,2016,,,1,5,"Data mining automates the finding predictive records procedure in big databases. Clustering is a most famous method in data mining and is an important methodology that is performed based on the similarity principle. The segregation of a big database is a stimulating and task of time consuming. It concludes two different stages: first, feature extraction maps all documents or record to a point in the space of high-dimensional, then algorithms for clustering automatically grouping the points into a cluster hierarchy. Clustering has various applications in different fields. Few of the fields include criminology, text mining, image resolution, machine learning. Crime detection has become one of the most attractive field as the crime rate in India and whole world is increasing at a greater pace. We as citizens of a country have to contribute towards its detection and removal. Thus, a comprehensive survey carried about the basics of clustering has given in this paper. Moreover, proposed work was given that gives the idea of the work going to be done in the upcoming time.",,978-1-5090-0669-4,10.1109/CDAN.2016.7570880,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570880,Clustering;partitioning;data mining;crime;k-means method optimization,Clustering algorithms;Data mining;Databases;Data analysis;Partitioning algorithms;Algorithm design and analysis;Clustering methods,data mining;feature extraction;pattern clustering;police data processing,optimized clustering;crime detection;data mining;big databases;similarity principle;feature extraction;high-dimensional space;cluster hierarchy;criminology;text mining;image resolution;machine learning;crime rate;India,,1,,17,,19-Sep-16,,,IEEE,IEEE Conferences
Document Representation and Dimension Reduction for Text Clustering,使用統計和語義數據進行文本聚類,M. Shafiei; S. Wang; R. Zhang; E. Milios; B. Tang; J. Tougas; R. Spiteri,"Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada",2007 IEEE 23rd International Conference on Data Engineering Workshop,10-Dec-07,2007,,,770,779,"Increasingly large text damsels and the high dimensionality associated with natural language create a great challenge in text mining, In this research, a systematic study is conducted. in which three different document representation methods for text are used, together with three Dimension Reduction Techniques (DRT), in the context of the text clustering problem. Several standard benchmark datasets are used. The three Document representation methods considered are based on the vector space model, and they include word, multi-word term, and character N-gram representations. The dimension reduction methods are. independent component analysis (ICA). latent semantic indexing (LSI), and a feature selection technique based on Document Frequency (DF). Results are compared in terms of clustering performance, using the k-means clustering algorithm. Experiments show that ICA and LSI are clearly belter than DF on all darascls. For word and N-gram representation. ICA generally gives better results compared with LSI. Experiments also show that the word representation gives better clustering results compared to term and N-gram representation. Finally, for the N-gram representation, it is demonstrated that a profile length (before dimensionality reduction) of 2000 is sufficient to capture the information and in most cases, a -4-gram representation gives better performance than 3-gram representation.",,978-1-4244-0831-3,10.1109/ICDEW.2007.4401066,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401066,,Independent component analysis;Large scale integration;Indexing;Clustering algorithms;Data mining;Computer science;Natural languages;Text mining;Frequency;Communications technology,data mining;data reduction;data structures;feature extraction;independent component analysis;natural languages;pattern clustering;text analysis,document representation;dimension reduction;text clustering;natural language;text mining;vector space model;multiword term;character N-gram representation;independent component analysis;latent semantic indexing;feature selection;document frequency;k-means clustering;word representation,,20,,26,,10-Dec-07,,,IEEE,IEEE Conferences
Using K-means cluster based techniques in external plagiarism detection,投影距離聚類和偽貝葉斯判別函數用於手寫數字識別,K. Vani; D. Gupta,"Department of Computer Science, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bangalore, India; Department of Mathematics, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bangalore, India",2014 International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-15,2014,,,1268,1273,"Text document categorization is one of the rapidly emerging research fields, where documents are identified, differentiated and classified manually or algorithmically. The paper focuses on application of automatic text document categorization in plagiarism detection domain. In today's world plagiarism has become a prime concern, especially in research and educational fields. This paper aims on the study and comparison of different methods of document categorization in external plagiarism detection. Here the primary focus is to explore the unsupervised document categorization/ clustering methods using different variations of K-means algorithm and compare it with the general N-gram based method and Vector Space Model based method. Finally the analysis and evaluation is done using data set from PAN-20131 and performance is compared based on precision, recall and efficiency in terms of time taken for algorithm execution.",,978-1-4799-6629-5,10.1109/IC3I.2014.7019659,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019659,Text Document Categorization;External Plagiarism;Candidate Retrieval;N-gram;Vector Space Model;K-means Clustering,Plagiarism;Clustering algorithms;Algorithm design and analysis;Vectors;Classification algorithms;Informatics;Partitioning algorithms,pattern clustering;text analysis,K-means cluster based techniques;external plagiarism detection;automatic text document categorization;plagiarism detection domain;world plagiarism;educational fields;unsupervised document categorization;clustering methods;K-means algorithm;N-gram based method;vector space model based method;data set;algorithm execution,,9,,20,,26-Jan-15,,,IEEE,IEEE Conferences
Persian Web Pages Clustering Improvement: Customizing the STC Algorithm,用於網頁聚類的K-Means混合和和諧搜索方法,M. Azadnia; S. Rezagholizadeh; A. Yari,"Inf. Technol. Dept., Iran Telecommun. Res. Center, Tehran, Iran; Inf. Technol. Dept., Inst. for Adv. Studies in Basic Sci., Zanjan, Iran; Inf. Technol. Dept., Iran Telecommun. Res. Center, Tehran, Iran",2009 Fourth International Conference on Computer Sciences and Convergence Information Technology,31-Dec-09,2009,,,717,722,"Today the Internet in almost all ethnic groups and cultures is found and the Web pages are developing very quickly in most countries and different languages. Considering the size and incoherent available information in the Internet has made the use of search engines obvious and necessary. Since search engines pay less attention to the linguistics and content features of documents in different languages and cultures, just uses the pages genuine content similarities, to provide the needs of users, will not be that successful. Regarding the fact, search engines for more effective retrieval and clustering Web pages should consider the linguistics, contents, characteristics and properties of languages. More over they should develop ways to eliminate the complexity of languages as well as using linguistic features to cluster Web pages more effective. In this paper a method for clustering and ranking Web pages in Persian language including its contents and linguistic properties has been developed. Clustering scheme provided based on STC algorithm is one of the best algorithms in clustering text documents. The main idea of this method includes some pre-processing phase to overcome the complexity of linguistic feature in Persian language. Open Source tools are available for these pre-processing steps and there is no need to implement them, simply some changes in their modules may be needed. Some of these pre-processing steps are extract phrases, parse sentences, remove stop words and also add neighbor pages pointed terms to the collection of phrases. All steps in this method have a linear behavior in time order and can apply to the large data sets. This means the proposed method in our research is scalable for mass document sources as the Web.",,978-1-4244-5244-6,10.1109/ICCIT.2009.295,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368875,Clustering;STC Algorithm;Persian Web,Web pages;Clustering algorithms;Information technology;Search engines;Internet;Data mining;Information retrieval;Clustering methods;Telecommunication computing;Content based retrieval,natural language processing;pattern clustering;search engines;text analysis,Web pages clustering;STC algorithm;search engines;Persian language;open source tools;mass document source;text document clustering,,,,7,,31-Dec-09,,,IEEE,IEEE Conferences
Analysis for classification of similar documents among various websites using rapid miner,文檔摘要中基於本體的結構化餘弦相似度：應用於基於移動音頻的知識管理,P. Kaur; S. S. Khurm; G. S. Josan,"Dept. of CSE, ACE, Ambala, India; Dept. of CSE &IT, BMSCE, Muktsar, India; Dept. of CSE & IT, Punjabi Univ., Patiala, India",2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT),3-Apr-14,2014,,,465,470,"The Web was intended to improve the management of general information about accelerators and experiments. It is also considered the most precious place for Information Retrieval and Knowledge Discovery. While retrieving information through queries inserted by the users, a search engine results in a large and non manageable collection of documents. Several web mining tools are used to classify, analyse and order the documents so that users can easily navigate through the search results and find the desired documents. A more efficient way to organize the documents can be a combination of similarity and ranking, where similarity can group the documents in terms of contents or distance and ranking can be applied for ordering the pages within each cluster or set. Based on this approach, in this paper, an analysis is being shown that provides ordered results in the form of similar documents among several set of website which are of users interest using an open source web mining tool called as rapid miner. This approach helps user to restrict their search to navigate less number of pages instead of huge documents in particular which are of their interest.",,978-1-4799-2900-9,10.1109/ICICICT.2014.6781327,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781327,World Wide Web;Document;Page Rank,Navigation;World Wide Web,data mining;document handling;pattern classification;public domain software;query processing;search engines;Web sites,similar document classification analysis;Websites;rapid miner;information retrieval;knowledge discovery;query;search engine;similarity;ranking;document organization;open source Web mining tool,,,,8,,3-Apr-14,,,IEEE,IEEE Conferences
Clustering for Traceability Managing in System Specifications,一種高效的隱私保護排名關鍵詞搜索方法,M. Mezghani; J. Kang; E. Kang; F. Sedes,"Semios for requirements, One Light studio; Fortia Financial Solutions; Semios for requirements, One Light Studio; IRIT, University of Toulouse, CNRS",2019 IEEE 27th International Requirements Engineering Conference (RE),5-Dec-19,2019,,,257,264,System specifications are generally organized according to several documents hierarchies levels linked in order to represent the traceability information. Requirements engineering experts verify manually the links between each specification which allows to generate a traceability matrix. The purpose of this paper is to automatize the generation of the traceability matrix since it is a time consuming and costly task. We propose an artificial intelligence based approach to deal with this problem through a clustering approach. This latter is an unsupervised algorithm that doesn't need any prior knowledge on the language neither the domain of the specifications. Our approach generates duplicates and clusters containing linked requirements. We experiment our approach in an aeronautic domain and a space domain. We obtain better results for high level specifications especially with a pre-processing.,2332-6441,978-1-7281-3912-8,10.1109/RE.2019.00035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920512,Requirements engineering;Traceability;Clustering;System specifications documents;Documents hierarchies,Tools;Task analysis;Clustering algorithms;Hypertext systems;Navigation;Manuals;Requirements engineering,artificial intelligence;formal specification;formal verification;pattern clustering;systems analysis,system specifications;documents hierarchies levels;traceability information;requirements engineering experts;traceability matrix;artificial intelligence;clustering approach;clusters;linked requirements;high level specifications,,,,14,,5-Dec-19,,,IEEE,IEEE Conferences
Web Text Clustering for Personalized E-learning Based on Maximal Frequent Itemsets,通過路徑通用性進行有效的XML聚類：一種高效且可擴展的算法,Z. Su; W. Song; M. Lin; J. Li,"Coll. of Inf. Eng., North China Univ. of Technol., Beijing; Coll. of Inf. Eng., North China Univ. of Technol., Beijing; Coll. of Inf. Eng., North China Univ. of Technol., Beijing; Coll. of Inf. Eng., North China Univ. of Technol., Beijing",2008 International Conference on Computer Science and Software Engineering,22-Dec-08,2008,6,,452,455,"With the rapid development of the network technique and the prevalence of the Internet, e-learning has become the major trend of the development of international education since 1980s, and the important access for the internationalization and the information of education. To meet the personalized needs of learners in e-learning, a new Web text clustering method for personalized e-learning based on maximal frequent itemsets is proposed. Firstly, the Web documents are represented by vector space model. Then, maximal frequent word sets are discovered. Finally, based on a new similarity measure of itemsets, maximal itemsets are used for clustering documents. Experimental results show that the proposed method is effective.",,978-0-7695-3336-0,10.1109/CSSE.2008.1639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4723295,,Electronic learning;Itemsets;Clustering algorithms;Data mining;Clustering methods;Text mining;Partitioning algorithms;Databases;Computer science;Software engineering,computer aided instruction;Internet;pattern clustering;text analysis,Web text clustering;personalized e-learning;maximal frequent itemset;Internet;international education;Web document;vector space model;maximal frequent word set;similarity measure;document clustering,,5,,13,,22-Dec-08,,,IEEE,IEEE Conferences
Short Text Clustering Algorithm Based on Frequent Closed Word Sets,使用相似性度量的基於頻繁術語的文本文檔聚類：一種新穎的方法,C. Jin; Q. Bai,"Huaiyin Institute of Technology,Faculty of Computer and Software Engineering,Huaian,China; Huaiyin Institute of Technology,Faculty of Automation,Huaian,China",2019 12th International Symposium on Computational Intelligence and Design (ISCID),14-May-20,2019,2,,267,270,"The text mining of micro-blog topic information can effectively obtain the attention degree of internet users for news events. It is of great significance in the field of public opinion monitoring and analysis. At the situation of the algorithm of traditional frequent word set is suitable for long text information clustering, this paper proposes to mine top-K frequent corpus in short text database and then to divide micro-blog topic texts covering the same frequent word sets into the same cluster. Combined with the largest frequent word-sets for similarity calculation, the overlapped document is re-divided to achieve micro-blog short text clustering. The experimental results of micro-blog topic dataset and the comparison with K-means clustering algorithm show that the proposed algorithm can effectively solve the sparseness and high-dimension problem of micro-blog topic short text clustering and greatly improve the micro-blog short text clustering effect.",2473-3547,978-1-7281-4653-9,10.1109/ISCID.2019.10144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092562,frequent closed word sets;microblog topic;text clustering,Clustering algorithms;Text mining;Prediction algorithms;Market research;Blogs;Itemsets,data mining;Internet;pattern clustering;Web sites,text mining;microblog topic information;Internet users;public opinion monitoring;long text information clustering;top-K frequent corpus;short text database;microblog topic dataset;microblog topic short text clustering;microblog short text clustering effect;frequent closed word sets,,,,10,,14-May-20,,,IEEE,IEEE Conferences
"Phoneme inventory, trigrams and geographic location as features for clustering different philippine languages",使用輕量級本體從多個集合中檢索文檔,A. Dela Cruz; N. Oco; L. R. Syliongka; R. E. Roxas,"College of Computer Studies, National University, Philippines; College of Computer Studies, National University, Philippines; College of Computer Studies, National University, Philippines; College of Computer Studies, National University, Philippines",2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA),4-May-17,2016,,,137,140,"In this paper, orthographic, geographic and phonetic features were explored to cluster 32 Philippine languages and identify closely-related languages. For the orthographic data, we collected religious text documents online and 100,000 words per language were used as training data. These words were cleaned and trigram profiles were generated. For the geographic feature, we used the location where the language is spoken. For the phonetic feature, the phoneme inventory of the languages was utilized. The languages were clustered using two clustering algorithms, hierarchical and k-means algorithm. Purity was used as an evaluation metric to validate the clusters made. For both hierarchical clustering and k-means algorithm, the highest purity value of a cluster is 0.67, this is an indication that members in a particular cluster have similar attributes. As future work, semantic features can be added to improve the data set and additional languages can be considered.",2472-7695,978-1-5090-3516-8,10.1109/ICSDA.2016.7918999,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918999,clustering;Philippine languages;language similarity;trigrams;geographic location;phoneme inventory,Clustering algorithms;Measurement;Standardization;Speech;Databases;Phylogeny;Algorithm design and analysis,natural language processing;pattern clustering;speech processing,phonetic features;trigram features;geographic location features;Philippine language clustering;phoneme inventory;k-means algorithm;hierarchical clustering;cluster purity value,,,,14,,4-May-17,,,IEEE,IEEE Conferences
Clustering method based on fuzzy equivalence relation,標記科學文章的層次聚類,A. B. Raut; G. R. Bamnote,"Department of Computer Science & Engg., HVPM's COET, Amravati, India; Department of Computer Science & Engg., PRMITR, Badnera, Amravati, India",2011 2nd International Conference on Computer and Communication Technology (ICCCT-2011),10-Nov-11,2011,,,666,671,"Fuzzy clustering techniques are used to construct clusters with uncertain boundaries and allows that one object belongs to overlapping clusters with some membership degree. In other words, the fuzzy clustering is to consider not only the belonging status of object to the clusters, but also to consider to what degree do the object belong to the cluster. In this paper, a technique called ?fuzzy hierarchical clustering??is being proposed that creates the clusters of web documents using fuzzy equivalence relation.",,978-1-4577-1386-6,10.1109/ICCCT.2011.6075209,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075209,Web Mining;Clustering;Search Engine;Fuzzy clustering,Clustering algorithms;Web mining;Partitioning algorithms;Clustering methods;Heuristic algorithms;Algorithm design and analysis,data mining;fuzzy set theory;Internet;pattern clustering;search engines;Web sites,fuzzy equivalence relation;uncertain boundaries;membership degree;fuzzy hierarchical clustering techniques;Web document clusters,,2,,21,,10-Nov-11,,,IEEE,IEEE Conferences
Automatic extraction of text regions from document images by multilevel thresholding and k-means clustering,高維稀疏數據子空間聚類的熵權k均值算法,H. N. Vu; T. A. Tran; I. S. Na; S. H. Kim,"Department of Computer Science, Chonnam National University, 77 Yongbong-ro, 500-757 South Korea; Department of Computer Science, Chonnam National University, 77 Yongbong-ro, 500-757 South Korea; Department of Computer Science, Chonnam National University, 77 Yongbong-ro, 500-757 South Korea; Department of Computer Science, Chonnam National University, 77 Yongbong-ro, 500-757 South Korea",2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS),27-Jul-15,2015,,,329,334,"Textual data plays an important role in a number of applications such as image database indexing, document understanding, and image-based web searching. The target of automatic real-life text extracting in document images without character recognition module is to identify image regions that contain only text. These textual regions can then be either input of optical character recognition application or highlighted for user focusing. In this paper we propose a method which consists of three stages-preprocessing which improves contrast of grayscale image, multi-level thresholding for separating textual region from non-textual object such as graphics, pictures, and complex background, and heuristic filter, recursive filter for text localizing in textual region. In many of these applications, it is not necessary to identify all the text regions, therefor we emphasize on identifying important text region with relatively large size and high contrast. Experimental results on real-life dataset images demonstrate that the proposed method is effective in identifying textual region with various illuminations, size and font from various types of background.",,978-1-4799-8679-8,10.1109/ICIS.2015.7166615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166615,Multilevel;K-mean;Connected Component,Feature extraction;Gray-scale;Image color analysis;Image segmentation;Lighting;Clustering algorithms;Data mining,database indexing;document image processing;image segmentation;optical character recognition;pattern clustering;text analysis;visual databases,document images;automatic extraction;multilevel thresholding;k-means clustering;image database indexing;document understanding;Web searching;optical character recognition application;user focusing;grayscale image;nontextual object;graphics;pictures;complex background;heuristic filter;recursive filter;text regions,,3,,20,,27-Jul-15,,,IEEE,IEEE Conferences
Keyword Based Information Retrieval System for Urdu Document Images,解決問題的數學文檔檢索,R. Hussain; H. A. Khan; I. Siddiqi; K. Khurshid; A. Masood,"Nat. Univ. of Sci. & Technol., Islamabad, Pakistan; Inst. of Space Technol., Islamabad, Pakistan; Bahria Univ., Islamabad, Pakistan; Inst. of Space Technol., Islamabad, Pakistan; Nat. Univ. of Sci. & Technol., Islamabad, Pakistan",2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),8-Feb-16,2015,,,27,33,"Various dynasties ruled the Indian sub-continent and left behind enormous and rich cultural heritage that also included intellectually enriched research in the shape of various documents scripted in Urdu. In order to provide efficient access to this knowledge, analysis though digitizing the existing work is the need of hour. In addition to digitization, efficient search mechanisms also need to be implemented to provide users a rapid access to the queried information. In most cases, the digitized documents are complemented by manually assigned tags which not only is a time consuming task but also provides a very limited search facility. Automating the transcription of these documents using Optical Character Recognition (OCR) systems is also challenging due to the very complex cursive nature of Urdu text. To overcome these limitations, a keyword spotting based information retrieval system for document images is introduced in this study. The proposed technique relies on two major modules, document indexing and retrieval. Images of documents are segmented into partial words (ligatures) and identical partial words (PWs) are grouped into clusters. We introduce the concept of considering each (partial) word as a unique shape and a set of shape descriptors is extracted to characterize the PWs. The clusters of PWs are used to index a given set of documents. During retrieval, the query word presented to the system is matched with the clusters in the database and all documents containing instances of the query word are retrieved and presented to the user. The system evaluated on a set of printed Urdu documents in Nastaliq font realized promising precision and recall rates.",,978-1-4673-9721-6,10.1109/SITIS.2015.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400541,Information Retrieval;Word Spotting;Partial words;Clustering;Indexing,Shape;Feature extraction;Indexing;Optical character recognition software;Image segmentation;Information retrieval,document image processing;feature extraction;image retrieval;image segmentation;pattern clustering,keyword based information retrieval system;Urdu document images;document indexing;document image segmentation;partial word segmentation;clustering;shape descriptor extraction;PW characterization;query word;Nastaliq font,,4,,28,,8-Feb-16,,,IEEE,IEEE Conferences
Survey on Graph and Cluster Based approaches in Multi-document Text Summarization,使用本體提取醫學文檔的概念,Y. K. Meena; A. Jain; D. Gopalani,"Department of Computer Engineering, Malaviya National Institute of Technology, Jaipur, Rajasthan, India; Department of Computer Engineering, Malaviya National Institute of Technology, Jaipur, Rajasthan, India; Department of Computer Engineering, Malaviya National Institute of Technology, Jaipur, Rajasthan, India",International Conference on Recent Advances and Innovations in Engineering (ICRAIE-2014),25-Sep-14,2014,,,1,5,"In today's era of World Wide Web, on-line information is increasing exponentially day by day. So there is a need to condense corpus of documents into useful information automatically. Automatic Text summarization plays an important role to extract salient feature from corpus of documents, which helps user to get useful information in short time and less effort. Summarization reduces the complexity of a document while retaining its important features. Recently, most researchers have transferred their efforts from single to multi document summarization but they have to be aware of the issues of redundancy, sentence ordering, fluency, etc. There are wide varieties of approaches in Multi-document Text Summarization like Graph Based, Cluster Based, Time Based and Term frequency -Inverse document frequency Based etc. The survey starts introducing Multi-document text Summarization (MDS) and then discusses various methods of MDS which fall under the Graph and Cluster Based methods. In this paper, we have analysed Graph and Cluster Based methods proposed by various researchers in the field and we sort out some of the problems in applied procedures and also pin out advantages, which would help future researchers working in the area, to get significant instruction for further analysis. Using this information one can generate new or even hybrid methods in Multi-document summarization.",,978-1-4799-4040-0,10.1109/ICRAIE.2014.6909126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909126,Automatic Text Summarization;Cluster Based;Graph Based;Multi-document Summarization,Web sites;Context;Convergence;Clustering algorithms;Algorithm design and analysis,Internet;text analysis,multidocument text summarization;World Wide Web;online information;automatic text summarization;salient feature extraction;graph based text summarization;cluster based text summarization;time based text summarization;term frequency-based text summarization;inverse document frequency based text summarization,,11,,17,,25-Sep-14,,,IEEE,IEEE Conferences
DT-Tree: A Semantic Representation of Scientific Papers,信息檢索指標：案例研究,S. R. A. Rizvi; S. X. Wang,"Dept. of Comput. Sci., California State Univ. Fullerton, Fullerton, CA, USA; Dept. of Comput. Sci., California State Univ. Fullerton, Fullerton, CA, USA",2010 10th IEEE International Conference on Computer and Information Technology,16-Sep-10,2010,,,1280,1284,"With the tremendous growth in electronic publication, locating the most relevant references is becoming a challenging task. Most effective document indexing structures represent a document as a vector of very high dimensionality. It is well known that such a representation suffers from the curse of dimensionality. In this paper, we introduce DT-Tree (DocumentTerm-Tree) - a new structure for the representation of scientific documents. DT-Tree represents a document using the 50 most frequent terms in that specific document. These terms are grouped into a tree structure according to where they appear in the document, such as title, abstract, or section title, etc. The distance between two documents is calculated based on their DT-Trees. Two DTTrees are compared using Dice coefficient between the corresponding nodes of the trees. To verify the effectiveness of our similarity measure, we conducted experiments to cluster 150 documents in three categories, namely biology, chemistry and physics. The experimental results indicated 100% accuracy.",,978-1-4244-7548-3,10.1109/CIT.2010.231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5577872,Analysis;Similarity Measure;Document clustering;dimension reduction;key term extraction;sparsity;kmeans,Clustering algorithms;Algorithm design and analysis;Biochemistry;Pediatrics;Physics,document handling;electronic publishing;indexing;trees (mathematics),semantic representation;DT tree;scientific paper;document indexing;document term tree;dice coefficient;electronic publication,,1,,19,,16-Sep-10,,,IEEE,IEEE Conferences
Topic Models for Unsupervised Cluster Matching,利用統計和語義信息進行文檔聚類：功能選擇評估,T. Iwata; T. Hirao; N. Ueda,"NTT Communication Science Laboratories, Seikacho, Kyoto, Japan; NTT Communication Science Laboratories, Seikacho, Kyoto, Japan; NTT Communication Science Laboratories, Seikacho, Kyoto, Japan",IEEE Transactions on Knowledge and Data Engineering,6-Mar-18,2018,30,4,786,795,"We propose topic models for unsupervised cluster matching, which is the task of finding matching between clusters in different domains without correspondence information. For example, the proposed model finds correspondence between document clusters in English and German without alignment information, such as dictionaries and parallel sentences/documents. The proposed model assumes that documents in all languages have a common latent topic structure, and there are potentially infinite number of topic proportion vectors in a latent topic space that is shared by all languages. Each document is generated using one of the topic proportion vectors and language-specific word distributions. By inferring a topic proportion vector used for each document, we can allocate documents in different languages into common clusters, where each cluster is associated with a topic proportion vector. Documents assigned into the same cluster are considered to be matched. We develop an efficient inference procedure for the proposed model based on collapsed Gibbs sampling. The effectiveness of the proposed model is demonstrated with real data sets including multilingual corpora of Wikipedia and product reviews.",1558-2191,,10.1109/TKDE.2017.2778720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8125189,Topic modeling;unsupervised object matching;clustering,Data models;Vocabulary;Dictionaries;Analytical models;Resource management;Sorting,document handling;inference mechanisms;pattern clustering;sampling methods;unsupervised learning;vectors,topic proportion vector;topic models;unsupervised cluster matching;document clusters;parallel sentences/documents;common latent topic structure;latent topic space;collapsed Gibbs sampling;Wikipedia;inference procedure,,1,,30,,30-Nov-17,,,IEEE,IEEE Journals
A comparative study on clustering techniques for Urdu ligatures in nastaliq font,一種基於分區的文檔聚類新算法,S. Shabbir; N. Javed; I. Siddiqi; K. Khurshid,"Department of Computer Science, Bahria University, Islamabad 44000, Pakistan; Department of Electrical Engineering, Institute of Space Technology, Islamabad 44000, Pakistan; Department of Computer Science, Bahria University, Islamabad 44000, Pakistan; Department of Electrical Engineering, Institute of Space Technology, Islamabad 44000, Pakistan",2017 13th International Conference on Emerging Technologies (ICET),8-Feb-18,2017,,,1,6,"Clustering is a pivotal step in any Optical Character Recognition (OCR) or Word Spotting system. It serves as a base for the classification and indexing of different words or characters depending upon the level of segmentation. Various clustering methodologies have been applied by different researchers on Latin script based document images. However for Urdu language, which belongs to the family of Arabic and Persian, clustering based indexing systems have not been extensively researched. In this paper, we present a comprehensive study of various known clustering techniques applied on printed Urdu Document Images. The images are segmented into ligatures or partial words and then they are grouped together using different clustering methods. Performance of these methods is evaluated using Calinski-Harabasz, Davis-Bouldin and Dunn indexes.",,978-1-5386-2260-5,10.1109/ICET.2017.8281724,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281724,Document Image Analysis;Oriental script;Urdu Ligatures;Clustering;Word Spotting;OCR,Indexes;Clustering algorithms;Self-organizing feature maps;Optical character recognition software;Image segmentation;Euclidean distance,document image processing;image segmentation;indexing;optical character recognition,Word Spotting system;clustering methodologies;Latin script;Urdu language;clustering based indexing systems;printed Urdu Document Images;different clustering methods;Dunn indexes;Urdu ligatures;nastaliq font;Optical Character Recognition;OCR,,,,20,,8-Feb-18,,,IEEE,IEEE Conferences
Incremental document clustering using Multi-representation Indexing Tree,使用主題和?本加權聚類檢測特定領域中的熱點及其趨勢,Lifeng Wang; Hui Song; Xiaoqiang Liu,"Department of computer science and technology, Donghua University, Shanghai, China; Department of computer science and technology, Donghua University, Shanghai, China; Department of computer science and technology, Donghua University, Shanghai, China",The 2nd International Conference on Information Science and Engineering,17-Jan-11,2010,,,3778,3781,"Incremental Document Clustering is a powerful technique for large-scale topic discovery from incremental documentation set. Indexing tree algorithm is advanced in efficiency. However, it tended to process spherical data. To address this problem, we present a novel Multi-Representation Indexing Tree (MRIT) algorithm for constructing a hierarchy that satisfies arbitrary shape clusters with a good performance. Compared with the Indexing tree algorithm, a cluster is decomposed into several sub clusters and is represented as a union of the sub clusters rather than the center of the cluster. Similarity of a document to one cluster is the distance to the nearest neighbor among the cluster's representative points. The experimental results on a variety of domains demonstrate that our algorithm can produce a quality cluster. It's insensitive to document input order, and efficient in terms of computational time.",2160-1291,978-1-4244-7618-3,10.1109/ICISE.2010.5690332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690332,Incremental Clustering;MRIT;Indexing Tree;Multi-representation,Indexing;Clustering algorithms;Algorithm design and analysis;Accuracy;Heuristic algorithms;Feature extraction;Nearest neighbor searches,,,,,,11,,17-Jan-11,,,IEEE,IEEE Conferences
A Concept-Driven Automatic Ontology Generation Approach for Conceptualization of Document Corpora,使用時間戳實現的多個文檔的基於質心的匯總,H. Zheng; C. Borchert; H. Kim,"Biomed. Knowledge Eng. Lab., Seoul Nat. Univ., Seoul; Biomed. Knowledge Eng. Lab., Seoul Nat. Univ., Seoul; Biomed. Knowledge Eng. Lab., Seoul Nat. Univ., Seoul",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,352,358,"In the age of increasing information availability, many techniques, such as document clustering and information visualization, have been developed to ease understanding of information for users. However, most of these methods do not help users directly understand key concepts and their semantic relationships in document corpora, which are critical for capturing their conceptual structures. Therefore, we propose a novel approach called 'Clonto' to identify the key concepts and automatically generate ontologies based on these concepts for conceptualization of document corpora. Clonto applies latent semantic analysis to identify key concepts, allocates documents based on these concepts, and utilizes WordNet to automatically generate a corpus-related ontology. The documents are linked to the ontology through the key concepts. The experimental results show that Clonto can identify key concepts with a high precision and the clustering results of Clonto outperform the STC (Suffix Tree Clustering) algorithm, the Lingo clustering algorithm, the Fuzzy Ants clustering algorithm, and clustering based on TRS (Tolerance Rough Set). Moreover, based on the same document corpus, the ontology generated by Clonto shows a significant informative conceptual structure.",,978-0-7695-3496-1,10.1109/WIIAT.2008.233,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740471,Clonto;Ontology;Suffix Tree Clustering;Lingo;Tolerance Rought Set,Ontologies;Visualization;Clustering algorithms;Intelligent agent;Knowledge engineering;Laboratories;Fuzzy sets;Semantic Web;Displays;Text analysis,document handling;information retrieval,concept-driven automatic ontology generation;document corpora conceptualization;information availability;document clustering;information visualization;Clonto;key concept identification;latent semantic analysis;WordNet;corpus-related ontology;document corpus;informative conceptual structure,,6,,20,,6-Jan-09,,,IEEE,IEEE Conferences
A Clustering and Ranking Based Approach for Multi-document Event Fusion,基於聚類圖的大數據提取與搜索模型,P. Li; Q. Zhu; X. Zhu,"Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China; Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China; Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China","2011 12th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",31-Oct-11,2011,,,159,165,"A complete event description is usually scattered over several sentences and documents, so that how to mine a complete event from several documents or event mentions is an issue currently. This paper proposes an event fusion approach to merge a set of event mentions which distributed over several HTML files into a complete event. Firstly it introduced plain features and structured features into the similarity calculation and applied the hierarchical clustering algorithm to cluster event mentions. Then it proposed an event fusion approach based on a ranking model to merge those argument instances with highest ranking rate in each cluster to form a complete event. The experimental result showed that our approach was effective and could achieve higher accuracy than the baseline.",,978-1-4577-0896-1,10.1109/SNPD.2011.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063560,Event fusion;Similarity;Event clustering;Ranking,Syntactics;Data mining;Feature extraction;Semantics;Accuracy;HTML;Mathematical model,document handling;hypermedia markup languages;pattern clustering;sensor fusion,clustering based approach;ranking based approach;multidocument event fusion;complete event description;event fusion approach;HTML files;hierarchical clustering algorithm;cluster event mentions,,,,17,,31-Oct-11,,,IEEE,IEEE Conferences
Multiresolution approach based on adaptive superpixels for administrative documents segmentation into color layers,文檔的並行比較器,E. Carel; J. Burie; V. Courboulay; J. Ogier; V. P. d'Andecy,"L3i, University of La Rochelle, Avenue Michel Cr矇peau, 17042 Cedex, France; L3i, University of La Rochelle, Avenue Michel Cr矇peau, 17042 Cedex, France; L3i, University of La Rochelle, Avenue Michel Cr矇peau, 17042 Cedex, France; L3i, University of La Rochelle, Avenue Michel Cr矇peau, 17042 Cedex, France; ITESOFT Headquarters: Parc d'Andron Le S矇quoia, 30470 Aimargues, France",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,566,570,"Administrative document images are usually processed in black and white what generates many problems due to the errors related to the binarization. Besides all semantic information provided by the color is lost. Document images have a rich and highly variable content. The presence of false colors and artefacts introduced by the scanning and the compression alter the segmentation of the regions. Problems arise when there is no correspondence between the point clouds which are detected in a color space and the real regions of an image. In order to help the segmentation, we propose the extraction of the main colors of an image as a set of binary layers. Due to the industrial context, our approach has to run unsupervised on a generic dataset of color administrative documents. The originality of this approach is the use of a multiresolution analysis to detect the number of colors automatically. At a low resolution, a set of local regions is obtained thanks to a SLIC-based approach which takes into account the structure of documents and which combines both colorimetric information and spatial information. Then, a merging stage is applied on each resolution separately based on the colors which have been extracted at a lower resolution. This contribution can both feed the traditional process and exploit colorimetric information.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333825,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333825,,Image resolution;Image segmentation,data compression;document image processing;feature extraction;image coding;image colour analysis;image resolution;image segmentation;pattern clustering,adaptive superpixels;administrative documents segmentation;color layers;administrative document images;binarization;compression;region segmentation;binary layers;main color extraction;color administrative documents;multiresolution analysis;SLIC-based approach;colorimetric information;spatial information;merging stage;colors extraction;simple linear iterative clustering,,2,,8,,23-Nov-15,,,IEEE,IEEE Conferences
Document clustering using particle swarm optimization,通過強制動態佈局可視化文檔空間,X. Cui; T. E. Potok; P. Palathingal,"Computational Sci. & Eng. Div., Oak Ridge Nat. Lab., TN, USA; Computational Sci. & Eng. Div., Oak Ridge Nat. Lab., TN, USA; Computational Sci. & Eng. Div., Oak Ridge Nat. Lab., TN, USA","Proceedings 2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005.",29-Aug-05,2005,,,185,191,"Fast and high-quality document clustering algorithms play an important role in effectively navigating, summarizing, and organizing information. Recent studies have shown that partitional clustering algorithms are more suitable for clustering large datasets. However, the K-means algorithm, the most commonly used partitional clustering algorithm, can only generate a local optimal solution. In this paper, we present a particle swarm optimization (PSO) document clustering algorithm. Contrary to the localized searching of the K-means algorithm, the PSO clustering algorithm performs a globalized search in the entire solution space. In the experiments we conducted, we applied the PSO, K-means and hybrid PSO clustering algorithm on four different text document datasets. The number of documents in the datasets ranges from 204 to over 800, and the number of terms ranges from over 5000 to over 7000. The results illustrate that the hybrid PSO algorithm can generate more compact clustering results than the K-means algorithm.",,0-7803-8916-6,10.1109/SIS.2005.1501621,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1501621,,Particle swarm optimization;Clustering algorithms;Partitioning algorithms;Software engineering;Laboratories;Software algorithms;Navigation;Organizing;Hybrid power systems;Data mining,document handling;data mining;very large databases;search problems;particle swarm optimisation,document clustering;particle swarm optimization;K-means algorithm;globalized search;partitional clustering algorithm,,137,,25,,29-Aug-05,,,IEEE,IEEE Conferences
Effects of clustering algorithms on typographic reconstruction,混合類型二進製文檔中一種可靠的文本提取技術,E. H. Barney Smith; B. Lamiroy,"Electrical and Computer Engineering Department, Boise State University, ID 83725-2075, USA; Universit矇 de Lorraine - LORIA (UMR 7503), Campus Scientifique - BP 239, 54506 Vand?uvre-l癡s-Nancy CEDEX - FRANCE",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,541,545,"Type designers and historians studying the typefaces and fonts used in historical documents can usually only rely on available printed material. The initial wooden or metal cast fonts have mostly disappeared. In this paper we address the creation of character templates from printed documents. Images of characters scanned from Renaissance era documents are segmented, then clustered. A template is created from each obtained cluster of similar appearance characters. In order for subsequent typeface analysis tools to operate, the template should reduce the noise present in the individual instances by using information from the set of samples, but the samples must be homogeneous enough to not introduce further noise into the process. This paper evaluates the efficiency of several clustering algorithms and the associated parameters through cluster validity statistics and appearance of the resulting template image. Clustering algorithms that form tight clusters produce templates that highlight details, even though the number of available samples is smaller, while algorithms with larger clusters better capture the global shape of the characters.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333820,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333820,,Image reconstruction;Image edge detection,document image processing;history;image reconstruction;image segmentation;pattern clustering,typographic reconstruction;clustering algorithm;historical document;Renaissance era document;image segmentation;subsequent typeface analysis,,,,13,,23-Nov-15,,,IEEE,IEEE Conferences
Proactive Documents--A New Paradigm to Access Document Information from Various Contexts,基於聚類的KNN改進算法CLKNN用於文本分類,K. E. S. Toni,"Technische Universitat Munchen, Germany",21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07),27-Aug-07,2007,1,,313,320,"In this paper we introduce proactive documents (PD) and proactive document agents (PDA) as a new paradigm to access document information from various contexts: browsing, other services and other agents. At large, the PD(A) system architecture is driven by one basic assumption about the nature of documents: natural language documents reflect the knowledge of the authors about a specific topic. PDA are intelligent agents, equipped with profound natural language processing (NLP) and information extraction (IE) algorithms. They create a complete representation of the structural and linguistic knowledge about the processed document. Depending on the context of usage, PD can present this knowledge in different ways. In a browsing- context, PD provide a text based browsing interface, providing typed, contextual links to other documents. If accessed by other services/agents PD are able to explicate their knowledge in a formal way, e.g. as an OWL- lite document. Thus, the information can easily be accessed and further processed by the requesting agents or services. Furthermore, we introduce a complete PD(A) architecture. It allows several PDA to communicate with other agents and to consolidate and evolve their knowledge, cluster documents into information items and create typed links between information items. Finally, we outline the interna of PDA: a mathematical model for information value calculation for document constituents.",,978-0-7695-2847-2,10.1109/AINAW.2007.296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221079,,Search engines;Context-aware services;Natural languages;Intelligent agent;Natural language processing;Data mining;Mathematical model;Uniform resource locators;Feeds;Web pages,document handling;natural language processing;software architecture;user interfaces,document information;proactive document agents;natural language documents;intelligent agents;natural language processing;information extraction;linguistic knowledge;text based browsing interface;cluster documents;mathematical model,,,,40,,27-Aug-07,,,IEEE,IEEE Conferences
Word Spotting in Bangla and English Graphical Documents,基於智能聚類技術的Web服務分類,A. Tarafdar; U. Pal; J. Ramel; N. Ragot; B. B. Chaudhuri,"CVPR Unit, Indian Stat. Inst., Kolkata, India; CVPR Unit, Indian Stat. Inst., Kolkata, India; Lab. d'Inf., Univ. Francois Rabelais Tours, Tours, France; Lab. d'Inf., Univ. Francois Rabelais Tours, Tours, France; CVPR Unit, Indian Stat. Inst., Kolkata, India",2014 22nd International Conference on Pattern Recognition,6-Dec-14,2014,,,3044,3049,"Word spotting in graphical documents is a very challenging task. With an increase usage of electronic media, we are in a need of searching objects in graphical documents by some labeled text. To address such scenarios we propose a word spotting system dedicated to graphical documents with Bangla and English scripts. In our proposed system, first text-graphics layers are separated using Gabor filter. In the text layer, character segmentation approach is applied using water reservoir based method to extract each character from the document. Then recognition of these isolated characters is done using rotation invariant feature, coupled with SVM classifier. Well recognized characters are then grouped based on their sizes. Initial spotting is started to find a query word among those groups of characters. In case if the system could spot a word partially due to any noise, SIFT is applied to identify missing portion of that partial spotting. Experimental results on English and Bangla script document images show that the method is feasible to spot a location in text labeled graphical documents.",1051-4651,978-1-4799-5209-0,10.1109/ICPR.2014.525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977237,Document Image Analysis;Word Spotting;Graphical documents;Information Retrieval;Gabor Filter;Water Reservoir Principle;Clustering;SIFT feature,Reservoirs;Character recognition;Gabor filters;Support vector machines;Feature extraction;Graphics,document image processing;Gabor filters;image classification;image retrieval;image segmentation;natural language processing;optical character recognition;support vector machines;text analysis;transforms,word spotting system;Bangla graphical documents;English graphical documents;electronic media usage;object searching;text labeled graphical documents;Bangla scripts;English scripts;text-graphics layers;Gabor filter;character segmentation approach;isolated character recognition;rotation invariant feature;SVM classifier;support vector machine;query word;SIFT;scale invariant feature transform,,,,14,,6-Dec-14,,,IEEE,IEEE Conferences
Fast Document Cosine Similarity Self-Join on GPUs,樹狀聚類，用於基於佈局的文檔圖像檢索,Y. Feng; J. Tang; M. Liu; C. Wang; J. Xie,"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci. & Eng., Wright State Univ., Dayton, OH, USA; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China",2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),16-Dec-18,2018,,,205,212,"Similarity Search has been studied in many different fields of computer science, including data mining, information retrieval, databases and so on. Document similarity self-join is a crucial part of lots of applications, such as near-duplicate document detection, document clustering and web search. On a collection of documents, document similarity self-join finds out all pairs of documents whose similarity values are no lower than a threshold value. However, similarity search is a computation-intensive procedure and consumes a large amount of time as the dataset size increases. Thus, many serial algorithms focus on speeding up the process by decreasing the possible similarity candidates for each query object on high-dimensional sparse datasets, including documents. However, the efficiency of those serial algorithms degrade badly as the threshold decreases. Parallel implementations based on OpenMP or MapReduce also adopt the pruning policy and do not solve the problem thoroughly. In this context, taking into account features of document datasets, we propose 2Step-SSJ, which solves the document similarity self-join in CUDA environment on GPUs. 2Step-SSJ performs the similarity self-join in two steps, i.e., similarity computing on the inverted list and similarity computing on the forward list, which compromises between the memory visiting and dot-product computation. The experimental results show that 2Step-SSJ could solve the problem much faster than existing methods on three benchmark text corpora, achieving the speedup of 2?-23? against the state-of-the-art parallel algorithm in general, while keep a relatively stable running time with different values of the threshold.",2375-0197,978-1-5386-7449-9,10.1109/ICTAI.2018.00040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576038,similarity search;document similarity;information retrieval;CUDA,Graphics processing units;Instruction sets;Indexes;Search problems;Parallel algorithms;Registers;Computer science,data mining;document handling;graphics processing units;information retrieval;parallel algorithms;parallel processing;pattern clustering;query processing;text analysis,similarity values;similarity search;2Step-SSJ;similarity computing;fast document cosine similarity self-join;near-duplicate document detection;document clustering;web search;OpenMP;MapReduce;GPU;data mining;information retrieval;parallel algorithm,,,,23,,16-Dec-18,,,IEEE,IEEE Conferences
Automatic Arabic text summarization using clustering and keyphrase extraction,基於復雜形狀匹配的中文文檔圖像壓縮,H. N. Fejer; N. Omar,"Center for AI Technology, FTSM, Universiti Kebangsaan Malaysia (UKM), 43600 Bangi, Selangor, Malaysia; Center for AI Technology, FTSM, Universiti Kebangsaan Malaysia(UKM), 43600 Bangi, Selangor, Malaysia",Proceedings of the 6th International Conference on Information Technology and Multimedia,26-Mar-15,2014,,,293,298,"As the number of electronic documents increases rapidly, the need for faster techniques to assess the relevance of these documents emerges. A summary is a concise representation of underlying text. A full understanding of the document is essential to form an ideal summary. However, achieving full understanding is either difficult or impossible for computers. Therefore, selecting important sentences from the original text and presenting these sentences as a summary present the most common techniques in automated text summarization. This paper propose a hybrid clustering method(partitioning and hierarchical) to group many Arabic documents into several clusters .Then keyphrase extraction module is applied to extract important Keyphrases from each cluster, which helps identify the most important sentences and find similar sentences based on several similarity algorithms. It applied to extract one sentence from a group of similar sentences while ignoring the other similar sentences (i.e., sentences that have a greater similarity than the predefined threshold). This model is designed for both single-and multi-document Arabic text summarization. The Recall-Oriented Understudy for Gisting Evaluation (ROGUE) matrix used for the evaluation. For the summarization dataset, Essex Arabic Summaries Corpus was used. It has many topic based articles with multiple human summaries. This model achieved an accuracy of 80 % for single-document and 62% for multi-document summarization.",,978-1-4799-5423-0,10.1109/ICIMU.2014.7066647,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066647,Text Summarization;Clustering;Keyphrase Extraction;Similarity;ROUGE Matrix,Couplings;Clustering algorithms;Information technology;Multimedia communication;Clustering methods;Feature extraction;Filtering,natural language processing;pattern clustering;text analysis;word processing,automatic Arabic text summarization;electronic document;hybrid clustering method;Arabic document;keyphrase extraction module;similarity algorithm;single-document Arabic text summarization;multidocument Arabic text summarization;recall-oriented understudy-for-gisting evaluation matrix;ROGUE matrix;Essex Arabic summary corpus,,6,,21,,26-Mar-15,,,IEEE,IEEE Conferences
E-VSM: Novel text representation model to capture contex-based closeness between two text documents,一種基於頻繁項集的文本聚類的主動學習方法,A. Bhakkad; S. C. Dharmadhikari; M. Emmanuel; P. Kulkarni,"Dept. of IT, Pune Institute of Computer Technology, India; Dept. of IT, Pune Institute of Computer Technology, India; Dept. of IT, Pune Institute of Computer Technology, India; Anomaly Sol. Pvt. Ltd., India",2013 7th International Conference on Intelligent Systems and Control (ISCO),21-Mar-13,2013,,,345,348,"In many applications of Information Retrieval and Text Mining, there is need for an intelligent system to calculate the closeness between two text documents. In this, representation of text document in terms of mathematical object plays vital role. Vector Space Model is most popular method to represent text document in mathematical form but it is lossy, loses ordering of terms in text document in turn the context of it. Existing measures of closeness between two text documents are Cosine Similarity, Euclidean Distance etc. which are efficient but lacks in consideration of context of document. Through this paper we propose E-VSM: Enhanced-Vector Space Model to overcome limitations of original Vector Space Model and new `Density-based Clustering' approach to calculate context-based closeness between two text documents which outperforms state of art in terms of accuracy. Experiments show good results specially when text document to be compared is very much close to a particular region of target text document.",,978-1-4673-4603-0,10.1109/ISCO.2013.6481176,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481176,Intelligent System;Vector Space Model;Density-Based Clustering;Context-Based Closeness,Noise;Optical imaging;Optical noise;Integrated optics,data mining;data structures;information retrieval;pattern clustering;text analysis;vectors,E-VSM;text representation model;context-based closeness;text document representation;information retrieval;text mining;intelligent system;cosine similarity;Euclidean distance;density-based clustering;enhanced vector space model,,3,,12,,21-Mar-13,,,IEEE,IEEE Conferences
Analyzing Term Weighting Schemes for Labeling Software Clusters,大型文本數據集聚類的MPRK算法,F. Siddique; O. Maqbool,"Dept. of Comput. Sci., Quaid-i-Azam Univ., Islamabad, Pakistan; Dept. of Comput. Sci., Quaid-i-Azam Univ., Islamabad, Pakistan",2011 15th European Conference on Software Maintenance and Reengineering,5-Apr-11,2011,,,85,88,"Clustering techniques have been widely employed for software modularization. The clusters formed as a result of the clustering process may be difficult to understand unless they are appropriately labeled. One method to assign labels is to use term weighting schemes from Information Retrieval and Text Categorization which use weights to assign importance to terms in a document. Some of these term weighting schemes have been used by researchers for labeling clusters, but there is a need to compare various schemes and analyze their strengths and weaknesses. In this paper, we analyze four different schemes in the context of software and identify cases where one may be better than the other. We also conduct experiments to verify the behavior of the weighting schemes according to software characteristics.",1534-5351,978-1-61284-259-2,10.1109/CSMR.2011.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741249,Software Cluster Labeling;Software Clustering;Term Weighting Schemes,Clustering algorithms;Radio frequency;Software algorithms;Labeling;Algorithm design and analysis;Software systems,pattern clustering;software engineering,term weighting scheme analysis;software cluster labelling;clustering techniques;software modularization;information retrieval;text categorization,,4,,13,,5-Apr-11,,,IEEE,IEEE Conferences
Web documents clustering with interest links,字符袋和SOM聚類，用於腳本識別和作者識別,Zifeng Cui; Baowen Xu; Weifeng Zhang; Junling Xu,"Dept of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Dept of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; NA; NA",IEEE International Workshop on Service-Oriented System Engineering (SOSE'05),12-Dec-05,2005,,,111,116,"Web documents clustering is a kind of effective Web mining technique. This paper proposes a novel Web documents clustering algorithm from the perspective of Web usage through analyzing WWW cache, in which Web documents reflect user's recent interests. According to the rich semantic information embedded in hyperlinks in Web documents, we first extracts hyperlinks from Web documents and the Web documents in WWW cache is modeled as an undirected Web graph in our approach. Then the clustering algorithm based on the Web graph model is given. Finally, Experimental results verify that the algorithm is efficient and feasible.",,0-7695-2438-9,10.1109/SOSE.2005.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1551138,,Clustering algorithms;World Wide Web;Partitioning algorithms;Search engines;Computer science;Laboratories;Software engineering;Web mining;Algorithm design and analysis;Greedy algorithms,Internet;document handling;data mining,Web documents clustering algorithm;Web user interest link;Web mining;Web usage;WWW cache analysis;World Wide Web;semantic information;Web document hyperlink;Web graph model,,,,12,,12-Dec-05,,,IEEE,IEEE Conferences
Feature Selection with 帣-Hill Climbing Search for Text Clustering Application,BiblioMapper：基於集群的信息可視化技術,L. M. Abualigah; A. T. Khader; M. A. Al-Betar; Z. A. A. Alyasseri; O. A. Alomari; E. S. Hanandeh,"Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia; Dept. of Comput. Inf. Syst., Zarqa Univ., Zarqa, Jordan",2017 Palestinian International Conference on Information and Communication Technology (PICICT),18-Sep-17,2017,,,22,27,"In the bases of increasing the volume of text information, the dealing with text information has become incredibly complicated. The text clustering is a suitable technique used in dealing with a tremendous amount of text documents by classifying these set of text documents into clusters. Ultimately, text documents hold sparse, non-uniform distribution and uninformative features are difficult to cluster. The text feature selection is a primary unsupervised learning method that is utilized to choose a new subset of informational text features. In this paper, a new algorithm is proposed based on 帣-hill climbing technique for text feature selection problem to improve the text clustering (B-FSTC). The results of the proposed method for 帣-hill climbing and original Hill climbing (i.e., H-FSTC) are examined using the k-mean text clustering and compared with each other. Experiments were conducted on four standard text datasets with varying characteristics. Interestingly, the proposed 帣-hill climbing algorithm obtains superior results in comparison with the other well-regard techniques by producing a new subset of informational text features. Lastly, the 帣-hill climbing-based feature selection method supports the k-mean clustering algorithm to achieve more precise clusters.",,978-1-5090-6538-7,10.1109/PICICT.2017.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038318,Unsupervised Feature Selection;帣-Hill Climbing;K-mean Text document Clustering;Informative features,Clustering algorithms;Optimization;Clustering methods;Text mining;Standards;Web pages;Benchmark testing,feature selection;pattern clustering;search problems;text analysis;unsupervised learning,帣-hill climbing search;text information;text documents;text feature selection;unsupervised learning;informational text features;B-FSTC;k-mean text clustering;text datasets,,4,,15,,18-Sep-17,,,IEEE,IEEE Conferences
A Novel Co-clustering Method with Intra-similarities,基於Google Earth的Web-3D虛擬交易社區的智能項目代理,J. Wu; J. Lai; C. Wang,"Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China",2011 IEEE 11th International Conference on Data Mining Workshops,23-Jan-12,2011,,,300,306,"Recently, co-clustering has become a topic of much interest because of its applications to many problems. It has been proved more effective than one-way clustering methods. But the existing co-clustering approaches just treat the document as a collection of words, disregarding the word sequences. They only consider the co-occurrence counts of words and documents, but do not take into account the similarities between words and similarities between documents. However, these similarity information can help improving the co-clustering. In this paper, we incorporate the word similarities and document similarities into the co-clustering algorithm, and propose a new co-clustering method. And we provide a theoretical analysis that our algorithm can converge to a local minimum. The empirical evaluation on publicly available data sets also shows that our algorithm is effective.",2375-9259,978-1-4673-0005-6,10.1109/ICDMW.2011.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137394,co-clustering;word similarities;document similarities,Clustering algorithms;Mutual information;Decision support systems;Prototypes;Algorithm design and analysis;Joints;Partitioning algorithms,pattern clustering;text analysis,coclustering method;document clustering;word collection;word sequence;cooccurrence word count;word similarity;document similarity;similarity information,,3,,22,,23-Jan-12,,,IEEE,IEEE Conferences
Coupled term-term relation analysis for document clustering,用於文檔聚類的耦合術語-術語關係分析,X. Cheng; D. Miao; C. Wang; L. Cao,"Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; Advanced Analytics Institute, University of Technology, Sydney, Australia; Advanced Analytics Institute, University of Technology, Sydney, Australia",The 2013 International Joint Conference on Neural Networks (IJCNN),9-Jan-14,2013,,,1,8,"Traditional document clustering approaches are usually based on the Bag of Words model, which is limited by its assumption of the independence among terms. Recent strategies have been proposed to capture the relation between terms based on statistical analysis, and they estimate the relation between terms purely by their co-occurrence across the documents. However, the implicit interactions with other link terms are overlooked, which leads to the discovery of incomplete information. This paper proposes a coupled term-term relation model for document representation, which considers both the intra-relation (i.e. co-occurrence of terms) and inter-relation (i.e. dependency of terms via link terms) between a pair of terms. The coupled relation for each pair of terms is further used to map a document onto a new feature space, which includes more semantic information. Substantial experiments verify that the document clustering incorporated with our proposed relation achieves a significant performance improvement compared to the state-of-the-art techniques.",2161-4407,978-1-4673-6129-3,10.1109/IJCNN.2013.6706853,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706853,,Semantics;Vectors;Data mining;Frequency measurement;Computer science;Context;Sparse matrices,data mining;document handling;pattern clustering;statistical analysis,coupled term-term relation analysis;document clustering;statistical analysis;document representation;feature space;semantic information,,24,,20,,9-Jan-14,,,IEEE,IEEE Conferences
Judicial precedents search supported by natural language processing and clustering,自然語言處理和聚類支持的司法判例搜索,M. A. Calamb獺s; A. Ord籀簽ez; A. Chac籀n; H. Ordo簽ez,"Intelligent Management Systems Group, Fundaci籀n Universitaria de Popay獺n, Cauca, Colombia; Intelligent Management Systems Group, Fundaci籀n Universitaria de Popay獺n, Cauca, Colombia; Universidad Aut籀noma del Cauca, Cauca, Colombia; Research Laboratory in Development of Software Engineering, Universidad San Buenaventura, Cali, Colombia",2015 10th Computing Colombian Conference (10CCC),23-Nov-15,2015,,,372,377,"The judicial precedent facilitates decision-making by judges based on previous sentences. However, legal professionals should look among a large number of documents sentences serve as support for its ongoing cases. This work shows the progress in developing a system that uses technologies of natural language processing and grouping (clustering) to optimize the process of search and analysis of these court documents.",,978-1-4673-9464-2,10.1109/ColumbianCC.2015.7333448,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333448,Document clustering;judicial document retrieval;judicial natural language processing;legal ontology;named entity recognition,,document handling;information retrieval;law administration;natural language processing;pattern clustering,judicial precedent search;natural language processing;clustering;legal professionals;court documents,,1,,16,,23-Nov-15,,,IEEE,IEEE Conferences
Microarray Gene Cluster Identification and Annotation Through Cluster Ensemble and EM-Based Informative Textual Summarization,通過聚類集成和基於EM的信息性文本摘要對微陣列基因聚類進行識別和註釋,X. Hu; E. K. Park; X. Zhang,"Henan Univ., Kaifeng, China; NA; NA",IEEE Transactions on Information Technology in Biomedicine,1-Sep-09,2009,13,5,832,840,"Generating high-quality gene clusters and identifying the underlying biological mechanism of the gene clusters are the important goals of clustering gene expression analysis. To get high-quality cluster results, most of the current approaches rely on choosing the best cluster algorithm, in which the design biases and assumptions meet the underlying distribution of the dataset. There are two issues for this approach: 1) usually, the underlying data distribution of the gene expression datasets is unknown and 2) there are so many clustering algorithms available and it is very challenging to choose the proper one. To provide a textual summary of the gene clusters, the most explored approach is the extractive approach that essentially builds upon techniques borrowed from the information retrieval, in which the objective is to provide terms to be used for query expansion, and not to act as a stand-alone summary for the entire document sets. Another drawback is that the clustering quality and cluster interpretation are treated as two isolated research problems and are studied separately. In this paper, we design and develop a unified system Gene Expression Miner to address these challenging issues in a principled and general manner by integrating cluster ensemble, text clustering, and multidocument summarization and provide an environment for comprehensive gene expression data analysis. We present a novel cluster ensemble approach to generate high-quality gene cluster. In our text summarization module, given a gene cluster, our expectation-maximization based algorithm can automatically identify subtopics and extract most probable terms for each topic. Then, the extracted top k topical terms from each subtopic are combined to form the biological explanation of each gene cluster. Experimental results demonstrate that our system can obtain high-quality clusters and provide informative key terms for the gene clusters.",1558-0032,,10.1109/TITB.2009.2023984,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072272,Cluster ensemble;expectation?maximization (EM);microarray gene expression analysis;text mining,Gene expression;Clustering algorithms;Data mining;Algorithm design and analysis;Data analysis;Genomics;Bioinformatics;Information science;Cities and towns;Biological tissues,bioinformatics;genetics;information retrieval;pattern clustering;statistical analysis,microarray gene cluster identification;annotation;informative textual summarization;extractive approach;information retrieval;query;Gene Expression Miner,"Algorithms;Cluster Analysis;Databases, Genetic;Genes, Fungal;Models, Statistical;Multigene Family;Oligonucleotide Array Sequence Analysis;Vocabulary, Controlled;Yeasts",13,,27,,12-Jun-09,,,IEEE,IEEE Journals
Binarization of Color Character Strings in Scene Images Using K-Means Clustering and Support Vector Machines,使用K均值聚類和支持向量機對場景圖像中的顏色字符串進行二值化,T. Wakahara; K. Kita,"Fac. of Comput. & Inf. Sci., Hosei Univ., Koganei, Japan; Fac. of Comput. & Inf. Sci., Hosei Univ., Koganei, Japan",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,274,278,"This paper addresses the problem of binalizing multicolored character strings in scene images subject to heavy image degradations and complex backgrounds. The proposed method consists of four steps. The first step generates tentatively binarized images via every dichotomization of K clusters obtained by K-means clustering of constituent pixels of a given image in the HSI color space. The total number of tentatively binarized images equals 2K - 2. The second step divides each binarized image into a sequence of ""single-character-like"" images using an average aspect ratio of a character. The third step is use of support vector machines (SVM) to determine whether each ""single-character-like"" image represents a character or non-character. We feed the SVM with the mesh feature to output the degree of ""character-likeness."" The fourth step selects a single binarized image with the maximum average of ""character-likeness"" as an optimal binarization result. Experiments using a total of 1000 character strings extracted from the ICDAR 2003 robust word recognition dataset show that the proposed method achieves a correct binarization rate of 80.8%.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065318,binarization of multicolored character strings;K-means clustering;support vector machines,Support vector machines;Image color analysis;Character recognition;Robustness;Feature extraction;Image recognition;Image segmentation,character recognition;pattern clustering;support vector machines,color character strings;scene images;k-means clustering;support vector machine;multicolored character strings;image degradation;dichotomization;binarized image;single-character-like image;SVM;character-likeness;optimal binarization;robust word recognition dataset;correct binarization rate,,22,,10,,3-Nov-11,,,IEEE,IEEE Conferences
Comparative Advantage Approach for Sparse Text Data Clustering,稀疏文本數據聚類的比較優勢方法,J. Ji; T. Y. T. Chan; Q. Zhao,"Univ. of Aizu, Aizu-Wakamatsu, Japan; Univ. of Akureyri, Akureyri, Iceland; Univ. of Aizu, Aizu-Wakamatsu, Japan",2009 Ninth IEEE International Conference on Computer and Information Technology,17-Nov-09,2009,1,,3,8,"Document clustering is the process of partitioning a set of unlabeled n documents into clusters such that documents in each cluster share some common concepts. Each concept is conveniently represented by some key terms. Using words as features, text data are represented as a vector in a very high dimensional vector space. However, most documents are sparse vectors, for example, more than ten thousand dimensions and sparsity of 98%. In this paper, we study a fast classification algorithm based on the idea of comparative advantage for clustering sparse data. The proposed algorithm uses one ""ruler"" instead of k centers to identify the comparative advantage of each cluster and define the cluster label for each document. Experimental results show that our algorithm has comparable performance but faster than k-means. It can produce clusters with smaller overlapping concepts in the sense of key terms.",,978-0-7695-3836-5,10.1109/CIT.2009.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329159,Document clustering;dimension reduction;key term extraction;sparsity;k-means;comparative advantage.,Frequency;Clustering algorithms;Information technology;Classification algorithms;Inverse problems;Virtual manufacturing;Unsupervised learning;Genetic algorithms,pattern classification;pattern clustering;text analysis;vectors,sparse text data clustering;document clustering;words;text data;high dimensional vector space;classification algorithm,,5,,14,,17-Nov-09,,,IEEE,IEEE Conferences
A novel approach for clustering Chinese blogs by embedded sentiment based on graph similarity,基於圖相似度的嵌入式情感聚類中文博客的新方法,J. Pang; D. Xu; S. Feng; F. Yang; D. Wang,"School of Computer Science and Technology, Wuhan University of Technology, 430070, China; School of Computer Science and Technology, Wuhan University of Technology, 430070, China; School of Information Science and Engineering, Northeastern University, Shenyang 110819, China; School of Information Science and Engineering, Northeastern University, Shenyang 110819, China; School of Information Science and Engineering, Northeastern University, Shenyang 110819, China",2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery,9-Sep-10,2010,5,,2344,2348,"Blog clustering is an important approach of web public opinion analysis. In this paper, an integrated graph-based approach for representing and clustering Chinese blogs by embedded sentiment is proposed. Graph-based representation and relevant clustering algorithm are applied. This graph-based blogs representation model considers not only sentiment words but also some structural information. Experimental results show that comparing with applying initial graph-based document representation model and vector space document representation model, the integrated graph-based document representation model has better quality in clustering Chinese blog documents.",,978-1-4244-5934-6,10.1109/FSKD.2010.5569832,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569832,blog mining;sentiment analysis;blog clustering;graph-based representation,Blogs;Clustering algorithms;Measurement;Tin;Semantics;Dictionaries;Entropy,document handling;graph theory;pattern clustering;Web sites,Chinese blogs clustering;embedded sentiment;graph similarity;Web public opinion analysis;Graph based representation;graph based document representation model;vector space document representation model,,,,10,,9-Sep-10,,,IEEE,IEEE Conferences
Three and Four Phase Scenarios for Dynamic Document Organization,動態文檔組織的三階段和四階段方案,T. Jo; N. Japkowicz,NA; NA,"2006 IEEE International Conference on Systems, Man and Cybernetics",16-Jul-07,2006,3,,2228,2233,"This research introduces a new paradigm: dynamic document organization, DDO, for managing documents. DDO consists of managing documents automatically under the assumption that the topic structure in a collection of documents is always variable and temporary. This paradigm is in contrast with static document organization, SDO, the currently used paradigm which assumes that the topic structure is fixed and permanent. In this work, we consider two scenarios, a three-phase-scenario and a four-phase-scenario, for managing documents based on DDO. In both scenarios, text clustering, cluster identification, and document classification are integrated into a cycle. In the four-phase-scenario, one more phase, classifier training, is added between the cluster identification and document classification phases. The goal of this research is to evaluate the two proposed scenarios and contrast the best one ti its best SDO counterpart. We show that the four-phase DDO scenario is more reliable than the three-phase DDO scenario, and that it generally outperforms the best SDO scenario.",1062-922X,1-4244-0099-6,10.1109/ICSMC.2006.385192,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274198,,Text categorization;Management training;Clustering algorithms;Text mining;Maintenance;Content management;Switches;Prototypes;Niobium;Support vector machines,document handling,four phase scenarios;three phase scenarios;dynamic document organization;documents collection;static document organization;cluster identification;text clustering;document classification,,,,14,,16-Jul-07,,,IEEE,IEEE Conferences
Clustering Sentence-Level Text Using a Novel Fuzzy Relational Clustering Algorithm,使用新型模糊關係聚類算法的句子級文本聚類,A. Skabar; K. Abdalgader,"La Trobe University, Victoria; La Trobe University, Victoria",IEEE Transactions on Knowledge and Data Engineering,19-Nov-12,2013,25,1,62,75,"In comparison with hard clustering methods, in which a pattern belongs to a single cluster, fuzzy clustering algorithms allow patterns to belong to all clusters with differing degrees of membership. This is important in domains such as sentence clustering, since a sentence is likely to be related to more than one theme or topic present within a document or set of documents. However, because most sentence similarity measures do not represent sentences in a common metric space, conventional fuzzy clustering approaches based on prototypes or mixtures of Gaussians are generally not applicable to sentence clustering. This paper presents a novel fuzzy clustering algorithm that operates on relational input data; i.e., data in the form of a square matrix of pairwise similarities between data objects. The algorithm uses a graph representation of the data, and operates in an Expectation-Maximization framework in which the graph centrality of an object in the graph is interpreted as a likelihood. Results of applying the algorithm to sentence clustering tasks demonstrate that the algorithm is capable of identifying overlapping clusters of semantically related sentences, and that it is therefore of potential use in a variety of text mining tasks. We also include results of applying the algorithm to benchmark data sets in several other domains.",1558-2191,,10.1109/TKDE.2011.205,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035706,Fuzzy relational clustering;natural language processing;graph centrality,Clustering algorithms;Prototypes;Convergence;Data models;Partitioning algorithms,data mining;data structures;expectation-maximisation algorithm;fuzzy set theory;Gaussian processes;graph theory;matrix algebra;natural language processing;pattern clustering;text analysis,sentence-level text clustering;fuzzy relational clustering algorithm;pattern clustering;sentence similarity measures;Gaussian mixtures;relational input data;pairwise similarity matrix;graph representation;data representation;expectation-maximization framework;graph centrality;overlapping cluster identification;semantically related sentences;text mining;natural language processing,,43,,59,,6-Oct-11,,,IEEE,IEEE Journals
An Ellipsoidal K-Means for Document Clustering,用於文檔聚類的橢球K均值,F. Dzogang; C. Marsala; M. Lesot; M. Rifqi,"LIP6, Univ. Pierre et Marie Curie - Paris 6, Paris, France; LIP6, Univ. Pierre et Marie Curie - Paris 6, Paris, France; LIP6, Univ. Pierre et Marie Curie - Paris 6, Paris, France; LIP6, Univ. Pantheon-Assas, Paris, France",2012 IEEE 12th International Conference on Data Mining,17-Jan-13,2012,,,221,230,"We propose an extension of the spherical K-means algorithm to deal with settings where the number of data points is largely inferior to the number of dimensions. We assume the data to lie in local and dense regions of the original space and we propose to embed each cluster into its specific ellipsoid. A new objective function is introduced, analytical solutions are derived for both the centroids and the associated ellipsoids. Furthermore, a study on the complexity of this algorithm highlights that it is of same order as the regular K-means algorithm. Results on both synthetic and real data show the efficiency of the proposed method.",2374-8486,978-1-4673-4649-8,10.1109/ICDM.2012.126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413900,clustering;feature selection;spherical k-means;information retrieval,Ellipsoids;Clustering algorithms;Vectors;Partitioning algorithms;Linear programming;Feature extraction;Tuning,computational complexity;document handling;pattern clustering,ellipsoidal k-means;document clustering;spherical k-means algorithm;ellipsoid;algorithm complexity,,4,,19,,17-Jan-13,,,IEEE,IEEE Conferences
Co-clustering for queries and corresponding advertisement,查詢和相應廣告的聯合集群,F. Yang; Bin An; Xizhao Wang,"Department of Computer Science, University of California, Santa Cruz, USA; Department of Electrical Engineering, University of California, Santa Cruz, USA; Machine Learning Center, Faculty of Mathematics and Computer Science, Hebei University, Baoding, China",2009 International Conference on Machine Learning and Cybernetics,25-Aug-09,2009,4,,2296,2299,Both documents clustering and words clustering are well studied problems. Most existing algorithms cluster documents (advertisement) and words (query) separately but not simultaneously. In this paper we present a novel idea of analyzing both queries and advertisements which occur with queries at the same time. We present an innovative co-clustering algorithm that suggests queries by co-clustering advertisements and queries. We pose the co-clustering problem as an optimization problem in information theory - the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters.,2160-1348,978-1-4244-3702-3,10.1109/ICMLC.2009.5212131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212131,Co-Clustering;Online advertisement;Query;DBSCAN;K-mean clustering;Singular Value Decomposition,Search engines;Machine learning;Cybernetics;Computer science;Clustering algorithms;Random variables;Advertising;Intrusion detection;Mathematics;Machine learning algorithms,advertising;query processing,coclustering;documents clustering;words clustering;advertisement;queries;optimization problem,,,,15,,25-Aug-09,,,IEEE,IEEE Conferences
A fast spectral cluster algorithm based on bipartite graph,基於二部圖的快速光譜聚類算法,H. Li-mao; X. Sen; Z. Tian,"School of Information Engineering, Yancheng Institute of Technology, Yancheng, 224051, China; School of Information Engineering, Yancheng Institute of Technology, Yancheng, 224051, China; Science and technology on Underwater Acoustic, Laboratory, Harbin Engineering University, Harbin 150001, China",Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology,19-Sep-11,2011,7,,3461,3464,"Recently, spectral clustering has been applied successfully in a variety of different areas. While choosing the similarity graph and its parameters for spectral clustering is not a trivial task and its computational complexity is rather high. In this paper we proposed a fast bipartite graph spectral algorithm which uses evidence accumulation method and algebraic transformation method to avoid the above shortcoming. Experiments on real-world document sets demonstrate the effectiveness of the proposed algorithm.",,978-1-61284-088-8,10.1109/EMEIT.2011.6023827,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6023827,clustering analysis;spectral clustering;normalized mutual information,Clustering algorithms;Bipartite graph;Eigenvalues and eigenfunctions;Clustering methods;Mutual information;Matrix decomposition;Laplace equations,algebra;computational complexity;graph theory;pattern clustering;spectral analysis,spectral cluster algorithm;bipartite graph;computational complexity;evidence accumulation method;algebraic transformation method,,1,,9,,19-Sep-11,,,IEEE,IEEE Conferences
Fuzzy Bag-of-Words Model for Document Representation,用於文檔表示的模糊詞袋模型,R. Zhao; K. Mao,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",IEEE Transactions on Fuzzy Systems,29-Mar-18,2018,26,2,794,804,"One key issue in text mining and natural language processing is how to effectively represent documents using numerical vectors. One classical model is the Bag-of-Words (BoW). In a BoW-based vector representation of a document, each element denotes the normalized number of occurrence of a basis term in the document. To count the number of occurrence of a basis term, BoW conducts exact word matching, which can be regarded as a hard mapping from words to the basis term. BoW representation suffers from its intrinsic extreme sparsity, high dimensionality, and inability to capture high-level semantic meanings behind text data. To address the aforementioned issues, we propose a new document representation method named fuzzy Bag-of-Words (FBoW) in this paper. FBoW adopts a fuzzy mapping based on semantic correlation among words quantified by cosine similarity measures between word embeddings. Since word semantic matching instead of exact word string matching is used, the FBoW could encode more semantics into the numerical representation. In addition, we propose to use word clusters instead of individual words as basis terms and develop fuzzy Bag-of-WordClusters (FBoWC) models. Three variants under the framework of FBoWC are proposed based on three different similarity measures between word clusters and words, which are named as FBoWCmean, FBoWCmax, and FBoWCmin , respectively. Document representations learned by the proposed FBoW and FBoWC are dense and able to encode high-level semantics. The task of document categorization is used to evaluate the performance of learned representation by the proposed FBoW and FBoWC methods. The results on seven real-word document classification datasets in comparison with six document representation learning methods have shown that our methods FBoW and FBoWC achieve the highest classification accuracies.",1941-0034,,10.1109/TFUZZ.2017.2690222,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891009,Document classification;document representation;fuzzy similarity;word embeddings,Semantics;Dogs;Computational modeling;Text mining;Vocabulary;Analytical models;Numerical models,fuzzy set theory;learning (artificial intelligence);natural language processing;pattern classification;pattern clustering;string matching;text analysis,FBoWC;text mining;natural language processing;numerical vectors;vector representation;BoW representation;document representation method;fuzzy mapping;word embeddings;word semantic matching;exact word string matching;high-level semantics;document categorization;document classification datasets;fuzzy bag-of-word clusters models;FBoWCmean;FBoWCmax;FBoWCmin,,12,,49,,31-Mar-17,,,IEEE,IEEE Journals
A novel approach for document clustering to criminal identification by using ABK-means algorithm,ABK-means算法的一種新的文檔聚類識別犯罪的方法,H. N. Gangavane; M. C. Nikose; P. C. Chavan,"Department of Computer Science & Engineering, BDCOE, Wardha, Maharashtra; Department of Computer Science & Engineering, BDCOE, Wardha, Maharashtra; Department of Computer Science & Engineering, BDCOE, Wardha, Maharashtra","2015 International Conference on Computer, Communication and Control (IC4)",11-Jan-16,2015,,,1,6,"The important role of digital forensics is to improve the investigation of criminal activities that involve gather, to preserve, analyze, digital devices and provide technical and scientific evidence, and to provide the important documentation to authorities. To automatically group the retrieved documents into a list of meaningful categories different clustering techniques can be used. In last few decades many researchers research is projected to analyze the criminal with that of crime. It is seen that there is a large amount of increase in the crime rate due to the gap between the optimal usage of investigation and technologies. Because of this there are many new opportunities for the development of new methodologies and techniques in the field of crime investigation using the methods based on data mining, forensic, image processing, and social mining. Document clustering involves descriptor and descriptors extraction. In this paper, presents a model using new methodology for evaluation of document clustering of criminal database by using k-means clustering technique. This model clusters the criminal data basing on the type crime.",,978-1-4799-8164-9,10.1109/IC4.2015.7375722,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375722,Automatic Speech Recognition;Speech recognition uses;Artificial Intelligence,Itemsets;Association rules;Yttrium;Computers;Digital forensics,data mining;digital forensics;document handling;natural language processing;pattern clustering,document clustering;criminal identification;ABK-means algorithm;digital forensics;criminal activity investigation;documentation;crime rate;crime investigation;data mining;image processing;social mining;descriptor extraction;criminal database;k-means clustering technique;NLP,,1,,17,,11-Jan-16,,,IEEE,IEEE Conferences
Incremental structural model for extracting relevant tokens of entity,用於提取實體相關令牌的增量結構模型,N. Rahal; M. Benjlaiel; A. M. Alimi,"REGIM-Lab.: REsearch Groups in Intelligent Machines, Tunis el Manar University, FST, Tunisia; REGIM-Lab.: REsearch Groups in Intelligent Machines, Sfax University, ENIS, Tunisia; REGIM-Lab.: REsearch Groups in Intelligent Machines, Sfax University, ENIS, Tunisia","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-Feb-17,2016,,,3452,3456,"This paper describes a method for extracting relevant tokens of entity from semi-structured administrative documents. This method is used for mislabeling correction by employing the entity tokens physically close in a document. Firstly, the entities are labeled. Secondly, each entity is modeled by a tokens structure graph in which the nodes represent the tokens and the arcs represent the distances. A clustering algorithm is then applied to incrementally concatenate the relevant tokens of entities and ignore the noisy parts. The obtained results with a dataset of real invoices are reported in experimental section.",,978-1-5090-1897-0,10.1109/SMC.2016.7844767,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844767,Extracting relevant tokens;mislabeling correction;tokens structure graph;clustering algorithm,Optical character recognition software;Labeling;Noise measurement;Conferences;Cybernetics;Clustering algorithms;Data mining,document handling;graph theory;pattern clustering,incremental structural model;relevant tokens extraction;semistructured administrative documents;tokens structure graph;clustering algorithm;noisy parts;experimental section,,,,5,,9-Feb-17,,,IEEE,IEEE Conferences
Clustering technical documents by stylistic features for authorship analysis,通過樣式特徵將技術文檔聚類以進行作者分析,D. Berry; E. Sazonov,"Department of Information Systems, Statistics, and Management Science, University of Alabama, Tuscaloosa, 35487, USA; Department of Electrical and Computer Engineering, University of Alabama, Tuscaloosa, 35487, USA",SoutheastCon 2015,25-Jun-15,2015,,,1,5,"While previous research has demonstrated the ability to discriminate between authors using purely stylistic features, the majority of studies have been conducted on large corpora of non-technical literature. We investigate the ability of unsupervised methods to recover the authorial structure of a collection of technical documents labeled by primary author. Experiments were conducted using 23 submitted conference and journal papers containing almost 100,000 words from a local engineering research group with papers authored by both the Principal Investigator and by graduate students. Stylistic information was extracted from the body of each text forming a feature vector representing the document. Spectral clustering was applied to the feature vectors and the resulting clustering had an Adjusted Rand Index of .306 which is significantly better than chance (p <; .05).",1558-058X,978-1-4673-7300-5,10.1109/SECON.2015.7132936,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7132936,Natural language processing;spectral clustering;adjusted Rand index;technical writing;authorship analysis,Feature extraction;Indexes;Writing;Clustering algorithms;Data mining;Plagiarism;Accuracy,feature extraction;natural language processing;pattern clustering;text analysis;unsupervised learning,technical document clustering;stylistic features;authorship analysis;nontechnical literature;unsupervised methods;journal papers;conference papers;principal investigator;graduate students;stylistic information extraction;spectral clustering;feature vectors;adjusted Rand index,,1,,15,,25-Jun-15,,,IEEE,IEEE Conferences
Short text clustering using numerical data based on n-gram,使用基於n-gram的數值數據進行短文本聚類,R. Kumar; R. P. Mathur,"Department of Computer Science Engineering, Lovely Professional University, Phagwara, Punjab, India; Department of Computer Science Engineering, Lovely Professional University, Phagwara, Punjab, India",2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence),10-Nov-14,2014,,,274,276,"Short text messages, especially mobile SMSs contain not only pure textual strings but also contain numeric values. Existing systems discard and filter out these numeric values. In our research, a new approach has been developed which makes usage of numeric values for feature extraction in the process of clustering. We are proposing an algorithm that uses n-gram approach to retrieve the pre-strings and post-strings of each numeric data and then similarity between documents is calculated. Partitioning is done to separate out two types of documents such as pure textual as well as mixed documents. Text messaging is gaining popularity in the field of pushing and providing short indication and informative notifications to users at any time. Use of numerical values through n-gram plays an important role for efficient clustering of text messages.",,978-1-4799-4236-7,10.1109/CONFLUENCE.2014.6949257,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949257,N-gram;Clustering;VSM,Vectors;Data mining;Feature extraction;Clustering algorithms;Electronic mail;Mobile communication;Java,data mining;electronic messaging;text analysis,numerical data;short-text message clustering;mobile SMS;textual strings;numeric values;feature extraction;n-gram approach;prestring retrieval;poststring retrieval;document similarity;document partitioning;pure-textual documents;mixed documents;pushing field,,1,,9,,10-Nov-14,,,IEEE,IEEE Conferences
anyOCR: A sequence learning based OCR system for unlabeled historical documents,anyOCR：基於序列學習的OCR系統，用於未標記的歷史文檔,M. Jenckel; S. S. Bukhari; A. Dengel,"University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Germany; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Germany",2016 23rd International Conference on Pattern Recognition (ICPR),24-Apr-17,2016,,,4035,4040,"Institutes and libraries around the globe are preserving the literary heritage by digitizing historical documents. However, to make this data easily accessible the scanned documents need to be transformed into search-able text. State of the art OCR systems using Long-Short-Term-Memory networks (LSTM) have been applied successfully to recognize text in both printed and handwritten form. Besides the general challenges with historical documents, e.g. poor image quality, damaged characters, etc., especially unknown scripts and old fonds make it difficult to provide the large amount of transcribed training data required for these methods to perform well. Transcribing the documents manually is very costly in terms of man-hours and require language specific expertise. The unknown fonds and requirement for meaningful context also make the use of synthetic data unfeasible. We therefore propose an end-to-end framework anyOCR that cuts the required input from language experts to a minimum and is therefore easily extendable to other documents. Our approach combines the strengths of segmentation-based OCR methods utilizing clustering on individual characters and segmentation-free OCR methods utilizing a LSTM architecture. The proposed approach is applied to a collection of 15th century Latin documents. Combining the initial clustering with segmentation-free OCR was able to reduce the initial error of about 16% to less than 8%.",,978-1-5090-4847-2,10.1109/ICPR.2016.7900265,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900265,OCR;Historical Documents;Clustering;OCRopus;LSTM Networks,Optical character recognition software;Training data;Image segmentation;Logic gates;Training;Shape;Graphical user interfaces,document image processing;history;image segmentation;learning (artificial intelligence);optical character recognition;text detection,anyOCR;sequence learning-based OCR system;unlabeled historical documents;literary heritage preservation;historical document digitization;searchable text;long-short-term-memory networks;text recognition;handwritten text;printed text;image quality;unknown scripts;end-to-end framework;language experts;segmentation-based OCR methods;character clustering;segmentation-free OCR methods;LSTM architecture;15th century Latin documents;initial error reduction,,14,,12,,24-Apr-17,,,IEEE,IEEE Conferences
Extracting Product Features from Reviews Using Feature Ontology Tree Applied on LDA Topic Clusters,使用LDA主題集群上的特徵本體樹從評論中提取產品特徵,D. T. Santosh; B. V. Vardhan; D. Ramesh,"Comput. Sci. & Eng., JNTU Kakinada, Kakinada, India; Comput. Sci. & Eng., JNTUHCEJ, Jagityal, India; Comput. Sci. & Eng., JNTUHCEJ, Jagityal, India",2016 IEEE 6th International Conference on Advanced Computing (IACC),18-Aug-16,2016,,,163,168,"Online product reviews provide data about the users perspective on the features that were experienced by them. Product features and corresponding opinions form a major part in analyzing the online product reviews. Extracting features from a huge number of reviews is categorized into three main categories such as utilizing language rules, sequence labeling and the topic modeling. Latent Dirichlet Allocation (LDA) is one such topic model which clusters the document words into unsupervised learned topics using Dirichlet priors. The words so clustered are the features and opinion words in the product reviews domain. These clusters contain words which are non features of the product. To identify appropriate product features from these clusters a hierarchical, domain independent Feature Ontology Tree (FOT) is applied to LDA clusters. This improves the accuracy of the features using extracted LDA topic clusters.",,978-1-4673-8286-1,10.1109/IACC.2016.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544828,Feature;Opinion word;Latent Dirichlet Allocation;cluster;Feature Ontology Tree,Feature extraction;Ontologies;Resource management;OWL;Vocabulary;Conferences;Data mining,Internet;knowledge representation languages;natural language processing;ontologies (artificial intelligence);pattern clustering;text analysis;trees (mathematics);unsupervised learning,product feature extraction;LDA topic clusters;online product reviews;opinions;language rules;sequence labeling;topic modeling;latent Dirichlet allocation;document word clustering;unsupervised learned topics;Dirichlet priors;opinion words;hierarchical domain independent feature ontology tree;FOT;LDA cluster;Web Ontology Language,,,,16,,18-Aug-16,,,IEEE,IEEE Conferences
A feature selection for Korean Web document clustering,韓文Web文檔聚類的功能選擇,Heum Park; Young-Gi Kim; Hyuk-Chul Kwon,"AI Lab. Dept. of Comput. Sci., Pusan Nat. Univ., South Korea; NA; NA","30th Annual Conference of IEEE Industrial Electronics Society, 2004. IECON 2004",23-May-05,2004,3,,2650,2654 Vol. 3,"This paper is a comparative study of feature selection methods for Korean Web documents clustering. First, we focused on how the term feature and the co-link of Web documents affect clustering performance. We clustered Web documents by native term feature, co-link and both, and compared the output results with the originally allocated category. And we selected term features for each category using X/sup 2/, information gain (IG), and mutual information (MI) from training documents, and applied these features to other experimental documents. In addition we suggested a new method named max feature selection, which selects terms that have the maximum count for a category in each experimental document, and applied X/sup 2/ (or MI or IG) values to each term instead of term frequency of documents, and clustered them. In the results, X/sup 2/ shows a better performance than IG or MI, but the difference appears to be slight. But when we applied the max feature selection method, the clustering performance improved notably. Max feature selection is a simple but effective means of feature space reduction and shows powerful performance for Korean Web document clustering.",,0-7803-8730-9,10.1109/IECON.2004.1432224,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432224,,Frequency;Information retrieval;Data mining;Artificial intelligence;Computer science;Libraries;Information science;Mutual information;Explosions;Internet,information retrieval;Internet;document handling;pattern clustering;feature extraction,Korean Web document clustering;feature selection method;information gain;mutual information;max feature selection;feature space reduction,,,,10,,23-May-05,,,IEEE,IEEE Conferences
A new non-negative matrix factorization algorithm with sparseness constraints,一種新的具有稀疏約束的非負矩陣分解算法,W. Zhao; H. Ma; N. Li,"College of Information Engineering, Xiangtan University, Xiangtan 411105, China; College of Mathematics and Information Science, Northwest Normal University, Lanzhou 730070, China; College of Mathematics and Computer Science, Hebei University, Baoding 071002, China",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1449,1452,"The non-negative matrix factorization (NMF) aims to find two matrix factors for a matrix X such that X ??W H, where W and H are both nonnegative matrices. The non-negativity constraint arises often naturally in applications in physics and engineering. In this paper, we propose a new NMF approach, which incorporates sparseness constraints explicitly. The new model can learn much sparser matrix factorization. Also, an objective function is defined to impose the sparseness constraint, in addition to the non-negative constraint. Experimental results on two document datasets show the effectiveness and efficiency of the proposed method.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016966,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016966,Non-negative matrix factorization;sparseness constraints;document clustering,Legged locomotion,document handling;matrix decomposition;pattern clustering;sparse matrices,nonnegative matrix factorization algorithm;sparseness constraints;NMF approach;document clustering,,8,,15,,12-Sep-11,,,IEEE,IEEE Conferences
A hybrid feature selection algorithm for web document clustering,Web文檔聚類的混合特徵選擇算法,A. Benghabrit; B. Ouhbi; E. M. Zemmouri; B. Frikh; H. Behja,"LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LM2I Laboratory, ENSAM, MoulayIsma簿l University, Mekn癡s, Morocco; LTTI Laboratory, EST-F癡s, Sidi Mohamed Ben Abdellah, F癡s, Morocco; Greentic Laobarory, ENSEM, Hassan 2 University, Casablanca, Morocco",2014 International Conference on Next Generation Networks and Services (NGNS),18-Dec-14,2014,,,216,222,"Knowing that not all the features in a dataset are important since some are redundant or irrelevant, the use of feature selection, an effective dimensionality reduction technique, is essential for web document clustering. For the clustering process, it represents the task of selecting important features for the underlying clusters. Therefore in order to pilot the web document clustering process, we propose a hybrid feature selection algorithm that selects simultaneously the most statistical and semantic informative features through a weighting model. The clustering process selects relevant features and performs document clustering iteratively until stability. The experimental results demonstrate the practical aspects of our algorithm and show that it generates more efficient clustering than the one obtained by other existing algorithms.",2327-6533,978-1-4799-6937-1,10.1109/NGNS.2014.6990255,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990255,Clustering;Web documents;Feature selection methods;Statistical and semantic analysis;Performance analysis,Clustering algorithms;Semantics;Vectors;Feature extraction;Mutual information;Algorithm design and analysis;Convergence,feature selection;Internet;stability;statistical analysis,hybrid feature selection algorithm;Web document clustering process;statistical informative features;semantic informative features;weighting model;stability,,,,36,,18-Dec-14,,,IEEE,IEEE Conferences
Clustering XML Documents Based on Data Type,根據數據類型對XML文檔進行聚類,C. Zhou; Y. Lu,"Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., China; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., China",2008 International Conference on Computational Intelligence and Security,22-Dec-08,2008,2,,122,127,"The existing so-called semantic XML document clustering algorithms usually use a synonymous word library to calculate semantic similarities among XML documents. However, when people create their own XML documents, they name the element randomly and often use lots of abbreviations. Many tags are not real words at all. The XML documents created by different people may appear very different from each other even if they describe the same object. The traditional methods do not work well in such case. To address the problem, we proposed a novel similarity measure standard based on data-type tree, a model integrating data types and tags of XML documents. A clustering algorithm DT2K-means is also proposed to cluster XML documents. Empirical experiment results on real world data sets show DT2K-means can group the semantic similar XML documents together correctly, which contain different tags but describe the same object.",,978-0-7695-3508-1,10.1109/CIS.2008.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724749,clustering;data type;XML,XML;Clustering algorithms;Educational institutions;Computer science;Computational intelligence;Computer security;Data security;Libraries;Measurement standards;Stability,pattern clustering;XML,semantic XML document clustering algorithm;synonymous word library;semantic similarity;similarity measure standard;data type tree;eXtensible Markup Language,,,,14,,22-Dec-08,,,IEEE,IEEE Conferences
Stamp Detection in Color Document Images,彩色文檔圖像中的圖章檢測,B. Micenkov織; J. v. Beusekom,NA; NA,2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1125,1129,"An automatic system for stamp segmentation and further verification is needed especially for environments like insurance companies where a huge volume of documents is processed daily. However, detection of a general stamp is not a trivial task as it can have different shapes and colors and, moreover, it can be imprinted with a variable quality and rotation. Previous methods were restricted to detection of stamps of particular shapes or colors. The method presented in the paper includes segmentation of the image by color clustering and subsequent classification of candidate solutions by geometrical and color-related features. The approach allows for differentiation of stamps from other color objects in the document such as logos or texts. For the purpose of evaluation, a data set of 400 document images has been collected, annotated and made public. With the proposed method, recall of 83% and precision of 84% have been achieved.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.227,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065485,stamp detection;image segmentation;computational forensics;color clustering,Image color analysis;Image segmentation;Shape;Seals;Feature extraction;Image resolution;Vectors,computer forensics;document image processing;feature extraction;image colour analysis;image segmentation;pattern clustering,stamp detection;color document images;automatic stamp segmentation system;insurance companies;image segmentation;color clustering;subsequent classification;color-related feature;geometrical feature,,14,,15,,3-Nov-11,,,IEEE,IEEE Conferences
A novel approach to sentence clustering,一種新穎的句子聚類方法,D. Sahoo; R. Balabantaray,"Dept. of Computer Science & Engineering, IIIT-Bhubaneswar, Odisha, India-751003; Dept. of Computer Science & Engineering, IIIT-Bhubaneswar, Odisha, India-751003","2016 International Conference on Computing, Communication and Automation (ICCCA)",16-Jan-17,2016,,,1,6,"Sentence clustering is often used as the first step in various information retrieval tasks like automatic text summarization, topic detection and tracking etc. Researchers face difficulty to cluster sentences because a single sentence is less informative compared to document. We present a sentence Feature Based Sentence Clustering, FBSC, which incorporates some sentence level relationship features like transition relationship, anaphoric relationship, and term (word) similarity relationship in association with Markov Clustering Algorithm (MCL) to generate sentence clusters. We observe that Purity of clusters generated by Feature Based Sentence Clustering (FBSC) compared to baseline k-means sentence clustering is better.",,978-1-5090-1666-2,10.1109/CCAA.2016.7813697,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813697,clustering;transition relationship;anaphoric relationship;term similarity,Clustering algorithms;Silicon;Information retrieval;Automation;Computational modeling;Markov processes;Text mining,information retrieval;Markov processes;pattern clustering;text analysis,information retrieval tasks;automatic text summarization;topic detection;topic tracking;feature based sentence clustering;FBSC;transition relationship;anaphoric relationship;term similarity relationship;word similarity relationship;Markov clustering algorithm;MCL,,1,,23,,16-Jan-17,,,IEEE,IEEE Conferences
RDF-based Model for Encoding Document Hierarchies,基於RDF的文檔層次結構編碼模型,R. O. Chavez; J. A. Sanchez; M. A. Medina; R. O. Chavez; J. A. Sanchez; M. A. Medina,NA; Univ. de las Americas - Puebla; Univ. de las Americas - Puebla; NA; Univ. de las Americas - Puebla; Univ. de las Americas - Puebla,"17th International Conference on Electronics, Communications and Computers (CONIELECOMP'07)",12-Mar-07,2007,,,22,22,"Some document clustering tools automatically or semi-automatically produce document hierarchies. Commonly, they are stored as text or graphical files. As a result, applications are not able to use them before some kind of processing. A more recent representation of document hierarchies employs Extensible Markup Language. However, the reusability of this representation is diminished because the semantic of XML elements is not machine accessible. This paper proposes the use of a resource description framework model to overcome these drawbacks",,0-7695-2799-X,10.1109/CONIELECOMP.2007.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127262,,Encoding;Resource description framework;Clustering algorithms;XML;Information retrieval;Natural languages;Ontologies;Tree graphs;Graphics;Data mining,pattern clustering;semantic networks;XML,RDF-based model;document hierarchy encoding;document clustering tools;Extensible Markup Language;XML elements;resource description framework model,,,,12,,12-Mar-07,,,IEEE,IEEE Conferences
Semantic explorative evaluation of document clustering algorithms,文檔聚類算法的語義探索性評估,H. S. Nguyen; S. H. Nguyen; W. ?wieboda,"Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Poland; Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Poland; Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Poland",2013 Federated Conference on Computer Science and Information Systems,7-Nov-13,2013,,,115,122,"In this paper, we investigate the problem of quality analysis of clustering results using semantic annotations given by experts. We propose a novel approach to construction of evaluation measure, which is based on the Minimal Description Length (MDL) principle. In fact this proposed measure, called SEE (Semantic Evaluation by Exploration), is an improvement of the existing evaluation methods such as Rand Index or Normalized Mutual Information. It fixes some of weaknesses of the original methods. We illustrate the proposed evaluation method on the freely accessible biomedical research articles from Pubmed Central (PMC). Many articles from Pubmed Central are annotated by the experts using Medical Subject Headings (MeSH) thesaurus. This paper is a part of the research on designing and developing a dialog-based semantic search engine for SONCA system which is a part of the SYNAT project. We compare different semantic techniques for search result clustering using the proposed measure.",,978-83-60810-52-1,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643985,,Clustering algorithms;Extraterrestrial measurements;Semantics;Moon;Decision trees;Biomedical measurement;Indexes,document handling;indexing;pattern clustering;search engines,search result clustering;SYNAT project;SONCA system;dialog-based semantic search engine;MeSH thesaurus;medical subject headings thesaurus;PMC;Pubmed Central;freely accessible biomedical research;normalized mutual information;rand index;semantic evaluation by exploration;SEE;MDL principle;minimal description length principle;semantic annotations;quality analysis;document clustering algorithms;semantic explorative evaluation,,1,,20,,7-Nov-13,,,IEEE,IEEE Conferences
Document Binarization Using Topological Clustering Guided Laplacian Energy Segmentation,使用拓撲聚類引導的拉普拉斯能量分割的文檔二值化,K. R. Ayyalasomayajula; A. Brun,"Dept. of Inf. Technol., Uppsala Univ., Uppsala, Sweden; Dept. of Inf. Technol., Uppsala Univ., Uppsala, Sweden",2014 14th International Conference on Frontiers in Handwriting Recognition,15-Dec-14,2014,,,523,528,"The current approach for text binarization proposes a clustering algorithm as a preprocessing stage to an energy-based segmentation method. It uses a clustering algorithm to obtain a coarse estimate of the background (BG) and foreground (FG) pixels. These estimates are used as a prior for the source and sink points of a graph cut implementation, which is used to efficiently find the minimum energy solution of an objective function to separate the BG and FG. The binary image thus obtained is used to refine the edge map that guides the graph cut algorithm. A final binary image is obtained by once again performing the graph cut guided by the refined edges on Laplacian of the image.",2167-6445,978-1-4799-4334-0,10.1109/ICFHR.2014.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6981073,Image Processing;Classification;Machine Learning;Graph-theoretic methods,Manganese;Clustering algorithms;Image edge detection;Optics;Topology;Laplace equations;Bandwidth,document image processing;graph theory;image classification;image segmentation;learning (artificial intelligence);pattern clustering,document binarization;topological clustering;guided Laplacian energy segmentation;text binarization;clustering algorithm;preprocessing stage;background pixels;BG pixels;foreground pixels;FG pixels;minimum energy solution;binary image;edge map;graph cut algorithm;refined edges;machine learning;classification,,4,,9,,15-Dec-14,,,IEEE,IEEE Conferences
Most Clusters Can Be Retrieved with Short Disjunctive Queries,可以使用簡短的析取查詢檢索大多數群集,V. Deolalikar,NA,2013 IEEE 13th International Conference on Data Mining,3-Feb-14,2013,,,1019,1024,"Simple keyword based searches are ubiquitous in today's internet age. It is hard to imagine an information system today that does not permit a simple keyword based search. This method of information retrieval has the obvious benefits of being highly interpretable, and having wide usage. However, a general perception is that keyword search may not be as powerful an information retrieval paradigm as those that utilize data mining technologies. At the same time, the tremendous growth in textual information in various domains has also given impetus to data mining technologies such as document clustering. Document clustering is a powerful technique, having wide applications in enterprise information management (EIM). However, there is a general perception that the clusters it produces are not always easily interpretable. This hampers its usage in certain settings. This leads us to the following question: can we retrieve a cluster (from a corpus) using a keyword search with precision and recall that are reasonable from the point of view of a retrieval system? What is the form of such a keyword search? How many keywords do we require? How do we arrive at these keywords? Not only are these questions natural, they have immediate use in several highly regulated applications in EIM such as eDiscovery and compliance, where document sets must be specified using keywords. In order to answer our question, we construct a framework that uses maximal frequent discriminative item sets. The novelty of our usage of these item sets is that although their definition as frequent item sets is conjunctive, we use them to form a disjunctive query upon the corpus. We then study the results of this query as an information retrieval problem whose target is the cluster. Our study yields a surprising result: most clusters can be retrieved, up to reasonable precision and recall, using a disjunctive query of only three terms. Among other ramifications, this gives us a readily interpretable description of a cluster in terms of the disjunctive query that returns it.",2374-8486,978-0-7695-5108-1,10.1109/ICDM.2013.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729591,Document Clustering;Keyword Search;Disjunctive Queries;Frequent Itemsets;Retrieval,Itemsets;Keyword search;Clustering algorithms;Data mining;Organizations;Standards,information systems;pattern clustering;query processing;text analysis,short disjunctive query;keyword based searches;Internet;information retrieval method;data mining technology;textual information;document clustering;enterprise information management;EIM;e-discovery;compliance;document sets;maximal frequent discriminative item sets;information system,,1,,16,,3-Feb-14,,,IEEE,IEEE Conferences
A pipeline for reconstructing cross-shredded English document,用於重建交叉粉碎的英語文檔的管道,Guanghao Chen; Jue Wu; Cundi Jia; Yunzhou Zhang,"School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)",20-Jul-17,2017,,,1034,1039,"Document shreds reconstruction is of great significance in the fields of file confidentiality, anti-disclosure, and investigative science. In this paper, a complete and practical pipeline is designed to reconstruct cross-shredded English documents. The pipeline firstly classifies the shreds into several clusters based on an improved K-means algorithm to reduce clustering imbalance. Especially, a preprocessing is needed before extracting feature vector for shredded English document because of the unaligned characters. Owing to its successful performance in reconstructing the strip-shredded documents, Hungarian algorithm is applied into the permutation for the cross-shredded shreds in the same row. Eventually the location of the connective horizontal paper slips are arranged by considering the complementary relationship of edge vectors between two neighboring shreds. Reconstruction experiment results indicate that the designed pipeline can acquire high precision and efficiency.",,978-1-5090-6238-6,10.1109/ICIVC.2017.7984711,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984711,cross-shredded document;reconstruction;improved K-means algorithm;hungarian algorithm;pipeline,Gold;IP networks;Fats,document image processing;feature extraction;image classification;image reconstruction;pattern clustering;vectors,cross-shredded English document reconstruction;document shreds reconstruction;file confidentiality;antidisclosure;investigative science;shred classification;improved K-means algorithm;clustering imbalance;feature vector extraction;unaligned characters;strip-shredded document reconstruction;Hungarian algorithm;cross-shredded shreds;connective horizontal paper slips;edge vectors,,,,14,,20-Jul-17,,,IEEE,IEEE Conferences
Architecture of personalized web search engine using suffix tree clustering,使用後綴樹聚類的個性化Web搜索引擎的體系結構,A. Annadurai; A. Annadurai,"MTech in Information Technology, Madras Institute of Technology, Anna University, Chennai, India; Data warehouse Engineer, Ness Technologies Ltd, Bangalore, India","2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies",22-Sep-11,2011,,,604,608,"Web search engines are designed to serve all users, independent of the special needs of any individual user. The objective of the project is to develop a personalized web search engine which considers users interest and generates search results based on the user's semantic profile. The proposed system utilizes clustering and re-ranking algorithms in order to organize the web documents and provide an order to the results displayed to the user. Web crawlers are utilized to get the links, images and allied information from the World Wide Web. The fetched documents are further clustered using suffix tree clustering algorithm, which enhances the performance of the web search engine. The results are organized using Page Re-Rank algorithm which considers hyperlink and link structure information to bring an order to the web. The system creates a semantic profile of the user by monitoring and analyzing the users search history. The search results generated will utilize an amalgamation of varied techniques including clustering, re-ranking and semantic user profiles to enhance the performance of the web search engine.",,978-1-61284-653-8,10.1109/ICSCCN.2011.6024622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024622,Suffix tree clustering;base clusters;search engine,Search engines;Clustering algorithms;Signal processing algorithms;Semantics;Web search;Engines;Web pages,document handling;Internet;pattern clustering;performance evaluation;search engines;tree data structures,personalized Web search engine architecture;suffix tree clustering algorithm;semantic profile;re-ranking algorithm;Web documents;Web crawlers;World Wide Web;page re-rank algorithm,,3,,24,,22-Sep-11,,,IEEE,IEEE Conferences
A Complete Optical Character Recognition Methodology for Historical Documents,完整的歷史文獻光學字符識別方法,G. Vamvakas; B. Gatos; N. Stamatopoulos; S. J. Perantonis,"Inst. of Inf. & Telecommun., Nat. Center for Sci. Res. Demokritos, Athens; Inst. of Inf. & Telecommun., Nat. Center for Sci. Res. Demokritos, Athens; Inst. of Inf. & Telecommun., Nat. Center for Sci. Res. Demokritos, Athens; Inst. of Inf. & Telecommun., Nat. Center for Sci. Res. Demokritos, Athens",2008 The Eighth IAPR International Workshop on Document Analysis Systems,11-Nov-08,2008,,,525,532,"In this paper a complete OCR methodology for recognizing historical documents, either printed or handwritten without any knowledge of the font, is presented. This methodology consists of three steps: The first two steps refer to creating a database for training using a set of documents, while the third one refers to recognition of new document images. First, a pre-processing step that includes image binarization and enhancement takes place. At a second step a top-down segmentation approach is used in order to detect text lines, words and characters. A clustering scheme is then adopted in order to group characters of similar shape. This is a semi-automatic procedure since the user is able to interact at any time in order to correct possible errors of clustering and assign an ASCII label. After this step, a database is created in order to be used for recognition. Finally, in the third step, for every new document image the above segmentation approach takes place while the recognition is based on the character database that has been produced at the previous step.",,978-0-7695-3337-7,10.1109/DAS.2008.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4670002,Optical Character Recognition;Historical Documents,Optical character recognition software;Character recognition;Image databases;Handwriting recognition;Image segmentation;Image recognition;Cultural differences;Image converters;Pattern recognition;Text analysis,document image processing;humanities;image enhancement;image segmentation;learning (artificial intelligence);optical character recognition;pattern clustering;text analysis;visual databases,optical character recognition methodology;historical document;image recognition;image binarization;image enhancement;top-down segmentation approach;text line detection;ASCII label;semiautomatic procedure,,33,,22,,11-Nov-08,,,IEEE,IEEE Conferences
Incorporating paragraph embeddings and density peaks clustering for spoken document summarization,結合段落嵌入和密度峰聚類以進行語音文檔摘要,K. Chen; K. Shih; S. Liu; B. Chen; H. Wang,"Institute of Information Science, Academia Sinica, Taiwan; National Taiwan Normal University, Taiwan; Institute of Information Science, Academia Sinica, Taiwan; National Taiwan Normal University, Taiwan; Institute of Information Science, Academia Sinica, Taiwan",2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),11-Feb-16,2015,,,207,214,"Representation learning has emerged as a newly active research subject in many machine learning applications because of its excellent performance. As an instantiation, word embedding has been widely used in the natural language processing area. However, as far as we are aware, there are relatively few studies investigating paragraph embedding methods in extractive text or speech summarization. Extractive summarization aims at selecting a set of indicative sentences from a source document to express the most important theme of the document. There is a general consensus that relevance and redundancy are both critical issues for users in a realistic summarization scenario. However, most of the existing methods focus on determining only the relevance degree between sentences and a given document, while the redundancy degree is calculated by a post-processing step. Based on these observations, three contributions are proposed in this paper. First, we comprehensively compare the word and paragraph embedding methods for spoken document summarization. Next, we propose a novel summarization framework which can take both relevance and redundancy information into account simultaneously. Consequently, a set of representative sentences can be automatically selected through a one-pass process. Third, we further plug in paragraph embedding methods into the proposed framework to enhance the summarization performance. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.",,978-1-4799-7291-3,10.1109/ASRU.2015.7404796,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404796,Spoken document;summarization;embedding;relevance;redundancy,Redundancy;Training;Context modeling;Predictive models;Context;Artificial neural networks,document handling;learning (artificial intelligence);natural language processing;pattern clustering,paragraph embeddings;density peaks clustering;spoken document summarization;representation learning;machine learning application;word embedding;natural language processing;extractive text;speech summarization;extractive summarization;relevance information;redundancy information,,3,,45,,11-Feb-16,,,IEEE,IEEE Conferences
Application of the discrete wavelet transform to identification of questioned documents,離散小波變換在可疑文件識別中的應用,D. Levin; R. Conn; M. Kam,"Dept. of Electr. & Comput. Eng., Drexel Univ., Philadelphia, PA, USA; Dept. of Electr. & Comput. Eng., Drexel Univ., Philadelphia, PA, USA; Dept. of Electr. & Comput. Eng., Drexel Univ., Philadelphia, PA, USA",Proceedings of IEEE-SP International Symposium on Time- Frequency and Time-Scale Analysis,6-Aug-02,1994,,,318,321,"Identification of questioned documents (QDs) is the process of associating documents produced by an unknown writer with labeled archival samples. Here we use the discrete wavelet transform (DWT) to extract characteristics of letters and words of archival database samples, and questioned documents that need to be identified. Using standard clustering techniques, database document characteristics are categorized into distinct classes. These classes are labeled by the names of the known database writers. Identification of QDs is then made on the basis of the distance between QD characteristics and the database classes. We provide preliminary examples on a database of documents that was generated by nine writers. Seven different DWTs (and a voting scheme) were employed to make decisions about the QD's associations. The high success rate in identification of these samples indicates the potential that DWT-based method have in QD identification.<>",,0-7803-2127-8,10.1109/TFSA.1994.467230,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=467230,,Discrete wavelet transforms;Vocabulary;Image databases;Algorithm design and analysis;Image segmentation;Application software;Voting;Pattern recognition;Forensics;Contracts,handwriting recognition;document handling;document image processing;transforms;wavelet transforms;visual databases,discrete wavelet transform;questioned documents identification;archival database samples;standard clustering techniques;database document characteristics;voting scheme,,1,,9,,6-Aug-02,,,IEEE,IEEE Conferences
Direct and latent modeling techniques for computing spoken document similarity,直接和潛在建模技術，用於計算語音文檔相似度,T. J. Hazen,"MIT Lincoln Laboratory, Lexington, Massachusetts, USA",2010 IEEE Spoken Language Technology Workshop,24-Jan-11,2010,,,366,371,"Document similarity measures are required for a variety of data organization and retrieval tasks including document clustering, document link detection, and query-by-example document retrieval. In this paper we examine existing and novel document similarity measures for use with spoken document collections processed with automatic speech recognition (ASR) technology. We compare direct vector space approaches using the cosine similarity measure applied to feature vectors constructed with various forms of term frequency inverse document frequency (TF-IDF) normalization against latent topic modeling approaches based on latent Dirichlet allocation (LDA). In document link detection experiments on the Fisher Corpus, we find that an approach that applies bagging to models derived from LDA substantially outperforms the direct vector space approach.",,978-1-4244-7902-3,10.1109/SLT.2010.5700880,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5700880,document similarity;document link detection;latent topic modeling,Bagging;Training;Probabilistic logic;Feature extraction;Lattices;Automatic speech recognition,document handling;file organisation;information retrieval;speech recognition,direct modeling techniques;latent modeling techniques;spoken document similarity computing;data organization;query-by-example document retrieval;automatic speech recognition technology;term frequency inverse document frequency normalization;latent Dirichlet allocation;document link detection experiments;Fisher corpus,,10,,14,,24-Jan-11,,,IEEE,IEEE Conferences
SOPHIA: an interactive cluster-based retrieval system for the OHSUMED collection,SOPHIA：OHSUMED集合的基於集群的交互式檢索系統,V. Dobrynin; D. Patterson; M. Galushka; N. Rooney,"Dept. of Programming Technol., St. Petersburg State Univ., Russia; NA; NA; NA",IEEE Transactions on Information Technology in Biomedicine,13-Jun-05,2005,9,2,256,265,"The ability to perform an exploratory search and retrieval of relevant documents from a large collection of domain-specific documents is an important requirement both in the field of medicine and other areas. In this paper, we present a unsupervised distributional clustering technique called SOPHIA. SOPHIA provides a semantically meaningful visual clustering of the document corpus in conjunction with an intuitive interactive search facility. We assess the effectiveness of SOPHIA's cluster-based information retrieval for the MEDLINE testset collection known as OHSUMED.",1558-0032,,10.1109/TITB.2005.847184,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1435423,Clustering;information retrieval;MEDLINE,Clustering algorithms;Information retrieval;Indexing;Knowledge engineering;Partitioning algorithms;Testing;Power system modeling;Vocabulary;Euclidean distance,medical information systems;information retrieval systems;interactive systems;information retrieval,SOPHIA;interactive cluster-based retrieval system;OHSUMED collection;exploratory search;domain-specific document;unsupervised distributional clustering technique;semantically meaningful visual clustering;document corpus;interactive search faculty;cluster-based information retrieval;MEDLINE testset collection,"Artificial Intelligence;Cluster Analysis;Information Storage and Retrieval;Vocabulary, Controlled",10,,22,,13-Jun-05,,,IEEE,IEEE Journals
Sequential speaker clustering based on second order statistical measures,基於二階統計量的順序說話人聚類,S. Ouamour; H. Sayoud,"Electronics and Computer engineering Institute, USTHB University, Algiers, Algeria; Electronics and Computer engineering Institute, USTHB University, Algiers, Algeria",2010 The 2nd International Conference on Industrial Mechatronics and Automation,3-Aug-10,2010,2,,42,45,"This paper presents an investigation on the sequential clustering, based on second order statistical measures, for the task of speaker diarization. This clustering technique, which represents the second part of a global system of speaker indexing, is made for gathering the homogeneous segments obtained during the segmentation step. Most existing clustering systems are based on hierarchical clustering as agglomerative techniques. Such systems present two problems: finding the stopping criterion and choosing the threshold of clustering decision. In this research work, we try to resolve the first problem (stopping criterion) by proposing a sequential clustering approach based on second order statistical measures in order to gather the similar homogeneous segments (obtained by the segmentation process). At the end, each class will contain the global intervention of only one speaker in the entire audio document. Our sequential clustering approach uses a mono gaussian measure, called 弮G帣, which is able to assess the degree of similarity between the different homogeneous segments. Experiments are done on a subset of Hub4 Broadcast-News database and the corresponding results show that the implemented algorithms are interesting for the task of speaker clustering.",,978-1-4244-7656-5,10.1109/ICINDMA.2010.5538371,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5538371,Speaker clustering;speech segmentation;speaker diarization;sequential clustering;speech processing,Industrial electronics;Indexing;Speech processing;Computer industry;Electronics industry;Mechatronics;Automation;Assembly systems;TV broadcasting;MONOS devices,Gaussian processes;indexing;pattern clustering;speech processing;statistical analysis,sequential speaker clustering;second order statistical measures;speaker diarization;speaker indexing;agglomerative techniques;mono Gaussian measure;Hub4 Broadcast-News database;speech segmentation;audio document,,,,10,,3-Aug-10,,,IEEE,IEEE Conferences
Document Modeling Using Syntactic and Semantic Information,使用句法和語義信息的文檔建模,?. Au; M. Bouguessa; S. Wang,"Dept. of Comput. Sci., Univ. of Sherbrooke, Sherbrooke, QC, Canada; Dept. of Comput. Sci., Univ. of Quebec at Montreal, Montreal, QC, Canada; Dept. of Comput. Sci., Univ. of Sherbrooke, Sherbrooke, QC, Canada",2013 27th International Conference on Advanced Information Networking and Applications Workshops,1-Jul-13,2013,,,203,206,"This paper proposes a novel document representation model that considers both semantic and syntactic information. The proposed model combines syntactic and semantic components using, respectively, syntactic dependency trees and a word sense disambiguation algorithm. The suitability of the proposed model has been demonstrated through an application of document clustering. The results show that the incorporation of semantic information in syntactic dependency trees improves the quality of the clustering solution compared with documents modeled by syntactic dependency trees only.",,978-0-7695-4952-1,10.1109/WAINA.2013.75,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550397,document analysis;syntactic dependency trees;semantic refienement,Syntactics;Semantics;Vegetation;Hidden Markov models;Data mining;Databases;Context,document handling;trees (mathematics),document modeling;syntactic information;semantic information;document representation model;syntactic components;semantic components;syntactic dependency trees;word sense disambiguation algorithm,,1,,7,,1-Jul-13,,,IEEE,IEEE Conferences
A Vector Matrix Iterative Self-Organizing Assistant Clustering Algorithm of XML Document,XML文檔的矢量矩陣迭代自組織輔助聚類算法,B. Liu; L. Yang; Y. Deng,"Coll. of Inf. Sci. & Eng., Central-South Univ., Changsha; Coll. of Inf. Sci. & Eng., Central-South Univ., Changsha; Xiangya Hosp., Central-South Univ., Changsha",2008 The 9th International Conference for Young Computer Scientists,12-Dec-08,2008,,,1629,1634,"To improve the clustering quality of massive extensible markup language (XML) document clustering, this paper proposes a vector matrix iterative self-organizing assistant clustering algorithm of XML document (VMISACAX). The algorithm bases on the XML key, and transforms XML document into vector matrix, then carries out the optimizations of canceling, dissociating and uniting etc. In order to improve the convergence of the algorithm, a assistant strategy is imported to shorten the algorithm time under settling for clustering, to obtain best result of clustering by XML key's weights, but it doesn't always obtain the maximum distance's target of matrix vector clustering. Contrasted with other vector clustering algorithms, a series of emulation experiments show that this algorithm has proper the effectiveness and feasibility.",,978-0-7695-3398-8,10.1109/ICYCS.2008.170,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709217,XML key;Vector Matrix;Weight Clustering;Iteration Algorithm,Clustering algorithms;Iterative algorithms;XML,iterative methods;matrix algebra;pattern clustering;vectors;XML,vector matrix iterative self-organizing assistant clustering algorithm;XML document;extensible markup language,,,,9,,12-Dec-08,,,IEEE,IEEE Conferences
A summarizer system based on a semantic analysis of web documents,基於Web文檔語義分析的摘要器系統,A. Florence; V. Padmadas,"Thadomal Shahani Engineering College, Mumbai, India; Computer Department Thadomal Shahani Engineering College Mumbai, India",2015 International Conference on Technologies for Sustainable Development (ICTSD),30-Apr-15,2015,,,1,6,"The availability of web and search engines has made the search easier nowadays. Information overload is one of the major problems which require algorithms and tools for faster access. Electronic documents are one of the major sources of information for business and academic information. In order to fully utilizing these on-line documents effectively, it is crucial to be able to extract the summary of these documents. Summarization system will be one of the solutions to the above problem. This project proposes a summarizer system which will be able to perform summarization of multiple documents. The input text documents are analyzed through a parser which parses the input documents and generates parse tree for each sentence. RDF triples are extracted from each sentence by analyzing the typed dependencies generated from the parser in the form of subject, verb and object. Semantic distance is computed between each pair of sentences and a matrix containing the semantic distance for sentences are computed. The measure adopted to compute semantic distance is Wu and Palmer distance. A clustering algorithm is applied to the extracted subject, verb and object space and the extracted RDF triples are grouped into clusters. The important sentences are selected for final summary are extracted using sentence selection algorithm.",,978-1-4799-8187-8,10.1109/ICTSD.2015.7095851,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095851,Summarization;RDF;NLP;Semantic analysis;parse tree,Semantics;Resource description framework;Clustering algorithms;Algorithm design and analysis;Fires;Sustainable development;Text analysis,document handling;information analysis;information retrieval systems;pattern clustering,summarizer system;semantic analysis;Web documents;search engines;information overload;business information;academic information;electronic documents;document extraction;RDF triples;resource description framework;semantic distance;Wu-Palmer distance;clustering algorithm;sentence selection algorithm,,1,,18,,30-Apr-15,,,IEEE,IEEE Conferences
Centroid Ratio for a Pairwise Random Swap Clustering Algorithm,成對隨機交換聚類算法的質心比,Q. Zhao; P. Fr瓣nti,"School of Software Engineering, Tongji University, Shanghai, China; School of Computing, University of Eastern Finland, Joensuu, Finland",IEEE Transactions on Knowledge and Data Engineering,13-May-14,2014,26,5,1090,1101,"Clustering algorithm and cluster validity are two highly correlated parts in cluster analysis. In this paper, a novel idea for cluster validity and a clustering algorithm based on the validity index are introduced. A Centroid Ratio is firstly introduced to compare two clustering results. This centroid ratio is then used in prototype-based clustering by introducing a Pairwise Random Swap clustering algorithm to avoid the local optimum problem of k -means. The swap strategy in the algorithm alternates between simple perturbation to the solution and convergence toward the nearest optimum by k -means. The centroid ratio is shown to be highly correlated to the mean square error (MSE) and other external indices. Moreover, it is fast and simple to calculate. An empirical study of several different datasets indicates that the proposed algorithm works more efficiently than Random Swap, Deterministic Random Swap, Repeated k-means or k-means++. The algorithm is successfully applied to document clustering and color image quantization as well.",1558-2191,,10.1109/TKDE.2013.113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552197,Algorithms;Similarity measures;Quantization;Modeling structured;textual and multimedia data;Data clustering;random/deterministic swap;clustering evaluation;$k$ -means,Clustering algorithms;Algorithm design and analysis;Time complexity;Partitioning algorithms;Indexes;Prototypes;Euclidean distance,data handling;image processing;mean square error methods;pattern clustering,centroid ratio;pairwise random swap clustering algorithm;cluster analysis;validity index;prototype based clustering;swap strategy;mean square error;MSE;deterministic random swap;repeated k-means;document clustering;color image quantization,,4,,33,,2-Jul-13,,,IEEE,IEEE Journals
Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres,頻率敏感型競爭學習，用於高維超球上的可伸縮平衡聚類,A. Banerjee; J. Ghosh,"Dept. of Electr. & Comput. Eng., Univ. of Texas, Austin, TX, USA; Dept. of Electr. & Comput. Eng., Univ. of Texas, Austin, TX, USA",IEEE Transactions on Neural Networks,10-May-04,2004,15,3,702,719,"Competitive learning mechanisms for clustering, in general, suffer from poor performance for very high-dimensional (>1000) data because of ""curse of dimensionality"" effects. In applications such as document clustering, it is customary to normalize the high-dimensional input vectors to unit length, and it is sometimes also desirable to obtain balanced clusters, i.e., clusters of comparable sizes. The spherical kmeans (spkmeans) algorithm, which normalizes the cluster centers as well as the inputs, has been successfully used to cluster normalized text documents in 2000+ dimensional space. Unfortunately, like regular kmeans and its soft expectation-maximization-based version, spkmeans tends to generate extremely imbalanced clusters in high-dimensional spaces when the desired number of clusters is large (tens or more). This paper first shows that the spkmeans algorithm can be derived from a certain maximum likelihood formulation using a mixture of von Mises-Fisher distributions as the generative model, and in fact, it can be considered as a batch-mode version of (normalized) competitive learning. The proposed generative model is then adapted in a principled way to yield three frequency-sensitive competitive learning variants that are applicable to static data and produced high-quality and well-balanced clusters for high-dimensional data. Like kmeans, each iteration is linear in the number of data points and in the number of clusters for all the three algorithms. A frequency-sensitive algorithm to cluster streaming data is also proposed. Experimental results on clustering of high-dimensional text data sets are provided to show the effectiveness and applicability of the proposed techniques.",1941-0093,,10.1109/TNN.2004.824416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296696,,Frequency;Clustering algorithms;Power capacitors;Learning systems;Scalability;Data visualization;Application software;Hilbert space;Clustering methods,unsupervised learning;maximum likelihood estimation;pattern clustering,frequency-sensitive competitive learning;scalable balanced clustering;high-dimensional hyperspheres;spherical kmeans algorithm;maximum likelihood formulation;von Mises-Fisher distributions;cluster streaming data;balanced clustering;expectation maximization;text clustering,Artificial Intelligence;Cluster Analysis,56,,58,,10-May-04,,,IEEE,IEEE Journals
Phrase Ranking and Wikipedia Based Cluster Labeling,短語排名和基於維基百科的集群標籤,P. R. Chinthala,"Goa Campus, Dept. of Comput. Sci., BITS Pilani, Zuarinagar, India",2013 International Conference on Machine Intelligence and Research Advancement,9-Oct-14,2013,,,199,202,"Automatically labeling document clusters with words which indicate their topics is a relatively new and active research field. The most frequently used process, labeling with the most frequent words in the clusters, turns out using several words that are virtually void of descriptive power even after traditional stop words are eliminated. Another procedure, labeling with the most anticipated words, often include rather obscure results. We present Phrase Rank, a variation of the Page Rank algorithm based on relational graph representation of the content of web document collections. Phrase Rank achieves segregation and ranking of discriminative phrases higher than the ambiguous Phrases followed by common phrases. Thus a set of important text features are first extracted from the cluster documents. Further we use these features to extract cluster labels from the external knowledge sources such as pre-categorized knowledge of Wikipedia. We experiment with a test dataset to demonstrate the efficacy of Phrase Rank algorithm.",,978-0-7695-5013-8,10.1109/ICMIRA.2013.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918821,Cluster labeling;Phrase ranking;PageRank;Wikipedia,Internet;Encyclopedias;Electronic publishing;Labeling;Games;Clustering algorithms,graph theory;pattern clustering;text analysis;Web sites,phrase ranking;Wikipedia based cluster labeling;document clusters;topics;active research field;most frequent words;most anticipated words;page rank algorithm;relational graph representation;Web document collections;text feature extraction;cluster documents,,,,8,,9-Oct-14,,,IEEE,IEEE Conferences
Collaborative Clustering of XML Documents,XML文檔的協同集群,S. Greco; F. Gullo; G. Ponti; A. Tagarelli,"Dept. of Electron., Comput. & Syst. Sci. (DEIS), Univ. of Calabria, Arcavacata di Rende, Italy; Dept. of Electron., Comput. & Syst. Sci. (DEIS), Univ. of Calabria, Arcavacata di Rende, Italy; Dept. of Electron., Comput. & Syst. Sci. (DEIS), Univ. of Calabria, Arcavacata di Rende, Italy; Dept. of Electron., Comput. & Syst. Sci. (DEIS), Univ. of Calabria, Arcavacata di Rende, Italy",2009 International Conference on Parallel Processing Workshops,28-Dec-09,2009,,,579,586,"This paper presents a distributed collaborative approach to XML document clustering. According to a previous study, XML documents are mapped to a transactional domain, based on a data representation model which exploits the notion of XML tree tuple. This XML transactional model is well-suited to the identification of semantically cohesive substructures from XML documents, according to structure as well as content information. The proposed clustering framework employs a centroid-based partitional clustering paradigm in a distributed environment. Each peer in the network is allowed to compute a local clustering solution over its own data, then exchanges cluster centroids with other peers. The exchanged centroids correspond to recommendations offered by a peer to peers allowed to compute global representatives. Exploiting these recommendations, each peer becomes responsible for computing a global set of centroids for a given set of clusters. The overall clustering solution is hence computed in a collaborative way according to data from all the peers. Our approach has been evaluated on real XML document collections varying the number of peers. Results have shown that collaborative clustering leads to accurate overall clustering solutions with a relatively low load in the network.",2332-5690,978-1-4244-4923-1,10.1109/ICPPW.2009.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366138,XML;collaborative distributed clustering;XML structure and content information;transactional data,XML;Peer to peer computing;Collaborative work;International collaboration;Parallel processing;Concurrent computing;Distributed computing;Computer networks;Query processing;Information retrieval,data structures;groupware;pattern clustering;peer-to-peer computing;trees (mathematics);XML,collaborative clustering;XML documents;distributed collaborative approach;data representation;XML tree tuple;centroid-based partitional clustering,,3,,18,,28-Dec-09,,,IEEE,IEEE Conferences
Semantic based clustering system with cloud computing,基於語義的雲計算集群系統,H. H. Tar; Z. Y. Zaw,"University of Computer Studeis, Yangon(UCSY), Yangon, Myanmar; Union Election Commission (UEC), Nay Pyi Taw, Myanmar",2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),5-Feb-18,2017,,,195,197,"This paper presented a novel approach for document clustering applying cloud technologies for the system's performance issue. It is critical for application service in cloud computing to provide precise information. Ontology servicing is one of the methods to deal with semantic ambiguity and information overload efficiently through appropriate semantic models and semantic technology. This system is the advanced and extended version of the system we have been published before. The experiments reveal that even the testing documents increased; the system may actually be able to produce useful result for text document clustering. In this paper, we propose a cloud service that exploits a novel ontology-based technique for identifying cloud service to improve the accuracy of cloud services searching. Our approach has the capability to perform cloud service concepts from cloud service sources. The main idea behind our method is cloud services using an ontology-based technique.",2189-8723,978-1-5090-6664-3,10.1109/ICIIBMS.2017.8279711,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279711,Clustering;Ontology;Cloud Technology,Cloud computing;Ontologies;Semantics;Semantic Web;Signal processing;Telecommunications,cloud computing;ontologies (artificial intelligence);pattern clustering;text analysis,testing documents;text document clustering;cloud services searching;cloud service concepts;cloud service sources;cloud computing;ontology servicing;semantic ambiguity;appropriate semantic models;semantic technology;semantic-based clustering system,,1,,6,,5-Feb-18,,,IEEE,IEEE Conferences
Intelligent Extended Clustering Genetic Algorithm,智能擴展聚類遺傳算法,N. El-Bathy; G. Azar; M. El-Bathy; G. Stein,"Department of Mathematics and Computer Science, Lawrence Technological University, Southfield, MI, United State of America; Department of Mathematics and Computer Science, Lawrence Technological University, Southfield, MI, United State of America; Department of Mathematics and Computer Science, Lawrence Technological University, Southfield, MI, United State of America; Department of Mathematics and Computer Science, Lawrence Technological University, Southfield, MI, United State of America",2011 IEEE INTERNATIONAL CONFERENCE ON ELECTRO/INFORMATION TECHNOLOGY,8-Aug-11,2011,,,1,5,"In this paper, the problem of clustering intelligent web using K-means algorithm has been analyzed and the need for a new data clustering algorithm such as Genetic Algorithm (GA) is justified. We propose an Intelligent Extended Clustering Genetic Algorithm (IECGA) using Business Process Execution Language (BPEL) to be an optimal solution for data clustering. It improves the efficiency and performance for retrieving a proper information results that satisfy user's needs. The proposed IECGA uses several mutation operators simultaneously to produce next generation. This series of random mutation process depend on chromosome best fitness in the population and rely on high relevancy as well. The mutation operation will guarantee the success of IECGA for data clustering since it expands the search. So the highly effective mutation operators the greater effects on the genetic process. Finally, IECGA for data clustering gives the user needed documents based on similarity between query matching and relevant document mechanism. The results obtained from the web intelligent search engine are optimal.",2154-0373,978-1-61284-466-4,10.1109/EIT.2011.5978607,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978607,BPEL;Clustering Genetic Algorithm;K-Means;Intelligent Agent,Genetic algorithms;Clustering algorithms;Algorithm design and analysis;Biological cells;Service oriented architecture;Publishing,business data processing;document handling;genetic algorithms;pattern clustering;query processing;random processes;search engines,intelligent extended clustering genetic algorithm;k-means algorithm;data clustering algorithm;business process execution language;information retrieval;IECGA;mutation operator;random mutation process;chromosome;query matching;document mechanism;Web intelligent search engine,,5,,11,,8-Aug-11,,,IEEE,IEEE Conferences
Unsupervised Exemplar-Based Learning for Improved Document Image Classification,基於無監督的基於示例的學習，用於改進的文檔圖像分類,S. Abuelwafa; M. Pedersoli; M. Cheriet,"?cole de technologie sup矇rieure, University of Quebec, Montreal, QC, Canada; ?cole de technologie sup矇rieure, University of Quebec, Montreal, QC, Canada; ?cole de technologie sup矇rieure, University of Quebec, Montreal, QC, Canada",IEEE Access,25-Sep-19,2019,7,,133738,133748,"Many recent state-of-the-art approaches for document image classification are based on supervised feature learning that requires a large amount of labeled training data. In real-world problem of document image classification, the available amount of labeled data is limited and scarce while a large amount of unlabeled data is often available at almost no cost. In this paper, we present an approach for learning visual features for document analysis in an unsupervised way, which improves the document image classification performance without increasing the amount of annotated data. The proposed approach trains a neural network model on an auxiliary task in which every training example is associated with a different label (exemplar) and expanded to multiple images through a data augmentation technique. Thus, the learned model, which is trained in an unsupervised way, is used to boost the document classification performance. In fact, this learned model has proved to be consistently efficient in two different settings: i) as an unsupervised feature extractor to represent document images for an unsupervised classification task (i.e., clustering); and ii) in the parameters initialization of a supervised classification task trained with a small amount of annotated data. We perform experiments on the Tobacco-3482 dataset and demonstrate the capability of our approach to improve i) the unsupervised classification accuracy up to 2.4%; and ii) the supervised classification accuracy by 1.5% without any extra data or by 5% when using 3000 additional not annotated samples.",2169-3536,,10.1109/ACCESS.2019.2940884,NSERC of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843852,Document image classification;document analysis;document image representation,Task analysis;Data models;Visualization;Training;Optical character recognition software;Training data;Feature extraction,document image processing;feature extraction;image classification;neural nets;unsupervised learning,data augmentation technique;document classification performance;unsupervised feature extractor;unsupervised classification task;supervised classification task;supervised feature learning;labeled training data;unlabeled data;document analysis;document image classification performance;unsupervised exemplar-based learning;Tobacco-3482 dataset;neural network,,,,42,CCBY,18-Sep-19,,,IEEE,IEEE Journals
Image data mining from financial documents based on wavelet features,基於小波特徵的財務憑證圖像數據挖掘,O. El Badawy; M. R. El-Sakka; K. Hassanein; M. S. Kamel,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada; NA; NA; NA",Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205),7-Aug-02,2001,1,,1078,1081 vol.1,"We present a framework for clustering and classifying cheque images according to their payee-line content. The features used in the clustering and classification processes are extracted from the wavelet domain by means of thresholding and counting of wavelet coefficients. The feasibility of this framework is tested on a database of 2620 cheque images. This database consists of cheques from 10 different accounts. Each account is written by a different person. Clustering and classification are performed separately on each account using distance-based techniques. We achieved correct-classification rates of 86% and 81% for the supervised and unsupervised learning cases, respectively. These rates are the average of correct-classification rates obtained from the 10 different accounts.",,0-7803-6725-1,10.1109/ICIP.2001.959236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=959236,,Data mining;Wavelet domain;Image databases;Feature extraction;Discrete wavelet transforms;Spatial databases;Unsupervised learning;Pattern recognition;Design engineering;Systems engineering and theory,feature extraction;data mining;document image processing;banking;pattern clustering;image classification;wavelet transforms;unsupervised learning;learning (artificial intelligence);handwritten character recognition;cheque processing,image data mining;financial documents;wavelet features;cheque image clustering;cheque image classifying;payee-line content;feature extraction;thresholding;distance-based techniques;supervised learning;unsupervised learning;handwritten text,,1,,11,,7-Aug-02,,,IEEE,IEEE Conferences
Towards Reliable Clustering of English Text Documents Using Correlation Coefficient,利用相關係數實現英語文本文檔的可靠聚類,H. Bhaumik; B. Chakraborty; A. Mukherjee; S. Bhattacharyya; M. Chattopadhyay,"Dept. of Inf. Technol., RCC Inst. of Inf. Technol., Kolkata, India; Dept. of Comput. Applic., RCC Inst. of Inf. Technol., Kolkata, India; Dept. of Inf. Technol., RCC Inst. of Inf. Technol., Kolkata, India; Dept. of Comput. Applic., RCC Inst. of Inf. Technol., Kolkata, India; Inf. Technol. Area, Indian Inst. of Manage., Raipur, India",2014 International Conference on Computational Intelligence and Communication Networks,26-Mar-15,2014,,,530,535,"This paper proposes a new approach for clustering English text documents, based on finding the pair wise correlation of documents in a given set of text documents. The correlation coefficient for each pair of documents is calculated on the basis of ranks given to the words in the documents. The ranking of the words occurring in a document is computed on the basis of weights of the words calculated according to the conventional TF-IDF factor. The proposed method is found to be able to cluster a given set of text documents into a number of classes depending on their contents where the number of classes is not known a priori. It is revealed from experimental results that the proposed method of text categorization using correlation coefficient performs better than some of the other text categorization methods, including methods that use artificial neural network.",,978-1-4799-6929-6,10.1109/CICN.2014.121,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065541,clustering;text classification;correlation coefficient,Correlation coefficient;Correlation;Vectors;Text categorization;Clustering algorithms;Classification algorithms;Equations,natural language processing;pattern clustering;statistical analysis;text analysis,reliable clustering;English text document;correlation coefficient;pairwise correlation;TF-IDF factor;text categorization,,2,,40,,26-Mar-15,,,IEEE,IEEE Conferences
A maximal frequent itemset approach for Web document clustering,Web文檔聚類的最大頻繁項集方法,Ling Zhuang; Honghua Dai,"Sch. of Inf. Technol., Deakin Univ., Burwood, Vic., Australia; Sch. of Inf. Technol., Deakin Univ., Burwood, Vic., Australia","The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.",30-Nov-04,2004,,,970,977,"To efficiently and yet accurately cluster Web documents is of great interests to Web users and is a key component of the searching accuracy of a Web search engine. To achieve this, this paper introduces a new approach for the clustering of Web documents, which is called maximal frequent itemset (MFI) approach. Iterative clustering algorithms, such as K-means and expectation-maximization (EM), are sensitive to their initial conditions. MFI approach firstly locates the center points of high density clusters precisely. These center points then are used as initial points for the K-means algorithm. Our experimental results tested on 3 Web document sets show that our MFI approach outperforms the other methods we compared in most cases, particularly in the case of large number of categories in Web document sets.",,0-7695-2216-5,10.1109/CIT.2004.1357322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357322,,Itemsets;Clustering algorithms;Iterative algorithms;Information technology;Search engines;Data mining;Road transportation;Australia;Web search;Testing,Internet;iterative methods;optimisation;pattern clustering;data mining;text analysis,maximal frequent itemset approach;Web document clustering;Web search engine;K-means iterative clustering;expectation-maximization iterative clustering,,12,,16,,30-Nov-04,,,IEEE,IEEE Conferences
Extract list data from semi-structured document using clustering,使用聚類從半結構化文檔中提取列表數據,Hui Xu; Juanzi Li; Peng Xu,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China",2005 International Conference on Natural Language Processing and Knowledge Engineering,27-Feb-06,2005,,,559,564,"This paper is concerned with list data extraction from semi-structured documents. By list data extraction, we mean extracting data from lists and grouping it by rows and columns. List, which has structured characteristics, is used to store highly structured and database-like information in many semi-structured documents, such as business annual reports, online airport listings, catalogs, hotel directories, etc. List data extraction is of benefit to text mining applications on semi-structured documents. Several research efforts have been done on structured data extraction from semi-structured documents by utilizing the word layout and arrangement information. However, as far as we know, few studies have been sufficiently investigated on list data extraction making use of the semantic information previously. In this paper, we propose a clustering based method making use of not only the layout and arrangement information but also the semantic information of words for this extraction task. We show experimental results on plain-text annual reports from Shanghai Stock Exchange, in which 73.49% of the lists were extracted correctly.",,0-7803-9361-9,10.1109/NLPKE.2005.1598800,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598800,,Data mining;Clustering algorithms;Databases;Airports;Catalogs;Stock markets;Computer science;Text mining;Humans;Computer applications,information retrieval;document handling;pattern clustering;data mining;text analysis,list data extraction;semistructured document;clustering;business annual reports;online airport listings;catalogs;hotel directories;text mining;semantic information,,,,18,,27-Feb-06,,,IEEE,IEEE Conferences
Graph Clustering-Based Ensemble Method for Handwritten Text Line Segmentation,基於圖聚類的文本行分割方法,V. Manohar; S. N. Vitaladevuni; H. Cao; R. Prasad; P. Natarajan,"Speech, Language, & Multimedia Bus. Unit, Raytheon BBN Technol., Cambridge, MA, USA; Speech, Language, & Multimedia Bus. Unit, Raytheon BBN Technol., Cambridge, MA, USA; Speech, Language, & Multimedia Bus. Unit, Raytheon BBN Technol., Cambridge, MA, USA; Speech, Language, & Multimedia Bus. Unit, Raytheon BBN Technol., Cambridge, MA, USA; Speech, Language, & Multimedia Bus. Unit, Raytheon BBN Technol., Cambridge, MA, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,574,578,"Handwritten text line segmentation on real-world data presents significant challenges that cannot be overcome by any single technique. Given the diversity of approaches and the recent advances in ensemble-based combination for pattern recognition problems, it is possible to improve the segmentation performance by combining the outputs from different line finding methods. In this paper, we propose a novel graph clustering-based approach to combine the output of an ensemble of text line segmentation algorithms. A weighted undirected graph is constructed with nodes corresponding to connected components and edge connecting pairs of connected components. Text line segmentation is then posed as the problem of minimum cost partitioning of the nodes in the graph such that each cluster corresponds to a unique line in the document image. Experimental results on a challenging Arabic field dataset using the ensemble method shows a relative gain of 18% in the F1 score over the best individual method within the ensemble.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.121,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065376,text line segmentation;handwriting;ensemble method;graph clustering,Image segmentation;Clustering algorithms;Handwriting recognition;Image edge detection;Conferences;Corporate acquisitions;Measurement,document image processing;graph theory;handwritten character recognition;pattern recognition;text analysis,graph clustering-based ensemble method;handwritten text line segmentation;hndwritten text line segmentation;real-world data;ensemble-based combination;pattern recognition problems;segmentation performance;line finding methods;graph clustering-based approach;text line segmentation algorithms;weighted undirected graph;document image;Arabic field dataset,,10,,18,,3-Nov-11,,,IEEE,IEEE Conferences
A contribution to modification of PART clustering algorithm for text processing,為修改PART聚類算法進行文本處理做出的貢獻,R. Forgac; R. Krakovsky; I. Mokris,"Institute of Informatics Slovak Academy of Sciences, Bratislava, Slovakia; Catholic University, Department of Informatics, Ruzomberok, Slovakia; Institute of Informatics Slovak Academy of Sciences, Bratislava, Slovakia",2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES),23-Nov-15,2015,,,421,425,"The paper presents one of the possible modifications of the Projective Adaptive Resonance Theory (PART) clustering algorithm and its application in the processing of text documents. Clustering on the basis of PART can be applied, for example, to generate a dictionary of keywords from a text. The principle of this method is based on clustering of words with the same root word. In order to demonstrate the increased clustering accuracy, the achieved clustering results with modified algorithm are compared with the results based on the standard PART algorithm.",,978-1-4673-7939-7,10.1109/INES.2015.7329747,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329747,,Clustering algorithms;Neurons;Dictionaries;Standards;Text processing;Accuracy;Subspace constraints,pattern clustering;text analysis,PART clustering algorithm;text document processing;projective adaptive resonance theory clustering algorithm;word clustering,,,,19,,23-Nov-15,,,IEEE,IEEE Conferences
Online PLSA: Batch Updating Techniques Including Out-of-Vocabulary Words,在線PLSA：包括詞彙外單詞在內的批處理更新技術,N. K. Bassiou; C. L. Kotropoulos,"Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece",IEEE Transactions on Neural Networks and Learning Systems,15-Oct-14,2014,25,11,1953,1966,"A novel method is proposed for updating an already trained asymmetric and symmetric probabilistic latent semantic analysis (PLSA) model within the context of a varying document stream. The proposed method is coined online PLSA (oPLSA). The oPLSA employs a fixed-size moving window over a document stream to incorporate new documents and at the same time to discard old ones (i.e., documents that fall outside the scope of the window). In addition, the oPLSA assimilates new words that had not been previously seen (out-of-vocabulary words), and discards the words that exclusively appear in the documents to be thrown away. To handle the new words, Good-Turing estimates for the probabilities of unseen words are exploited. The experimental results demonstrate the superiority in terms of accuracy of the oPLSA over well known PLSA updating methods, such as the PLSA folding-in (PLSA fold.), the PLSA rerun from the breakpoint, the quasi-Bayes PLSA, and the Incremental PLSA. A comparison with respect to the CPU run time reveals that the oPLSA is the second fastest method after the PLSA fold. However, the better accuracy of the oPLSA than that of the PLSA fold. pays off the longer computation time. The oPLSA and the other PLSA updating methods together with online LDA are tested for document clustering and F1 scores are also reported.",2162-2388,,10.1109/TNNLS.2014.2299806,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737290,Document clustering;document modeling;information retrieval;out-of-vocabulary (OOV) words;PLSA updating;probabilistic latent semantic analysis (PLSA);unsupervised learning.;Document clustering;document modeling;information retrieval;out-of-vocabulary (OOV) words;PLSA updating;probabilistic latent semantic analysis (PLSA);unsupervised learning,Vocabulary;Data models;Mathematical model;Probabilistic logic;Computational modeling;Matrix decomposition;Semantics,document handling;natural language processing;probability;vocabulary,document clustering;second fastest method;online PLSA updating method;oPLSA;asymmetric probabilistic latent semantic analysis model;symmetric probabilistic latent semantic analysis;out-of-vocabulary words;batch updating techniques,,11,,45,,11-Feb-14,,,IEEE,IEEE Journals
LIMTopic: A Framework of Incorporating Link Based Importance into Topic Modeling,LIMTopic：將基於鏈接的重要性納入主題建模的框架,D. Duan; Y. Li; R. Li; R. Zhang; X. Gu; K. Wen,"School of Computer Science and Technology, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China; Department of Computing and Information Systems, University of Melbourne, Austral; School of Computer Science and Technology, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China",IEEE Transactions on Knowledge and Data Engineering,4-Sep-14,2014,26,10,2493,2506,"Topic modeling has become a widely used tool for document management. However, there are few topic models distinguishing the importance of documents on different topics. In this paper, we propose a framework LIMTopic to incorporate link based importance into topic modeling. To instantiate the framework, RankTopic and HITSTopic are proposed by incorporating topical pagerank and topical HITS into topic modeling respectively. Specifically, ranking methods are first used to compute the topical importance of documents. Then, a generalized relation is built between link importance and topic modeling. We empirically show that LIMTopic converges after a small number of iterations in most experimental settings. The necessity of incorporating link importance into topic modeling is justified based on KL-Divergences between topic distributions converted from topical link importance and those computed by basic topic models. To investigate the document network summarization performance of topic models, we propose a novel measure called log-likelihood of ranking-integrated document-word matrix. Extensive experimental results show that LIMTopic performs better than baseline models in generalization performance, document clustering and classification, topic interpretability and document network summarization performance. Moreover, RankTopic has comparable performance with relational topic model (RTM) and HITSTopic performs much better than baseline models in document clustering and classification.",1558-2191,,10.1109/TKDE.2013.2297912,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702464,Link Importance;topic modeling;model framework;document network;log-likelihood of ranking-integrated document-word matrix,Computational modeling;Mathematical model;Communities;Equations;Vectors;Analytical models;Algorithm design and analysis,convergence of numerical methods;document handling;iterative methods;matrix algebra;pattern classification;pattern clustering;statistical analysis,LIMTopic;link based importance;topic modeling;document management;RankTopic;HITSTopic;topical PageRank;topical HITS;KL-divergences;document network summarization performance;ranking-integrated document-word matrix log-likelihood;document clustering;document classification;topic interpretability,,13,,41,,9-Jan-14,,,IEEE,IEEE Journals
Web Services Clustering Based on HDP and SOM Neural Network,基於HDP和SOM神經網絡的Web服務聚類,Q. Xiao; B. Cao; X. Zhang; J. Liu; R. Hu; B. Li,"Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of Comput., Wuhan Univ., Wuhan, China","2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",6-Dec-18,2018,,,397,404,"Discovering appropriate, user-desired Web services from massive Web services quickly and accurately for users to build valuable Web applications has become a significant challenge. Web services clustering has been proved to be an effective way for Web service discovery. Some recent research works exploit additional information (such as, tags or word clusters information) to argument LDA-based topic modeling for better service clustering. Although they definitely boost service clustering via mining more implicit topics and semantic correlation of service document, their performance still can be improved due to inherent disadvantage of LDA topic model and adopted clustering algorithms. To address this problem, we propose a Web services clustering method based on HDP topic model and SOM neural network. This method, firstly uses Word2Vec to expand the original Web services description document from Wikipedia English corpus and applies HDP topic model to model the expanded Web services description document to obtain the document-topic vector. At last, it employs SOM algorithm on the document-topic vector to achieve Web services clustering. The comparative experiments are performed on ProgrammableWeb dataset. The experimental results show that the proposed method respectively achieves significant improvements of 486%, 54.0%, 35.8%, 47.1%, 39.0%, 28.6%, and 9.4%, compared with other seven service clustering methods.",,978-1-5386-9380-3,10.1109/SmartWorld.2018.00097,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560075,Web Services;Word2Vec;HDP topic model;Self Organizing Maps;Web Service Clustering,,data mining;pattern clustering;self-organising feature maps;Web services,document-topic vector;Web services clustering;seven service clustering methods;SOM neural network;appropriate user-desired Web services;massive Web services;valuable Web applications;Web service discovery;word clusters information;argument LDA-based topic;service document;HDP topic model;original Web services description document;expanded Web services description document,,1,,23,,6-Dec-18,,,IEEE,IEEE Conferences
Segmentation-free word spotting using SIFT,使用SIFT的無分段單詞發現,D. Lee; W. Hong; I. Oh,"Department of Computer Science and Engineering, Chonbuk National University, Deokjin-dong 664-14, Jeonju, Korea; Department of Computer Science and Engineering, Chonbuk National University, Deokjin-dong 664-14, Jeonju, Korea; Department of Computer Science and Engineering, Chonbuk National University, Deokjin-dong 664-14, Jeonju, Korea",2012 IEEE Southwest Symposium on Image Analysis and Interpretation,21-May-12,2012,,,65,68,"This paper presents a method for word spotting using SIFT. The method performs well for multi-lingual document images and it works without skew correction and without any segmentation process. The algorithm for detecting words in test images is described, including the processes of detecting keypoints, matching them, and clustering matching pairs. Experiments on English and Korean document image collections showed that the proposed method is promising.",,978-1-4673-1830-3,10.1109/SSIAI.2012.6202454,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6202454,document image retrieval;local descriptor;scale and rotation invariance,Frequency locked loops;Image segmentation;Manganese,document image processing;image matching;image segmentation;natural language processing;pattern clustering;word processing,segmentation-free word spotting;SIFT;muItilingual document image;word detection;keypoint detection;keypoint matching;clustering matching pairs;English document image collection;Korean document image collection,,8,,10,,21-May-12,,,IEEE,IEEE Conferences
Topic based semantic clustering using Wikipedia knowledge,使用維基百科知識的基於主題的語義聚類,B. Ramudu; M. N. Murty,"Centre for Artificial Intelligence And Robotics, DRDO, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India",2012 International Conference on Data Science & Engineering (ICDSE),27-Aug-12,2012,,,1,7,"Most of the existing academic social networks like DBLP and ArnetMiner suggest co-authors as being related to each author based on author and co-author details, which are extracted from academic researchers publications. Instead of this approach, we propose grouping of researchers on basis of topics they have in common that would make more semantic sense for a user searching for publications in an area. Identification of relevant complete topic phrases, discovery of equivalent topics (synonyms) using terms in document collections are fundamental problems in information retrieval, natural language processing, pattern recognition, etc. In this paper, we propose a novel approach, wherein we identify a document's topic using Wikipedia. Wikipedia is the largest freely available crowd-source with up-to-date knowledge containing around 20 millions topics. The document collection we consider is the collection of academic researchers' publications. The documents are mapped to topics extracted from documents using Wikipedia to generate a document-topic representation. Then clustering algorithm is applied on document-topic representation to group semantically related researchers and generate a topic based academic researchers' Social Network. We use Adaptive Rough Fuzzy Leader (ARFL) a soft clustering algorithm, since each researcher can have expertise in more than one area and they can belong to more than one group. We present the empirical evaluation of our proposed scheme. We also demonstrate how our proposed solution is scalable to various domain areas and can be used to design topic based retrieval systems.",,978-1-4673-2149-5,10.1109/ICDSE.2012.6281905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6281905,Semantic clustering;Topic model;Academic Social networking;Wikipedia knowledge,Internet;Encyclopedias;Electronic publishing;Data mining;Artificial intelligence,document handling;fuzzy set theory;information retrieval;pattern clustering;rough set theory;semantic Web;social networking (online),topic based semantic clustering;Wikipedia knowledge;academic social networks;DBLP;ArnetMiner;coauthors;academic researchers publications;relevant complete topic phrases;equivalent topic discovery;document collections;information retrieval;natural language processing;pattern recognition;crowd-source;document-topic representation;adaptive rough fuzzy leader;ARFL;soft clustering algorithm;topic based retrieval systems,,2,,26,,27-Aug-12,,,IEEE,IEEE Conferences
Investigating Semantic Measures in XML Clustering,調查XML群集中的語義測度,R. Nayak,"Queensland University of Technology, Australia",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,1042,1045,This paper discusses the influence of semantic computation in structural data such as XML for clustering similar data. We study how the semantic similarity at individual element level influences the overall similarity of documents. The empirical results indicate that the semantic measures do not play an important role in finding clusters in structural data such as XML,,0-7695-2747-7,10.1109/WI.2006.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061518,,XML;Clustering algorithms;Information systems;Australia;Pattern matching;Document handling;Tree graphs;Data models;Dictionaries;Ontologies,computational linguistics;information retrieval;Internet;pattern clustering;XML,semantic measures;XML clustering;semantic computation;semantic similarity;data mining;data retrieval;Web intelligence,,12,,11,,15-Jan-07,,,IEEE,IEEE Conferences
An efficient and scalable algorithm for clustering XML documents by structure,一種高效且可擴展的按結構對XML文檔進行聚類的算法,Wang Lian; D. W. -. Cheung; N. Mamoulis; Siu-Ming Yiu,"Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China; Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China; Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China; Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China",IEEE Transactions on Knowledge and Data Engineering,19-Feb-04,2004,16,1,82,96,"With the standardization of XML as an information exchange language over the Internet, a huge amount of information is formatted in XML documents. In order to analyze this information efficiently, decomposing the XML documents and storing them in relational tables is a popular practice. However, query processing becomes expensive since, in many cases, an excessive number of joins is required to recover information from the fragmented data. If a collection consists of documents with different structures (for example, they come from different DTDs), mining clusters in the documents could alleviate the fragmentation problem. We propose a hierarchical algorithm (S-GRACE) for clustering XML documents based on structural information in the data. The notion of structure graph (s-graph) is proposed, supporting a computationally efficient distance metric defined between documents and sets of documents. This simple metric yields our new clustering algorithm which is efficient and effective, compared to other approaches based on tree-edit distance. Experiments on real data show that our algorithm can discover clusters not easily identified by manual inspection.",1558-2191,,10.1109/TKDE.2004.1264824,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1264824,,Clustering algorithms;XML;Query processing;Relational databases;Object oriented databases;Computer Society;Standardization;Information analysis;Tree graphs;Inspection,data mining;XML;pattern clustering;Internet;graph theory;query processing,scalable algorithm;XML document clustering;XML standardization;information exchange language;relational tables;query processing;cluster mining;fragmentation problem;hierarchical algorithm;S-GRACE;structural information;structure graph;tree-edit distance;data mining;semistructured data,,112,,24,,19-Feb-04,,,IEEE,IEEE Journals
An improved web information summarization method using Sentence Similarity-Based Soft clustering,一種基於句子相似度的軟聚類的改進的Web信息匯總方法,Jun Tang; Xiaojuan Zhao,"Department of Information Engineering, Hunan Urban Construction College, XiangTan, China; Department of Information Engineering, Hunan Urban Construction College, XiangTan, China",2009 International Conference on Future BioMedical Information Engineering (FBIE),5-Feb-10,2009,,,292,295,"For the explosion of information in the World Wide Web, this paper proposed a new method of web news summarization via soft clustering algorithm. It used search engine to extract relevant documents, and mixed query sentence into sentences set which segmented from multi-document set, then this paper adopted efficient soft cluster algorithm SSSC (sentence similarity-based soft clustering) to cluster all the sentences. Experimental result shows that the proposed summarization method can improve the performance of summary, soft clustering algorithm is efficient.",2157-9601,978-1-4244-4690-2,10.1109/FBIE.2009.5405911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5405911,summarization;clustering;sentence similarity,Clustering algorithms;Biomedical engineering;Educational institutions;Data mining;Explosions;Search engines;Biomedical measurements;Web sites;Text processing;Clustering methods,Internet;search engines;Web sites,sentence similarity-based soft clustering;Web information summarization method;World Wide Web;search engine;document extraction;mixed query sentence;soft clustering algorithm;multidocument set,,,,7,,5-Feb-10,,,IEEE,IEEE Conferences
Improved clustering technique using metadata for text mining,使用元數據進行文本挖掘的改進聚類技術,R. E. Thomas; S. S. Khan,"Department of Computer Engineering, St. Francis Institute of Technology, Mumbai, India; Department of Computer Engineering, St. Francis Institute of Technology, Mumbai, India",2016 International Conference on Communication and Electronics Systems (ICCES),30-Mar-17,2016,,,1,5,"In many text mining applications, information from Document is present in the form of Text along with Side Information or Metadata. Examples of this side information include links to other web pages, title of the document, author name or date of Publication which are present in the text document. Such metadata may possess a lot of information for the clustering purposes. But this Side information may be sometimes noisy. Using such Side Information for producing clusters without filtering it, can result to bad quality of Clusters. So we use an efficient Feature Selection method to perform the mining process to select that Side Information which is useful for Clustering so as to maximize the advantages from using it. The proposed technique makes use of the process of Two-mode clustering which is a data mining technique that allows producing groups by Clustering both Text and Side Information.",,978-1-5090-1066-0,10.1109/CESYS.2016.7889835,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889835,Co-Clustering;Data Mining;Text Mining;Side Information,Clustering algorithms;Indexes;Algorithm design and analysis;Partitioning algorithms;Noise measurement;Spatial databases;Data mining,data mining;feature selection;meta data;pattern clustering;text analysis,clustering technique;metadata;text mining;feature selection;side information;text information,,1,,12,,30-Mar-17,,,IEEE,IEEE Conferences
Search Results Clustering in Chinese Context Based on a New Suffix Tree,基於新後綴樹的中文上下文搜索結果聚類,J. Wu; Z. Wang,"Inst. of Syst. Eng. Dalian, Univ. of Technol., Dalian; NA",2008 IEEE 8th International Conference on Computer and Information Technology Workshops,18-Jul-08,2008,,,110,115,"Searching for information by search engines has been gaining popularity in recent years. However, results returned by most Chinese Web search engines usually reach up to thousands or even millions documents, so search results clustering is of critical need for on-line grouping of similar documents to improve user experience while searching collections of Web pages and facilitate browsing Chinese Web pages in a more compact and thematic form. This paper presents a new suffix tree clustering (STC) algorithm for Web search results clustering, which is more suitable for Chinese context. It is built in terms of Chinese words, of which meaningless phrases are ignored by an efficient strategy we proposed. Meanwhile the Chinese synonymy is introduced into the suffix tree to improve the quality of the clusters. Experiments show that the proposed novel STC algorithm has a better performance in precision and speed than original STC.",,978-0-7695-3242-4,10.1109/CIT.2008.Workshops.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568488,Search engines;Suffix tree clustering;Chinese synonymy,Clustering algorithms;Web search;Search engines;Web pages;Systems engineering and theory;Information technology;Conferences;Clustering methods;Information filtering;Information filters,document handling;Internet;pattern clustering;search engines;trees (mathematics),Chinese context clustering;Chinese Web search engines;Web pages;suffix tree clustering algorithm;thematic form,,1,,13,,18-Jul-08,,,IEEE,IEEE Conferences
Mining conceptual rules for web document using sentence ranking conditional probability,使用句子排序條件概率挖掘Web文檔的概念規則,V. M. Navaneethakumar,"Department of Computer Applications K.S.R College of Engineering Tiruchengode, India","2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering",15-Apr-13,2013,,,169,174,"Text classification and information mining are two significant objectives of natural language processing. Whereas handcrafting rules for both tasks has an extensive convention, learning strategies increased much attention in the past. Existing work presented concept based mining model for text, sentence mining and does not support text classification. To enhance the text clustering approach, we first presented Conceptual Rule Mining On Text clusters to evaluate the more related and influential sentences contributing the document topic. But this model might discriminate terms with semantic variation and negligible authority on the sentence meaning. In addition, we plan to extend conceptual text clustering to web documents, by assigning sentence weights based on conditional probability. Probability ratio is identified for the sentence similarity from which unique sentence meaning contributing to the document topic are listed. In this work, our plan is to implement ranking of the sentences which are calculated using the weights assigned to each and every sentences. With sentence rank conceptual rules are defined for the text cluster documents. The conceptual rule depicts finer tuned document topic and sentence meaning utilized to evaluate the global document contribution. Experiments are conducted with the web documents extracted from the research repositories to evaluate the efficiency of the proposed efficient conceptual rule mining on web document clusters using sentence ranking conditional probability [CRMSRCP] and compared with an existing Model for Concept Based Clustering and Classification and our previous works in terms of Sentence Term Relation, Cluster Object weights, and cluster quality.",,978-1-4673-5845-3,10.1109/ICPRIME.2013.6496467,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496467,Concept-based mining model;Conceptual rule mining;text clustering;conditional probability;probability ratio;ranking,Text mining;Informatics;Mobile communication;Semantics;Natural language processing,data mining;Internet;pattern classification;pattern clustering;probability;text analysis,conceptual rule mining;Web document;sentence ranking conditional probability;text classification;information mining;natural language processing;learning strategy;concept based mining;sentence mining;text clustering approach;document topic;semantic variation;sentence meaning;probability ratio;CRMSRCP;sentence term relation;cluster object weights;cluster quality,,,,11,,15-Apr-13,,,IEEE,IEEE Conferences
Trend Analysis of Machine Learning - A Text Mining And Document Clustering Methodology,機器學習趨勢分析-文本挖掘和文檔聚類方法,J. Yang; W. Liao; W. Wu; C. Yin,"Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan",2009 International Conference on New Trends in Information and Service Science,25-Sep-09,2009,,,481,486,"The machine learning is certificated as one of the most important technologies in todaypsilas world. There are several various researches applying machine learning to improve its operation efficiency in many different aspects.Based on the social science citation index (SSCI) database,this research is using text mining technology which collecting the homogeneous glossaries in the articles, conducting to the literature cluster analysis. To select the term frequency index which generated by various glossaries aggregation from each article as well as an input variable for self-organization map(SOM) network, following by utilizing the network neuron automatic clustering function, dividing into 10 application domains of machine learning, finally proceeding the trend analysis coordinated with the articles by published year,discovering the historical vein and collecting the results by each research area, and further forecasting the future possible tendency.",,978-0-7695-3687-3,10.1109/NISS.2009.176,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260843,text mining;document clustering;neural network,Machine learning;Text mining;Terminology;Indexes;Databases;Citation analysis;Frequency conversion;Input variables;Neurons;Veins,data mining;learning (artificial intelligence);neural nets;pattern clustering;self-organising feature maps;text analysis,trend analysis;machine learning;text mining;document clustering;social science citation index;term frequency index;self-organization map network;network neuron automatic clustering function,,1,,24,,25-Sep-09,,,IEEE,IEEE Conferences
Web Pages Classification and Clustering by Means of Genetic Algorithms: A Variable Size Page Representing Approach,網頁分類和聚類的遺傳算法：可變大小頁面表示方法,Z. Hossaini; A. M. Rahmani; S. Setayeshi,"Sci. & Res. Branch, Islamic Azad Univ., Tehran, Iran; Sci. & Res. Branch, Islamic Azad Univ., Tehran, Iran; Sci. & Res. Branch, Islamic Azad Univ., Tehran, Iran",2008 International Conference on Computational Intelligence for Modelling Control & Automation,24-Jul-09,2008,,,436,440,"Arranging mass of data in related groups is an important way that helps us to decide about them better, clustering and classification are two efficient methods of grouping huge volume of data, most of clustering and classification methods that work on Web pages grouping problems, use fixed size vectors in their learning algorithm. In the real world of WWW this assumption is not reliable. In this paper we use genetic algorithm (GA) for classification and clustering, the algorithm works on variable size vectors. At the GA part we combined standard crossover and mutation operators with K-means algorithm, for improving diversity and correctness of results. By means of this method more accurate classes are achieved, and their subclasses are defined as clusters. This method shows more accurate results than fixed size methods, the accuracy rate is about 90.7%, and also overload of unnecessary elements in vectors is bypassed.",,978-0-7695-3514-2,10.1109/CIMCA.2008.151,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172665,clustering;classification;genetic algorithm;variable size vector,Web pages;Genetic algorithms,dictionaries;document handling;genetic algorithms;Internet;learning (artificial intelligence);mathematical operators;pattern classification;pattern clustering,Web pages classification;Web pages clustering;genetic algorithm;variable size page representing approach;Web pages grouping problem;fixed size vector;learning algorithm;WWW;World Wide Web;K-means algorithm;mutation operator;standard crossover operator,,,,11,,24-Jul-09,,,IEEE,IEEE Conferences
Incremental clustering algorithm based on phrase-semantic similarity histogram,基於短語語義相似度直方圖的增量聚類算法,W. K. Gad; M. S. Kamel,"Department of Electrical and Computer Engineering, University of Waterloo, Waterloo Ontario N2L 3G1, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo Ontario N2L 3G1, Canada",2010 International Conference on Machine Learning and Cybernetics,20-Sep-10,2010,4,,2088,2093,"Incremental document clustering is an important key in organizing, searching, and browsing large datasets. Although, many incremental document clustering methods have been proposed, they do not focus on linguistic and semantic properties of the text Incremental clustering algorithms are preferred to traditional clustering techniques with the advent of online publishing in the World Wide Web. In this paper, an incremental document clustering algorithm is introduced. The proposed algorithm integrates the text semantic to the incremental clustering process. The clusters are represented using semantic histogram which measures the distribution of semantic similarities within each cluster. Experimental results show that the proposed algorithm has a promising clustering performance compared to standard clustering methods.",2160-1348,978-1-4244-6527-9,10.1109/ICMLC.2010.5580499,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580499,Ontology;WordNet;semantic similarity;incremental document clustering;semantic histogram,Semantics;Histograms,data mining;document handling;natural language processing;pattern clustering,phrase-semantic similarity histogram;incremental document clustering;text semantic,,4,,10,,20-Sep-10,,,IEEE,IEEE Conferences
Multi-objectives-based text clustering technique using K-mean algorithm,基於K均值算法的基於多目標的文本聚類技術,L. M. Abualigah; A. T. Khader; M. A. Al-Betar,"School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; Department of information technology, Al-Huson University College, Irbid-Jordan",2016 7th International Conference on Computer Science and Information Technology (CSIT),25-Aug-16,2016,,,1,6,"Text documents clustering is a popular unsupervised text mining tool. It is used for partitioning a collection of text documents into similar clusters based on the distance or similarity measure as decided by an objective function. Text clustering algorithm often makes prior assumptions to satisfy objective function, which is optimized either through traditional techniques or meta-heuristic techniques. In text clustering techniques, the right decision for any document distribution is done using an objective function. Normally, clustering algorithms perform poorly when the configuration of the well-formulated objective function is not sound and complete. Therefore, we proposed multi-objectives-based method namely, combine distance and similarity measure for improving the text clustering technique. Multi-objectives text clustering method is combined with two evaluating criteria which emerge as a robust alternative in several situations. In particular, the multi-objective function in the text clustering domain is not a popular, and it is a core issue that affects the performance of the text clustering technique. The performance of multi-objectives function is investigated using the k-mean text clustering technique. The experiments were conducted using seven standard text datasets. The results showed that the proposed multi-objectives based method outperforms the other measures in term of the performance of the text clustering, evaluated by using two common clustering measures, namely, Accuracy and F-measure.",,978-1-4673-8914-3,10.1109/CSIT.2016.7549464,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549464,Multi-objectives function;Text clustering;K-mean technique;Distance measure;Similarity measure;Clustering performance,Clustering algorithms;Linear programming;Standards;Euclidean distance;Information technology;Text mining;Mathematical model,data mining;pattern clustering;text analysis;unsupervised learning,multi-objectives-based text clustering technique;K-mean algorithm;text documents clustering;unsupervised text mining;meta-heuristic techniques;multi-objectives-based method;accuracy;F-measure,,22,,17,,25-Aug-16,,,IEEE,IEEE Conferences
RankTopic: Ranking Based Topic Modeling,RankTopic：基於排名的主題建模,D. Duan; Y. Li; R. Li; R. Zhang; A. Wen,"Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Dept. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia; Dept. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia; Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China",2012 IEEE 12th International Conference on Data Mining,17-Jan-13,2012,,,211,220,"Topic modeling has become a widely used tool for document management due to its superior performance. However, there are few topic models distinguishing the importance of documents on different topics. In this paper, we investigate how to utilize the importance of documents to improve topic modeling and propose to incorporate link based ranking into topic modeling. Specifically, topical pagerank is used to compute the topic level ranking of documents, which indicates the importance of documents on different topics. By retreating the topical ranking of a document as the probability of the document involved in corresponding topic, a generalized relation is built between ranking and topic modeling. Based on the relation, a ranking based topic model Rank Topic is proposed. With Rank Topic, a mutual enhancement framework is established between ranking and topic modeling. Extensive experiments on paper citation data and Twitter data are conducted to compare the performance of Rank Topic with that of some state-of-the-art topic models. Experimental results show that Rank Topic performs much better than some baseline models and is comparable with the state-of-the-art link combined relational topic model (RTM) in generalization performance, document clustering and classification by setting a proper balancing parameter. It is also demonstrated in both quantitative and qualitative ways that topics detected by Rank Topic are more interpretable than those detected by some baseline models and still competitive with RTM.",2374-8486,978-1-4673-4649-8,10.1109/ICDM.2012.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413901,Ranking;Topic Modeling;Document Network;Clustering;Classification,Mathematical model;Computational modeling;Equations;Noise;Data models;Educational institutions;Web pages,document handling;generalisation (artificial intelligence);pattern classification;pattern clustering;probability,RankTopic tool;ranking based topic modeling;document management;document importance;topical pagerank;document ranking;probability;generalized relation;mutual enhancement framework;paper citation data;Twitter data;relational topic model;generalization performance;document clustering;document classification;balancing parameter,,13,,30,,17-Jan-13,,,IEEE,IEEE Conferences
Proportional data clustering using K-means algorithm: A comparison of different distances,用於文檔聚類的耦合術語-術語關係分析,J. P. Singh; N. Bouguila,"CIISE, Concordia University, Montr矇al, Qu矇bec, Canada; CIISE, Concordia University, Montr矇al, Qu矇bec, Canada",2017 IEEE International Conference on Industrial Technology (ICIT),4-May-17,2017,,,1048,1052,"In this paper, we discuss proportional data clustering. It emerges In many applications such as document clustering and Image classification using bag of visual words approach. When deploying mixture models for clustering, there Is always a problem of initialization, and It Is common to initialize using K-means algorithm. In proposed work, we present K-means clustering approach using different distance metrics. In particular, we propose the consideration of the Altchlson distance. Experimental results are presented using silhouette plots for showing divergence from the center, and confusion matrix Is used to validate our clustering of synthetic and real data sets of Images and texts. The algorithm with Altchlson distance metric results Into lower error rates.",,978-1-5090-5320-9,10.1109/ICIT.2017.7915506,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915506,K-means;unsupervised learning;Aitchison distance;clustering,Clustering algorithms;Euclidean distance;Mathematical model;Visualization;Algorithm design and analysis;Analytical models,matrix algebra;mixture models;pattern clustering,proportional data clustering;bag of visual words approach;mixture models;K-means clustering approach;Altchison distance metrics;silhouette plots;confusion matrix,,6,,32,,4-May-17,,,IEEE,IEEE Conferences
An Improved Semantic Smoothing Model for Model-Based Document Clustering,自然語言處理和聚類支持的司法判例搜索,J. Cai; Y. Liu; J. Yin,"Sun Yat-Sen University, China; Sun Yat-Sen University, China; Sun Yat-Sen University, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",13-Aug-07,2007,3,,670,675,"Recently, semantic smoothing is proposed as an efficient solution for the improvement of document cluster quality. However, the existing semantic smoothing model is not effective for partitional clustering to enhance the document clustering quality. In this paper, inspired by the TF*IDF schema and background elimination strategy, we first introduce an improved semantic smoothing model, which is suitable for both agglomerative and partitional clustering. Based on the improved semantic smoothing model, two model-document clustering algorithms, the partitional clustering algorithm and the agglomerative clustering algorithm, are also presented. The experimental results show our algorithms are more effective than the previous methods to improve the cluster quality.",,978-0-7695-2909-7,10.1109/SNPD.2007.155,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287935,,Smoothing methods;Planets;Clustering algorithms;Partitioning algorithms;Data mining;Information retrieval;Software engineering;Artificial intelligence;Distributed computing;Computer science,pattern clustering;text analysis,semantic smoothing model;model-based text document clustering;agglomerative-partitional clustering,,1,,13,,13-Aug-07,,,IEEE,IEEE Conferences
No-reference document image quality assessment based on high order image statistics,通過聚類集成和基於EM的信息性文本摘要對微陣列基因聚類進行識別和註釋,J. Xu; P. Ye; Q. Li; Y. Liu; D. Doermann,"Beijing University of Posts and Telecommunications, Beijing, China; Airbnb, San Francisco, CA, USA; Nanyang Technological University, Singapore; Beijing University of Posts and Telecommunications, Beijing, China; University of Maryland, College Park, MD, USA",2016 IEEE International Conference on Image Processing (ICIP),19-Aug-16,2016,,,3289,3293,"Document image quality assessment (DIQA) aims to predict the visual quality of degraded document images. Although the definition of ?visual quality??can change based on the specific applications, in this paper, we use OCR accuracy as a metric for quality and develop a novel no-reference DIQA method based on high order image statistics for OCR accuracy prediction. The proposed method consists of three steps. First, normalized local image patches are extracted with regular grid and a comprehensive document image codebook is constructed by K-means clustering. Second, local features are softly assigned to several nearest codewords, and the direct differences between high order statistics of local features and codewords are calculated as global quality aware features. Finally, support vector regression (SVR) is utilized to learn the mapping between extracted image features and OCR accuracies. Experimental results on two document image databases show that the proposed method can accurately predict OCR accuracy and outperforms previous algorithms.",2381-8549,978-1-4673-9961-6,10.1109/ICIP.2016.7532968,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532968,Document image quality assessment;high order statistics;OCR accuracy;no-reference,Optical character recognition software;Databases;Feature extraction;Image quality;Prediction algorithms;Nonlinear distortion,document image processing;feature extraction;optical character recognition;pattern clustering;regression analysis;statistics;support vector machines,no-reference document image quality assessment;no-reference DIQA;high order image statistics;visual quality prediction;degraded document images;OCR accuracy prediction;normalized local image patch extraction;regular grid;comprehensive document image codebook;K-means clustering;codewords;global quality aware features;support vector regression;SVR;map learning;image feature extraction,,8,,20,,19-Aug-16,,,IEEE,IEEE Conferences
Writer Identification and Writer Retrieval Using the Fisher Vector on Visual Vocabularies,使用K均值聚類和支持向量機對場景圖像中的顏色字符串進行二值化,S. Fiel; R. Sablatnig,"Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,545,549,"In this paper a method for writer identification and writer retrieval is presented. Writer identification is the task of identifying the writer of a document out of a database of known writers. In contrast to identification, writer retrieval is the task of finding documents in a database according to the similarity of handwritings. The approach presented in this paper uses local features for this task. First a vocabulary is calculated by clustering features using a Gaussian Mixture Model and applying the Fisher kernel. For each document image the features are calculated and the Fisher Vector is generated using the vocabulary. The distance of this vector is then used as similarity measurement for the handwriting and can be used for writer identification and writer retrieval. The proposed method is evaluated on two datasets, namely the ICDAR 2011 Writer Identification Contest dataset which consists of 208 documents from 26 writers, and the CVL Database which contains 1539 documents from 309 writers. Experiments show that the proposed methods performs slightly better than previously presented writer identification approaches.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.114,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628679,Writer Identification;Writer Retrieval;Document Analysis,Databases;Vectors;Writing;Vocabulary;Kernel;Image segmentation;Training,document image processing;feature extraction;Gaussian processes;handwritten character recognition;image retrieval;pattern clustering;visual databases;vocabulary,document writer retrieval;document writer identification;local features;visual vocabulary;feature clustering;Gaussian mixture model;Fisher kernel;document image;Fisher vector;handwriting similarity measurement;ICDAR 2011 Writer Identification Contest dataset;CVL Database,,43,,16,,15-Oct-13,,,IEEE,IEEE Conferences
Document Classification with Unsupervised Nonnegative Matrix Factorization and Supervised Percetron Learning,稀疏文本數據聚類的比較優勢方法,P. C. Barman; S. Lee,"Brain Science Research Center, Dept. of BioSystems, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, pcbarman@neuron.kaist.ac.kr; Brain Science Research Center, Dept. of BioSystems, &Dept. of Electrical Engineering and Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, sylee@kaist.ac.kr",2007 International Conference on Information Acquisition,27-Aug-07,2007,,,182,186,"A new hybrid neural network model is proposed for the document classification. The NMF-SLP model consists of 2 layers, in which the first non-negative matrix factorization (NMF) layer decomposes a document into several clusters, and the second single-layer-perceptron (SLP) layer classifies the document based on the clusters. The NMF layer is trained by factorizing the document word frequency matrix into feature matrix and coefficient matrix, and then estimating the pseudo-inverse of the feature matrix. The SLP layer is trained by standard error minimization algorithm. Classification performances are investigated as a function of the cluster number, i.e., the number of hidden neurons, and also slope of sigmoidal nonlinearity at the hidden neurons. The developed model demonstrates much better classification accuracy compared to the simple NMF and k-NN classifiers, while standard multi-layer Perceptron is almost impractical to train properly due to high dimensional inputs and large number of adaptive elements.",,1-4244-1219-6,10.1109/ICIA.2007.4295722,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4295722,,Clustering algorithms;Transmission line matrix methods;Neurons;Matrix decomposition;Unsupervised learning;Frequency estimation;Supervised learning;Feature extraction;Neural networks;Cities and towns,document handling;matrix decomposition;matrix inversion;minimisation;pattern classification;perceptrons;unsupervised learning,document classification;unsupervised nonnegative matrix factorization;supervised perceptron learning;hybrid neural network model;NMF-SLP model;single-layer-perceptron layer;NMF layer;SLP layer;document word frequency matrix factorization;feature matrix pseudoinverse estimation;error minimization algorithm,,3,,10,,27-Aug-07,,,IEEE,IEEE Conferences
Web content management by self-organization,基於圖相似度的嵌入式情感聚類中文博客的新方法,R. T. Freeman; H. Yin,"Sch. of Electr. & Electron. Eng., Univ. of Manchester, UK; Sch. of Electr. & Electron. Eng., Univ. of Manchester, UK",IEEE Transactions on Neural Networks,19-Sep-05,2005,16,5,1256,1268,"We present a new method for content management and knowledge discovery using a topology-preserving neural network. The method, termed topological organization of content (TOC), can generate a taxonomy of topics from a set of unannotated, unstructured documents. The TOC consists of a hierarchy of self-organizing growing chains (GCs), each of which can develop independently in terms of size and topics. The dynamic development process is validated continuously using a proposed entropy-based Bayesian information criterion (BIC). Each chain meeting the criterion spans child chains, with reduced vocabularies and increased specializations. This results in a topological tree hierarchy, which can be browsed like a table of contents directory or web portal. A brief review is given on existing methods for document clustering and organization, and clustering validation measures. The proposed approach has been tested and compared with several existing methods on real world web page datasets. The results have clearly demonstrated the advantages and efficiency in content organization of the proposed method in terms of computational cost and representation. The TOC can be easily adapted for large-scale applications. The topology provides a unique, additional feature for retrieving related topics and confining the search space.",1941-0093,,10.1109/TNN.2005.853415,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510724,Bayesian information criterion (BIC);content management;document categorization;hierarchical clustering;information retrieval (IR);self-organizing maps (SOMs);taxonomy generation;topic hierarchy;topological tree structure,Content management;Neural networks;Taxonomy;Bayesian methods;Vocabulary;Portals;Testing;Web pages;Computational efficiency;Large-scale systems,content management;Internet;data mining;document handling;self-organising feature maps;pattern clustering;information retrieval;classification;entropy,Web content management;knowledge discovery;topology-preserving neural network;topological content organization;topic taxonomy;unannotated unstructured documents;self-organizing growing chains;dynamic development process;entropy-based Bayesian information criterion;topological tree hierarchy;document clustering;document organization;clustering validation;document categorization;information retrieval;self-organizing maps,"Abstracting and Indexing as Topic;Artificial Intelligence;Database Management Systems;Documentation;Information Storage and Retrieval;Internet;Natural Language Processing;Pattern Recognition, Automated;Signal Processing, Computer-Assisted",13,,48,,19-Sep-05,,,IEEE,IEEE Journals
A pragmatic analysis of query expansion based on unsupervised learning,動態文檔組織的三階段和四階段方案,A. Muthulakshmi; R. Kaviya; M. I. Devi,"Department of Information Technology, KLN College of Information Technology, Sivagangai, India; Department of Information Technology, KLN College of Information Technology, Sivagangai, India; Information Technology, KLN College of Information Technology, Sivagangai, India",2013 IEEE Conference on Information & Communication Technologies,15-Jul-13,2013,,,890,893,"In this paper, we examine the significance of expansion of the user query by two techniques namely Efficient Clustering-By-Direction and Theme Clustering. These two techniques produce the clusters of keywords extracted from the set of retrieved documents for the user query. The former clustering is based on statistical approach whereas the latter clustering is based on semantic approach. Empirical analysis of set of keywords that provides the importance of user's need produce the narrow search results. The clusters are further analyzed to provide the most appropriate keywords from the group of documents.",,978-1-4673-5758-6,10.1109/CICT.2013.6558221,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6558221,Query Expansion;clustering;Theme clustering;Clustering-By-Direction,Clustering algorithms;Semantics;Vectors;Internet;Tagging;Search engines;Conferences,pattern clustering;query processing;statistical analysis;unsupervised learning,pragmatic analysis;query expansion;unsupervised learning;efficient clustering-by-direction;theme clustering;document retrieval;statistical approach;semantic approach,,,,18,,15-Jul-13,,,IEEE,IEEE Conferences
Clustering for Web information hierarchy mining,使用新型模糊關係聚類算法的句子級文本聚類,Hung-Yu Kao; Ming-Syan Chen; Jan-Ming Ho,"Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),27-Oct-03,2003,,,698,701,"Benefiting from the growth of techniques of dynamic page generation, the amount and the complexity of Web pages increase explosively. The structures of Web pages which are dynamically generated by the same templates are thus similar to one another and are usually assembled by a set of fundamental information clusters These neighboring information clusters usually represent the similar semantics and form a larger cluster with the more generalized information. The hierarchical structure generated by information clusters in a bottom-up manner is called the information hierarchy of a page. We study the problem of mining the information hierarchies of pages in Web sites to recognize the information distribution of pages within the multilevel, multigranularity configurations. Explicitly, we propose an information clustering system that applies a top-down information centroid searching algorithm and a multigranularity centroid converging process on the document object model (DOM) trees of pages to build the information hierarchies of pages. Experiments on several real news Web sites show the high precision and recall rates of the proposed method on determining information clusters of pages and also validate its practical applicability to real Web sites.",,0-7695-1932-6,10.1109/WI.2003.1241299,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241299,,Web pages;Electronic mail;Assembly;Information science;Clustering algorithms;Scalability;Joining processes;Merging;HTML;Data mining,Internet;data mining;Web sites;document handling;information retrieval;statistical analysis,Web information hierarchy mining;dynamic Web page generation;Web site;information page distribution;information clustering system;top-down information centroid searching algorithm;multigranularity centroid converging process;document object model;DOM tree,,,,8,,27-Oct-03,,,IEEE,IEEE Conferences
Fuzzy Clustering in a Complex Network Based on Content Relevance and Link Structures,用於文檔聚類的橢球K均值,L. Hu; K. C. C. Chan,"School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong",IEEE Transactions on Fuzzy Systems,30-Mar-16,2016,24,2,456,470,"Many real-world problems can be represented as complex networks with nodes representing different objects and links between nodes representing relationships between objects. As different attributes can be considered as associating with different objects, other than nontrivial link structures, complex networks also contain rich content information, and it can be a big challenge to find interesting clusters in such networks by fully exploiting the knowledge of both content and link information in them. Although some attempts have been made to tackle this clustering problem, few of them have considered the feasibility of identifying clusters in complex networks using a fuzzy-based clustering approach. We believe that, if the degree of membership to a cluster that a node belongs to can be considered, we will be able to better identify clusters in complex networks, as we may be able to identify overlapping clusters. In this paper, we, therefore, propose a fuzzy-based clustering algorithm for this task. The algorithm, which we call Fuzzy Clustering Algorithm for Complex Networks (FCAN), can discover clusters by taking into consideration both link and content information. It does so by first processing the content information by introducing a measure to quantify the relevance of contents between each pair of nodes within the network. It then proceeds to leverage the link information in the clustering process by considering a measure of cluster density. Based on these measures, FCAN identifies fuzzy clusters that are more densely connected and more highly relevant in their contents to optimize the degrees of memberships of each node belonging to different clusters. The performance of FCAN has been evaluated with several synthetic and real datasets involving those of document classification and social community detection. The results show that, in terms of accuracy, computation efficiency, and scalability, FCAN can be a very promising approach.",1941-0034,,10.1109/TFUZZ.2015.2460732,National High-tech R&D Program of China; 863 Program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166325,Fuzzy clustering;link structure;content relevance;complex network;Complex network;content relevance;fuzzy clustering;link structure,Clustering algorithms;Complex networks;Optimization;Communities;Classification algorithms;Computational modeling;Density measurement,complex networks;fuzzy set theory;network theory (graphs);pattern classification;pattern clustering,content relevance based complex network;link structure based complex network;nontrivial link structures;link information;content information;fuzzy-based clustering approach;fuzzy clustering algorithm for complex networks;FCAN;cluster density;document classification;social community detection;computation efficiency,,27,,42,,24-Jul-15,,,IEEE,IEEE Journals
Hierarchical Clustering based on IndoorGML Document,查詢和相應廣告的聯合集群,J. Tamas,Eszterhazy Karoly University,2019 IEEE 15th International Scientific Conference on Informatics,17-Jun-20,2019,,,177,182,"Grouping of symbolic positions using hierarchical clustering based on IndoorGML document is presented in this paper. Using hierarchical clustering information of symbolic positions, the accuracy of symbolic indoor positioning algorithms can be improved in the case of low confidence level. The symbolic positions of the physical space can be described using IndoorGML standard. Hierarchical grouping can be determined among symbolic positions by heuristic. Although it requires knowledge gained from firsthand experience. Moreover, existing hierarchical clustering algorithms can be applied using the topological description to generate tree structures. In every tree, the leaf nodes are the symbolic positions. In this paper, these generated trees are examined.",,978-1-7281-3180-1,10.1109/Informatics47936.2019.9119255,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119255,indoorgml;hierarchical clustering;symbolic indoor positioning,,document handling;pattern clustering;trees (mathematics),IndoorGML document;symbolic indoor positioning algorithms;hierarchical grouping;hierarchical clustering;topological description,,,,16,,17-Jun-20,,,IEEE,IEEE Conferences
A Novel Approach of Neural Topic Modelling for Document Clustering,基於二部圖的快速光譜聚類算法,S. Subramani; V. Sridhar; K. Shetty,"Telecommunication Engineering, BMS College of Engineering, Bengaluru, India; Computer Science Engineering, BMS Institute of Technology, Bengaluru, India; Computer Science Engineering, Sri Jayachamrajendra College of Engineering, Mysore, India",2018 IEEE Symposium Series on Computational Intelligence (SSCI),31-Jan-19,2018,,,2169,2173,"Topic modelling is a text mining technique to discover common topics in a collection of documents. The proposed methodology of topic modelling used artificial neural networks to improve the clustering mechanism of similar documents by modelling probabilistic relations between the topics, documents and vocabulary. Currently, while topic modelling and clustering are considered to be manifestations of unsupervised learning, and neural networks on the other hand are used for supervised learning problems, Neural Topic Modelling reformulated topic modelling into a supervised learning task by defining an objective function whose loss function had to be minimized. Custom input embedding layers were designed in order to extract the semantic relationships between the words in the corpus, and the output of the model presented a topic probability distribution for each document. The documents with similar distributions were then bucketed together based on the criteria of meeting the threshold value of a simple distance based similarity metric, such as cosine similarity. The model was implemented using Keras with TensorFlow backend and the effectiveness of the clustering was validated on the IMDB Movie dataset and the News Aggregator dataset from UCI. On comparison with other commonly used clustering mechanisms in combination with traditional topic models, the proposed model delivered an improved Silhouette Co-efficient Score and Davies-Bouldin Index, along with an increased data handling capacity, thereby making the solution scalable.",,978-1-5386-9276-9,10.1109/SSCI.2018.8628912,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628912,Deep Learning;Artificial Neural Networks;Topic Modelling;Clustering;Neural Topic Modelling,Computational modeling;Data models;Neural networks;Computer architecture;Probabilistic logic;Task analysis;Linear programming,data mining;learning (artificial intelligence);neural nets;pattern clustering;probability;statistical distributions;text analysis;unsupervised learning,text mining technique;artificial neural networks;clustering mechanism;modelling probabilistic relations;supervised learning problems;supervised learning task;topic probability distribution;document clustering;loss function;topic models;neural topic modelling reformulated topic,,,,14,,31-Jan-19,,,IEEE,IEEE Conferences
Web documents mining,用於文檔表示的模糊詞袋模型,Qin-Bao Song; Nai-Qian Li; Jun-Yi Shen; Li-Ming Chen,"Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; NA",Proceedings. International Conference on Machine Learning and Cybernetics,19-Feb-03,2002,2,,791,795 vol.2,"By grouping similar Web documents into clusters, the search space can be reduced, the search accelerated, and its precision improved. In the paper, a clustering algorithm is introduced. In the proposed clustering method, topics are represented according to a vector space model, documents are represented according to the topics, and the relation between the documents and the topics is viewed as a transaction, one document corresponds to a transaction and one topic corresponds to an item. An association rules mining algorithm discovers the frequent item sets, and the corresponding documents are seen as the initial clusters. The clusters are merged if the distance between them is small enough, or the cluster is divided if the connection strength between its documents is smaller than the given threshold. Experiments are conducted on real Web documents, results show the algorithm's effectivity and suitability for tackling the overlapping clusters inherent in documents.",,0-7803-7508-4,10.1109/ICMLC.2002.1174490,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1174490,,Clustering algorithms;Association rules;Data mining;Computer science;Space technology;Acceleration;Search engines;Mathematics;Electronic mail;Clustering methods,data mining;Web sites;information retrieval,Web documents mining;search space;clustering algorithm;association rule;vector space model;topics;overlapping clusters,,1,,7,,19-Feb-03,,,IEEE,IEEE Conferences
Use of figures in literature mining for biomedical digital libraries,ABK-means算法的一種新的文檔聚類識別犯罪的方法,Nawei Chen; H. Shatkay; D. Blostein,"Sch. of Comput., Queen's Univ., Kingston, Ont., Canada; Sch. of Comput., Queen's Univ., Kingston, Ont., Canada; Sch. of Comput., Queen's Univ., Kingston, Ont., Canada",Second International Conference on Document Image Analysis for Libraries (DIAL'06),8-May-06,2006,,,18 pp.,197,"The maintenance of biomedical digital libraries (including organism databases and protein databases) involves analysis of a large number of documents. Much work is done manually: curators study large numbers of biomedical documents while updating and annotating organism databases such as MGI (mouse genome informatics) and Flybase (a database of the fruit-fly genome). We summarize the annotation process in organism databases, and describe some of the roles played by the gene ontology and by document databases such as PubMed. Efforts are ongoing to automate parts of the annotation process. Biomedical text mining contests, such as the TREC Genomics Track (Hersh et al., 2004, 2005), define annotation subtasks, and provide training and test data. So far, these efforts have focused on the analysis of the text content of documents. We are investigating the analysis of figures in biomedical documents; the information derived from figure analysis may later be combined with the information derived from text analysis. We present an algorithm for using figures in document triage; triage involves determining which documents are relevant to a given annotation task. In our triage algorithm, we segment figures into subfigures and classify the subfigures as graphical, gel, fluorescence microscopy, and other microscopy. A secondary classification into subcategories is performed by clustering, using clusters created from the subfigures in the labeled training data. The classifications of all subfigures in a document are combined to form a document descriptor. The document descriptor is then classified using a naive Bayes classifier, as either relevant or irrelevant to the given annotation task",,0-7695-2531-8,10.1109/DIAL.2006.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1612961,,Software libraries;Databases;Organisms;Genomics;Bioinformatics;Information analysis;Clustering algorithms;Microscopy;Proteins;Data analysis,Bayes methods;data mining;digital libraries;medical computing;pattern classification;pattern clustering;text analysis,literature mining;biomedical digital library;library maintenance;organism databases;protein databases;document analysis;mouse genome informatics;Flybase;gene ontology;document database;document annotation;biomedical text mining;biomedical documents;text analysis;document triage;document classification;document clustering;document descriptor;naive Bayes classifier,,1,,22,,8-May-06,,,IEEE,IEEE Conferences
Categorization and keyword identification of unlabeled documents,用於提取實體相關令牌的增量結構模型,Ning Kang; C. Domeniconi; D. Barbara,"Dept. of ISE, George Mason Univ., Fairfax, VA, USA; Dept. of ISE, George Mason Univ., Fairfax, VA, USA; Dept. of ISE, George Mason Univ., Fairfax, VA, USA",Fifth IEEE International Conference on Data Mining (ICDM'05),3-Jan-06,2005,,,4 pp.,,"In this paper, we first propose a global unsupervised feature selection approach for text, based on frequent itemset mining. As a result, each document is represented as a set of words that co-occur frequently in the given corpus of documents. We then introduce a locally adaptive clustering algorithm, designed to estimate (local) word relevance and, simultaneously, to group the documents. We present experimental results to demonstrate the feasibility of our approach. Furthermore, the analysis of the weights credited to terms provides evidence that the identified keywords can guide the process of label assignment to clusters. We take into consideration both spam email filtering and general classification datasets. Our analysis of the distribution of weights in the two cases provides insights on how the spam problem distinguishes from the general classification case.",2374-8486,0-7695-2278-5,10.1109/ICDM.2005.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565755,,Clustering algorithms;Data mining;Itemsets;Algorithm design and analysis;Predictive models;Filtering;Data analysis;Functional analysis;Dictionaries;Indexing,classification;feature extraction;text analysis;data mining;pattern clustering;unsolicited e-mail;information filtering,unlabeled document categorization;keyword identification;global unsupervised feature selection;frequent itemset mining;adaptive clustering;word relevance;label assignment;spam email filtering;general classification dataset;text mining,,6,,9,,3-Jan-06,,,IEEE,IEEE Conferences
A Graph-Based Model for Keyword Extraction and Tagging of Research Documents,通過樣式特徵將技術文檔聚類以進行作者分析,M. Thushara; A. S; M. N. M,"Amrita Vishwa Vidyapeetham,Department of Computer Science and Applications,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Applications,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department of Computer Science and Applications,Amritapuri,India","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",13-Feb-20,2019,1,,942,946,"This paper presents an unsupervised approach for extracting keywords from documents and tagging of research documents with the help of extracted keywords. Keywords are useful for checking the similarity between two documents. This will help us to correctly cluster the related documents with respect to their domains. In this paper, we are describing how research papers are clustered using keywords. A report is generated with respect to the author's contribution in different domains. The system uses Keyword Extraction using Collective Node Weight for extraction of keywords, Node-Edge Rank for ranking Keywords and WordNet Similarity Measurement for tagging of Research papers.",,978-1-7281-0283-2,10.1109/ICICICT46008.2019.8993142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993142,Extraction;Document Tagging;Graph-based Models;Collective Node Weight,,graph theory;pattern clustering;query processing;text analysis,graph-based model;keyword extraction;research documents;unsupervised approach;related documents,,,,17,,13-Feb-20,,,IEEE,IEEE Conferences
Using Self Organizing Map to cluster Arabic crime documents,使用基於n-gram的數值數據進行短文本聚類,M. Alruily; A. Ayesh; A. Al-Marghilani,"Software Technology Research Laboratory, De Montfort University, The Gateway, Leicester, LE1 9BH UK; Software Technology Research Laboratory, De Montfort University, The Gateway, Leicester, LE1 9BH UK; Software Technology Research Laboratory, De Montfort University, The Gateway, Leicester, LE1 9BH UK",Proceedings of the International Multiconference on Computer Science and Information Technology,6-Jan-11,2010,,,357,363,"This paper presents a system that combines two text mining techniques; information extraction and clustering. A rule-based approach is used to perform the information extraction task, based on the dependency relation between some intransitive verbs and prepositions. This relationship helps in extracting types of crime from documents within the crime domain. With regard to the clustering task, the Self Organizing Map (SOM) is used to cluster Arabic crime documents based on crime types. This work is then validated through experiments, the results of which show that the techniques developed here are promising.",2157-5533,978-83-60810-27-9,10.1109/IMCSIT.2010.5679616,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679616,,Data mining;Organizing;Neurons;Pragmatics;Data visualization;Grammar;Context,data mining;document handling;information retrieval;knowledge based systems;natural language processing;pattern clustering;self-organising feature maps;task analysis,self organizing map;text mining;information clustering;information extraction task;dependency relation;intransitive verbs;intransitive prepositions;Arabic crime document clustering;rule based approach,,8,,23,,6-Jan-11,,,IEEE,IEEE Conferences
Implementing subtopic recall and plummeting shadowing in Suffix Tree clustering,anyOCR：基於序列學習的OCR系統，用於未標記的歷史文檔,R. Mahalakshmi; V. L. Praba,"Manonmaniam Sundaranar University, Tirunelveli, Tamilnadu; Government arts College, Sivaganga, Madurai, India",2013 IEEE International Conference on Computational Intelligence and Computing Research,27-Jan-14,2013,,,1,4,"Mining data to find patterns has been studied for some time and current work on automatically grouping documents has roots in this research. We propose a dexterous partitioning strategy for Web search result presentation. We identify a canon to which a clustering must stick on to perk up the user's search experience, avoid the redundant effect of query aspect recurrence, which is called shadowing. We present measures to quantify the shadowing effect, and we introduce an algorithm SSTC Subtopic Suffix Tree Clustering that optimizes the identified principles. The key idea of this is a dynamic reorganization of a clustering, similar to a faceted navigation system, We have also concentrated on the elimination of excessive clustering, i.e., either the number of cluster labels or the number of documents per cluster should not exceed the size of the result list Evaluations are done using the AMBIENT corpus and demonstrate the latent of our approach by a comparison with two well-known clustering search result algorithms STC and Lingo.",,978-1-4799-1597-2,10.1109/ICCIC.2013.6724219,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724219,subtopic retrieval;clustering;search results;diversification,Clustering algorithms;Shadow mapping;Web search;Algorithm design and analysis;Search engines;Heuristic algorithms;Educational institutions,data mining;Internet;pattern clustering;query processing;trees (mathematics),AMBIENT corpus;query aspect recurrence;Web search result presentation;dexterous partitioning strategy;data mining;suffix tree clustering;plummeting shadowing;subtopic recall,,,,14,,27-Jan-14,,,IEEE,IEEE Conferences
Automatic Reconstruction of Cross-Cut Chinese Document Shreds Based on the Feature of Typesetting and Strokes,使用LDA主題集群上的特徵本體樹從評論中提取產品特徵,Y. Wang; B. Wu; L. Gao; H. Yang,"School of Information Science and Technology, Yunnan Normal University, Kunming, China; West China School of Medicine, Sichuan University, Chengdu, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China",2019 IEEE 4th International Conference on Signal and Image Processing (ICSIP),17-Oct-19,2019,,,727,731,"This paper proposed an algorithm for automatic reconstruction of cross-cut Chinese document shreds based on the similarities of stroke and typesetting features. We started with extracting the typesetting feature including row height, line spacing and word width based on horizontal projection vector of strokes among the shreds, and clustered shreds into groups by means of k nearest neighbor mutual voting clustering algorithm with rejection region on the basis of horizontal projection vector similarity, word height and line spacing match measurement. Then, vertical sorting among the cluster groups was carried out according to word height and line spacing match measurement. Finally, intra-group splicing was accomplished based on greedy algorithm by using similarity of strokes and word width match measurement on both sides of vertical cut. The experiment result showed that the proposed algorithm achieved 100% reconstruction accuracy with the run time less than 5 seconds, and worked well in environment with 10 standard deviation Gauss noise.",,978-1-7281-3660-8,10.1109/SIPROCESS.2019.8868511,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868511,automaticl reconstruction;strokes feature;typesetting feature;mutual voting method;clustering;greedy algorithm,Image reconstruction;Clustering algorithms;Splicing;Typesetting;Greedy algorithms;Feature extraction;Reconstruction algorithms,document handling;feature extraction;greedy algorithms;natural language processing;pattern clustering;typesetting,line spacing match measurement;cluster groups;word height;greedy algorithm;word width match measurement;vertical cut;reconstruction accuracy;automatic reconstruction;cross-cut chinese document shreds;stroke;typesetting features;typesetting feature including row height;k nearest neighbor mutual voting clustering algorithm;horizontal projection vector similarity,,,,10,,17-Oct-19,,,IEEE,IEEE Conferences
Improvement of Cluster Importance Algorithm with Sentence Position for News Summarization,韓文Web文檔聚類的功能選擇,N. Hayatin; G. I. Marthasari; S. Anggraini,"Department of Informatics Engineering, University of Muhammadiyah Malang, Malang, Indonesia; Department of Informatics Engineering, University of Muhammadiyah Malang, Malang, Indonesia; Department of Informatics Engineering, University of Muhammadiyah Malang, Malang, Indonesia","2018 5th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",8-Jul-19,2018,,,483,488,"Text summarization is one of the ways to reduce large document dimension to obtain important information from the document. News is one of information which usually has several sub-topics from a topic. In order to get the main information from a topic as fast as possible, multi-document summarization is the solution, but sometimes it can create redundancy. In this study, we used cluster importance algorithm by considering sentence position to overcome the redundancy. Stages of cluster importance algorithm are sentence clustering, cluster ordering, and selection of sentence representative which will be explained in the subsections below. The contribution of this research was to add the position of sentence in the selection phase of representative sentence. For evaluation, we used 30 topics of Indonesian news tested by using ROUGE-1, there were 2 news topics that had different ROUGE-1 score between using cluster importance algorithm by considering sentence position and using cluster importance. However, those 2 news topics which used cluster importance by considering sentence position have a greater score of Rouge-1 than the one which only used cluster importance. The use of sentence position had an effect on the order of sentence on each topic, but there were only 2 news topics that affected the outcome of the summary.",,978-1-5386-8402-3,10.1109/EECSI.2018.8752633,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752633,News Summarization;Redundancy;Cluster Importance;Sentence Position,Clustering algorithms;Testing;Redundancy;Mathematical model;Flowcharts;Feature extraction;Couplings,document handling;information retrieval;pattern clustering;text analysis,multidocument summarization;cluster importance algorithm;sentence position;sentence clustering;cluster ordering;sentence representative;representative sentence;news summarization;news topics;text summarization;ROUGE-1,,,,15,,8-Jul-19,,,IEEE,IEEE Conferences
Ontology learning method for Web services clustering,一種新的具有稀疏約束的非負矩陣分解算法,B. T. G. S. Kumara; I. Paik; G. Lee,"School of Computer Science and Engineering, University of Aizu, Wakamatsu, Fukushima, Japan; School of Computer Science and Engineering, University of Aizu, Wakamatsu, Fukushima, Japan; School of Computer Science and Engineering, University of Aizu, Wakamatsu, Fukushima, Japan",2012 7th International Conference on Computing and Convergence Technology (ICCCT),13-Jun-13,2012,,,705,710,"Web service discovery is becoming a challenging and time consuming task due to large number of Web services available on the Internet. Organizing the Web services into functionally similar clusters is one of a very efficient approach for reducing the search space. To cluster Web services, take out the Web services description languages documents and extract the features (e.g., service name) to measure the similarities. Complex terms are used as Web service features in some contexts. Current approaches do not consider about the hidden semantic pattern exists within the complex terms. We present an approach to cluster the Web services into functionally similar Web service clusters that mine Web Service Description Language (WSDL) documents and generate ontologies by using complex terms for the measuring purpose of similarity. We use both logic based reasoning and edge base similarity measuring techniques for calculating the similarity using generated ontology. Experimental results show our clustering approach with ontology learning, has better performance comparing with approaches which are not considering about the latent pattern exists within the complex terms.",,978-89-94364-22-3,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530425,Web service clustering;Ontology Learning,,data mining;document handling;feature extraction;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;search problems;Web services,ontology learning method;Web service clustering approach;Internet;search space;Web service feature extraction;hidden semantic pattern;functionally similar Web service clusters;Web service description language document mining;WSDL document mining;complex terms;logic based reasoning;edge base similarity measuring techniques,,,,14,,13-Jun-13,,,IEEE,IEEE Conferences
Evaluation of Text Clustering Based on Iterative Classification,Web文檔聚類的混合特徵選擇算法,X. Wang; J. Lou,"Inst. of Comput. Applic. Technol., Hang Zhou Dian Zi Univ., Hangzhou, China; Inst. of Comput. Applic. Technol., Hang Zhou Dian Zi Univ., Hangzhou, China",2009 International Conference on Computational Intelligence and Software Engineering,28-Dec-09,2009,,,1,5,"Text clustering is a useful and inexpensive way to organize vast text repositories into meaningful topics categories. Although text clustering can be seen as an alternative to supervised text categorization, the question remains of how to determine if the resulting clusters are of sufficient quality in a real-life application. However, it is difficult to evaluate a given clustering of documents. Furthermore, the existing quality measures rely on the labor standard, which is difficult and time-consuming. The need for fair methods that can assess the validation of clustering results is becoming more and more critical. In this paper, we propose and experiment an innovative evaluation measure that allows one to effectively and correctly assess the clustering results.",,978-1-4244-4507-3,10.1109/CISE.2009.5364099,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364099,,Text categorization;Computer applications;Testing;Measurement standards;Iterative methods;Internet;Software libraries;Navigation;Clustering algorithms;Management information systems,iterative methods;pattern clustering;text analysis,text clustering;iterative classification;text repository;topics category;supervised text categorization;document clustering,,,,13,,28-Dec-09,,,IEEE,IEEE Conferences
A k-means clustering algorithm based on the distribution of SIFT,根據數據類型對XML文檔進行聚類,H. Lv; X. Huang; L. Yang; T. Liu; P. Wang,"School of Computer Science at Communication University of China, China; School of Computer Science, Communication University of China, China; Department at Communication University of China, China; Patent Examination Cooperation Center of The Patent Office, SIPO in Beijing, China; Anhui Vocational College of Press and Pulishing, China",2013 IEEE Third International Conference on Information Science and Technology (ICIST),27-Feb-14,2013,,,1301,1304,"Bag-of-Words based Image retrieval recently became the research hotspot. To improve the performance of visual word training in Bag-of-Words based image retrieval system, a k-means clustering algorithm based on the distribution of SIFT (Scale Invariant Feature Transform) feature data on each dimension is proposed. The initial clustering centers are obtained by analyzing the distribution of SIFT feature data on each dimension, and combing the iDistance method which is used to partition the data space in high-dimensional indexing according to the data distribution adaptively. Then the AKM (Approximate k-means) is used to do cluster on the sample feature data, train the visual words and get the visual vocabulary finally. In AKM, the k-d tree is built on the cluster centers at the beginning of each iteration to increase speed. The image retrieval system is constructed to verify the performance of our proposed method. Experiments are carried out on the oxford buildings 5k datasets which have 11 landmarks and the mAP (mean Average Precision) is used to evaluate the performance of image retrieval. Our proposed method achieves 31.9% compared to the AKM's 29.8%, so it is clear that our proposed method optimizes the visual words training process and finally improves the bag-of-words based image retrieval performance.",2164-4357,978-1-4673-2764-0,10.1109/ICIST.2013.6747776,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747776,,Visualization;Image retrieval;Vocabulary;Clustering algorithms;Feature extraction;Vectors;Quantization (signal),document image processing;feature extraction;image retrieval;indexing;pattern clustering;trees (mathematics),approximate k-means clustering algorithm;SIFT feature data distribution;bag-of-words based image retrieval system;visual word training;scale invariant feature transform;iDistance method;data space partition;high-dimensional indexing;AKM;visual vocabulary;k-d tree;mAP;mean average precision,,4,,14,,27-Feb-14,,,IEEE,IEEE Conferences
K-Means for Search Results Clustering Using URL and Tag Contents,彩色文檔圖像中的圖章檢測,S. Poomagal; T. Hamsapriya,"Dept. of Comput. & Inf. Sci., PSG Coll. of Technol., Coimbatore, India; Dept. of Inf. Technol., PSG Coll. of Technol., Coimbatore, India","2011 International Conference on Process Automation, Control and Computing",11-Aug-11,2011,,,1,7,"Increasing volume of web has resulted in the flooding of huge collection of web documents in search results creating difficulty for the user to browse the necessary document. Clustering is a solution to organize search results in a better way for browsing. It is a process of combining similar web documents into groups. For web page clustering, terms (features) can be extracted from different parts of a web page. Giansalvatore, Salvatore and Alessandro have extracted terms from entire web page for clustering Stanis law Osinski et al., have considered terms only from snippets. A new method is introduced in this paper which extract terms from URL, Title tag and Meta tag to produce clusters of web documents. The reason for selecting these parts of a web page is that they contain keywords which are available in a web page. Clustering algorithm used in this paper is K-means. Proposed method of clustering is compared with snippet based clustering in terms of intra-cluster distance and inter-cluster distance.",,978-1-61284-764-1,10.1109/PACC.2011.5978906,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978906,,Clustering algorithms;Web pages;Feature extraction;Partitioning algorithms;Search engines;Frequency measurement;Ear,document handling;feature extraction;information retrieval;pattern clustering;search problems;Web sites,search result clustering;URL;tag content;k-means clustering;Web documents;feature extraction;Web page;title tag;meta tag;snippet based clustering,,3,,10,,11-Aug-11,,,IEEE,IEEE Conferences
An axiomatic approach to clustering line-segments,一種新穎的句子聚類方法,A. Jonk; A. W. M. Smeulders,"Dept. of Math. & Comput. Sci., Amsterdam Univ., Netherlands; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,386,389 vol.1,In this paper we consider the problem of clustering line-segments into new ones. The clustering-hierarchy gives an answer to the question what original line segments are combined into larger ones. Such a clustering is defined as a hierarchical ordering of a set of line-segments. Criteria on a clustering-method are presented. The difference between edges and lines in relation to scale-invariant clustering is demonstrated. Existing approaches are evaluated using the presented criteria. It is shown that these approaches do not meet desirable criteria such as scale-invariance. A new method is described that adheres the formulated criteria. Finally an experiment is presented that illustrates the usefulness of the new method.,,0-8186-7128-9,10.1109/ICDAR.1995.599019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599019,,Image segmentation;Robustness;Image edge detection;Clustering methods;Mathematics;Computer science;Computer vision;Image converters;Shape,computer vision;pattern recognition,axiomatic approach;line-segments clustering;clustering-hierarchy;hierarchical ordering;edges;lines;scale-invariant clustering;scale-invariance,,4,,5,,6-Aug-02,,,IEEE,IEEE Conferences
Soft clustering algorithms based on neural networks,基於RDF的文檔層次結構編碼模型,L. Skovajsov獺; M. Roj?ek,"Institute of Informatics, Slovak Academy of Sciences, D繳bravsk獺 cesta 9, 845 07 Bratislava 45, Slovakia; Department of Informatics, Faculty of Pedagogic, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1, 03401, Slovakia",2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI),22-Dec-11,2011,,,439,442,"This paper is oriented into the text document retrieval area. The aim of the paper is to compare two soft document clustering methods by using neural networks, the modification of KMART and the nonlinear Hebbian neural network with Oja learning rule.",,978-1-4577-0045-3,10.1109/CINTI.2011.6108545,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6108545,,Neural networks;Vectors;Clustering algorithms;Training;Subspace constraints;Informatics;Information retrieval,Hebbian learning;information retrieval;neural nets;pattern clustering;text analysis,soft clustering algorithm;neural networks;text document retrieval area;soft document clustering methods;KMART;nonlinear Hebbian neural network;Oja learning rule,,3,,14,,22-Dec-11,,,IEEE,IEEE Conferences
Automatic thematic categorization of documents using a fuzzy taxonomy and fuzzy hierarchical clustering,文檔聚類算法的語義探索性評估,M. Wallace; G. Akrivas; G. Stamou,"Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece","The 12th IEEE International Conference on Fuzzy Systems, 2003. FUZZ '03.",25-Jun-03,2003,2,,1446,1451 vol.2,"In this paper we formally define the problem of automatic detection of thematic categories in a semantically indexed document, and identify the main obstacles to overcome in this process. Furthermore, we explain how detection of thematic categories can be achieved, with the use of a fuzzy quasi-taxonomic relation. Our approach relies on a fuzzy hierarchical clustering algorithm; this algorithm uses a similarity measure that is based on the notion of context.",,0-7803-7810-5,10.1109/FUZZ.2003.1206644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1206644,,Taxonomy;Ontologies;Information retrieval;Fuzzy systems;Clustering algorithms;Document handling;Multimedia systems;Laboratories;Upper bound;Text analysis,fuzzy logic;document handling;information retrieval systems;knowledge based systems,automatic thematic categorization;fuzzy taxonomy;automatic detection;semantically indexed document;main obstacles;fuzzy quasi-taxonomic relation;fuzzy hierarchical clustering algorithm;similarity measure;context notion;thematic categories detection,,6,,8,,25-Jun-03,,,IEEE,IEEE Conferences
Unsupervised style classification of document page images,使用拓撲聚類引導的拉普拉斯能量分割的文檔二值化,Song Mao; Lan Nie; G. R. Thoma,"U.S. Nat. Library of Medicine, Bethesda, MD, USA; NA; NA",IEEE International Conference on Image Processing 2005,14-Nov-05,2005,2,,II,510,"Style classification of document page images is crucial for logical structure analysis of heterogeneous collections of documents. Both layout and contextual features contain significant information about document styles. Most existing methods are supervised methods in which specific document models or classifiers are learned from a training set of document page images with known class labels. In this paper, we propose an unsupervised classification method that involves no training or manual selection of algorithm parameters. In particular, we first represent each document page as an ordered labeled X-Y tree. A tree matching algorithm is then used to compute style dissimilarity between two document pages. We propose a set of tree edit cost functions based on Karl Pearson distance between two multivariate feature observations, which is robust to the over-segmentation problem and zone length variations of same logical entities. Finally, the K-medoids algorithm is used to find an optimal grouping of the trees into K clusters, each of which corresponds to a distinct document style. We evaluate our algorithm on test datasets with different cluster sizes and degrees of style similarity. Experimental results show our algorithm achieved an average classification accuracy of 95.69% over six datasets consisting of 150 pages of 11 different styles.",2381-8549,0-7803-9134-9,10.1109/ICIP.2005.1530104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1530104,,Clustering algorithms;Image analysis;Cost function;Text analysis;Classification tree analysis;Algorithm design and analysis;Training data;Libraries;Biomedical imaging;Robustness,image classification;document image processing;image representation,unsupervised style classification;document page images;logical structure analysis;tree matching algorithm;Karl Pearson distance;K-medoids algorithm,,1,,8,,14-Nov-05,,,IEEE,IEEE Conferences
Clustering-based burst-detection algorithm for web-image document stream on social media,可以使用簡短的析取查詢檢索大多數群集,S. Tamura; K. Tamura; H. Kitakami; K. Hirahara,"Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan; Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-Ku, 731-3194, Japan","2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",13-Dec-12,2012,,,703,708,"With an increasing interest in social media, a large number of Web images have been created on the Internet. Therefore, extracting useful knowledge from a large-scale set of Web images has become a new type of challenge. In this paper, we focus on Web images that have been posted onto the Internet through social media sites. The main objective of this study is to extract the events and track the topics of a document stream that includes Web images. To address this challenge, this paper proposes a novel method for burst detection in a Web-image document stream. The proposed method integrates a clustering technique with Kleinberg's burst detection. To evaluate the proposed method, we used actual tweets from Twitter users. The experimental results show that the proposed method can extract the events and track the topics related to Web images posted on social media sites.",1062-922X,978-1-4673-1714-6,10.1109/ICSMC.2012.6377809,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6377809,text mining;social media;multimedia data;burst detection;topic detection,Media;Streaming media;Clustering algorithms;Earthquakes;Twitter;Data mining;Internet,document handling;pattern clustering;social networking (online),clustering-based burst-detection algorithm;Web image document stream;Internet;social media site;event extraction;topics tracking;Twitter,,4,,21,,13-Dec-12,,,IEEE,IEEE Conferences
A Multilingual Patent Text-Mining Approach for Computing Relatedness Evaluation of Patent Documents,用於重建交叉粉碎的英語文檔的管道,C. Lee; H. Yang; C. Wu; Y. Li,"Dept. of Electr. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Kaohsiung, Taiwan; Dept. of Inf. Manage., Nat. Univ. of Kaohsiung, Kaohsiung, Taiwan; Dept. of Electr. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Kaohsiung, Taiwan; Dept. of Inf. Manage., Nat. Univ. of Kaohsiung, Kaohsiung, Taiwan",2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing,17-Nov-09,2009,,,612,615,"This paper describes our work on developing a language-independent technique for discovery of implicit knowledge about patents from multilingual patent information sources. Traditional techniques of multi- and cross-language patent retrieval are mostly based on the process of translation. One major problem of those is that it is difficult to find related patents produced from other countries in a stand-alone patent information system. In this paper, we present a novel system platform to support locating similar and relevant multilingual patent documents. The platform is developed using a multilingual vector space based on the latent semantic indexing (LSI) model, and utilizing collected professional Chinese-English parallel corpora for training the system model. These multilingual patent documents can then be mapped into the semantic vector space for evaluating their similarity by means of text clustering techniques. The preliminary results show that our platform framework has potential for retrieval and relatedness evaluation of multilingual patent documents.",,978-1-4244-4717-6,10.1109/IIH-MSP.2009.162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337401,Neural networks;Multilingual patent retrieval;Text mining;Latent semantic indexing;Text clustering,Indexing;Information retrieval;Large scale integration;Information systems;Text mining;Information processing;Dictionaries;Multimedia computing;Signal processing;Information management,data mining;indexing;information retrieval;patents;pattern clustering;text analysis,multilingual patent text-mining approach;language-independent technique;multilingual patent information sources;multilingual patent documents;multilingual vector space;latent semantic indexing model;text clustering techniques,,1,,19,,17-Nov-09,,,IEEE,IEEE Conferences
Multiprocessor document allocation: a genetic algorithm approach,使用後綴樹聚類的個性化Web搜索引擎的體系結構,O. Frieder; H. T. Siegelmann,"Fac. of Comput. Sci. & Comput. Eng., Florida Inst. of Technol., Melbourne, FL, USA; NA",IEEE Transactions on Knowledge and Data Engineering,6-Aug-02,1997,9,4,640,642,"We formally define the Multiprocessor Document Allocation Problem (MDAP) and prove it to be computationally intractable (NP complete). Once it is shown that MDAP is NP complete, we describe a document allocation algorithm based on genetic algorithms. This algorithm assumes that the documents are clustered using any one of the many clustering techniques. We later show that our allocation algorithm probabilistically converges to a good solution. For a behavioral evaluation, we present sample experimental results.",1558-2191,,10.1109/69.617055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617055,,Genetic algorithms;Clustering algorithms;Information retrieval;Parallel processing;Phased arrays;Information systems;Databases;Concurrent computing;Memory architecture;Costs,multiprocessing systems;parallel algorithms;information retrieval;genetic algorithms;resource allocation,Multiprocessor Document Allocation Problem;genetic algorithm approach;computationally intractable;NP complete;document allocation algorithm;document clustering;clustering techniques;probabilistic convergence;behavioral evaluation;experimental results,,10,,12,,6-Aug-02,,,IEEE,IEEE Journals
Towards Scaleable and Adaptive Document Routing Services,完整的歷史文獻光學字符識別方法,D. Ramachandran; N. Boyette; I. Cheng; V. Krishna; S. Srinivasan,"University of Illinois at Urbana-Champaign; IBM Almaden Research Center, 650 Harry Road, San Jose, CA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA; IBM Almaden Research Center, 650 Harry Road, San Jose, CA",2006 IEEE International Conference on Services Computing (SCC'06),11-Dec-06,2006,,,311,314,"We propose a framework for intelligent document routing that extends XML technologies and enables realtime update of business routing logic. A novel feature of our system is the ability to semi-automate the process of creating and updating the routing rules. This is achieved by a closed loop system that performs clustering and inductive rule learning of document features. This framework was tested and deployed in a large real-world enterprise environment resulting in reduced development costs and simplified administration. The user interacts with the system using an application to graphically create, edit and publish the business processes",,0-7695-2670-5,10.1109/SCC.2006.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026943,,Routing;Data mining;XML;Engines;Runtime;Logic testing;User interfaces;Document handling;Dissolved gas analysis;Computer science,business data processing;document handling;learning by example;user interfaces;XML,intelligent document routing service;XML technology;business routing logic;closed loop system;inductive rule learning;enterprise environment,,,,9,,11-Dec-06,,,IEEE,IEEE Conferences
Entity Local Structure Graph Matching for Mislabeling Correction,結合段落嵌入和密度峰聚類以進行語音文檔摘要,N. Kooli; A. Bela?d; A. Joseph; V. P. D'Andecy,"LORIA, Univ. de Lorraine Campus Sci., Vandoeuvre-les-Nancy, France; LORIA, Univ. de Lorraine Campus Sci., Vandoeuvre-les-Nancy, France; ITESOFT Groupe, YOOZ, Aimargues, France; ITESOFT Groupe, YOOZ, Aimargues, France",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,257,262,"This paper proposes an entity local structure comparison approach based on inexact subgraph matching. The comparison results are used for mislabeling correction in the local structure. The latter represents a set of entity attribute labels which are physically close in a document image. It is modeled by an attributed graph describing the content and presentation features of the labels by the nodes and the geometrical features by the arcs. A local structure graph is matched with a structure model which represents a set of local structure model graphs. The structure model is initially built using a set of well chosen local structures based on a graph clustering algorithm and is then incrementally updated. The subgraph matching adopts a specific cost function that integrates the feature dissimilarities. The matched model graph is used to extract the missed labels, prune the extraneous ones and correct the erroneous label fields in the local structure. The evaluation of the structure comparison approach on 525 local structures extracted from 200 business documents achieves about 90% for recall and 95% for precision. The mislabeling correction rates in these local structures vary between 73% and 100%.",,978-1-5090-1792-8,10.1109/DAS.2016.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490127,entity local structure;subgraph matching;mislabeling correction;structure model;graph clustering,Semantics;Databases;Cost function;Labeling;Clustering algorithms;Optical character recognition software;Noise measurement,document image processing;graph theory;pattern clustering,entity local structure graph matching;mislabeling correction;entity local structure comparison approach;inexact subgraph matching;entity attribute labels;document image;presentation features;geometrical features;graph clustering algorithm;business documents,,2,,13,,13-Jun-16,,,IEEE,IEEE Conferences
Web Search Using Summarization on Clustered Web Documents Retrieved by User Queries,離散小波變換在可疑文件識別中的應用,R. Qumsiyeh; Y. Ng,"Comput. Sci. Dept., Brigham Young Univ., Provo, UT, USA; Comput. Sci. Dept., Brigham Young Univ., Provo, UT, USA",2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),4-Feb-16,2015,1,,401,404,"Web search engines, such as Google, Bing, and Yahoo!, rank the set of documents S retrieved in response to a user query and represent each document D in S using a title and a snippet, which serves as an abstract of D. Snippets, however, are not as useful as they are designed for, i.e., assisting its users to quickly identify results of interest. These snippets are inadequate in providing distinct information and capture the main contents of the corresponding documents. Moreover, when the intended information need specified in a search query is ambiguous, it is very difficult, if not impossible, for a search engine to identify precisely the set of documents that satisfy the user's intended request without requiring additional information. Furthermore, a document title is not always a good indicator of the content of the corresponding document either. All of these design problems can be solved by our proposed query-based summarizer, called QSum. QSum generates a concise/comprehensive summary for each cluster of documents retrieved in response to a user query, which saves the user's time and effort in searching for specific information of interest by skipping the step to browse through the retrieved documents one by one. Experimental results show that QSum is effective and efficient in creating a high-quality summary for each cluster to enhance web search.",,978-1-4673-9618-9,10.1109/WI-IAT.2015.220,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396837,Web search;query processing;summarization,Web search;Google;Silicon;Engines;Search engines;HTML,document handling;Internet;query processing;search engines,summarization;clustered Web document;user query;Web search engine;Google;Bing;Yahoo!;snippet;search query;document title;query-based summarizer;QSum;retrieved document,,,,10,,4-Feb-16,,,IEEE,IEEE Conferences
Multilingual hyperdocument recognition: a document mining approach,直接和潛在建模技術，用於計算語音文檔相似度,T. D. Nguyen; K. Zreik,"Doctorant en Informatique, Univ. de Caen, France; NA","Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.",6-Jul-04,2004,,,443,444,"This paper suggests a new distributive analysis approach to retrieve multilingual hyperdocument. To learn about the number of languages involved in a Web site, a set of general and computational knowledge is used, which is completely independent of the linguistic domains. The mining process considers three main stages: preprocessing (hyperdocument vectoring), processing (clustering), post processing (clusters pruning). A prototype of this system has been developed and tested two clustering approaches on several international Web - sites with very high and completely satisfactory performances.",,0-7803-8482-2,10.1109/ICTTA.2004.1307822,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307822,,Clustering algorithms;Prototypes;System testing;Performance evaluation;Machine learning;Data mining;Machine learning algorithms;Distributed computing;Unsupervised learning;Computer architecture,data mining;information retrieval;computational linguistics;Web sites;pattern clustering,multilingual hyperdocument recognition;document mining approach;distributive analysis;multilingual hyperdocument retrieval;language learning;Web site;computational knowledge;data mining process;preprocessing;hyperdocument vectoring;data clustering;post processing;cluster pruning,,1,,4,,6-Jul-04,,,IEEE,IEEE Conferences
Directional Clustering Through Matrix Factorization,SOPHIA：OHSUMED集合的基於集群的交互式檢索系統,T. Blumensath,"Institute of Sound and Vibration Research Signal Processing and Control Group, University of Southampton, Southampton, U.K.",IEEE Transactions on Neural Networks and Learning Systems,20-May-17,2016,27,10,2095,2107,"This paper deals with a clustering problem where feature vectors are clustered depending on the angle between feature vectors, that is, feature vectors are grouped together if they point roughly in the same direction. This directional distance measure arises in several applications, including document classification and human brain imaging. Using ideas from the field of constrained low-rank matrix factorization and sparse approximation, a novel approach is presented that differs from classical clustering methods, such as seminonnegative matrix factorization, K-EVD, or k-means clustering, yet combines some aspects of all these. As in nonnegative matrix factorization and K-EVD, the matrix decomposition is iteratively refined to optimize a data fidelity term; however, no positivity constraint is enforced directly nor do we need to explicitly compute eigenvectors. As in k-means and K-EVD, each optimization step is followed by a hard cluster assignment. This leads to an efficient algorithm that is shown here to outperform common competitors in terms of clustering performance and/or computation speed. In addition to a detailed theoretical analysis of some of the algorithm's main properties, the approach is empirically evaluated on a range of toy problems, several standard text clustering data sets, and a high-dimensional problem in brain imaging, where functional magnetic resonance imaging data are used to partition the human cerebral cortex into distinct functional regions.",2162-2388,,10.1109/TNNLS.2015.2505060,Engineering and Physical Sciences Research Council within the Faculty of Engineering through the Environment Funded New Frontiers Fellowship; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374711,Clustering;iterative hard thresholding;K-EVD;k-means;K-SVD;seminonnegative matrix factorization (semi-NMF),Matrix decomposition;Cost function;Brain;Clustering algorithms;Magnetic resonance imaging,matrix decomposition;pattern clustering;singular value decomposition,directional clustering problem;feature vectors;directional distance measure;document classification;human brain imaging;constrained low-rank matrix factorization;sparse approximation;K-EVD;k-means clustering;data fidelity term;positivity constraint;functional magnetic resonance imaging data,Algorithms;Brain;Cluster Analysis;Humans;Neural Networks (Computer),6,,44,,7-Jan-16,,,IEEE,IEEE Journals
Multi-document summarization systems comparison,基於二階統計量的順序說話人聚類,L. Li; W. Heng; P. Liu,"School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China",2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems,14-Nov-13,2012,3,,1409,1413,"This paper compared two multi-document summarization systems we developed. One system used hierarchical sentence clustering algorithm to find the important information, while the other system mainly adopted hierarchical Latent Dirichlet Allocation (hLDA) topic model to obtain the sub-topics of multi-document data. Both of the two systems are evaluated and compared on TAC 2010/TAC 2011 data using the ROUGE testing method with same parameters' setting. The results have shown that the hLDA system has got some improvement compared with the clustering system. And normally in ROUGE testing, results from non-stopwords are better than those from stopwords.",2376-595X,978-1-4673-1857-0,10.1109/CCIS.2012.6664617,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664617,multi-document summarization;system comparison;hierarchical sentence clustering;HLDA,Resource management;Computational modeling;Feature extraction;Clustering algorithms;Probabilistic logic;Vectors;Semantics,document handling;pattern clustering,multidocument summarization systems comparison;hierarchical sentence clustering algorithm;hierarchical Latent Dirichlet Allocation;hLDA;multidocument data;ROUGE testing method,,,,19,,14-Nov-13,,,IEEE,IEEE Conferences
Event Information Organization Algorithm about Enterprises Based on Timeline,使用句法和語義信息的文檔建模,Q. Ding; P. Wang; Y. Fang; Z. Zhang,"Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China; Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China; Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China","2017 International Conference on Computer Technology, Electronics and Communication (ICCTEC)",8-Aug-19,2017,,,627,631,"To organize process of a number of events containing common topics, the paper proposed an event information organization algorithm by two text clustering, topic clustering which combines Latent dirichlet allocation model (LDA model) and TF-IDF model and event Hierarchical Clustering based on timeline. Topic clustering is a topic clustering through dynamic K-means in which the document vector distance is calculated. It can be known that events sets have a common topic by this clustering method. And another clustering method event hierarchical clustering is a clustering events phase based on timeline. In a word, this paper organizes event information by combining the above two clustering methods.",,978-1-5386-5784-3,10.1109/ICCTEC.2017.00141,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789310,event information organization algorithm;topic clustering;event hierarchical clustering,Clustering algorithms;Organizations;Heuristic algorithms;Clustering methods;Computational modeling;Computer science;Indexes,pattern clustering;text analysis,event information organization algorithm;text clustering;topic clustering;events sets;clustering method event hierarchical clustering;clustering events phase;enterprises,,,,14,,8-Aug-19,,,IEEE,IEEE Conferences
Fuzzy Conceptualization Model for Document Representation,XML文檔的矢量矩陣迭代自組織輔助聚類算法,P. Sijin; H. N. Champa,"University Visvesvaraya College of Engineering Bangalore University,Department of Computer Science and Engineering,Bangalore,India; University Visvesvaraya College of Engineering,Department of Computer Science and Engineering,Bangalore,India","2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",16-Sep-20,2020,,,1,4,The Fuzzy Conceptualization Model (FCM) performs a fuzzy mapping of query set to the given word corpus to obtain fuzzy membership degree for a document based on semantic correlation among words quantified by cosine similarity measures between word embeddings of queries and word corpus. A fuzzy membership function is defined for the given search query with word attributes of the given corpus. The generated numerical vector representation is the measure of similarity between word attributes of document and base terms set. The ground truth fuzzy set obtained for the popular terms in the given data corpus is used for creating semantic table for the base term cluster and hence formulates a matrix representation for both accurate and semantically related terms to be getting processed and counted together with out much over head to typical Fuzzy Bag of Words (FBoW) models and its variants. The proposed FCM outperforms other comparative models with high confidence level under paired t-test. FCM performs well when the mapping bound 弇 is zero since the increased 弇 values increases the sparsity of the vector representation of the documents.,,978-1-7281-6828-9,10.1109/CONECCT50063.2020.9198656,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198656,word embedding;salience;FBoW;coherence;n-DCG,Semantics;Fuzzy sets;Numerical models;Data models;Vocabulary;Indexes;Clustering algorithms,fuzzy set theory;matrix algebra;pattern clustering;query processing;vectors;word processing,document representation;FCM;word corpus;fuzzy membership degree;semantic correlation;cosine similarity measures;fuzzy membership function;search query;word attributes;vector representation;ground truth fuzzy set;semantic table;base term cluster;matrix representation;fuzzy conceptualization model;word embeddings,,,,26,,16-Sep-20,,,IEEE,IEEE Conferences
A new text clustering method based on Huffman encoding algorithm,基於Web文檔語義分析的摘要器系統,M. Muntean; L. C?bulea; H. V?lean,"Science and Engineering Department, ?? Decembrie 1918??University of Alba Iulia, Alba Iulia, Romania; Science and Engineering Department, ?? Decembrie 1918??University of Alba Iulia, Alba Iulia, Romania; Automation Department, Technical University of Cluj Napoca, Cluj Napoca, Romania","2014 IEEE International Conference on Automation, Quality and Testing, Robotics",17-Jul-14,2014,,,1,6,"Clustering of text data is a widely studied data mining problem and has a number of applications such as spam detection, document organization and indexing, IP-address streams, credit-card transaction streams, and so on. However, the clustering of text data is still in early stage, because the research focused so far on the case of quantitative or categorical data. In this paper we propose a new method for improving the clustering accuracy of text data. Our method encodes the string values of a dataset using Huffman encoding algorithm, and declares these attributes as integer in the cluster evaluation phase. In the experimental part, we compared the cluster label assigned by the proposed method to each instance of the dataset with its real category, and we obtained a better clustering accuracy than the one found with traditional methods. This method is useful when the dataset to be clustered has only string attributes, because in this case, a traditional clustering method does not recognize, or recognize with a low accuracy, the category of instances.",,978-1-4799-3732-5,10.1109/AQTR.2014.6857838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6857838,accuracy;cluster analysis;huffman encoding;machine learning,Clustering algorithms;Encoding;Unsolicited electronic mail;Clustering methods;Data mining;Accuracy;Algorithm design and analysis,data mining;encoding;Huffman codes;pattern clustering;text analysis,Huffman encoding algorithm;data mining problem;spam detection;document organization;indexing;IP-address streams;credit-card transaction streams;quantitative data;categorical data;text data clustering accuracy improvement;dataset string value encoding;cluster evaluation;cluster label assignment;dataset instance;string attributes,,2,,16,,17-Jul-14,,,IEEE,IEEE Conferences
Keyword-Labeled Classification with Auxiliary Unlabeled Documents,成對隨機交換聚類算法的質心比,C. Zhang; D. Xing; K. Zhou,"Shanghai Jiaotong Univ., Shanghai; Shanghai Jiaotong Univ., Shanghai; Shanghai Jiaotong Univ., Shanghai",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,3,,463,466,"To reduce the human effort in labeling the training set for document classification, some learning algorithms ask users to give the representative keywords for each class rather than any labeled documents. The key challenge in such \emph {keyword-labeled classification} is how to learn the high quality classifier with very small number of keywords. In this paper, we propose a novel co-clustering based classification algorithm for keyword-labeled classification (CCKC) by utilizing auxiliary unlabeled documents. The experimental results show our algorithm greatly improves the classification performance over existing approaches.",,978-0-7695-3496-1,10.1109/WIIAT.2008.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740822,,Testing;Clustering algorithms;Labeling;Intelligent agent;Classification algorithms;Humans;Training data;Internet;Bridges;Text processing,classification;learning (artificial intelligence);pattern clustering;text analysis,keyword-labeled classification;auxiliary unlabeled document;training set;co-clustering based classification algorithm;learning algorithm,,,,9,,6-Jan-09,,,IEEE,IEEE Conferences
An Incremental Algorithm for Clustering Search Results,頻率敏感型競爭學習，用於高維超球上的可伸縮平衡聚類,Y. Liu; Y. Ouyang; H. Sheng; Z. Xiong,"Sch. of Comput. Sci. & Technol., Beihang Univ., Beijing; NA; NA; NA",2008 IEEE International Conference on Signal Image Technology and Internet Based Systems,22-Dec-08,2008,,,112,117,"When Internet users are facing massive search results, document clustering techniques are very helpful. Generally, existing clustering methods start with a known set of data objects, measured against a known set of attributes. However, there are numerous applications where the attribute set can only obtained gradually as processing data objects incrementally. This paper presents an incremental clustering algorithm (ICA) for clustering search results, which relies on pair-wise search result similarity calculated using Jaccard method. We use a measure namely, cluster average similarity area to score cluster cohesiveness. Experimental results show that our algorithm leads to less computational time than traditional clustering method while achieving a comparable or better clustering quality.",,978-0-7695-3493-0,10.1109/SITIS.2008.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725794,incremental clustering;search results;internet,Clustering algorithms;Clustering methods;Independent component analysis;Histograms;Internet;User-generated content;Computer science;Area measurement;Web search;Search engines,Internet;pattern clustering;search engines,incremental algorithm;clustering search results;Internet users;document clustering;incremental clustering algorithm;pair-wise search result;Jaccard method,,4,,11,,22-Dec-08,,,IEEE,IEEE Conferences
Logo spotting for document categorization,短語排名和基於維基百科的集群標籤,V. P. Le; M. Visani; C. De Tran; J. Ogier,"Laboratory L3I, Faculty of Science and Technology, La Rochelle University, France; Laboratory L3I, Faculty of Science and Technology, La Rochelle University, France; College of Information and Communication Technology, Can Tho University, Vietnam; Laboratory L3I, Faculty of Science and Technology, La Rochelle University, France",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,3484,3487,"Logo spotting is of a great interest because it enables to categorize the document images of a digital library of scanned documents according to their sources, without any costly semantic analysis of their textual transcript. In this paper, we present an approach for logo spotting, based on the matching of keypoints extracted both from the query document images and a given set of logos (gallery) using SIFT. In order to filter the matching points and keep only the most relevant, we compare the spatial distribution of the matching keypoints in the query image and in the logo gallery. We test our approach using a large collection of real world documents using a well-known benchmark database of logos and show that our approach achieves good performances compared to state-of-the-art approaches.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460915,,Databases;Feature extraction;Matched filters;Noise;Histograms;Image segmentation;Clustering algorithms,digital libraries;document image processing;image retrieval;information filtering;visual databases,logo spotting;keypoint extraction;query document image categorization;SIFT;matching point filtering;matching keypoint spatial distribution;logo gallery;real world documents;logos database;scanned document digital library,,2,,9,,14-Feb-13,,,IEEE,IEEE Conferences
Topic Detection from Short Text: A Term-based Consensus Clustering method,XML文檔的協同集群,Hao Lin; Bo Sun; J. Wu; H. Xiong,"School of Economics and Management, Beihang University, Beijing, China; National Computer Network Emergency Response Technical Team/Coordination Center of China, Beijing, China; School of Economics and Management, Beihang University, Beijing, China; School of Economics and Management, Beihang University, Beijing, China",2016 13th International Conference on Service Systems and Service Management (ICSSSM),11-Aug-16,2016,,,1,6,"The process of Topic Detection from Short Text Systems (SMS) is to extract distinct topics hidden inside short text collections, such as Twitter, Weibo, and instant messages. With the recent emergence of large volume user generated content collections enabled by online social media, topic detection from SMS becomes a challenging yet promising means for online public opinion analysis. In available literature, many forms and methods of topic detection have been proposed, but obtaining meaningful and coherent data is still difficult to reliably obtain for the extreme sparsity brought by SMS. To this end, we developed a Term-based Consensus Clustering topic detection (TCC) framework to provide an unsupervised methodology for finding distinct topics from within SMS collections. Specifically, we adopt a consensus clustering technique called K-means-based Consensus Clustering to handle SMS clustering, due to its low computational complexity and robust clustering performance. To further enrich the features of the information of the sparse SMS data, we conduct term clustering in the highly dense term space instead of the conventionally targeted sparse document space. To be more specific, we first use a feature space transfer technique to represent short text collections as a pseudo-document matrix, where rows, namely instances, correspond to terms and columns, namely features, correspond to adjacent terms. Basic partitions are generated from the pseudo-document matrix for term clustering and consensus clustering is followed to obtain the final term clustering result. Finally, a document classification process is adopted and a document is assigned to a cluster, where most terms occurred. Extensive experiments on real-world data sets demonstrate that TCC is comparable to several widely used methods in terms of topic detection quality. Particularly, we demonstrate that TCC obtains best clustering performance when observing a large number of the predefined topics across short text collections.",2161-1904,978-1-5090-2842-9,10.1109/ICSSSM.2016.7538624,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538624,Topic Detection;SMS;Consensus Clustering;Feature Space Transfer,,document handling;electronic messaging;matrix algebra;pattern clustering;social networking (online);user interfaces,topic detection;term-based consensus clustering method;short text systems;SMS;Twitter;Weibo;instant messages;user generated content collections;online social media;TCC framework;K-means-based consensus clustering;feature space transfer;short text collections;pseudo-document matrix,,,,26,,11-Aug-16,,,IEEE,IEEE Conferences
Algorithm for clustering analysis of gene expression data using MapReduce framework,基於語義的雲計算集群系統,P. P. A. Priya; R. Lawrance,"Department of Computer Science, Ayya Nadar Janaki Ammal College, Sivakasi; Department of Computer Applications, Ayya Nadar Janaki Ammal College, Sivakasi",2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16),31-Oct-16,2016,,,1,4,"Bioinformatics is a fast growing field in data mining techniques to solve biological problems. Few decades' fast developments in genomic and other molecular research in information technologies have combined to produce an enormous amount of information relevant to molecular biology. The study of gene expression data investigation has developed in the few years from being purely data-centric to interrelate. The developments in gene expression based analysis methods are association, classification, clustering, and prediction studies. In recent times, industries have a rapid growth of data; a data analysis tool is required to satisfy the need for analyzing a huge volume of data. The MapReduce framework is designed to compute data demanding applications to support effective decision making. This document afford an outline of the MapReduce programming model, different methods to implement MapReduce models to process large-scale datasets and used to cluster the given gene dataset based on specified features.",,978-1-4673-8437-7,10.1109/ICCTIDE.2016.7725376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725376,Clustering;Microarray;Gene expression data;MapReduce;Canopy Clustering Algorithm,Clustering algorithms;Algorithm design and analysis;Data mining;Gene expression;Big data;Partitioning algorithms;Bioinformatics,bioinformatics;data analysis;genetics;parallel programming;pattern classification;pattern clustering,clustering analysis;gene expression data;MapReduce framework;bioinformatics;data mining;genomic;molecular research;information technology;molecular biology;association;classification;prediction;data analysis tool;decision making;MapReduce programming model,,1,,11,,31-Oct-16,,,IEEE,IEEE Conferences
Arabic handwritten texts clusterization based on Feature Relation Graph (FRG),智能擴展聚類遺傳算法,V. A. Pavlov; D. S. Shalymov,"Saint-Petersburg State University, Russian Federation; Saint Petersburg State University, Russian Federation",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,941,945,"Text clustering is known as the problem of grouping texts so that all texts within a group share a similar measure (similar author, similar genre, etc.) This task became very important for the last two decades because of increased number of text documents in digital form which needs to be organized and processed. We investigate a new metric based on the Feature Relation Graph (FRG) for Arabic handwritten texts clustering. This metric has proved to be effective for the text independent Persian writer identification. We have used it to solve more general problem of texts clustering. Pattern based features are extracted from handwritten texts using Gabor and XGabor filters. The extracted features are represented for each cluster by using the FRG. We apply several clustering algorithms in a space of FRGs. Numerical experiments to demonstrate effectiveness of proposed metric and compare effectiveness of different algorithms are provided.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333900,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333900,,Image edge detection;Clustering algorithms;Chlorine,feature extraction;Gabor filters;graph theory;handwritten character recognition;natural language processing;pattern clustering;text detection,Arabic handwritten text clusterization;feature relation graph;FRG;text clustering;text grouping problem;text documents;text independent Persian writer identification;pattern based features;XGabor filters;feature extraction,,2,,12,,23-Nov-15,,,IEEE,IEEE Conferences
Incrementally Clustering Legislative Interpellation Documents,基於無監督的基於示例的學習，用於改進的文檔圖像分類,F. Lin; Y. Huang; D. Liao,"Inst. of Service Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Inst. of Technol. Manage., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Inst. of Political Sci., Nat. Sun Yat-sen Univ., Kaohsiung, Taiwan",2012 45th Hawaii International Conference on System Sciences,9-Feb-12,2012,,,2521,2530,"The Parliamentary Library of Legislative Yuan website provides a fair and objective channel for the public to trace daily activities of the Legislative Yuan and legislators' inquiries in Taiwan. However, the increased information content causes information overloading problem. To mitigate such information overloading problem, this study proposes an incremental clustering mechanism to renew the information regularly by presenting it as a categorical structure to ease the efforts on tracing issue development. This study first initiates a basic categorical structure by a two-stage clustering approach. Then, the incremental clustering method is applied to clustering a collection of incoming documents corresponding to the same topic into clusters, and designates these clusters into existing categories or creates a new category. Experimental results show the effectiveness of the proposed incremental clustering method, which enables the management of the hierarchical structure of categories on legislative interpellation. This study contributes to e-government initiatives on facilitating the public to trace the legislative activities periodically.",1530-1605,978-0-7695-4525-7,10.1109/HICSS.2012.322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149320,Incremental clustering,Vectors;Libraries;Periodic structures;Clustering methods;Maintenance engineering;Educational institutions;Support vector machine classification,government data processing;government policies;legislation;pattern clustering,legislative interpellation document;information overloading problem;e-government;incremental clustering mechanism,,,,11,,9-Feb-12,,,IEEE,IEEE Conferences
Deriving Semantic Sessions from Semantic Clusters,基於小波特徵的財務憑證圖像數據挖掘,B. Safarkhani; M. Talabeigi; M. Mohsenzadeh; M. R. Meybodi,"Dept. of Comput. Eng., Islamic Azad Univ., Tehran; Dept. of Comput. Eng., Islamic Azad Univ., Tehran; Dept. of Comput. Eng., Islamic Azad Univ., Tehran; NA",2009 International Conference on Information Management and Engineering,19-Jun-09,2009,,,523,528,"A important phase in any Web personalization system is transaction identification. Recently a number of researches have been done to incorporate semantics of a website in representation of transactions. Building a hierarchy of concepts manually is time consuming and expensive. In this paper we intend to address these shortcomings. Our contribution is that we introduce a mechanism to automatically improve the representation of the user in the Website using a comprehensive lexical semantic resource and semantic clusters. We utilize Wikipedia, the largest encyclopedia to date, as a rich lexical resource to enhance the automatic construction of vector model representation of user sessions. We cluster Web pages based on their content with hierarchical unsupervised fuzzy clustering algorithms ,are effective methods, for exploring the structure of complex real data where grouping of overlapping and vague elements is necessary. Entries in Web server logs are used to identify users and visit sessions, while Web page or resources in the site are clustered based on their content and their semantic. Theses clusters of Web documents are used to scrutinize the discovered web sessions in order to identify what we call sub-sessions. Each subsession have consistent goal. This process engendered to improving deriving semantic sessions from Web site user page views. Our experiments show that proposed system significantly improves the quality of Web personalization process.",,978-0-7695-3595-1,10.1109/ICIME.2009.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5077090,Semantic vectors;Semantic sub-session;Semantic cluster;Wikipedia,Wikipedia;Web pages;Clustering algorithms;Web server;Data mining;Ontologies;Information management;Encyclopedias;Feature extraction;Taxonomy,document handling;fuzzy set theory;pattern clustering;personal computing;user interfaces;Web sites,semantic sessions;semantic clusters;Web personalization system;transaction identification;Web site;semantic resource;Wikipedia;hierarchical unsupervised fuzzy clustering algorithms;Web pages;Web server logs;Web documents,,6,,21,,19-Jun-09,,,IEEE,IEEE Conferences
Automatically Determining the Number of Clusters in Unlabeled Data Sets,利用相關係數實現英語文本文檔的可靠聚類,L. Wang; C. Leckie; K. Ramamohanarao; J. Bezdek,"The University of Melbourne, Melbourne; The University of Melbourne, Melbourne; The University of Melbourne, Melbourne; The University of Melbourne, Melbourne",IEEE Transactions on Knowledge and Data Engineering,23-Jan-09,2009,21,3,335,350,"Clustering is a popular tool for exploratory data analysis. One of the major problems in cluster analysis is the determination of the number of clusters in unlabeled data, which is a basic input for most clustering algorithms. In this paper we investigate a new method called DBE (dark block extraction) for automatically estimating the number of clusters in unlabeled data sets, which is based on an existing algorithm for visual assessment of cluster tendency (VAT) of a data set, using several common image and signal processing techniques. Basic steps include: 1) generating a VAT image of an input dissimilarity matrix; 2) performing image segmentation on the VAT image to obtain a binary image, followed by directional morphological filtering; 3) applying a distance transform to the filtered binary image and projecting the pixel values onto the main diagonal axis of the image to form a projection signal; 4) smoothing the projection signal, computing its first-order derivative, and then detecting major peaks and valleys in the resulting signal to decide the number of clusters. Our new DBE method is nearly ""automatic"", depending on just one easy-to-set parameter. Several numerical and real-world examples are presented to illustrate the effectiveness of DBE.",1558-2191,,10.1109/TKDE.2008.158,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4759628,Clustering;Cluster Tendency;Data and knowledge visualization;Database Applications;Database Management;Information Technology,Signal processing algorithms;Clustering algorithms;Data analysis;Algorithm design and analysis;Data mining;Image generation;Signal generators;Image segmentation;Filtering;Pixel,data analysis;document image processing;image segmentation;matrix algebra;pattern clustering,data analysis;cluster analysis;dark block extraction;unlabeled data sets;cluster tendency visual assessment;input dissimilarity matrix;image segmentation;directional morphological filtering,,58,,46,,23-Jan-09,,,IEEE,IEEE Journals
Probabilistic Clustering and Classification for Textual Data: An Online and Incremental Approach,Web文檔聚類的最大頻繁項集方法,T. F. Rodrigues; P. M. Engel,"Inf. Inst., Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil; Inf. Inst., Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil",2014 Brazilian Conference on Intelligent Systems,15-Dec-14,2014,,,288,293,"Given the amount of information stored in textual data and the fact that it is unstructured, algorithms able to process and transform it to a format useful to solve real world problems are desirable. Tasks like organization and exploration of large document collections can benefit from the design of such methods. This work proposes an incremental, online and probabilistic clustering algorithm for textual data, based on a mixture of Multinomial distributions. The main advantage of the model is that only a single step over the training data is necessary to learn from it. As more texts are processed, the model improves its structure to better represent the data stream.",,978-1-4799-5618-0,10.1109/BRACIS.2014.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984845,Document Clustering;Document Classification;Topic Modeling;Online Learning;Incremental Learning,Mathematical model;Vocabulary;Vectors;Clustering algorithms;Data models;Training;Probabilistic logic,pattern classification;pattern clustering;text analysis,textual data classification;incremental clustering algorithm;online clustering algorithm;probabilistic clustering algorithm;multinomial distributions,,,,16,,15-Dec-14,,,IEEE,IEEE Conferences
Intelligent Web-History Based on a Hybrid Clustering Algorithm for Future-Internet Systems,使用聚類從半結構化文檔中提取列表數據,A. Marin; F. Pop,"Fac. of Automatics & Comput. Sci., Univ. Politeh. of Bucharest, Bucharest, Romania; Fac. of Automatics & Comput. Sci., Univ. Politeh. of Bucharest, Bucharest, Romania",2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing,15-Mar-12,2011,,,145,152,"Internet grows larger year by year and makes users to be confronted with large quantities of data that they cannot fully comprehend. The ongoing transition from Web 2.0 to the Semantic Web makes the development of intelligent services with the ability to discern, classify and simplify web information of vital importance. In this paper we present a new model for web-history organizing in order to improve the user action over the Internet. Based on this model we proposed an application, delivered as a Google Chrome browser extension, which organizes the web-history into semantic clusters, providing the user with an easy-to-follow hierarchal structure. The paper covers the main algorithms in the field, offering a comprehensive critical analysis, such as document vectorization, relational clustering, fuzzy and genetic variations and the item-set-based approach. Our work consists of adapting these algorithms to support an ever-increasing set of input data. The result is a hybrid variation that rapidly offers an acceptable solution, which is optimized in time, a quality preserved during the extensive web explorations a user may undergo. A variety of test results is presented in the end, with under-stress behavior and a selection of user experience.",,978-1-4673-0207-4,10.1109/SYNASC.2011.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169514,Web History;Itemset;Web Mining;Semantic Clustering;Fuzzy Algorithms;Genetic Algorithms,Clustering algorithms;Itemsets;Vectors;History;Algorithm design and analysis;Internet;Genetics,data mining;document handling;fuzzy set theory;pattern clustering;semantic Web,intelligent Web history;hybrid clustering algorithm;Internet;Web 2.0;semantic Web;Web history organizing;Google Chrome browser extension;semantic clusters;hierarchal structure;document vectorization;relational clustering;fuzzy variation;genetic variation;item-set-based approach;Web mining,,,,8,,15-Mar-12,,,IEEE,IEEE Conferences
An efficient similarity matching for clustering XML element,基於圖聚類的文本行分割方法,S. S. Tan; G. K. Hoon,"School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia",2016 Third International Conference on Information Retrieval and Knowledge Management (CAMP),5-Jan-17,2016,,,101,106,"In recent year, XML has become the major means for information representation and exchange on the Web. Due to the increasing number of XML documents, XML similarity becomes essential in a wide range of applications like information extraction, data integration etc. In this paper we present a clustering approach that calculate similarity between XML elements to identify appropriate information unit for retrieval task. Preliminary experimental results conducted using XML data sets of a biliographical site and a conference site shows that the proposed approach is promising to be further extended for other possible XML sources.",,978-1-5090-2954-9,10.1109/INFRKM.2016.7806343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806343,XML;content similarity;structural similarity,XML;Measurement;Semantics;Information retrieval;Computers;Information representation;Natural language processing,document handling;information retrieval;pattern clustering;string matching;XML,similarity matching;XML element clustering;information representation;information exchange;XML documents;information retrieval task;XML data sets;biliographical site;conference site;eXtensible Markup Language,,,,15,,5-Jan-17,,,IEEE,IEEE Conferences
Sparse data for document clustering,為修改PART聚類算法進行文本處理做出的貢獻,I. Veritawati; I. Wasito; Mujiono,"Computer Science, University of Indonesia, Depok, West Java, Indonesia; Computer Science, University of Indonesia, Depok, West Java, Indonesia; Computer Science, University of Indonesia, Depok, West Java, Indonesia",2013 International Conference of Information and Communication Technology (ICoICT),5-Aug-13,2013,,,38,43,"Document clustering which is a part of text mining framework is used to process models and real data collection of cancer documents into several groups. A vector space model of the documents based on their key phrases are formed and called sparse matrix which contains many zero values. A sparse dimensional reduction and several methods of clustering include K-means, Self Organizing and Non-negative Matrices Factorization (NMF) are applied to the data, then the results are compared. Sparse method in dimensional reduction step using Arnoldi Method provides a better result of clustering validity twice more than standard dimensional reduction result.",,978-1-4673-4992-5,10.1109/ICoICT.2013.6574546,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574546,sparse;arnoldi method;k-means;competitive learning;self-organizing;non-negative matrices factorization,Principal component analysis;Indexes;Vectors;Sparse matrices;Organizing;Data models;Standards,data mining;matrix decomposition;pattern clustering;self-organising feature maps;sparse matrices;text analysis,sparse data;document clustering;text mining framework;real data collection;cancer document;vector space model;key phrase;sparse matrix;zero value;sparse dimensional reduction;k-means clustering;self organizing;nonnegative matrices factorization;NMF;sparse method;Arnoldi method;clustering validity,,,,22,,5-Aug-13,,,IEEE,IEEE Conferences
Topic word set-based text clustering,在線PLSA：包括詞彙外單詞在內的批處理更新技術,A. M. Ghazifard; M. Shams; Z. Shamaee,"E-Learning Department, University of Isfahan, Isfahan, Iran; Department, University of Tehran, Tehran Iran; ECE Department, Isfahan University of Technology, Isfahan, Iran",7th International Conference on e-Commerce in Developing Countries:with focus on e-Security,15-Jul-13,2013,,,1,10,"Clustering is the task of grouping related and similar data without any prior knowledge about the labels. In some real world applications, we face huge amounts of unstructured textual data with no organization. In these situations, clustering is a primitive operation that needs to be done to help future e-commerce tasks. Clustering can be used to enhance different e-commerce applications like re-commender systems, customer relationship management systems or personal assistant agents. In this paper we propose a new method for text clustering, by constructing a term correlation graph, and then extracting topic word sets from it and finally, categorizing each document to its related topic with the help of a classification algorithm like SVM. This method provides a natural and understandable description for clusters by their topic word sets, and it also enables us to decide the cluster of documents only when needed and in a parallel fashion, thus significantly reducing the offline processing time. Our clustering method also outperforms the well-known k-means clustering algorithm according to clustering quality measures.",,978-1-4799-0393-1,10.1109/ECDC.2013.6556740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6556740,e-commerce;clustering;classification;term correlation graph;topic word set,Correlation;Clustering algorithms;Classification algorithms;Clustering methods;Indexing;Organizations;Recommender systems,classification;customer relationship management;electronic commerce;graph theory;Internet;multi-agent systems;pattern clustering;set theory;support vector machines;text analysis,topic word set-based text clustering;related data grouping task;similar data grouping task;unstructured textual data;e-commerce tasks;recommender systems;customer relationship management systems;personal assistant agents;term correlation graph;topic word set extraction;document categorization;SVM;support vector machine;offline processing time reduction;k-means clustering algorithm;classification algorithm;World Wide Web,,2,,16,,15-Jul-13,,,IEEE,IEEE Conferences
Experimental study on fuzzy word memberships for multi-document summarization,LIMTopic：將基於鏈接的重要性納入主題建模的框架,W. Tjhi; L. Chen,"Div of Information Engineeering, School of EEE, Nanyang Technological University, Singapore; Div of Information Engineeering, School of EEE, Nanyang Technological University, Singapore","2007 6th International Conference on Information, Communications & Signal Processing",12-Feb-08,2007,,,1,5,"Fuzzy co-clustering (FCC) is a technique that performs simultaneous fuzzy clustering of objects and features. Recently, several FCC algorithms have been proposed to handle clustering of high-dimensional datasets. The success of these FCC efforts is obvious as it results in both document and word clusters with fuzzy memberships. This paper reports our efforts made on multi-document summarization (MDS) using fuzzy co- clustering approach. The word-memberships are utilized in the MDS, which appear a good alternative interpretation to a document cluster comparing with the conventional frequency- based approaches. We explain the key differences between a summarizer based on memberships approach against the conventional approach and closely investigate on why in principle the fuzzy co-clustering approach has the high potential to outperform the frequency based approaches for MDS. Experiential study on benchmark dataset DUC 2004 shows very promising results, which encourages the further research in the area.",,978-1-4244-0982-2,10.1109/ICICS.2007.4449876,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449876,fuzzy co-clustering;fuzzy memberships;multi-document summarization,FCC;Clustering algorithms;Engines;Frequency shift keying;Data mining;Fuzzy sets;Information technology;Web sites;World Wide Web;Weight measurement,document handling;fuzzy set theory;pattern clustering,fuzzy word membership;multidocument summarization;fuzzy coclustering,,,,11,,12-Feb-08,,,IEEE,IEEE Conferences
A Three-Stage Clustering Framework Based on Multiple Feature Combination for Chinese Person Name Disambiguation,基於HDP和SOM神經網絡的Web服務聚類,F. Wang; Y. Yang; Z. Ma; L. Li,"Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China; Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China; Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China; Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China",2013 International Conference on Information Science and Cloud Computing Companion,4-Dec-14,2013,,,103,109,"To solve name ambiguity problems and improve the performance of person name disambiguation, we propose a three-stage clustering algorithm in the paper. In the first stage, organizations and locations (OLs) are used to cluster documents about the same person, therefore some texts with more resemblance will be assigned to one category. This stage is simply document clustering based on the similarity of OLs. In the second stage, the clustered documents are used as a new data source from which some novel features (like co-author names) are extracted. We used these new extracted features to make additional clustering between documents. Meanwhile, a method was proposed to solve name ambiguity problems by using social networks construction based on the relationships among co-authors. In the third stage, Web pages are further clustered using content-based hierarchical agglomerative clustering (HAC) algorithm, then analyzing the useful content including title and abstract and keywords (TAKs) to disambiguate the ambiguous names. Experimental results show that our three-stage clustering algorithm can availably enhance the performance of person name disambiguation.",,978-1-4799-5245-8,10.1109/ISCC-C.2013.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973577,person name disambiguation;social networks;hierarchical agglomerative clustering,Feature extraction;Clustering algorithms;Organizations;Social network services;Vectors;Educational institutions;Abstracts,natural language processing;pattern clustering;social networking (online);text analysis,three-stage clustering framework;multiple feature combination;Chinese person name disambiguation performance enhancement;name ambiguity problems;organization-and-location;document clustering;OL similarity;data source;co-author names;feature extraction;social network construction;co-author relationships;Web page clustering;content-based hierarchical agglomerative clustering algorithm;content-based HAC algorithm;useful content analyzing;title-and-abstract-and-keywords;TAK;ambiguous name disambiguation,,,,14,,4-Dec-14,,,IEEE,IEEE Conferences
Text-Line Detection in Camera-Captured Document Images Using the State Estimation of Connected Components,使用SIFT的無分段單詞發現,H. I. Koo,"Department of Electrical and Computer Engineering, Ajou University, Suwon, South Korea",IEEE Transactions on Image Processing,26-Sep-16,2016,25,11,5358,5368,"Camera-based text processing has attracted considerable attention and numerous methods have been proposed. However, most of these methods have focused on the scene text detection problem and relatively little work has been performed on camera-captured document images. In this paper, we present a text-line detection algorithm for camera-captured document images, which is an essential step toward document understanding. In particular, our method is developed by incorporating state estimation (an extension of scale selection) into a connected component (CC)-based framework. To be precise, we extract CCs with the maximally stable extremal region algorithm and estimate the scales and orientations of CCs from their projection profiles. Since this state estimation facilitates a merging process (bottom-up clustering) and provides a stopping criterion, our method is able to handle arbitrarily oriented text-lines and works robustly for a range of scales. Finally, a text-line/non-text-line classifier is trained and non-text candidates (e.g., background clutters) are filtered out with the classifier. Experimental results show that the proposed method outperforms conventional methods on a standard dataset and works well for a new challenging dataset.",1941-0042,,10.1109/TIP.2016.2607418,"Basic Science Research Program through the National Research Foundation of Korea, Ministry of Science, ICT and Future Planning; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563454,Document image processing;scene text detection;text-line detection;text-line segmentation,Image segmentation;State estimation;Detection algorithms;Cost function;Clutter;Text processing;Clustering algorithms,cameras;document image processing;image classification;merging;pattern clustering;state estimation;text analysis;text detection,text-line detection;camera-captured document images;connected component state estimation;camera-based text processing;scene text detection problem;document understanding;connected component-based framework;maximally stable extremal region algorithm;merging process;arbitrarily oriented text-lines;nontext-line classifier;nontext candidates,,10,,46,,8-Sep-16,,,IEEE,IEEE Journals
A Fuzzy C-Means Based Approach Towards Efficient Document Image Binarization,使用維基百科知識的基於主題的語義聚類,P. Jana; S. Ghosh; R. Sarkar; M. Nasipuri,"Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India",2017 Ninth International Conference on Advances in Pattern Recognition (ICAPR),30-Dec-18,2017,,,1,6,"Many traditional binarization techniques fail to overcome the challenging impediments fostered by degraded historical handwritten document images. In this paper, we present a fast and competent, yet simple binarization technique that uses a Fuzzy C-Means based global thresholding approach, aided by background separation. The proposed method uses a superset of foreground regions to correctly assess background of the document image. Background is estimated based on a sliding interpolation window of variable dimension, judged by appraising the nature of text stroke. Ultimately a global approach is undertaken to binarize the background-separated normalized and enhanced image by clustering the pixels using Fuzzy C-Means. This helps considering indeterministic nature of each pixel and the bland nature of the normalized image. The proposed technique is applied on the most recent (2016) benchmarking dataset of Handwritten counterpart of Document Image Binarization Contest (H-DIBCO). In order to substantiate its competence and accuracy, the experimental results are compared with the top-three winning techniques in the contest and other well-known techniques, in terms of an ensemble of parameters. It is observed that the proposed technique outperforms the best competing techniques in almost all the measuring parameters.",,978-1-5386-2241-4,10.1109/ICAPR.2017.8592936,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8592936,Document image binarization;background estimation;fuzzy c-means;image normalization;H-DIBCO,Estimation;Image edge detection;Microsoft Windows;Interpolation;Histograms;Lighting;Clustering algorithms,document image processing;estimation theory;fuzzy set theory;handwritten character recognition;interpolation;pattern clustering,simple binarization technique;global thresholding approach;background separation;foreground regions;sliding interpolation window;text stroke;document image binarization;handwritten document images;image enhancement;fuzzy C-means based approach;background estimation,,,,22,,30-Dec-18,,,IEEE,IEEE Conferences
Mesh Cluster Based Routing Protocol: Enhancing Multi-hop Internet Access using Cluster paradigm,調查XML群集中的語義測度,R. O. Schoeneich; M. Golanski,"Institute of Telecommunications, Warsaw University of Technology, Warsaw, Poland, e-mail: rschoeneich@tele.pw.edu.pl; Institute of Telecommunications, Warsaw University of Technology, Warsaw, Poland, e-mail: mgolanski@tele.pw.edu.pl","EUROCON 2007 - The International Conference on Computer as a Tool""""",26-Dec-07,2007,,,962,965,"The purpose of this document is to describe an approach to using the clustering paradigm in a routing protocol for multi-hop ad-hoc wireless networks with Internet access. In the mesh cluster based routing protocol (MCBRP) wireless nodes are grouped into clusters with a cluster-head responsible for communication management. Moreover an existence of special cluster-heads called hard-cluster-heads has been assumed. Hard-cluster-heads have the functionality of an interface between wireless and other types of network, usually the Internet. This paper describes each node type: cluster-member, cluster-soft-head, and cluster-hard-head. Furthermore assumptions, used solutions and discussion have been presented.",,978-1-4244-0812-2,10.1109/EURCON.2007.4400318,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4400318,Wireless LAN;MANET;mesh networks,Routing protocols;Internet;IP networks;Spread spectrum communication;Wireless LAN;Proposals;Wireless mesh networks;Nominations and elections;Clustering algorithms;Computer networks,ad hoc networks;Internet;routing protocols;telecommunication network management;workstation clusters,mesh cluster based routing protocol;multihop Internet access;multihop ad-hoc wireless networks;communication management;hard-cluster-heads,,2,,12,,26-Dec-07,,,IEEE,IEEE Conferences
Weighted Feature Subset Non-negative Matrix Factorization and Its Applications to Document Understanding,一種高效且可擴展的按結構對XML文檔進行聚類的算法,D. Wang; T. Li; C. Ding,"Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA; Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA; Dept. of Comput. Sci. & Eng., Univ. of Texas at Arlington, Arlington, TX, USA",2010 IEEE International Conference on Data Mining,20-Jan-11,2010,,,541,550,"Keyword (Feature) selection enhances and improves many Information Retrieval (IR) tasks such as document categorization, automatic topic discovery, etc. The problem of keyword selection is usually solved using supervised algorithms. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together. The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. The proposed approach is further extended to a weighted version in which each document is also assigned a weight to assess its importance in the cluster. This work considers both theoretical and empirical weighted feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering. We apply our proposed approaches to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks.",2374-8486,978-1-4244-9131-5,10.1109/ICDM.2010.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694008,Non-negative matrix factorization;feature selection;weighted feature subset non-negative matrix factorization,Clustering algorithms;Semantics;Hidden Markov models;Optimization;Data visualization;Computer science;Large scale integration,document handling;information retrieval;matrix decomposition;pattern clustering;unsupervised learning,information retrieval;document clustering;nonnegative matrix factorization;unsupervised feature selection;data clustering;keyword selection;weighted feature subset selection,,14,,43,,20-Jan-11,,,IEEE,IEEE Conferences
Aspect based multi-document summarization,一種基於句子相似度的軟聚類的改進的Web信息匯總方法,D. Sahoo; R. Balabantaray; M. Phukon; S. Saikia,"Dept. of Computer Science & Engineering, IIIT-Bhubaneswar, Odisha, India; Dept. Of Computer Science& Engineering, IIIT Bhubaneswar, Odisha, India; Department of IT, GUIST, Gauhati University, Assam, India; Department of IT, GUIST, Gauhati University, Assam, India","2016 International Conference on Computing, Communication and Automation (ICCCA)",16-Jan-17,2016,,,873,877,"Multi-document summarization is useful when a user deals with a group of heterogeneous documents and wants to compile the important information present in the collection, or there is a group of homogeneous documents, taken out from a large corpus as a result of a query. We present an approach to automatic multi-document summarization that depends on clustering and sentence extraction. User provides a query, based on the query; documents that are relevant to the query are extracted from a document corpus containing documents from various domains. An n ? n similarity matrix is created among the sentences having sentence level similarity in all extracted documents. Then clusters of similar sentences are formed using Markov clustering algorithm. In each cluster, each sentence is assigned five different weights 1. Chronological weight of sentence (Document level) 2. Position weight of sentence (position of sentence in the document) 3. Sentence weight (based on term weight) 4. Aspect based weight (sentence containing aspect words) and 5. Synonymy and Hyponym Weight. Then top ranked sentences having highest weight are extracted from each cluster and presented to user.",,978-1-5090-1666-2,10.1109/CCAA.2016.7813838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813838,Summarization;Clusteri;Term Weigt;Positional Weigh;Chronological Weight;Aspect Weight,Automation;Markov processes;Feature extraction;Computer science;Clustering algorithms;Semantics;History,Markov processes;matrix algebra;pattern clustering;query processing;text analysis,multidocument summarization;text extraction;sentence extraction;query extraction;n?n similarity matrix;Markov clustering algorithm;sentence weight;synonymy weight;hyponym weight,,3,,20,,16-Jan-17,,,IEEE,IEEE Conferences
A fast tree-based search algorithm for cluster search engine,使用元數據進行文本挖掘的改進聚類技術,C. Tsai; K. Huango; M. Chiang; C. Yang,"Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer and Communication Engineering, National Cheng Kung University, Tainan, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan, R.O.C.","2009 IEEE International Conference on Systems, Man and Cybernetics",4-Dec-09,2009,,,1603,1608,"In this paper, we present an Intelligent Cluster Search Engine System, called ICSE. This system is motivated by the observation that traditional search engines present to the users a set of non-classified web pages based on its ranking mechanism, and the unfortunate results are that they usually can not satisfy the need of users. For this reason, ICSE provides to the user a set taxonomic web pages in response to a user's query, and thus it would greatly help the users filter out irrelevant or redundant information. The proposed system can be divided into two parts. The first is the knowledge base constructed by Open Directory Project and Yahoo! Directory. The second is the fast clustering algorithm described herein for clustering the web pages. In addition, in response to a user's query, the proposed system will first send the query to a meta-search engine. Then, it will create a clustered document set using the given knowledge base and the clustering algorithm of ICSE. Our simulation result showed that the proposed system can enhance the relevance and coverage of the search results that the users need compared with traditional search engines.",1062-922X,978-1-4244-2793-2,10.1109/ICSMC.2009.5346100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346100,clustering search engine;meta-search engine;document clustering,Clustering algorithms;Search engines;Internet;Web pages;Information filtering;Information filters;Metasearch;Humans;Animals;Cybernetics,knowledge based systems;pattern clustering;query processing;search engines;tree searching,tree-based search algorithm;intelligent cluster search engine system;set taxonomic Web pages;knowledge base;Open Directory Project;Yahoo! Directory;fast clustering algorithm;meta-search engine;query,,3,,23,,4-Dec-09,,,IEEE,IEEE Conferences
A Concept Based Indexing Approach for Document Clustering,基於新後綴樹的中文上下文搜索結果聚類,S. Barresi; S. Nefti; Y. Rezgui,"Univ. of Salford, Salford; Univ. of Salford, Salford; Univ. of Salford, Salford",2008 IEEE International Conference on Semantic Computing,12-Aug-08,2008,,,26,33,"The research presented in this paper focuses on the pre-processing stage of the clustering process, proposing a novel indexing technique which goes beyond the syntax of terms; trying to capture their unambiguous meaning from their context and to derive a set of concepts to be used to represent the documents. This approach overcomes some of the major drawbacks deriving from the use of bag of words and term frequency based indexing techniques. The proposed approach is evaluated by using unsupervised performance measures and by comparing the clustering results achieved against the ones obtained when using a traditional indexing method. The experimental results show that better clustering results are achieved through the use of the proposed indexing approach, which also led to a substantial reduction of the index term dimension.",,978-0-7695-3279-0,10.1109/ICSC.2008.75,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597170,Concept Indexing;Clustering,Indexing;Clustering algorithms;Abstracts;Classification algorithms;Partitioning algorithms;Organizations;Algorithm design and analysis,classification;document handling;indexing;vocabulary,concept based indexing approach;document clustering;bag of words technique;term frequency based indexing technique;unsupervised classification method,,1,,27,,12-Aug-08,,,IEEE,IEEE Conferences
Kernel spectral document clustering using unsupervised precision-recall metrics,使用句子排序條件概率挖掘Web文檔的概念規則,R. Mall; J. A. K. Suykens,"KU Leuven, ESAT-STADIUS, Kasteelpark Arenberg 10, B-3001, Belgium; KU Leuven, ESAT-STADIUS, Kasteelpark Arenberg 10, B-3001, Belgium",2015 International Joint Conference on Neural Networks (IJCNN),1-Oct-15,2015,,,1,7,"Kernel Spectral Clustering (KSC) solves a weighted kernel principal component analysis problem in a primal-dual optimization framework. The KSC model is built on a small subset of data using a proper training, model selection and a test phase. The clustering model is obtained using the dual solution of the problem and has a powerful out-of-sample extensions property which allows cluster affiliation for previously unseen data points. In the model selection phase, we estimate the appropriate number of clusters using a metric that evaluates the quality of the clusters. Traditional quality indices like inertia, Davies-Bouldin (DB) index and silhouette (SIL) are known to be method-dependent and not perform well in case of complex heterogeneous data like textual data. In this paper, we utilize the quality evaluation techniques based on an unsupervised version of Precision, Recall and F-measure proposed in [1] to come up with a new kernel spectral document clustering (KSDC) model which generates homogeneous clusters of documents. We compare the quality of the clusters obtained by the proposed KSDC technique with k-means and neural gas algorithm, which are more oriented towards these metrics, on several real world textual data.",2161-4407,978-1-4799-1960-4,10.1109/IJCNN.2015.7280654,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280654,,Measurement;Decoding,document handling;optimisation;pattern clustering;principal component analysis,unsupervised precision-recall metrics;weighted kernel principal component analysis problem;primal-dual optimization framework;KSC model;model selection phase;Davies-Bouldin index;DB index;silhouette;SIL;quality evaluation technique;kernel spectral document clustering model;KSDC model;k-means algorithm;neural gas algorithm,,1,,30,,1-Oct-15,,,IEEE,IEEE Conferences
A Hybrid Information Retrieval for Indonesian Translation of Quran by Using Single Pass Clustering Algorithm,機器學習趨勢分析-文本挖掘和文檔聚類方法,Z. Indra; A. Adnan; R. Salambue,"Abdurrab University,Department of Informatics Engineering,Pekanbaru,Indonesia; Riau University,Department of Mathematics,Pekanbaru,Indonesia; Riau University,Department of Computer Science,Pekanbaru,Indonesia",2019 Fourth International Conference on Informatics and Computing (ICIC),10-Feb-20,2019,,,1,5,"The holy Quran as a scripture and the source of law must be believed by more than one billion Muslim in the world. To be a good Muslim, someone must be able to understand the contents of the Quran and underlie all of their activity to the guidance that is in the Quran. However, the holy Quran is scripture that is very long document and derived in Arabic. Thus it can cause ordinary people those who do not understand Arabic find it increasingly difficult to find specific topics to learn the contents content of the Qur'an. This research aims to develop a web-based verse search system for Al-Qur'an. This paper proposed Quran information retrieval software which is integrated with clustering algorithm to ease Muslim to discover the relevant information in Quran verse by grouping the Quran verse into their own similar group. This proposed hybrid Quran information retrieval is composed of two main parts namely Quran indexing and Quran information searching.",,978-1-7281-2207-6,10.1109/ICIC47613.2019.8985737,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985737,Quran;information retrieval;search engine;data clustering;single pass clustering,Clustering algorithms;Search engines;Indexing;Mathematical model;Labeling;Semantics,information retrieval;Internet;language translation;natural language processing;pattern clustering;text analysis,holy Quran;scripture;Arabic;Web-based verse search system;good Muslim;single pass clustering algorithm;indonesian translation;hybrid information retrieval;Quran information searching;hybrid Quran information retrieval;Quran verse;Quran information retrieval software,,,,10,,10-Feb-20,,,IEEE,IEEE Conferences
Conceptual summarization using ontologies and nearest neighborhood clustering,網頁分類和聚類的遺傳算法：可變大小頁面表示方法,E. Gavagsaz; M. Naghibzadeh; M. Jalali,"Department of Software Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran; Department of Software Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran; Department of Software Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran",2011 International Conference on Semantic Technology and Information Retrieval,22-Aug-11,2011,,,1,6,"Conceptual summarization aims to provide a database which comprises an abstraction of the entire document content. To effectively provide conceptual summarization, we have presented an approach that is used for conceptual querying. The approach is based on utilizing an ontology for similarity measure between concepts and the nearest neighborhood clustering algorithm for concepts clustering. The results show an improvement in the runtime and tolerant as regards noise.",2166-0700,978-1-61284-353-7,10.1109/STAIR.2011.5995756,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995756,conceptual summarization;ontology;nearest neighborhood clustering,Ontologies;Clustering algorithms;Clustering methods;Noise;Classification algorithms;Semantics;Software algorithms,document handling;ontologies (artificial intelligence);pattern clustering;query processing,document content;conceptual summarization;conceptual querying;ontology;nearest neighborhood clustering;similarity measure,,,,8,,22-Aug-11,,,IEEE,IEEE Conferences
Handwritten text line extraction based on minimum spanning tree clustering,基於短語語義相似度直方圖的增量聚類算法,Fei Yin; Cheng-Lin Liu,"National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, P.O. Box 2728, Beijing 100080, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, P.O. Box 2728, Beijing 100080, China",2007 International Conference on Wavelet Analysis and Pattern Recognition,7-Jan-08,2007,3,,1123,1128,"Text line extraction from unconstrained handwritten documents is a challenge because the text lines are often skewed and curved and the space between lines is not obvious. To solve this problem, we propose an approach based on minimum spanning tree (MST) clustering with new distance measures. First, the connected components of the document image are grouped into a tree by MST clustering with a new distance measure. The edges of the tree are then dynamically cut to form text lines by using a new objective function for finding the number of clusters. This approach is totally parameter-free and can apply to various documents with multi-skewed and curved lines. Experiments on handwritten Chinese documents demonstrate the effectiveness of the approach.",2158-5709,978-1-4244-1065-1,10.1109/ICWAPR.2007.4421601,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4421601,OCR;Handwritten text line extraction;Connected component labeling;MST clustering;Multi-skewed document,Pattern recognition;Optical character recognition software;Text analysis;Wavelet analysis;Pattern analysis;Character recognition;Performance analysis;Strips;Pixel,document image processing;feature extraction;handwritten character recognition;pattern clustering;trees (mathematics),handwritten text line extraction;minimum spanning tree clustering;unconstrained handwritten document image,,6,,20,,7-Jan-08,,,IEEE,IEEE Conferences
Binarization of Color Characters in Scene Images Using k-means Clustering and Support Vector Machines,基於K均值算法的基於多目標的文本聚類技術,K. Kita; T. Wakahara,"Fac. of Comput. & Inf. Sci., Hosei Univ., Koganei, Japan; Fac. of Comput. & Inf. Sci., Hosei Univ., Koganei, Japan",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,3183,3186,This paper proposes a new technique for binalizing multicolored characters subject to heavy degradations. The key ideas are threefold. The first is generation of tentatively binarized images via every dichotomization of k clusters obtained by k-means clustering in the HSI color space. The total number of tentatively binarized images equals 2k-2. The second is use of support vector machines (SVM) to determine whether and to what degree each tentatively binarized image represents a character or non-character. We feed the SVM with mesh and weighted direction code histogram features to output the degree of ?character-likeness.??The third is selection of a single binarized image with the maximum degree of ?character likeness??as an optimal binarization result. Experiments using a total of 1000 single-character color images extracted from the ICDAR 2003 robust OCR dataset show that the proposed method achieves a correct binarization rate of 93.7%.,1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.779,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597180,figure-ground discrimination;binarization of multicolored characters;k-means clustering;support vector machines,Support vector machines;Image color analysis;Pixel;Feature extraction;Training data;Character recognition;Histograms,document image processing;image colour analysis;pattern clustering;support vector machines;text analysis,color characters binarization;scene images;k-means clustering;support vector machines;multicolored characters,,28,,10,,7-Oct-10,,,IEEE,IEEE Conferences
A Cluster Center Initialization Method using Hyperspace-based Multi-level Thresholding (HMLT): Application to Color Ancient Document Image Denoising,RankTopic：基於排名的主題建模,W. Elhedda; M. Mehri; M. A. Mahjoub,"Universit矇 de Sousse, Ecole Nationale d'Ing矇nieurs de Sousse, LATIS-Laboratory of Advanced Technology and Intelligent Systems,Sousse,Tunisie;,4023; Universit矇 de Sousse, Ecole Nationale d'Ing矇nieurs de Sousse, LATIS-Laboratory of Advanced Technology and Intelligent Systems,Sousse,Tunisie;,4023; Universit矇 de Sousse, Ecole Nationale d'Ing矇nieurs de Sousse, LATIS-Laboratory of Advanced Technology and Intelligent Systems,Sousse,Tunisie;,4023",2020 21st International Arab Conference on Information Technology (ACIT),4-Jan-21,2020,,,1,6,"Many iterative supervised clustering algorithms such as K-means and its derivatives depend closely on the initial cluster center positions. In order to overcome the convergence problems inherent in the clustering algorithms (i.e., local optimum), and subsequently to avoid a drop in clustering performance, many researchers continue to propose novel efficient methods able to determine automatically the optimal cluster centers. Therefore, in this paper, we propose a simple and efficient cluster center initialization method, called hyperspace-based multi-level thresholding (HMLT). The proposed HMLT method is based on using a novel multi-level thresholding approach on the multi-dimensional representation of color images (called hyperspace). In order to show the high performance of the HMLT method, experiments have been conducted using a recent clustering method, called the hyperkernel-based intuitionistic fuzzy c-means (HKIFCM), and after initializing the cluster center positions randomly and by means of the HMLT method. The HKIFCM clustering method that its performance tightly depends on the cluster center initialization, is applied for color ancient document image denoising (i.e., separate noise from text and background). Qualitative and quantitative assessments of results are deduced from a number of ancient document images collected from two different datasets.",,978-1-7281-8855-3,10.1109/ACIT50332.2020.9300075,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9300075,Ancient color document images;denoising;hyperspace;multi-level thresholding;cluster center initialization,Image color analysis;Color;Clustering algorithms;Clustering methods;Quaternions;Mathematical model;Iterative algorithms,,,,,,24,,4-Jan-21,,,IEEE,IEEE Conferences
Recent trends in Data Mining (DM): Document Clustering of DM Publications,數據挖掘（DM）的最新趨勢：DM出版物的文檔聚類,Y. Peng; G. Kou; Z. Chen; Y. Shi,"Peter Kiewit Institute ofInformation Science, Technology & Engineering, University of Nebraska, Omaha, NE 68182, USA; Peter Kiewit Institute ofInformation Science, Technology & Engineering, University of Nebraska, Omaha, NE 68182, USA. Tel: ++1 402 6393893; Peter Kiewit Institute ofInformation Science, Technology & Engineering, University of Nebraska, Omaha, NE 68182, USA; Chinese Academy of Sciences Research Center on Data Technology & Knowledge Economy, Graduate University ofthe Chinese Academy of Sciences, Beijing 100080, China",2006 International Conference on Service Systems and Service Management,26-Feb-07,2006,2,,1653,1659,"Data mining (DM) brings knowledge and theories from several fields including databases, machine learning, optimization, statistics, and data visualization and has been applied to various real-life applications. A large amount of data mining articles have been published. The goal of this study is to establish an overview of the past and current data mining research activities from the title and abstract more than 1400 textual documents collected from premier data mining journals and conference proceedings. Specifically, this study applied document clustering approaches to determine which subjects had been studied over the last several years, which subjects are currently popular, and describe the longitudinal changes of data mining publications",2161-1904,1-4244-0450-9,10.1109/ICSSSM.2006.320794,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114740,Document Clustering;Data mining field;Content analysis;Categorization,Delta modulation;Data mining;Machine learning;Conference proceedings;IEL;Data engineering;Visual databases;Statistics;Data visualization;Software libraries,data mining;electronic publishing;pattern clustering;text analysis,data mining publication;data mining journal;document clustering;text categorization,,7,,27,,26-Feb-07,,,IEEE,IEEE Conferences
"Using of cloud computing, clustering and document-oriented database for enterprise content management",使用雲計算，集群和麵向文檔的數據庫進行企業內容管理,J. Rats; G. Ernestsons,"Rix Technologies Riga, Latvia; Clusterpoint Riga, Latvia",2013 Second International Conference on Informatics & Applications (ICIA),31-Oct-13,2013,,,72,76,"The paper deals with NoSQL Document-oriented database technology and its implementation in Enterprise Content Management area. The results of performance tests of the SQL and NoSQL solutions and suggestions on the conceptual architecture of the ECM system based on NoSQL Document-oriented database are provided. Using of cloud computing, clustering, data ranking and other Big Data related technologies is discussed.",,978-1-4673-5256-7,10.1109/ICoIA.2013.6650232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650232,Big Content;Big Data;cloud computing;clustering;data ranking;document-oriented databases;ECM;NoSQL,Databases;Electronic countermeasures;Information management;Data handling;Data storage systems;Internet;Content management,business data processing;cloud computing;content management;pattern clustering;SQL,SQL;Big Data related technologies;data ranking;ECM system;conceptual architecture;NoSQL document-oriented database technology;enterprise content management;clustering;cloud computing,,,,20,,31-Oct-13,,,IEEE,IEEE Conferences
Image annotation with semi-supervised clustering,具有半監督聚類的圖像註釋,A. Sayar; F. T. Yarman-Vural,"Bilgisayar M羹hendisli聶i B繹l羹m羹, Orta Do聶u Teknik ?niversitesi, Turkey; Bilgisayar M羹hendisli聶i B繹l羹m羹, Orta Do聶u Teknik ?niversitesi, Turkey","2008 IEEE 16th Signal Processing, Communication and Applications Conference",26-Sep-08,2008,,,1,4,"Methods developed for image annotation usually make use of region clustering algorithms. Visual codebooks generated for region clusters, using low level features are matched with words in various ways. In this work, we ensured that clustering is more meaningful by using words in associated text in addition to image data in clustering of image regions to generate a codebook. We first compute topic probabilities of text documents associated with each image in the training set. Next, we eliminate low probability topics and use highly probable ones in the supervision of region clustering algorithm. To implement this supervision, we force our region clustering algorithm to assign each region to one of the clusters reserved for high probability topics of the associated text. Consequently, regions in generated clusters not only become visually closer, but also the probability of them to belong to the same topic increases. Experiment results show that image annotation with semi-supervised clustering is more successful compared to existing methods. To implement the algorithm parallel computation methods have been used.",2165-0608,978-1-4244-1998-2,10.1109/SIU.2008.4632665,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4632665,,Clustering algorithms;Analytical models;Indexing;Data models;Computational modeling;Probabilistic logic;Artificial neural networks,image classification;pattern clustering;text analysis,image annotation;semisupervised clustering;visual codebooks;image data;topic probabilities;text documents;high probability topics;associated text;parallel computation,,1,,6,,26-Sep-08,,,IEEE,IEEE Conferences
Novelty-based Incremental Document Clustering for On-line Documents,基於新穎性的在線文檔增量文檔聚類,S. Khy; Y. Ishikawa; H. Kitagawa,"University of Tsukuba, Japan; NA; NA",22nd International Conference on Data Engineering Workshops (ICDEW'06),24-Apr-06,2006,,,40,40,"Document clustering has been used as a core technique in managing vast amount of data and providing needed information. In on-line environments, generally new information gains more interests than old one. Traditional clustering focuses on grouping similar documents into clusters by treating each document with equal weight. We proposed a novelty-based incremental clustering method for on-line documents that has biases on recent documents. In the clustering method, the notion of ?novelty??is incorporated into a similarity function and a clustering method, a variant of the K-means method, is proposed. We examine the efficiency and behaviors of the method by experiments.",,0-7695-2571-7,10.1109/ICDEW.2006.100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1623835,,Clustering methods;Data engineering;Clustering algorithms;Radio broadcasting;TV broadcasting;Systems engineering and theory;Engineering management;Electronic mail;Web and internet services;Data mining,,,,3,,12,,24-Apr-06,,,IEEE,IEEE Conferences
A General Framework for Agglomerative Hierarchical Clustering Algorithms,聚集層次聚類算法的通用框架,R. J. Gil-Garcia; J. M. Badia-Contelles; A. Pons-Porrata,"Universidad de Oriente, Cuba; NA; NA",18th International Conference on Pattern Recognition (ICPR'06),18-Sep-06,2006,2,,569,572,"This paper presents a general framework for agglomerative hierarchical clustering based on graphs. Different hierarchical agglomerative clustering algorithms can be obtained from this framework, by specifying an inter-cluster similarity measure, a subgraph of the 13-similarity graph, and a cover routine. We also describe two methods obtained from this framework called hierarchical compact algorithm and hierarchical star algorithm. These algorithms have been evaluated using standard document collections. The experimental results show that our methods are faster and obtain smaller hierarchies than traditional hierarchical algorithms while achieving a similar clustering quality",1051-4651,0-7695-2521-0,10.1109/ICPR.2006.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699269,,Clustering algorithms;Pattern recognition;Data mining;Data visualization;Taxonomy,graph theory;pattern clustering,agglomerative hierarchical clustering algorithms;inter-cluster similarity measure specification;subgraph;similarity graph;hierarchical compact algorithm;hierarchical star algorithm;standard document collections;hierarchical algorithms,,19,,7,,18-Sep-06,,,IEEE,IEEE Conferences
An adaptive neural network approach to hypertext clustering,自適應神經網絡的超文本聚類方法,N. Vlajic; H. C. Card,"Dept. of Electr. & Comput. Eng., Manitoba Univ., Winnipeg, Man., Canada; NA",IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),6-Aug-02,1999,6,,3722,3726 vol.6,"The WWW is an online hypertextual collection, and a more sophisticated algorithm for Web page clustering may have to be based on combined term-similarity and hyperlink-similarity measures. It has been observed that nearly all currently employed techniques for document classification on the Web make use of textual information only. In addition, most of these techniques are incapable of discovering the real nature of the collection to which they are applied due to rather inefficient clustering algorithms employed. This paper describes a novel technique for hypertext clustering, called an adaptive hypertext clustering (AHC) algorithm. This algorithm has been derived from a modified neural network algorithm, and adjusted to the problem of combined term-similarity and hyperlink-similarity measures. The results presented in the paper show that AHC can be easily adapted to enable the most appropriate Web page classification within collections of various thematic and functional profiles, suggesting its main benefits over the traditional techniques.",1098-7576,0-7803-5529-6,10.1109/IJCNN.1999.830743,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=830743,,Adaptive systems;Neural networks;Clustering algorithms;Artificial neural networks;Books;Web pages;Libraries;Resonance;Internet;Technological innovation,hypermedia;information resources;indexing;self-organising feature maps;pattern clustering,adaptive neural network;hypertext clustering;WWW;online hypertextual collection;Web page clustering;term-similarity measures;hyperlink-similarity measures;document classification;term-similarity;hyperlink-similarity;Web page classification;thematic profiles;functional profiles;World Wide Web,,2,,4,,6-Aug-02,,,IEEE,IEEE Conferences
Information theoretic clustering of sparse cooccurrence data,稀疏共現數據的信息理論聚類,I. S. Dhillon; Y. Guan,"Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA",Third IEEE International Conference on Data Mining,19-Dec-03,2003,,,517,520,"A novel approach to clustering cooccurrence data poses it as an optimization problem in information theory which minimizes the resulting loss in mutual information. A divisive clustering algorithm that monotonically reduces this loss function was recently proposed. We show that sparse high-dimensional data presents special challenges which can result in the algorithm getting stuck at poor local minima. We propose two solutions to this problem: (a) a ""prior"" to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and (b) local search to escape local minima. Finally, we combine these solutions to get a robust algorithm that is computationally efficient. We present experimental results to show that the proposed method is effective in clustering document collections and outperform previous information-theoretic clustering approaches.",,0-7695-1978-4,10.1109/ICDM.2003.1250966,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1250966,,Clustering algorithms;Character generation;Random variables;Information theory;Mutual information;Entropy;Probability distribution;Robustness;Unsupervised learning;Loss measurement,information theory;Bayes methods;optimisation;pattern clustering;learning (artificial intelligence),information theory;divisive clustering algorithm;sparse high-dimensional cooccurrence data;local minima;supervised Naive Bayes algorithm;document clustering,,12,,9,,19-Dec-03,,,IEEE,IEEE Conferences
PDF-TREX: An Approach for Recognizing and Extracting Tables from PDF Documents,PDF-TREX：一種從PDF文檔中識別和提取表格的方法,E. Oro; M. Ruffolo,"Dept. of Comput. & Syst. Sci., Univ. of Calabria, Rende, Italy; High Performance Comput. & Networking Inst., Italian Nat. Res. Council, Rende, Italy",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,906,910,"This paper presents PDF-TREX, an heuristic approach for table recognition and extraction from PDF documents.The heuristics starts from an initial set of basic content elements and aligns and groups them, in bottom-up way by considering only their spatial features, in order to identify tabular arrangements of information. The scope of the approach is to recognize tables contained in PDF documents as a 2-dimensional grid on a Cartesian plane and extract them as a set of cells equipped by 2-dimensional coordinates. Experiments, carried out on a dataset composed of tables contained in documents coming from different domains, shows that the approach is well performing in recognizing table cells.The approach aims at improving PDF document annotation and information extraction by providing an output that can be further processed for understanding table and document contents.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277546,Table Recognition and Extraction;Hierarchical Clustering;Document Analysis;Information Extraction,Data mining;Text analysis;High performance computing;Councils;Humans;Encoding;HTML;Visualization;Layout;XML,data visualisation;document image processing;image recognition,PDF-TREX approach;PDF documents;table recognition and extraction;2D Cartesian plane grid,,27,,10,,2-Oct-09,,,IEEE,IEEE Conferences
Cluster Labeling for the Blogosphere,Blogosphere的群集標籤,P. Hennig; P. Berger; C. Steuer; C. Wuerz; C. Meinel,"Hasso-Plattner-Inst., Univ. of Potsdam, Potsdam, Germany; Hasso-Plattner-Inst., Univ. of Potsdam, Potsdam, Germany; Hasso-Plattner-Inst., Univ. of Potsdam, Potsdam, Germany; Hasso-Plattner-Inst., Univ. of Potsdam, Potsdam, Germany; Hasso-Plattner-Inst., Univ. of Potsdam, Potsdam, Germany",2014 IEEE Fourth International Conference on Big Data and Cloud Computing,9-Feb-15,2014,,,416,423,"Hierarchical Cluster Labeling helps users to quickly understand and analyze hierarchical clusters. This may be used to enhance search engine results or interactive browsing like it is being used in the Blog Intelligence application. The hierarchical organization of data helps to represent different levels of detail. Hierarchical clustering may be quite common, but there are few good solutions for labeling those clusters. We decided to lay the focus of this work on labeling binary hierarchical clusters. Current approaches focus either on statistical features of the clustered documents or external sources like Wikipedia. We combined those ideas to profit from both advantages and created an algorithm, that can handle clustered documents as well as terms.",,978-1-4799-6719-3,10.1109/BDCloud.2014.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034824,Clustering;Cluster Labeling;Text Mining;Blogosphere;Social Network,Clustering algorithms;Blogs;Labeling;Encyclopedias;Electronic publishing;Internet,pattern clustering;search engines;statistical analysis;Web sites,Blogosphere;hierarchical cluster labeling;search engine;Blog Intelligence;hierarchical organization;statistical features;clustered documents;Wikipedia,,,,24,,9-Feb-15,,,IEEE,IEEE Conferences
The Use of Clustering Algorithms Ensemble with Variable Distance Metrics in Solving Problems of Web Mining,可變距離度量的聚類算法在解決Web挖掘問題中的應用,P. V. Bochkaryov; A. I. Guseva,"MEPhI, Nat. Res. Nucl. Univ., Moscow, Russia; MEPhI, Nat. Res. Nucl. Univ., Moscow, Russia",2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),20-Nov-17,2017,,,41,46,The article focuses on the results of the research into scientific publications of the All-Russian Institute for Scientific and Technical Information of the Russian Academy of Sciences database (VINITI Database RAS) in different fields. The purpose of operation was to increase partition accuracy on the directions of large volumes of scientific data. This analysis was carried out on summaries of scientific publications by methods of cluster analysis of text documents. The authors have used clustering algorithms ensemble with variable distance metrics as the cluster algorithm. The proposed algorithm increases the accuracy of scientific publications analysis. The developed algorithm was tested on the basis of documents VINITI Database RAS in five scientific directions. The authors acknowledge support from the MEPhI Academic Excellence Project (Contract No. 02.a03.21.0005).,,978-1-5386-3281-9,10.1109/FiCloudW.2017.82,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113768,data mining;content analisys;clustering;clustering algorithms ensemble;metrics;k-means;k-median,Clustering algorithms;Partitioning algorithms;Measurement;Algorithm design and analysis;Physics;Psychology;Chemistry,data mining;Internet;natural sciences computing;pattern clustering;text analysis,scientific publications analysis;variable distance metrics;scientific data;cluster analysis;cluster algorithm;clustering algorithms;Web mining;VINITI Database RAS;Russian Academy of Sciences;All-Russian Institute for Scientific and Technical Information;text documents;MEPhI Academic Excellence Project,,,,13,,20-Nov-17,,,IEEE,IEEE Conferences
Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation,使用模糊邏輯來利用HTML標記進行網頁表示,A. P. Garc穩a-Plaza; V. Fresno; R. M. Unanue; A. Zubiaga,"NLP&IR Group, National University of Distance Education, Madrid, Spain; NLP&IR Group, National University of Distance Education, Madrid, Spain; NLP&IR Group, National University of Distance Education, Madrid, Spain; Department of Computer Science, University of Warwick, Coventry, U.K.",IEEE Transactions on Fuzzy Systems,3-Aug-17,2017,25,4,919,933,"The selection of a suitable document representation approach plays a crucial role in the performance of a document clustering task. Being able to pick out representative words within a document can lead to substantial improvements in document clustering. In the case of web documents, the HTML markup that defines the layout of the content provides additional structural information that can be further exploited to identify representative words. In this paper, we introduce a fuzzy term weighing approach that makes the most of the HTML structure for document clustering. We set forth and build on the hypothesis that a good representation can take advantage of how humans skim through documents to extract the most representative words. The authors of web pages make use of HTML tags to convey the most important message of a web page through page elements that attract the readers' attention, such as page titles or emphasized elements. We define a set of criteria to exploit the information provided by these page elements, and introduce a fuzzy combination of these criteria that we evaluate within the context of a web page clustering task. Our proposed approach, called abstract fuzzy combination of criteria (AFCC), can adapt to datasets whose features are distributed differently, achieving good results compared with other similar fuzzy logic based approaches and TF-IDF across different datasets.",1941-0034,,10.1109/TFUZZ.2016.2586971,Spanish Ministry of Science and Innovation; PHEME FP7; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505655,Document representation;fuzzy systems;term weighting function;web page clustering,Web pages;HTML;Fuzzy logic;Internet;Encyclopedias;Electronic publishing,document handling;fuzzy logic;hypermedia markup languages;pattern clustering;Web sites,fuzzy logic;HTML markup;Web page representation;document representation;document clustering;representative words;Web documents;fuzzy term weighing approach;HTML structure;HTML tags;page elements;abstract fuzzy combination of criteria;AFCC;datasets,,,,42,,7-Jul-16,,,IEEE,IEEE Journals
Comparing the Effectiveness of Query-Document Clusterings Using the QDSM and Cosine Similarity,使用QDSM和余弦相似度比較查詢文檔聚類的有效性,C. Guti矇rrez-Soto; A. C. D穩az; G. Hubert,"Universidad del B穩o-B穩o,Departamento de Sistemas de Informaci籀n),Concepci籀n,Chile; Universidad Veracruzana,Facultad de Estad穩stica e Inform獺tica,Veracruz,M矇xico; IRIT UMR 5505 CNRS, Universit矇 de Toulouse,Toulouse,Francia",2019 38th International Conference of the Chilean Computer Science Society (SCCC),23-Jan-20,2019,,,1,8,"Typically, approaches based on query clustering in IR only take the keywords that belong to the queries into consideration, with the aim of calculating the similarity among them. In search engines, one of the factors that affects precision is the short lengths of queries. One way to tackle this problem is through the use of the context. Aiming to consider the context of queries, several measures have been proposed to calculate their inter-document similarity, which are known as Query-Sensitive Similarity Measure (QSSM). This paper presents a new QSSM called Query-Document Similarity Measure (QDSM), which is used to build clusters of similar search results. The goal of this measure is to provide clusters of better quality (i.e., clusters with more relevant documents) taking into account the context in which the queries are submitted. To achieve this goal, several experiments were carried out using four well-known algorithms in Information Retrieval (IR) along with the AOL Query Logs Dataset (AOL). Final results using Bisection K-Means and Average link show that the clusters built with QDSM provide better effectiveness than the clusters built with Cosine Similarity (Sc).",1522-4902,978-1-7281-5613-2,10.1109/SCCC49216.2019.8966432,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966432,Information retrieval;clustering;query-sensitive similarity,Search engines;Clustering algorithms;Automobiles;Engines;Support vector machines;Probabilistic logic,pattern clustering;query processing;search engines;text analysis,query-sensitive similarity measure;query-document similarity measure;QDSM;cosine similarity;inter-document similarity;AOL query logs dataset;query document clustering;search engines;information retrieval,,,,36,,23-Jan-20,,,IEEE,IEEE Conferences
A PROV-O based approach to web content provenance,基於PROV-O的Web內容來源方法,Ni Jing,"Economic Management Department, Beijing Institute of Petrochemical Technology, 102617, China","2015 International Conference on Logistics, Informatics and Service Sciences (LISS)",4-Jan-16,2015,,,1,6,"Data provenance is currently a hot issue, and many webpages still lack provenance annotation. PROV-O is an emerging W3C recommendation for a provenance data model and language. In this paper, through the analysis of web document derivation, we define a document as an entity and extract a number of semantic properties about document features. A semantic similarity clustering method is used to determine the relationship during the changes of documents. Feature words variation and the responsible person can be found with the aid of PROV-O. Then, taking ?genetically modified??news Webpages as test documents, we verify the proposed approach.",,978-1-4799-1891-1,10.1109/LISS.2015.7369688,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7369688,semantic web;data provenance;linked data;automatic discovery;web content similarity,Semantics;Metadata;Vocabulary;Ontologies;Feature extraction;Dictionaries;Clustering algorithms,document handling;pattern clustering;semantic Web,PROV-O based approach;Web content provenance;data provenance;W3C recommendation;Web document derivation analysis;document features;semantic similarity clustering method;feature words variation;news Web pages,,,,11,,4-Jan-16,,,IEEE,IEEE Conferences
Graph-Based Technique for Extracting Keyphrases in a Single-Document (GTEK),基於圖的技術在單文檔（GTEK）中提取關鍵短語,M. R. Alfarra; A. Alfarra,"Comput. Sci. & Inf. Technol. Dept., Univ. Coll. of Sci. & Technol., Khan Younis, Palestinian Authority; Comput. Sci. & Inf. Technol. Dept., Univ. Coll. of Sci. & Technol., Khan Younis, Palestinian Authority",2018 International Conference on Promising Electronic Technologies (ICPET),11-Nov-18,2018,,,92,97,"In this paper, a novel Graph-based Technique for Extracting Keyphrases in a single document (GTEK) is introduced to be used in extractive summarization of text. GTEK is based on the graph-based representation of text, which depends on terms and phrase numeration in sentences rather than some structural document features. GTEK considers the impact of the sentence on the phrases in a document, motivated by the fact that a phrase may be important if it appears in the most important sentences in the document. The Graph-based Growing Self-Organizing Map (G-GSOM) is used to group the sentences into graph-based clusters. TextRank algorithm is applied on graphs of clusters under the assumption that the top-ranked nodes should represent the most important sentences, where the most frequent phrases in these sentences are selected as document keyphrases. Experimental results show that our innovative technique extracts the most keyphrases of two datasets.",,978-1-5386-5697-6,10.1109/ICPET.2018.00023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531236,Keyphrase-extraction;graph-based;clustering;single-document-summarization,Clustering algorithms;Feature extraction;Data mining;Task analysis;Semantics;Indexes;Computer science,graph theory;self-organising feature maps;text analysis,single-document;GTEK;phrase numeration;structural document features;graph-based clusters;document keyphrases;Graph-based Technique for Extracting Keyphrases;Graph-based Growing Self-Organizing Map;G-GSOM;extractive text summarization;graph-based text representation;TextRank algorithm,,2,,19,,11-Nov-18,,,IEEE,IEEE Conferences
Web-Service Clustering with a Hybrid of Ontology Learning and Information-Retrieval-Based Term Similarity,結合了本體學習和基於信息檢索的術語相似性的Web服務集群,B. T. G. S. Kumara; I. Paik; W. Chen,"Sch. of Comput. Sci. & Eng., Univ. of Aizu, Aizu-Wakamatsu, Japan; Sch. of Comput. Sci. & Eng., Univ. of Aizu, Aizu-Wakamatsu, Japan; Sch. of Comput. Sci. & Eng., Univ. of Aizu, Aizu-Wakamatsu, Japan",2013 IEEE 20th International Conference on Web Services,31-Oct-13,2013,,,340,347,"Organizing Web services into functionally similar clusters, is an efficient approach to discovering Web services efficiently. An important aspect of the clustering process is calculating the semantic similarity of Web services. Most current clustering approaches are based on similarity-distance measurement, including keyword, ontology and information-retrieval-based methods. Problems with these approaches include a shortage of high quality ontologies and a loss of semantic information. In addition, there has been little fine-grained improvement in existing approaches to service clustering. In this paper, we present a new approach to grouping Web services into functionally similar clusters by mining Web service documents and generating an ontology via hidden semantic patterns present within the complex terms used in service features to measure similarity. If calculating the similarity using the generated ontology fails, the similarity is calculated by using an information-retrieval-based term-similarity method that adopts term-similarity measuring techniques used by thesaurus and search engines. Another important aspect of high performance in clustering is identifying the most suitable cluster center. To improve the utility of clusters, we propose an approach to identifying the cluster center that combines service similarity with the term frequency-inverse document frequency values of service names. Experimental results show that our clustering approach performs better than existing approaches.",,978-0-7695-5025-1,10.1109/ICWS.2013.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649597,Web Service Clustering;Ontology Learning,Ontologies;Semantics;Feature extraction;Web services;Clustering algorithms;Educational institutions;Matched filters,data mining;information retrieval;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;search engines;text analysis;thesauri;Web services,Web service clustering;ontology learning;information retrieval-based term similarity;similarity distance measurement;keyword;Web service document mining;hidden semantic pattern;service feature;semantic similarity measure;complex term;thesaurus;search engine;cluster center identification;term frequency inverse document frequency value;service name,,14,,18,,31-Oct-13,,,IEEE,IEEE Conferences
WE-LDA: A Word Embeddings Augmented LDA Model for Web Services Clustering,WE-LDA：用於Web服務群集的Word嵌入增強型LDA模型,M. Shi; J. Liu; D. Zhou; M. Tang; B. Cao,"Sch. of CoPputer Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of CoPputer Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of CoPputer Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of CoPputer Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of CoPputer Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China",2017 IEEE International Conference on Web Services (ICWS),11-Sep-17,2017,,,9,16,"Due to the rapid growth in both the number and diversity of Web services on the web, it becomes increasingly difficult for us to find the desired and appropriate Web services nowadays. Clustering Web services according to their functionalities becomes an efficient way to facilitate the Web services discovery as well as the services management. Existing methods for Web services clustering mostly focus on utilizing directly key features from WSDL documents, e.g., input/output parameters and keywords from description text. Probabilistic topic model Latent Dirichlet Allocation (LDA) is also adopted, which extracts latent topic features of WSDL documents to represent Web services, to improve the accuracy of Web services clustering. However, the power of the basic LDA model for clustering is limited to some extent. Some auxiliary features can be exploited to enhance the ability of LDA. Since the word vectors obtained by Word2vec is with higher quality than those obtained by LDA model, we propose, in this paper, an augmented LDA model (named WE-LDA) which leverages the high-quality word vectors to improve the performance of Web services clustering. In WE-LDA, the word vectors obtained by Word2vec are clustered into word clusters by K-means++ algorithm and these word clusters are incorporated to semi-supervise the LDA training process, which can elicit better distributed representations of Web services. A comprehensive experiment is conducted to validate the performance of the proposed method based on a ground truth dataset crawled from ProgrammableWeb. Compared with the state-of-the-art, our approach has an average improvement of 5.3% of the clustering accuracy with various metrics.",,978-1-5386-0752-7,10.1109/ICWS.2017.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029739,Web services;clustering;Word2vec;LDA;K-means++,Feature extraction;Clustering algorithms;Semantics;Probabilistic logic;Mashups;Tools,pattern clustering;text analysis;Web services,Word embeddings augmented LDA model;Web services clustering;appropriate Web services;Web services discovery;services management;word vectors;Word2vec;word clusters;WSDL documents;WSDL documents;latent Dirichlet allocation;k-means++ algorithm;Programmable Web,,13,,29,,11-Sep-17,,,IEEE,IEEE Conferences
Clustering sentence level-text using fuzzy hierarchical algorithm,使用模糊層次算法對句子層次文本進行聚類,G. K. Priya; G. Anupriya,"Dr. Mahalingam College of Engineering and Technology, Pollachi, Tamil Nadu, India; Dr. Mahalingam College of Engineering and Technology, Pollachi, Tamil Nadu, India",2013 International Conference on Human Computer Interactions (ICHCI),1-Sep-14,2013,,,1,8,"Clustering is a popular technique for unsupervised text analysis, often used to explore the content of large amounts of sentences. It is performed based on the similarity of sentences. Sentences may contain interrelated concepts and implementing flat clustering algorithms allows one sentence to be present only in one cluster. Also, sentences are semantically related to each other and so word co-occurrence is not a valid measure for sentence level flat clustering. So, WordNet based semantic similarity measure along with fuzzy sentence clustering algorithm is proposed. The existing system uses the Fuzzy C-Means algorithm where the cluster size should be specified as an input. Due to the rigorous convergence criteria, the time complexity is much larger. Most of the NLP documents are hierarchical in nature and so fuzzy hierarchical sentence clustering algorithm is used here. Labeling is performed for each cluster depending on the hierarchy formed and instead of considering all the terms in a sentence, only the verbs and nouns are considered for the similarity computation. Agglomerative clustering based on the verbs and divisive clustering based on nouns is proposed. This methodology is validated through various performance measures like Purity, Entropy and Time. Upon comparing the results for various datasets, it was observed that the overall improvement in purity is 36.6% and entropy is 31%. The time complexity of the hierarchical algorithm is very much less than the EM algorithm. Thus better quality clusters are formed in comparatively less time by using the Fuzzy Hierarchical Sentence Clustering Algorithm.",,978-1-4673-5703-6,10.1109/ICHCI-IEEE.2013.6887778,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6887778,Natural Language Processing(NLP);Fuzzy C-Means(FCM) Clustering;WordNet Similarity;Agglomerative and Divisive Clustering,Clustering algorithms;Semantics;Algorithm design and analysis;Time complexity;Convergence;Natural languages;Speech,computational complexity;fuzzy set theory;natural language processing;pattern clustering;text analysis,sentence level-text clustering;fuzzy hierarchical sentence clustering algorithm;unsupervised text analysis;WordNet based semantic similarity measure;fuzzy c-means algorithm;time complexity;NLP documents;natural language processing,,,,18,,1-Sep-14,,,IEEE,IEEE Conferences
Topic based Summarization of Multiple Documents using Semantic Analysis and Clustering,使用語義分析和聚類的多文檔基於主題的摘要,R. Hafeez; S. Khan; M. A. Abbas; F. Maqbool,"School of Electrical Engineering and Computer Sciences, National University of Sciences and Technology Islamabad, Pakistan; School of Electrical Engineering and Computer Sciences, National University of Sciences and Technology Islamabad, Pakistan; University Institute of Information Technology, PMAS-Arid Agriculture University Rawalpindi, Pakistan; School of Electrical Engineering and Computer Sciences, National University of Sciences and Technology Islamabad, Pakistan",2018 15th International Conference on Smart Cities: Improving Quality of Life Using ICT & IoT (HONET-ICT),29-Nov-18,2018,,,70,74,"Document summarization addresses the problem of presenting the information in a compact form to the readers. Different approaches to summarize documents have been proposed and evaluated in literature. Common research problems in multi-document summarization are Redundancy and Extraction of sentences; that are important and semantically linked with other sentences. With the combination of agglomerative hierarchical clustering and Latent Semantic Analysis (LSA); which measures semantic similarity between sentences and reduces dimensions by preserving only highly weighted vectors, we propose a novel multi document summarization approach. Latent Dirichlet Allocation Model is used to identify important topic terms in the resultant summary. We have used Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric to evaluate our system against other state-of-the art techniques using Document Understanding Conference (DUC) dataset 2004. Experimental results show that there is substantial performance improvement using our system and it makes better summary as compared to the other state-of-art techniques.",1949-4106,978-1-5386-8354-5,10.1109/HONET.2018.8551325,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8551325,,Semantics;Task analysis;Clustering algorithms;Redundancy;Weight measurement;Resource management,pattern clustering;sentiment analysis,topic based summarization;compact form;agglomerative hierarchical clustering;semantic similarity;multidocument summarization approach;Latent Dirichlet Allocation Model;important topic terms;Document Understanding Conference dataset 2004;document summarization;sentences extraction;latent semantic analysis;redundancy;LSA;ROUGE;recall oriented understudy for gisting evaluation metric,,,,20,,29-Nov-18,,,IEEE,IEEE Conferences
Robust Document Image Authentication,強大的文檔圖像認證,M. Jiang; E. K. Wong; N. Memon,"Dept. of Computer and Information Science, Polytechnic University, Brooklyn, New York 11201; Dept. of Computer and Information Science, Polytechnic University, Brooklyn, New York 11201. wong@poly.edu; Dept. of Computer and Information Science, Polytechnic University, Brooklyn, New York 11201",2007 IEEE International Conference on Multimedia and Expo,8-Aug-07,2007,,,1131,1134,"In this paper, we propose a novel method for robust document image authentication. In the proposed method, characters and symbols on a binary document are first grouped into different classes based on k-means clustering in the feature space. Labels are then assigned to the different classes. An ordered sequence of labels formed from the characters and symbols on the document is then used to compute a digital signature. This is achieved by using a cryptographic hash function and a secret key. The computed signature can be appended at the end of an electronic document file, or printed on a hardcopy document in the form of a bar code. Using this method, we will be able to detect any intentional content alteration to the document, but at the same time tolerate moderate amount of noise introduced by printing, scanning, and photocopying of the document. Experimental results demonstrate the effectiveness of the proposed approach.",1945-788X,1-4244-1016-9,10.1109/ICME.2007.4284854,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4284854,,Robustness;Authentication;Watermarking;Digital signatures;Printing;Pixel;Image converters;Information science;Cryptography;Instruments,bar codes;cryptography;digital signatures;document image processing;image sequences,robust document image authentication;k-means clustering;label sequence;digital signature;cryptographic hash function;secret key;electronic document file;bar code,,1,,13,,8-Aug-07,,,IEEE,IEEE Conferences
An Efficient Concept-Based Mining Model for Enhancing Text Clustering,基於概念的高效挖掘模型，用於增強文本聚類,S. Shehata; F. Karray; M. Kamel,"University of Waterloo, Waterloo; University of Waterloo, Waterloo; University of Waterloo, Waterloo",IEEE Transactions on Knowledge and Data Engineering,19-Aug-10,2010,22,10,1360,1371,"Most of the common techniques in text mining are based on the statistical analysis of a term, either word or phrase. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying text mining model should indicate terms that capture the semantics of text. In this case, the mining model can capture terms that present the concepts of the sentence, which leads to discovery of the topic of the document. A new concept-based mining model that analyzes terms on the sentence, document, and corpus levels is introduced. The concept-based mining model can effectively discriminate between nonimportant terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed mining model consists of sentence-based concept analysis, document-based concept analysis, corpus-based concept-analysis, and concept-based similarity measure. The term which contributes to the sentence semantics is analyzed on the sentence, document, and corpus levels rather than the traditional analysis of the document only. The proposed model can efficiently find significant matching concepts between documents, according to the semantics of their sentences. The similarity between documents is calculated based on a new concept-based similarity measure. The proposed similarity measure takes full advantage of using the concept analysis measures on the sentence, document, and corpus levels in calculating the similarity between documents. Large sets of experiments using the proposed concept-based mining model on different data sets in text clustering are conducted. The experiments demonstrate extensive comparison between the concept-based analysis and the traditional analysis. Experimental results demonstrate the substantial enhancement of the clustering quality using the sentence-based, document-based, corpus-based, and combined approach concept analysis.",1558-2191,,10.1109/TKDE.2009.174,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184843,Concept-based mining model;sentence-based;document-based;corpus-based;concept analysis;conceptual term frequency;concept-based similarity.,Data mining;Frequency;Text mining;Statistical analysis;Clustering methods;Artificial intelligence;Clustering algorithms;Text analysis;Humans;History,data mining;pattern clustering;statistical analysis;text analysis;vocabulary;word processing,concept based mining model;text clustering enhancement;statistical analysis;text mining model;text semantics;sentence based concept analysis;document based concept analysis;corpus based concept analysis;concept based similarity measure;sentence semantics,,54,,30,,31-Jul-09,,,IEEE,IEEE Journals
Document Clustering Using Multi-Objective Genetic Algorithms on MATLAB Distributed Computing,基於MATLAB分佈式計算的多目標遺傳算法的文檔聚類,J. S. Lee; S. C. Park,"Div. of Electron. & Inf. Eng., Jeonbuk Nat. Univ., Jeonbuk, South Korea; Div. of Electron. & Inf. Eng., Jeonbuk Nat. Univ., Jeonbuk, South Korea",2012 International Conference on Information Science and Applications,21-Jun-12,2012,,,1,6,"Genetic Algorithm (GA), one of the artificial intelligence algorithms, performs much better than the other algorithms for the document clustering. However, it has problem known as the premature convergence occurrence. So, Fuzzy Logic based GA (FLGA) was proposed to solve it. Nevertheless, it has still weakness such as the parameter dependence problem. In order to overcome this problem, the Multi-Objective Genetic Algorithms (MOGAs), NSGA-II and SPEA2, have been proposed. The MOGAs showed high performance compared to other algorithms including the general GA, but their computational times have increased. In order to reduce these computational times, the distributed computing method, MATLAB Distributed Computing (MDC) with 10 processors, is applied to the MOGAs in this paper. The performances of MOGAs on MDC show about 12% higher than others.",2162-9048,978-1-4673-1401-5,10.1109/ICISA.2012.6220980,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6220980,,Genetic algorithms;Clustering algorithms;Indexes;Distributed computing;Application software;Arrays;Biological cells,distributed processing;document handling;fuzzy logic;genetic algorithms;mathematics computing;pattern clustering,document clustering;multiobjective genetic algorithms;MATLAB distributed computing;artificial intelligence algorithms;premature convergence;fuzzy logic based GA;FLGA;parameter dependence problem;MOGA;MDC,,,,20,,21-Jun-12,,,IEEE,IEEE Conferences
Tweets clustering: Adaptive PSO,Tweets聚類：自適應PSO,K. S. Gaikwad; M. S. Patwardhan,"M.Tech (Computer Department), VIT, Pune, India; (Computer Department), VIT, Pune, India",2014 Annual IEEE India Conference (INDICON),5-Feb-15,2014,,,1,6,"In the today's world, huge amount of online data is required for various analysis purposes. It is difficult to store, manage and retrieve such a massive data efficiently, especially when the data is continuously getting appended during run-time. This generates the need to organize such a data in similar groups in a more dynamic fashion. Not being adaptive in nature, traditional algorithms like K-means fail to accommodate the newly arrived data during run-time, without re-initialization. In this approach, we have carried out clustering of streaming tweets using adaptive particle swarm optimization (APSO) algorithm. The algorithm is adaptive, because it can accommodate the streaming data effectively and efficiently, without having to go for re-initialization. Unlike previous approaches, we have initialized the particles, only at the commencement of the algorithm, in such a way that they are well-distributed and thus cover the complete problem space leading to the algorithm not getting stuck at the local optimum. We have also devised a mutation-like operation, which at the arrival of new tweets; re-initialize only a subset of the converged particles away from the convergence point. This accommodates the latest data effectively, again covering the complete problem space. With this approach, we have achieved a trade-off between the cluster quality and the execution time.",2325-9418,978-1-4799-5364-6,10.1109/INDICON.2014.7030584,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030584,Dynamic Document Clustering;Tweets;Swam Intelligence;Adaptive Algorithm;Particle Swarm Optimization,Clustering algorithms;Vectors;Heuristic algorithms;Algorithm design and analysis;Convergence;Particle swarm optimization;Atmospheric measurements,document handling;particle swarm optimisation;pattern clustering;social networking (online),adaptive PSO;online data;data storage;data management;data retrieval;data organization;streaming tweet data clustering;adaptive particle swarm optimization algorithm;APSO algorithm;particle initialization;local optimum;mutation-like operation;convergence point;problem space;cluster quality;execution time,,2,,14,,5-Feb-15,,,IEEE,IEEE Conferences
A comparative study for Arabic Multi-Document Summarization Systems (AMD-SS),阿拉伯文多文檔摘要係統（AMD-SS）的比較研究,M. N. Ibrahim; K. A. Maria; K. M. Jaber,"Faculty of Science and Information Technology, Al-Zaytoonah University of Jordan, Amman, Jordan; Faculty of Science and Information Technology, Al-Zaytoonah University of Jordan, Amman, Jordan; Faculty of Science and Information Technology, Al-Zaytoonah University of Jordan, Amman, Jordan",2017 8th International Conference on Information Technology (ICIT),23-Oct-17,2017,,,1013,1022,"This paper demonstrates a comparative study of Arabic Multi-Document Summarization System (AMD-SS). These methods are compared and analyzed, aiming to detect which method generates a genuine summary and achieves the best results in comparison with the human summarization techniques. The comparative study shows that there is a lack in the area of Arabic Automatic Text Summarization systems. Therefore, we proposed an Arabic Text Summarization model that built in linear algorithms based on parallel computing techniques. The proposed model is built in order to generate an Arabic Document Summary (ADS) that is fully coherent, grammatical and meaningful Arabic sentences, closing to a human summarization. Recent researches have not provided perfect Arabic summary.",,978-1-5090-6332-1,10.1109/ICITECH.2017.8079984,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079984,Multi-Document Summarization (MDS);Clustering;Graph;Cross Document Structure Theory (CST);Features Selection (FS);Human Summarization,Feature extraction;Semantics;Information technology;Computational modeling;Clustering algorithms;Redundancy;Parallel processing,document handling;natural language processing;parallel algorithms;text analysis,Arabic MultiDocument Summarization systems;AMD-SS;human summarization techniques;Arabic Automatic Text Summarization systems;Arabic Document Summary;grammatical sentences;meaningful Arabic sentences;perfect Arabic summary;human summarization;ADS;parallel computing techniques;linear algorithms,,,,87,,23-Oct-17,,,IEEE,IEEE Conferences
Technology matching of the patent documents using clustering algorithms,使用聚類算法對專利文件進行技術匹配,M. Dra鱉i?; D. Kukolj; M. Vitas; M. Pokri?; S. Manojlovi?; Z. Teki?,"RT-RK Institute for Computer Based Systems, Novi Sad, Serbia; University of Novi Sad/Faculty of Technical Science, Novi Sad, Serbia; RT-RK Institute for Computer Based Systems, Novi Sad, Serbia; University of Novi Sad/Faculty of Technical Science, Novi Sad, Serbia; University of Novi Sad/Faculty of Sciences, Novi Sad, Serbia; RT-RK Institute for Computer Based Systems, Novi Sad, Serbia",2013 IEEE 14th International Symposium on Computational Intelligence and Informatics (CINTI),9-Jan-14,2013,,,405,409,"This paper analyzes the accuracy of different clustering algorithms to handle different parts of the patent documents. The algorithms are part of the software package which is used as a tool for business intelligence purposes. The tool assembles patent data from publicly available data bases, collects and analyzes patents bibliographic parameters and performs text mining. Performances of clustering algorithms: k-means, the neural-gas; fuzzy c-means and ronn algorithm are examined when run on different parts of the patent document, such as abstract, claim, international patent code description and detailed patent description, but applied on the same patent data set. Patent data set was previously classified in technology groups by the experts and obtained results are compared with the purpose of selection of the most suitable algorithm and patent document part.",,978-1-4799-0197-5,10.1109/CINTI.2013.6705231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705231,,Patents;Clustering algorithms;Classification algorithms;Accuracy;Technological innovation;Abstracts;Economics,data mining;patents;pattern clustering;pattern matching;text analysis,technology matching;patent documents;clustering algorithms;software package;business intelligence;patents bibliographic;text mining;k-means;neural gas;fuzzy c-means;ronn algorithm;international patent code description;patent description;patent data set;technology groups;patent document part,,1,,30,,9-Jan-14,,,IEEE,IEEE Conferences
A Study on Detecting Patterns in Twitter Intra-topic User and Message Clustering,Twitter主題內用戶檢測模式和消息聚類研究,M. Cheong; V. Lee,"Fac. of IT, Monash Univ., Clayton, VIC, Australia; Fac. of IT, Monash Univ., Clayton, VIC, Australia",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,3125,3128,"Timely detection of hidden patterns is the key for the analysis and estimating of driving determinants for mission critical decision making. This study applies Cheong and Lee's ?context-aware??content analysis framework to extract latent properties from Twitter messages (tweets). In addition, we incorporate an unsupervised Self-organizing Feature Map (SOM) as a machine learning-based clustering tool that has not been investigated in the context of opinion mining and sentimental analysis using microblogging. Our experimental results reveal the detection of interesting patterns for topics of interest which are latent and cannot be easily detected from the observed tweets without the aid of machine learning tools.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.765,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597282,Online documents;Group interaction: analysis of verbal and non-verbal communication;Pattern recognition systems and applications,Twitter;Visualization;Media;Nominations and elections;Communities;Clustering algorithms,decision making;pattern clustering;social networking (online),pattern detection;twitter intratopic user;message clustering;decision making;context-aware content analysis framework;Twitter messages;self-organizing feature map;machine learning-based clustering tool;opinion mining;sentimental analysis;microblogging,,15,,10,,7-Oct-10,,,IEEE,IEEE Conferences
Automated Thai Online Assignment Scoring,自動泰國在線作業評分,T. Thongyoo; S. Saelee; S. Krootjohn,"Department of Computer Education, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand; Department of Computer Education, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand; Department of Computer Education, Faculty of Technical Education, King Mongkut's University of Technology North Bangkok, Thailand",2016 Fifth ICT International Student Project Conference (ICT-ISPC),25-Jul-16,2016,,,33,36,"This paper presents a new approach of Thai assignment scoring called Automated Thai Online Assignment Scoring (ATOAS). The study was designed by using the principles of text mining including Document Clustering and Document Classification. The experimental results showed that the performance of the model could be as effective as the human performance with the 86.51 percentage of average accuracy and correlation coefficient at moderate level (R= .60. The research findings indicated that the proposed model could reduce the scoring times. This model also could help instructors to teach big-sized classes as well as decrease the budget to hire teacher assistants. Additionally, it could increase times for instructors to conduct their research, develop teaching materials, and create other innovative works needed in teaching and learning processes.",,978-1-5090-1125-4,10.1109/ICT-ISPC.2016.7519229,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519229,Document Clustering;Document Classification;Automated Essay Scoring;K-Means;SVM,Support vector machines,computer aided instruction;data mining;Internet;pattern classification;pattern clustering;teaching;text analysis,automated Thai online assignment scoring;ATOAS;text mining;document clustering;document classification;teaching process;e-Learning,,1,,20,,25-Jul-16,,,IEEE,IEEE Conferences
Clustering of Word Contexts as a Method of Eliminating Polysemy of Words,詞語境聚類作為消除詞多義性的一種方法,A. Kapitanov; I. Kapitanova; V. Troyanovskiy; V. Ilyushechkin; E. Dorogova,"Chair of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Chair of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Chair of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Chair of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Chair of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia",2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus),3-Mar-19,2019,,,1861,1864,"When processing text documents in natural language, you may encounter such problem as polysemy of words. If the algorithm determines the terms regardless of the semantic component of the words, considering homonymous words as one term, there is a loss of data. As the analysis of the literature data shows, at present no algorithm can unambiguously divide polysemous words into separate terms by semantic groups. In this paper, we propose a method of eliminating polysemy of words, based on the clustering of word contexts, which will improve the quality of processing text documents in natural language.",2376-6565,978-1-7281-0339-6,10.1109/EIConRus.2019.8656851,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8656851,NLP;clustering;polysemy;semantic analysis;word embeddings,Hidden Markov models;Semantics;Clustering algorithms;Natural languages;Markov processes;Dictionaries;Mathematical model,natural language processing;pattern clustering;text analysis,processing text documents;natural language;polysemy;homonymous words;polysemous words;separate terms;word contexts,,1,,4,,3-Mar-19,,,IEEE,IEEE Conferences
Search-based software module clustering techniques: A review article,基於搜索的軟件模塊集群技術：評論文章,F. Morsali; M. R. Keyvanpour,"Department of Computer Engineering, Alzahra University, Tehran, Iran; Department of Computer Engineering, Alzahra University, Tehran, Iran",2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation (KBEI),26-Mar-18,2017,,,977,983,"A software module clustering is a technique of organizing software entities into clusters to enhance the quality of a software system. It provides easier navigation and easier tracking among software parts and enhances comprehension. Therefore, a good distribution of the module facilitates the development and maintenance of a software system. The software module clustering problem is often solved using the traditional analytical methods. These methods are useful for small size of software problems. When the size of the problem grows, these methods become infeasible as they cannot solve the problem within reasonable amount of time. To overcome these difficulties, the software module clustering problem for large software systems is formulated as search-based optimization problem. In this regard, many techniques have been proposed. Since it is very important and helpful to have an overall view to all of aspects in every research field, the existence of classified documents of different techniques in every aspect of a research filed is necessary. Therefore, this paper provides a classification for analyzing each of the approaches presented in this area. Utilizing from proposed classification can be effective in analyzing and evaluation of different methods as well as deal with different challenges. Software module clustering techniques classified into three main classes: mono-objective, multi-objective, and many-objective which every class have several subclasses. All in all, software module clustering techniques classified into some classes, namely, Single-Factor Module Clustering (SFMC), Multi-Factor Module Clustering (MFMC), Single-View Software Module Clustering, Multi-View Software Module Clustering.",,978-1-5386-2640-5,10.1109/KBEI.2017.8324941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324941,Search-Based Software Engineering;Search-based Software Module clustering;Mono-objective Optimization;Multi-objective Optimization;Many-objective Optimization;Single-Factor Module Clustering (SFMC);Multi-Factor Module Clustering (MFMC),Optimization;Knowledge based systems;Technological innovation;Software engineering;Search problems;Software,evolutionary computation;optimisation;pattern clustering;search problems;software maintenance;software quality,software system;optimization problem;Software module clustering techniques;Single-Factor Module Clustering;MultiFactor Module Clustering;Single-View Software Module Clustering;MultiView Software Module Clustering;software entities;software parts;software problems;search-based software module clustering problem,,2,,35,,26-Mar-18,,,IEEE,IEEE Conferences
Text categorization of Marathi documents using modified LINGO,使用改進的LINGO對Marathi文檔進行文本分類,S. A. Narhari; R. Shedge,"Department of Computer Engineering, Ramrao Adik Institute of Technology, Mumbai, India; Department of Computer Engineering, Ramrao Adik Institute of Technology, Mumbai, India","2017 International Conference on Advances in Computing, Communication and Control (ICAC3)",19-Mar-18,2017,,,1,5,"In the 21st century we have seen tremendous growth in the users who are accessing internet. At the same time users across the universe are giving preferences to the regional languages while accessing internet. It is seen that there is a huge scope for doing categorization in regional languages, considering the fact that search engine has worked on many regional languages. Moreover, super-fast acceleration in the information technology has driven the variety of applications of text categorization. We have chosen Marathi as a regional language for automatic text categorization. Categorization defines various classification and clustering techniques and Label Induction Grouping (LINGO) is one of these algorithms used for categorization of Marathi text documents. This uses Single Value Decomposition (SVD) for dimension reduction. We are proposing a modified LINGO algorithm which is considering morphology of Marathi text and Principle Component analysis (PCA) for improving results.",,978-1-5386-3852-1,10.1109/ICAC3.2017.8318771,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8318771,Text Categorization;Dimension reduction;Morphological analysis LINGO;SVD;PCA,Text categorization;Clustering algorithms;Feature extraction;Principal component analysis;Supervised learning;Dimensionality reduction;Text mining,information retrieval;natural language processing;pattern classification;pattern clustering;principal component analysis;query processing;search engines;singular value decomposition;text analysis,search engine;clustering technique;classification technique;label induction grouping;single value decomposition;SVD;dimension reduction;principle component analysis;PCA;automatic text categorization;regional language;modified LINGO algorithm;Marathi text documents,,,,19,,19-Mar-18,,,IEEE,IEEE Conferences
Ancient document analysis based on text line extraction,基於文本行提取的古代文獻分析,F. Kleber; R. Sablatnig; M. Gau; H. Miklas,"Pattern Recognition and Image Processing Group, Institute of Computer Aided Automation, Vienna University of Technology, 1040, Austria; Pattern Recognition and Image Processing Group, Institute of Computer Aided Automation, Vienna University of Technology, 1040, Austria; Institute for Slavonic Studies, University of Vienna, 1010, Austria; Institute for Slavonic Studies, University of Vienna, 1010, Austria",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"In order to preserve our cultural heritage and for automated document processing libraries and national archives have started digitizing historical documents. In the case of degraded manuscripts (e.g. by mold, humidity, bad storage conditions) the text or parts of it can disappear. The remaining parts of the text can be segmented and the ruling can be extrapolated with the a priori knowledge. Since the ruling defines the position of the text within a page, it can be used for layout analysis and as a basis for the enhancement of the readability. Furthermore, information about the scribe (hand) of the manuscript, its spatiotemporal origin can be gained by analyzing the ruling. This paper presents an algorithm for ruling estimation of Glagolitic texts based on text line extraction and is suitable for degraded manuscripts by extrapolating the baselines with the a priori knowledge of the ruling. The algorithm was tested on 30 pages of the Missale Sinaiticum and the evaluation was based on visual criteria.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761530,,Text analysis;Degradation;Algorithm design and analysis;Image segmentation;Image analysis;Spatiotemporal phenomena;Information analysis;Data mining;Software libraries;Clustering algorithms,document handling,ancient document analysis;text line extraction;automated document processing libraries;national archives;historical documents;degraded manuscripts;Glagolitic texts;Missale Sinaiticum,,12,,20,,23-Jan-09,,,IEEE,IEEE Conferences
A Combined Edge Detection Analysis and Clustering based Approach for Real Time Text Detection,結合邊緣檢測分析和聚類的實時文本檢測方法,R. A. P. Putro; F. P. Putri; M. I. Prasetiyowati,"Universitas Multimedia Nusantara,Department of Informatics,Tangerang,Indonesia; Universitas Multimedia Nusantara,Department of Informatics,Tangerang,Indonesia; Universitas Multimedia Nusantara,Department of Informatics,Tangerang,Indonesia",2019 5th International Conference on New Media Studies (CONMEDIA),6-Feb-20,2019,,,59,62,"Recently, scene text detection has become an active research topic in computer vision and document analysis because of its great importance. However, due to the number of factors, such as variety in font size, style, colors, language, as well as background noise, blur, and occlusion, makes text detection is still challenging. The proposed methods directly run on the frame image of the recording video to detect text. Sobel operator used to perform edge detection on the image while K-means clustering used to extract text from the background of the image. After the image has been segmented based on the cluster, the text will be identified using the Maximum Stable External Region (MSER). Applications run on Android devices and built by using OpenCV for image retrieval processes, while algorithms written using C++. The experimental result shows 71.79%, 62.11%, and 64.56% for average recall, precision, and F1-score, respectively.",,978-1-7281-0726-4,10.1109/CONMEDIA46929.2019.8981811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981811,K-Means Clustering;Optical Character Recognition;Real Time Text Detection;Sobel,Clustering algorithms;Image edge detection;Real-time systems;Image segmentation;Feature extraction;Image retrieval;Streaming media,computer vision;edge detection;feature extraction;image colour analysis;image retrieval;image segmentation;pattern clustering;text analysis;text detection;video signal processing,image retrieval processes;combined edge detection analysis;time text detection;scene text detection;computer vision;document analysis;font size;background noise;frame image;recording video;sobel operator;maximum stable external region;MSER;Android devices;OpenCV;C++;F1-score,,,,11,,6-Feb-20,,,IEEE,IEEE Conferences
Robust Skew Estimation of Handwritten and Printed Documents Based on Grayvalue Images,基於灰度圖像的手寫和打印文檔的魯棒偏斜估計,F. Kleber; M. Diem; R. Sablatnig,"Inst. of Comput. Aided Autom., Vienna Univ. of Technol., Vienna, Austria; Inst. of Comput. Aided Autom., Vienna Univ. of Technol., Vienna, Austria; Inst. of Comput. Aided Autom., Vienna Univ. of Technol., Vienna, Austria",2014 22nd International Conference on Pattern Recognition,6-Dec-14,2014,,,3020,3025,"Skew estimation is a preprocessing step in document image analysis to determine the global dominant orientation of a document's text lines. A skew angle can be introduced during scanning, or if a document is photographed. The correction of the skew angle is necessary for further image analysis, to avoid an influence to the performance of skew sensitive methods, e.g. Optical Character Recognition (OCR) or page segmentation. The performance of current skew estimation methods is shown at the ICDAR2013 Document Image Skew Estimation Contest (DISEC), which uses a benchmark dataset of binarized printed documents with varying layouts and languages like English, Chinese or Greek. The proposed method is based on a Focused Nearest Neighbour Clustering (FNNC) of interest points and the analysis of paragraphs/lines and achieved rank 5 at the contest. In this paper it is shown, that the use of gray value images can outperform the results restricted to binarized images, thus the proposed method avoids the binarization step which is still an open research topic in document image analysis. The robustness of the method is also shown on a dataset comprising historical documents and on low resolution images. The method is evaluated on the DISEC dataset and three additional datasets (historical documents, low resolution documents, and machine printed documents).",1051-4651,978-1-4799-5209-0,10.1109/ICPR.2014.521,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977233,,Estimation;Image resolution;Robustness;Benchmark testing;Layout;Image edge detection;Accuracy,document image processing;image resolution;image segmentation;optical character recognition;pattern clustering;visual databases,handwritten documents;grayvalue images;global dominant orientation;skew angle;skew sensitive methods;optical character recognition;OCR;page segmentation;ICDAR2013 document image skew estimation contest;benchmark dataset;binarized printed documents;focused nearest neighbour clustering;FNNC;DISEC dataset;interest points;paragraph analysis;line analysis;historical documents;low resolution images,,3,,21,,6-Dec-14,,,IEEE,IEEE Conferences
Forum topic detection based on hierarchical clustering,基於層次聚類的論壇主題檢測,H. Li; Q. Li,"School of Computer Engineering and Science, Shanghai University, 200444, China; School of Computer Engineering and Science, Shanghai University, 200444, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)",9-Feb-17,2016,,,529,533,"Forum has become one of the main platforms for people to express their personal point of view, with a lot of information surging in the forum everyday. How to detect automatically a forum topic among the massive information becomes an important and hard task. Though there are plenty of studies for topic detection, it is still a challenge to make it fast and accurately. This paper introduces the principle of maximum entropy and information gain when calculating feature weight. Our algorithm is based on the agglomerative hierarchical clustering (AHC). Experiments are focused on a game forum and handling sparse forum short texts. The result shows that the improved method can detect the forum topic more effectively.",,978-1-5090-0654-0,10.1109/ICALIP.2016.7846583,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846583,Forum on network;topic detection;entropy;information gain,Games;Feature extraction;Entropy;Uncertainty;Clustering algorithms;Internet;Social network services,document image processing;entropy;pattern clustering;social networking (online);text detection,sparse forum short texts;game forum;AHC;agglomerative hierarchical clustering;feature weight;information gain;maximum entropy;forum topic detection,,2,,17,,9-Feb-17,,,IEEE,IEEE Conferences
An Event Detection Algorithm Based on Improved STC,基於改進STC的事件檢測算法,L. Qiu; Bin-Pang; L. Zhao,"State Key Lab. of Software Development Environment, Beihang University, 100083. qiuliqing@nlsde.buaa.edu.cn; State Key Lab. of Software Development Environment, Beihang University, 100083. pangbin@nlsde.buaa.edu.cn; State Key Lab. of Software Development Environment, Beihang University, 100083. zhaolp@nlsde.buaa.edu.cn","2008 IEEE International Conference on Networking, Sensing and Control",20-May-08,2008,,,528,532,"In order to overcome some shortcomings of traditional algorithms in event detection, we propose an event detection algorithm based on improved STC (suffix tree clustering), which detects significant events from large volumes of news and presents the main content of the events to the user as summaries. The experimental results on TDT indicate that the new algorithm is an effective document clustering algorithm.",,978-1-4244-1685-1,10.1109/ICNSC.2008.4525274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525274,,Event detection;Clustering algorithms;Merging;Distance measurement;Text mining;Unsupervised learning;Density functional theory;Mice,document handling;information resources;Internet;pattern clustering;tree data structures,news document event detection algorithm;suffix tree clustering;Web document clustering algorithm,,,,10,,20-May-08,,,IEEE,IEEE Conferences
Local Co-occurrence and Contrast Mapping for Document Image Binarization,用於文檔圖像二值化的局部同現和對比度映射,N. Mitianoudis; N. Papamarkos,"Image Process. & Multimedia Lab., Democritus Univ. of Thrace, Xanthi, Greece; Image Process. & Multimedia Lab., Democritus Univ. of Thrace, Xanthi, Greece",2014 14th International Conference on Frontiers in Handwriting Recognition,15-Dec-14,2014,,,609,614,"Document Image Binarization refers to the task of transforming a scanned image of a handwritten or printed document into a bi-level representation containing only characters and background. Here, we address the historic document image binarization problem using a three-stage methodology. Firstly, we remove possible stains and noise from the document image by estimating the document background image. The remaining background and character pixels are separated using a Local Co-occurrence Mapping, local contrast and a two-state Gaussian Mixture Model. In the last stage, possible isolated misclassified blobs are removed by a morphology operator. The proposed scheme offers robust and fast performance, especially for handwritten documents.",2167-6445,978-1-4799-4334-0,10.1109/ICFHR.2014.107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6981086,Binarization;historic documents;background estimation,Text analysis;Frequency modulation;Histograms;Clustering algorithms;Noise;Image edge detection;Estimation,document image processing;Gaussian processes;image denoising;image representation;mixture models,contrast mapping;scanned image transformation;handwritten document;printed document;bilevel representation;historic document image binarization problem;three-stage methodology;stains removal;noise removal;document background image estimation;background pixels;character pixels;local co-occurrence mapping;two-state Gaussian mixture model;misclassified blobs;morphology operator,,2,,18,,15-Dec-14,,,IEEE,IEEE Conferences
A Focus + Context Technique for Visualizing a Document Collection,用於可視化文檔集合的Focus + Context技術,D. Dunsmuir; E. Lee; C. D. Shaw; M. Stone; R. Woodbury; J. Dill,NA; NA; NA; NA; NA; NA,2012 45th Hawaii International Conference on System Sciences,9-Feb-12,2012,,,1835,1844,"Investigative analysts need overviews of large amounts of data, which is a challenge when working with non-numerical data such as document collections. We present Semantic Zoom View (SZV), an interactive document collection visualization implemented as part of the CZSaw visual analytics system. SZV uses a focus + context technique to provide an overview with details on demand through interactive semantic zooming. SZV lets an analyst easily and quickly see the main topics of a document collection while keeping surrounding documents visible for context. Working within a single integrated visualization, an analyst can also quickly find related documents and break a large document collection into smaller meaningful groups. SZV's focus + context technique was compared to an overview + detail version for finding answers within a document collection and results indicated its strength for maintaining visibility of a full overview when document contents are accessed.",1530-1605,978-0-7695-4525-7,10.1109/HICSS.2012.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149109,semantic zoom;focus + context;document collection;visual analytics,Layout;Context;Clustering algorithms;Semantics;Algorithm design and analysis;Text analysis;Data visualization,data visualisation;document handling;interactive systems,focus technique;context technique;nonnumerical data;semantic zoom view;SZV;interactive document collection visualization;CZSaw visual analytics system;interactive semantic zooming;single integrated visualization;large document collection;document contents;document collection visualization,,1,,16,,9-Feb-12,,,IEEE,IEEE Conferences
Fuzzy concept graph and application in Web document clustering,模糊概念圖及其在Web文檔聚類中的應用,Chen An; Chen Ning; Weijia Jia; Sanding Luo,"Inst. of Software, Chinese Acad. of Sci., Beijing, China; NA; NA; NA",2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479),6-Aug-02,2001,3,,101,106 vol.3,"This paper introduces the fuzzy concept graph method firstly. Based on the related definitions, it discusses its main characteristics and application in an intelligent customized search system.",,0-7803-7010-4,10.1109/ICII.2001.983043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=983043,,Clustering algorithms;Data mining;Application software;Web mining;Information retrieval;Search engines;Partitioning algorithms;Iterative algorithms;Computer science;Intelligent systems,data mining;information resources;Internet;directed graphs;fuzzy set theory;search engines;information retrieval,fuzzy concept graph;intelligent customized search system;World Wide Web;directed graph;Internet;information retrieval;data mining;Web document clustering,,,,18,,6-Aug-02,,,IEEE,IEEE Conferences
An Adaptive Page Clustering Based Weighting Method for Information Retrieval,基於自適應頁面聚類的信息檢索加權方法,Y. Lin; H. Kao,"Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan",2013 Conference on Technologies and Applications of Artificial Intelligence,7-Apr-14,2013,,,199,204,"With the coming of the era of information explosion, using Internet to obtain information has become the most convenient pipeline information flow. However, the found information mostly based on keyword matching through the search engines, and the search engines do not generally conduct filtering and screening in order to enhance the returns. If the web pages pass a systematic arrangement and are divided into multiple categories or clusters, the users will be guided to obtain real help of information. In this paper, we propose an adaptive web pages clustering algorithm to perform this task. It extracts features to reduce feature dimensions, then filters automatically web pages into its appropriate cluster and enhances the features of the pages to site features for different coefficients to improve the effect. Finally, providing users a more accurate search data model. The experimental results show that compared to the traditional TF-IDF, the proposed approach can find the needed web pages and the topics of the web pages in the corresponding cluster that are highly similar.",2376-6824,978-1-4799-2529-2,10.1109/TAAI.2013.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6783867,Page Clustering;Information Retrieval;Topic Feature;Correlation Coefficient,Web pages;Feature extraction;Clustering methods;Keyword search;Clustering algorithms;Search engines,document handling;information retrieval;Internet;pattern clustering;search engines;Web sites,adaptive page clustering based weighting method;information retrieval;information explosion;Internet;pipeline information flow;keyword matching;search engines;systematic arrangement;Web pages clustering algorithm;TF-IDF,,,,27,,7-Apr-14,,,IEEE,IEEE Conferences
Joint factor analysis and latent clustering,聯合因素分析和潛在聚類,Bo Yang; Xiao Fu; N. D. Sidiropoulos,"Dept. ECE, Univ. Minnesota, Minneapolis, 55455, USA; Dept. ECE, Univ. Minnesota, Minneapolis, 55455, USA; Dept. ECE, Univ. Minnesota, Minneapolis, 55455, USA",2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),21-Jan-16,2015,,,173,176,"Many real-life datasets exhibit structure in the form of physically meaningful clusters - e.g., news documents can be categorized as sports, politics, entertainment, and so on. Taking these clusters into account together with low-rank structure may yield parsimonious matrix and tensor factorization models and more powerful data analytics. Prior works made use of data-domain similarity to improve nonnegative matrix factorization. Here we are instead interested in joint low-rank factorization and latent-domain clustering; that is, in clustering the latent reduced-dimension representations of the observed entities. A unified algorithmic framework that can deal with both matrix and tensor factorization and latent clustering is proposed. Numerical results obtained from synthetic and real document data show that the proposed approach can significantly improve factor analysis and clustering accuracy.",,978-1-4799-1963-5,10.1109/CAMSAP.2015.7383764,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383764,,Tensile stress;Loading;Electronic mail;Clustering algorithms;Load modeling;Cost function,data analysis;document handling;matrix decomposition;pattern clustering;tensors,joint factor analysis;latent clustering;physically meaningful clusters;news documents;parsimonious matrix;tensor factorization models;data analytics;data-domain similarity;nonnegative matrix factorization;joint low-rank factorization;latent-domain clustering;latent reduced-dimension representations,,,,15,,21-Jan-16,,,IEEE,IEEE Conferences
A Rule-Based Framework of Metadata Extraction from Scientific Papers,基於規則的科學論文元數據提取框架,Z. Guo; H. Jin,"Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China","2011 10th International Symposium on Distributed Computing and Applications to Business, Engineering and Science",2-Jan-12,2011,,,400,404,"Most scientific documents on the web are unstructured or semi-structured, and the automatic document metadata extraction process becomes an important task. This paper describes a framework for automatic metadata extraction from scientific papers. Based on a spatial and visual knowledge principle, our system can extract title, authors and abstract from scientific papers. We utilize format information such as font size and position to guide the metadata extraction process. The experiment results show that our system achieves a high accuracy in header metadata extraction which can effectively assist the automatic index creation for digital libraries.",,978-1-4577-0327-0,10.1109/DCABES.2011.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118700,document metadata;information extraction;rule-based approach,Portable document format;Data mining;Semantics;Libraries;Accuracy;XML;Layout,digital libraries;document handling;indexing;information retrieval;Internet;knowledge based systems;meta data;natural sciences computing,rule-based framework;automatic document metadata extraction;scientific papers;scientific documents;Web;spatial knowledge principle;visual knowledge principle;header metadata extraction;automatic index creation;digital libraries,,4,,12,,2-Jan-12,,,IEEE,IEEE Conferences
Minimum tree edit distance between XML and Probabilistic XML documents,XML和概率XML文檔之間的最小樹編輯距離,H. Ma; C. Xu; M. Fang; C. Yu,"College of Information Science and Engineering, Northeastern University Shenyang, China 110819; College of Information Science and Engineering, Northeastern University Shenyang, China 110819; College of Information Science and Engineering, Northeastern University Shenyang, China 110819; College of Information Science and Engineering, Northeastern University Shenyang, China 110819","2014 IEEE Workshop on Electronics, Computer and Applications",30-Jun-14,2014,,,391,394,"A Probabilistic XML document is a data model representing data as a probabilistic distribution over ordinary XML documents, which are called possible worlds. This paper studies the problem of tree edit distance between an XML document and a probabilistic XML document. In particular, We define a minimum tree edit distance between an XML document and the possible worlds of a probabilistic XML document. We investigate the problem and propose an algorithm for computing the minimum tree edit distance, which runtime is polynomial in the size of the probabilistic XML document. Finally, our experimental evaluation on synthetic XML documents confirms our analytic results.",,978-1-4799-4565-8,10.1109/IWECA.2014.6845639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845639,XML documents;probabilistic XML;tree edit distance,XML;Probabilistic logic;Classification algorithms;Clustering algorithms;Computational modeling;Programming;Algorithm design and analysis,data models;probability;XML,minimum tree edit distance;probabilistic XML document;probabilistic distribution;tree edit distance;polynomial runtime;XML document,,,,9,,30-Jun-14,,,IEEE,IEEE Conferences
Diversified Clustering Methods and Algorithms in Data Mining used for Document Representation,用於文檔表示的數據挖掘中的多種聚類方法和算法,P. P. Jorvekar; R. Shardanand Prasad; J. R. Prasad,"NBN Sinhgad School of Engineering,Computer Engineering,Pune,India; Sinhgad Institute of Technology and Science,Computer Engineering,Pune,India; Sinhgad College of Engineering,Computer Engineering,Pune,India","2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)",30-Jun-20,2019,,,1,7,"In this World of Words the rate of arriving new born worlds are tremendously increase with respect to time zone, as it is obvious that it will create a lack of much better and reliable word management with us. Now to survive we possess good clusters of similar bunch of worlds and their making process which called clustering are available. In this paper will see various clustering methods, qualities of good clusters with various Algorithms used on ground of Data mining. This will help to refill the lacks in future development in this area.",,978-1-7281-4042-1,10.1109/ICCUBEA47591.2019.9129300,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9129300,pyGCluster;CURE;BIRCH;DBSCAN;K-means;OPTICS,,data mining;document handling;pattern clustering,data mining;clustering methods;document representation;clustering algorithms,,,,19,,30-Jun-20,,,IEEE,IEEE Conferences
"Sparse Subspace Clustering: Algorithm, Theory, and Applications",稀疏子空間聚類：算法，理論和應用,E. Elhamifar; R. Vidal,"Johns Hopkins University, Baltimore; Johns Hopkins University, Baltimore",IEEE Transactions on Pattern Analysis and Machine Intelligence,17-Sep-13,2013,35,11,2765,2781,"Many real-world problems deal with collections of high-dimensional data, such as images, videos, text, and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called sparse subspace clustering, to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.",1939-3539,,10.1109/TPAMI.2013.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482137,High-dimensional data;intrinsic low-dimensionality;subspaces;clustering;sparse representation;$(\ell_1)$-minimization;convex programming;spectral clustering;principal angles;motion segmentation;face clustering,Clustering algorithms;Noise;Optimization;Sparse matrices;Vectors;Computer vision;Face,computational complexity;convex programming;data structures;minimisation;pattern clustering,sparse subspace clustering algorithm;high-dimensional data collection;data point clustering;data point representation;sparse representation;sparse optimization program;spectral clustering framework;general NP-hard problem;convex relaxation;minimization program;motion segmentation;face clustering;synthetic data,"Algorithms;Artificial Intelligence;Biometry;Face;Humans;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Sample Size",1167,,68,,19-Mar-13,,,IEEE,IEEE Journals
GDClust: A Graph-Based Document Clustering Technique,GDClust：基於圖的文檔聚類技術,M. S. Hossain; R. A. Angryk,"Montana State Univ., Bozeman; Montana State Univ., Bozeman",Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007),31-Mar-08,2007,,,417,422,"This paper introduces a new technique of document clustering based on frequent senses. The proposed system, GDClust (graph-based document clustering) works with frequent senses rather than frequent keywords used in traditional text mining techniques. GDClust presents text documents as hierarchical document-graphs and utilizes an apriori paradigm to find the frequent subgraphs, which reflect frequent senses. Discovered frequent subgraphs are then utilized to generate sense-based document clusters. We propose a novel multilevel Gaussian minimum support approach for candidate subgraph generation. GDClust utilizes English language ontology to construct document-graphs and exploits graph-based data mining technique for sense discovery and clustering. It is an automated system and requires minimal human interaction for the clustering purpose.",2375-9259,978-0-7695-3019-2,10.1109/ICDMW.2007.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476701,,Data mining;Humans;Ontologies;Books;Association rules;Clustering algorithms;Chemical analysis;Chemical technology;Conferences;Computer science,data mining;Gaussian processes;graph theory;natural language processing;ontologies (artificial intelligence);pattern clustering;text analysis,graph-based document clustering technique;frequent senses;text mining techniques;apriori paradigm;frequent subgraphs;multilevel Gaussian minimum support approach;candidate subgraph generation;English language ontology;sense discovery,,27,,27,,31-Mar-08,,,IEEE,IEEE Conferences
Clustering-Guided Sparse Structural Learning for Unsupervised Feature Selection,聚類指導的稀疏結構學習的無監督特徵選擇,Z. Li; J. Liu; Y. Yang; X. Zhou; H. Lu,"School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, QLD, Australia; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Knowledge and Data Engineering,5-Aug-14,2014,26,9,2138,2150,"Many pattern analysis and data mining problems have witnessed high-dimensional data represented by a large number of features, which are often redundant and noisy. Feature selection is one main technique for dimensionality reduction that involves identifying a subset of the most useful features. In this paper, a novel unsupervised feature selection algorithm, named clustering-guided sparse structural learning (CGSSL), is proposed by integrating cluster analysis and sparse structural analysis into a joint framework and experimentally evaluated. Nonnegative spectral clustering is developed to learn more accurate cluster labels of the input samples, which guide feature selection simultaneously. Meanwhile, the cluster labels are also predicted by exploiting the hidden structure shared by different features, which can uncover feature correlations to make the results more reliable. Row-wise sparse models are leveraged to make the proposed model suitable for feature selection. To optimize the proposed formulation, we propose an efficient iterative algorithm. Finally, extensive experiments are conducted on 12 diverse benchmarks, including face data, handwritten digit data, document data, and biomedical data. The encouraging experimental results in comparison with several representative algorithms and the theoretical analysis demonstrate the efficiency and effectiveness of the proposed algorithm for feature selection.",1558-2191,,10.1109/TKDE.2013.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509368,Computing Methodologies;Pattern Recognition;Design Methodology;Feature evaluation and selection;Clustering;Information Technology and Systems;Database Management;Database Applications;classification;and association rules;Feature selection;nonnegative spectral clustering;latent structure;row-sparsity,Clustering algorithms;Algorithm design and analysis;Optimization;Integrated circuits;Prediction algorithms;Correlation;Machine learning algorithms,feature selection;iterative methods;learning (artificial intelligence);pattern clustering,clustering-guided sparse structural learning;unsupervised feature selection;CGSSL;cluster analysis;sparse structural analysis;nonnegative spectral clustering;feature selection;hidden structure;feature correlations;row-wise sparse models;iterative algorithm;face data;handwritten digit data;document data;biomedical data,,159,,46,,26-Apr-13,,,IEEE,IEEE Journals
MMMs-induced k-member co-clustering for k-anonymization of cooccurrence information,MMM誘導的k成員共聚用於共現信息的k匿名化,K. Honda; H. Sakamoto; S. Ubukata; A. Notsu,"Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531, Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531, Japan",2016 International Joint Conference on Neural Networks (IJCNN),3-Nov-16,2016,,,2961,2966,"k-anonymization is a basic technique for utilizing sensitive information in data mining without violating personal privacy, and can be efficiently achieved by a greedy k-member clustering, where each data record is coded reflecting cluster structures so that each anonymized record is indistinguishable from at least other k - 1 records. In this paper, with the goal of utilizing cooccurrence information, a k-member co-clustering-based k-anonymization approach is proposed induced by the MMMs-induced fuzzy co-clustering concept. Co-clustering is a useful method for extracting mutually familiar object-item pairwise clusters from cooccurrence information such as document-keyword frequencies in document analysis. The proposed k-member co-clustering model sequentially extracts clusters one-by-one such that the aggregation degree of each k-objects cluster is maximized from the viewpoint of MMMs-induced co-clustering. The advantage of the proposed method is demonstrated through numerical experiments.",2161-4407,978-1-5090-0620-5,10.1109/IJCNN.2016.7727574,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727574,,Data mining;Minimization,data mining;data privacy;document handling;greedy algorithms;pattern clustering,MMM-induced k-member coclustering;cooccurrence information;sensitive information;data mining;personal privacy;greedy k-member clustering;data record;cluster structures;k-member coclustering-based k-anonymization;mutually familiar object-item pairwise clusters;document-keyword frequencies;document analysis,,,,18,,3-Nov-16,,,IEEE,IEEE Conferences
Clustering and classification in structured data domains using Fuzzy Lattice Neurocomputing (FLN),使用模糊格神經計算（FLN）在結構化數據域中進行聚類和分類,V. Petridis; V. G. Kaburlasos,"Dept. of Electr. & Comput. Eng., Aristotelian Univ. of Thessaloniki, Greece; NA",IEEE Transactions on Knowledge and Data Engineering,7-Aug-02,2001,13,2,245,260,"A connectionist scheme, namely, /spl sigma/-Fuzzy Lattice Neurocomputing scheme or /spl sigma/-FLN for short, which has been introduced in the literature lately for clustering in a lattice data domain, is employed for computing clusters of directed graphs in a master-graph. New tools are presented and used, including a convenient inclusion measure function for clustering graphs. A directed graph is treated by /spl sigma/-FLN as a single datum in the mathematical lattice of subgraphs stemming from a master-graph. A series of experiments is detailed where the master-graph emanates from a thesaurus of spoken language synonyms. The words of the thesaurus are fed to /spl sigma/-FLN in order to compute clusters of semantically related words, namely hyperwords. The arithmetic parameters of /spl sigma/-FLN can be adjusted so as to calibrate the total number of hyperwords computed in a specific application. It is demonstrated how the employment of hyperwords implies a reduction, based on the a priori knowledge of semantics contained in the thesaurus, in the number of features to be used for document classification. In a series of comparative experiments for document classification, it appears that the proposed method favorably improves classification accuracy in problems involving longer documents, whereas performance deteriorates in problems involving short documents.",1558-2191,,10.1109/69.917564,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=917564,,Lattices;Thesauri;Neural networks;Employment;Data processing;Natural languages;Arithmetic;Fuzzy neural networks;Parallel processing;Function approximation,fuzzy neural nets;pattern clustering;data handling;directed graphs;thesauri;natural languages;linguistics;document handling,structured data domains;classification;clustering;Fuzzy Lattice Neurocomputing;FLN;connectionist scheme;/spl sigma/-Fuzzy Lattice Neurocomputing scheme;/spl sigma/-FLN;lattice data domain;directed graphs;master-graph;inclusion measure function;clustering graphs;mathematical lattice;subgraphs;thesaurus;spoken language synonyms;semantically related words;hyperwords;arithmetic parameters;a priori knowledge;semantics;document classification;classification accuracy;longer documents;short documents,,33,,36,,7-Aug-02,,,IEEE,IEEE Journals
A modified fuzzy ART for soft document clustering,用於軟文檔聚類的改進模糊ART,R. Kondadadi; R. Kozma,"Dept. of Math. Sci., Univ. of Memphis, TN, USA; Dept. of Math. Sci., Univ. of Memphis, TN, USA",Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),7-Aug-02,2002,3,,2545,2549 vol.3,"Document clustering is a very useful application in recent days especially with the advent of the World Wide Web. Most of the existing document clustering algorithms either produce clusters of poor quality or are highly computationally expensive. In this paper we propose a document-clustering algorithm, KMART, that uses an unsupervised fuzzy adaptive resonance theory (fuzzy-ART) neural network. A modified version of the fuzzy ART is used to enable a document to be in multiple clusters. The number of clusters is determined dynamically. Some experiments are reported to compare the efficiency and execution time of our algorithm with other document-clustering algorithm like fuzzy c-means. The results show that KMART is both effective and efficient.",1098-7576,0-7803-7278-6,10.1109/IJCNN.2002.1007544,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007544,,Subspace constraints;Clustering algorithms;Partitioning algorithms;Search engines;Iterative algorithms;Web sites;Fuzzy neural networks;Data mining;Computer science;Application software,Internet;data mining;pattern clustering;ART neural nets;fuzzy neural nets,modified fuzzy ART neural network;soft document clustering;World Wide Web;computational expense;KMART;unsupervised fuzzy adaptive resonance theory neural network;fuzzy c-means;data mining;knowledge discovery,,19,,26,,7-Aug-02,,,IEEE,IEEE Conferences
A new density based clustering algorithm for Binary Data sets,一種新的基於密度的二元數據集聚類算法,S. J. Nanda; R. Raman; S. Vijay; A. Bhardwaj,"Department of Electronics and Communication Engineering, Malaviya National Institute of Technology, Jaipur, India; School of Electronics and Communication Engineering, Shri Mata Vaishno Devi University, Katra, India; School of Electronics and Communication Engineering, Shri Mata Vaishno Devi University, Katra, India; School of Electronics and Communication Engineering, Shri Mata Vaishno Devi University, Katra, India",2014 International Conference on High Performance Computing and Applications (ICHPCA),19-Feb-15,2014,,,1,6,"Binary Data clustering finds tremendous applications in fault analysis of machineries, document classification, image retrievals and analysis, medical diagnosis of diseases etc. Accurate clustering of binary databases provides numerous help to develop accurate designs of above systems. In this manuscript a density based clustering algorithm is proposed to effectively cluster binary datasets. The proposed algorithm automatically determines the number of clusters based upon the density of data present in a region. The number of clusters evolve during the clustering process due to merging of several smaller clusters. Simulation studies were carried out on two synthetic datasets and it is observed that the proposed algorithm can effectively clusters both correlated and random binary datasets.",,978-1-4799-5958-7,10.1109/ICHPCA.2014.7045336,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045336,Density based clustering;DBSCAN;Binary Datasets;Similarity Measure,DNA,pattern clustering,density based clustering algorithm;binary data clustering;binary database clustering;data density;cluster merging;synthetic datasets;correlated binary datasets;random binary datasets,,1,,22,,19-Feb-15,,,IEEE,IEEE Conferences
Automatic category generation for text documents by self-organizing maps,通過自組織地圖自動生成文本文檔的類別,Hsin-Chang Yang; Chung-Hong Lee,"Dept. of Inf. Manage., Chang Jung Univ., Tainan, Taiwan; NA",Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,6-Aug-02,2000,3,,581,586 vol.3,"One important task for text data mining is automatic text categorization, which assigns a text document to some predefined category according to their correlations. Traditionally, these categories as well as the correlations among them are determined bp human experts. In this paper, we devised a novel approach to automatically generate categories. The self-organizing map model is used to generate two maps, namely the word cluster map and the document cluster map, in which a neuron represents a cluster of words and documents respectively. Our approach is to analyze the document cluster map to find centroids of some super-clusters. We also devised a method to select the category term from the word cluster map. The hierarchical structure of categories may be generated by recursively applying the same method. Text categorization is the natural consequence of such automatic category generation process.",1098-7576,0-7695-0619-4,10.1109/IJCNN.2000.861377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=861377,,Self organizing feature maps;Text categorization;Neurons;Data mining;Humans;Information management;Natural language processing;Indexing;Ergonomics;Clustering algorithms,Data mining;Category theory;Text analysis;Self-organising feature maps;Generalization (artificial intelligence),automatic category generation;text document;self-organizing maps;text data mining;word cluster map;document cluster map;hierarchical structure;text categorisation,,6,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Application Research on Latent Semantic Analysis for Information Retrieval,潛在語義分析在信息檢索中的應用研究,C. Wenli,"Sch. of Foreign Language & Literature, Longdong Univ., Qingyang, China",2016 Eighth International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),13-Jun-16,2016,,,118,121,"The basic principle of Classic traditional information retrieval model is the machine matching of the key word, namely retrieval based on keywords. This paper proposes a pre-clustering-based latent semantic analysis algorithm for document retrieval. The algorithm can solve the problem of time consuming computation of the similarity between the query vector and each text vector in the traditional latent semantic algorithm for document retrieval. It first clusters the documents using k-means clustering based on the latent semantic analysis, finds out the central point of each cluster, and then calculates the similarity between the query vector and each cluster's central points for retrieval. In view of the characteristics of document retrieval, it proposes a new method for calculating the feature weights and adopts the method of pre-clustering to preprocess document collection. The results of the experiment show that the new algorithm can reduce the search time, and improve the retrieval efficiency.",2157-1481,978-1-5090-2312-7,10.1109/ICMTMA.2016.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488513,Document Retrieval;k-means;Latent Semantic Analysis;Singular Value Decomposition,Matrix decomposition;Semantics;Clustering algorithms;Information retrieval;Analytical models;Algorithm design and analysis;Singular value decomposition,pattern clustering;query processing;semantic networks;text analysis,information retrieval;machine matching;preclustering-based latent semantic analysis algorithm;document retrieval;time consuming computation;query vector;text vector;k-means clustering,,3,,13,,13-Jun-16,,,IEEE,IEEE Conferences
Co-occurrence-based clustering of odor descriptors for predicting structure-odor relationship,基於共現的氣味描述符聚類，用於預測結構與氣味的關係,C. Liu; L. Shang; K. Hayashi,"Research Laboratory, U.S.E. Co., Ltd., Shibuya-ku, Tokyo 150-0013, Japan; Graduate School of Information Science and Electrical Engineering, Kyushu University, Nishi-ku, Fukuoka 819-0395, Japan; Graduate School of Information Science and Electrical Engineering, Kyushu University, Nishi-ku, Fukuoka 819-0395, Japan",2019 IEEE International Symposium on Olfaction and Electronic Nose (ISOEN),5-Sep-19,2019,,,1,4,"One problem of machine-learning-based prediction of structure-odor relationship is that odorant molecules are usually labeled with ambiguous descriptors when they are collected from different sources. This study focused on the clustering of the odor descriptors by text mining approaches as well as the prediction of newly established labels from physicochemical parameters of the classified odorant molecules. An odor database was established by web scraping and transferred to a document-term matrix including 4011 odorants and 100 odor descriptors. The clustering of the odor descriptors was carried out by using different co-occurrence matrix and clustering approaches. A hierarchical cluster analysis combined with a co-occurrence probability distribution matrix has shown good results in the descriptor clustering. The attribute labels of each class were established and then predicted from physicochemical parameters of the classified odorants by using random forest model. An average accuracy higher than 82.42% was obtained, indicating the effectiveness of the proposed approaches for predicting structure-odor relationship.",,978-1-5386-8327-9,10.1109/ISOEN.2019.8823446,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823446,structure-odor relationship;text mining;machine learning;odor descriptors;physicochemical parameters;clustering;prediction,Predictive models;Dairy products;Indexes;Machine learning;Probability distribution;Olfactory,data mining;electronic noses;learning (artificial intelligence);matrix algebra;pattern classification;pattern clustering;probability,structure-odor relationship;machine-learning-based prediction;classified odorant molecules;odor database;cooccurrence-based clustering;Web scraping;physicochemical parameters;document-term matrix;random forest model;text mining,,,,10,,5-Sep-19,,,IEEE,IEEE Conferences
A Framework of Web-Based Social Network Search System,基於Web的社交網絡搜索系統框架,X. Zhang; L. Yuan; G. Li; Y. Pang,"Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China",2011 International Conference on Internet Technology and Applications,29-Aug-11,2011,,,1,4,"Social network mining technology and its implementation are the subject of much interest and difficulty in recent research. A framework of web-based social network search system is designed to carry out social network search. The paper presents a method to implement web downloader in order to obtain web documents containing social network information. Characteristic vector weight is calculated with C-value and Inverse Document Frequency, and identical name judgment is carried out by cataloguing web documents with clustering algorithm. The paper also gives a definition of the strength of personal relation, and current commercial search engines are used to design and implement algorithms that can retrieve personal relation information and calculate the strength of those relations. We tested the method on a search engine for name search, and the results show that identical name judgement based on a modified hierarchical clustering algorithm can significantly improve performance.",,978-1-4244-7255-0,10.1109/ITAP.2011.6006218,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006218,,Social network services;Clustering algorithms;Algorithm design and analysis;Search engines;Internet;Crawlers;Natural language processing,cataloguing;data mining;document handling;information retrieval;pattern clustering;search engines;social networking (online);statistical analysis,Web based social network search system;Web downloader;Web document;characteristic vector weight;C-value;inverse document frequency;identical name judgment;cataloguing;commercial search engines;personal relation information retrieval;modified hierarchical clustering algorithm;social network mining technology,,,,7,,29-Aug-11,,,IEEE,IEEE Conferences
A clustering analysis of news text based on co-occurrence matrix,基於共現矩陣的新聞文本聚類分析,S. Liu; X. Fan; J. Chai,"School of Information Engineering, Communication University of China, Beijing, China; School of Information Engineering, Communication University of China, Beijing, China; School of Information Engineering, Communication University of China, Beijing, China",2017 3rd IEEE International Conference on Computer and Communications (ICCC),26-Mar-18,2017,,,2281,2285,"In this paper, we use the improved TF-IDF formula to calculate the weight of the feature word of News. The keyword extraction takes into account the factors such as the parts of speech of feature words and inverse document frequency (IDF). And the K-core theory is used to determine the range of keywords. This study analyzes the co-occurrence strength of news keywords in a certain period of time, and obtains the cluster analysis of the co-occurrence intensity distance of news keywords. And the clustering results are analyzed in the end. In this paper, the quantitative research methods commonly used in bibliometrics are used in the news field to analyze the news content.",,978-1-5090-6352-9,10.1109/CompComm.2017.8322941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322941,component;Co-occurrence matrix;clustering analysis,Feature extraction;Speech;Software;Time-frequency analysis;Business;Speech recognition;Frequency conversion,feature extraction;matrix algebra;pattern clustering;statistical analysis;text analysis;word processing,clustering analysis;news text;improved TF-IDF formula;keyword extraction;feature words;inverse document frequency;K-core theory;news keywords;cluster analysis;news content;cooccurrence matrix;IDF;cooccurrence strength;cooccurrence intensity distance,,,,10,,26-Mar-18,,,IEEE,IEEE Conferences
Web Page Clustering Using a Fuzzy Logic Based Representation and Self-Organizing Maps,基於模糊邏輯的表示和自組織映射的網頁聚類,A. P. Garc穩a-Plaza; V. Fresno; R. Mart穩nez,"NLP & IR Group, UNED, Madrid; NLP & IR Group, UNED, Madrid; NLP & IR Group, UNED, Madrid",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,851,854,This article introduces and evaluates a fuzzy logic based representation for HTML document clustering using Self-Organizing Maps. This representation is built on heuristic combinations of criteria by means of a fuzzy rules system and based on the HTML markup. We evaluate the model using different feature vector sizes. Experimental results show an improvement in clustering quality when the fuzzy logic-based model is used instead of the vector space model with traditional term weighting functions in a standard benchmark dataset.,,978-0-7695-3496-1,10.1109/WIIAT.2008.249,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740563,Web Page Clustering;Fuzzy Logic;Self-Organizing Maps;Web Page Representation,Web pages;Fuzzy logic;Self organizing feature maps;Frequency;HTML;Intelligent agent;Humans;Fuzzy systems;Vocabulary;Ontologies,fuzzy logic;hypermedia markup languages;Internet;knowledge based systems;pattern clustering;self-organising feature maps,Web page clustering;self-organizing map;HTML document clustering;fuzzy rule system;fuzzy logic based representation,,6,,9,,6-Jan-09,,,IEEE,IEEE Conferences
Overviewing the Knowledge of a Query Keyword by Clustering Viewpoints of Web Search Information Needs,通過對Web搜索信息需求的觀點進行聚類來概述查詢關鍵字的知識,I. Moriya; Y. Inoue; T. Imada; T. Utsuro; Y. Kawada; N. Kando,"Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Grad. Sch. of Syst. & Inf. Eng., Univ. of Tsukuba, Tsukuba, Japan; Logworks Co., Ltd., Tokyo, Japan; Nat. Inst. of Inf., Tokyo, Japan",2015 IEEE 29th International Conference on Advanced Information Networking and Applications Workshops,30-Apr-15,2015,,,535,540,"In this paper, we address the issue of how to overview the knowledge of a given query keyword. We especially focus on concerns of those who search for Web pages with a given query keyword, and study how to efficiently overview the whole list of Web search information needs of a given query keyword. First, we collect Web search information needs of a given query keyword through search engine suggests. In the case of a Japanese search engine, we collect up to around 1,000 suggests given a query keyword. However, some of them are redundant in that they originate from almost the same Web search information needs. In order to aggregate such redundant search engine suggests, we take an approach of clustering search engine suggests based on document vectors generated from the snippets shown by the search engine. Evaluation result shows that the proposed clustering approach proves to be quite useful for efficiently over viewing Web search information needs of a given query keyword. We also develop an interface system for over viewing those aggregated search engine suggests of a given query keyword as well slinks to top ranked Web pages that are closely related to those aggregated search engine suggests.",,978-1-4799-1775-4,10.1109/WAINA.2015.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096232,search engine suggest;Web search information need;overview;clustering;search engine queries,Search engines;Web search;Web pages;Training;Production facilities;Knowledge engineering;Search problems,pattern clustering;query processing;search engines;Web sites,query keyword;viewpoints clustering;Web search information needs;knowledge overview;Web pages;Japanese search engine;search engine suggests clustering;document vectors;snippets;aggregated search engine suggests,,1,,7,,30-Apr-15,,,IEEE,IEEE Conferences
Automated document indexing via intelligent hierarchical clustering: A novel approach,通過智能分層聚類自動建立文檔索引：一種新穎的方法,R. K. Roul; S. R. Asthana; S. K. Sahay,"Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, Zuarinagar, India - 403726; Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, Zuarinagar, India - 403726; Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, Zuarinagar, India - 403726",2014 International Conference on High Performance Computing and Applications (ICHPCA),19-Feb-15,2014,,,1,6,"With the rising quantity of textual data available in electronic format, the need to organize it become a highly challenging task. In the present paper, we explore a document organization framework that exploits an intelligent hierarchical clustering algorithm to generate an index over a set of documents. The framework has been designed to be scalable and accurate even with large corpora. The advantage of the proposed algorithm lies in the need for minimal inputs, with much of the hierarchy attributes being decided in an automated manner using statistical methods. The use of topic modeling in a pre-processing stage ensures robustness to a range of variations in the input data. For experimental work 20-Newsgroups dataset has been used. The F-measure of the proposed approach has been compared with the traditional K-Means and K-Medoids clustering algorithms. Test results demonstrate the applicability, efficiency and effectiveness of our proposed approach. After extensive experimentation, we conclude that the framework shows promise for further research and specialized commercial applications.",,978-1-4799-5958-7,10.1109/ICHPCA.2014.7045347,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045347,Hierarchical Clustering;Indexing;Latent Dirichlet Allocation;Latent Semantic Indexing;Topic Modeling,,indexing;pattern clustering;statistical analysis;text analysis,automated document indexing;textual data;electronic format;document organization framework;intelligent hierarchical clustering algorithm;hierarchy attributes;statistical methods;pre-processing stage;20-newsgroups dataset;F-measure,,2,,18,,19-Feb-15,,,IEEE,IEEE Conferences
Serialized unsupervised classifier for adaptative color image segmentation: application to digitized ancient manuscripts,用於自適應彩色圖像分割的序列化無監督分類器：在數字化古代手稿中的應用,Y. Leydier; F. Le Bourgeois; H. Emptoz,"Archimed, Lille, France; NA; NA","Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",20-Sep-04,2004,1,,494,497 Vol.1,"This paper presents an adaptative algorithm for the segmentation of color images suited for document image analysis. The algorithm is based on a serialization of the k-means algorithm that is applied sequentially by using a sliding window over the image. The algorithm reuses information about the clusters computed by the previous classification and automatically adjusts the clusters during the windows displacement in order to better adapt the classifier to any new local modification of the colors. For digitized documents, we propose to define several different clusters in the color feature space for the same logical class. We also reintroduce the user into the initialization step who must define the different samples of colors for each class and the number of classes. This algorithm has been tested successfully on ancient color manuscripts having heavy defects, showing lighting variation and transparency. Nevertheless, the proposed algorithm is generic enough to be applied on a large variety of images using other features for different purposes like color image segmentation as well as image binarization.",1051-4651,0-7695-2128-2,10.1109/ICPR.2004.1334174,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1334174,,Image segmentation;Clustering algorithms;Image color analysis;Image restoration;Text analysis;Testing;Cultural differences;Internet;Software libraries;Costs,image segmentation;image colour analysis;image classification;document image processing;pattern clustering;feature extraction,serialized unsupervised classifier;adaptative color image segmentation;digitized ancient manuscripts;adaptative algorithm;document image analysis;k means algorithm;sliding window;cluster analysis;windows displacement;digitized documents;color manuscripts;image binarization,,16,,5,,20-Sep-04,,,IEEE,IEEE Conferences
Hypergraph Based Document Categorization: Frequent Itemsets vs Hypercliques,基於Hypergraph的文檔分類：頻繁項集與Hypercliques,T. Hu; J. Ouyang; C. Qu; S. Y. Sung,"DongGuan University of Technology, DongGuan, GuangDong 523808, China. E-MAIL: tmhu05@gmail.com; DongGuan University of Technology, DongGuan, GuangDong 523808, China; DongGuan University of Technology, DongGuan, GuangDong 523808, China; South Texas College, McAllen, TX 78501, USA",2007 International Conference on Machine Learning and Cybernetics,29-Oct-07,2007,2,,824,829,"This paper describes a new hypergraph formulation for document categorization, where hyperclique patterns, strongly affiliated documents in this case, are used as hyperedges. Compared to frequent itemsets, the objects in a hyperclique pattern have a guaranteed level of global pairwise similarity to one another as measured by the cosine or Jaccard similarity measure. Since hypergraph partitioning is mainly based on vertex similairty on the hyperedge, hypercliques may serve as better quality hyperedges. Besides, due to the additional confidence constraint, we can cover more items in the mined patterns while keep the pattern size reasonable. Hence, the difficulty in partitioning dense hypergraphs, which is often encountered in frequent itemset based hypergraph partitioning, is alleviated considerably. Finally, experiments with real-world datasets show that, with hyperclique patterns as hyperedges, we can improve the clustering results in terms of various external validation measures.",2160-1348,978-1-4244-0972-3,10.1109/ICMLC.2007.4370256,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370256,Hypergraph partitioning;Document categorization;Frequent itemset;Hyperclique,Itemsets;Clustering algorithms;Partitioning algorithms;Association rules;Machine learning;Cybernetics;Frequency;Transaction databases;Pattern analysis;Machine learning algorithms,document handling;graph theory;pattern clustering,hypergraph partitioning;document categorization;frequent itemsets;hypercliques;hyperedges;global pairwise similarity;cosine similarity;Jaccard similarity;vertex similairty;mined patterns;clustering,,1,,20,,29-Oct-07,,,IEEE,IEEE Conferences
Dynamic and adaptive self organizing maps applied to high dimensional large scale text clustering,動態和自適應自組織映射應用於高維大規模文本聚類,Z. Feng; J. Bao; J. Shen,"Institute of Computer Software, Xi'an Jiaotong University, Xi'an 710049, China; Institute of Computer Software, Xi'an Jiaotong University, Xi'an 710049, China; Institute of Computer Software, Xi'an Jiaotong University, Xi'an 710049, China",2010 IEEE International Conference on Software Engineering and Service Sciences,19-Aug-10,2010,,,348,351,"The self organizing maps(SOM) has been used as a tool for mapping high-dimensional input data into a low-dimensional feature map, which has significant advantages for text clustering applications. In this paper, a novel dynamic and adaptive SOM algorithm applied to high dimensional large scale text clustering is proposed. The characteristic feature of this novel neural network model is its dynamic architecture which grows (when the similarity between input pattern (text vector) and weight vector of the winning node is smaller than a given threshold) during its training process to find the inherent topology structure of the document set. By using unsupervised competitive learning in network, the weight vectors of the winning node and its nearest neighbors are adjusted adaptively (where learning rate is related to similarity in amended learning rule) in this algorithm. The results of the experiments indicated that the algorithm successfully improve quality of text clustering and learning speed of neural network.",2327-0594,978-1-4244-6055-7,10.1109/ICSESS.2010.5552449,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552449,text clustering;dynamic and adaptive self organizing maps;text vector,Heuristic algorithms;Clustering algorithms;Training;Self organizing feature maps;Optimization;Computer architecture,learning (artificial intelligence);pattern clustering;self-organising feature maps;text analysis,adaptive self organizing maps;large scale text clustering;neural network model;topology structure;unsupervised competitive learning,,4,,9,,19-Aug-10,,,IEEE,IEEE Conferences
A YOLO-Based Table Detection Method,基於YOLO的表格檢測方法,Y. Huang; Q. Yan; Y. Li; Y. Chen; X. Wang; L. Gao; Z. Tang,"Peking University; State Key Laboratory of Digital Publishing Technology, Founder Group Co., LTD, Beijing; Peking University; State Key Laboratory of Digital Publishing Technology, Founder Group Co., LTD, Beijing; State Key Laboratory of Digital Publishing Technology, Founder Group Co., LTD, Beijing; Peking University; Peking University",2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,813,818,"Due to various table layouts and styles, table detection is always a difficult task in the field of document analysis. Inspired by the great progress of deep learning based methods on object detection, in this paper, we present a YOLO-based method for this task. Considering the large difference between document objects and natural objects, we introduce some adaptive adjustments to YOLOv3, including an anchor optimization strategy and two post processing methods. For anchor optimization, we use k-means clustering to find anchors which are more suitable for tables rather than natural objects and make it easier for our model to find exact positions of tables. In post-processing process, the extra whitespaces and noisy page objects (e.g. page headers, page footers) are removed from the predicted results, so that our model can get more accurate table margins and higher IoU scores. The proposed method is evaluated on two datasets from ICDAR 2013 Table Competition and ICDAR 2017 Page Object Detection (POD) Competition and achieves state-of-the-art performance.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978047,table detection;document analysis;deep learning;post processing,Task analysis;Deep learning;Adaptation models;Layout;Feature extraction;Optimization;Portable document format,document image processing;learning (artificial intelligence);object detection;text analysis,table layouts;document analysis;deep learning;document objects;natural objects;anchor optimization strategy;post processing methods;post-processing process;accurate table margins;achieves state-of-the-art performance;YOLO-based table detection method;page object detection competition;k-means clustering;noisy page objects;POD;YOLOv3,,1,,30,,3-Feb-20,,,IEEE,IEEE Conferences
Probabilistic matrix tri-factorization,概率矩陣三因子分解,J. Yoo; S. Choi,"Department of Computer Science, POSTECH, Korea; Department of Computer Science, POSTECH, Korea","2009 IEEE International Conference on Acoustics, Speech and Signal Processing",26-May-09,2009,,,1553,1556,"Nonnegative matrix tri-factorization (NMTF) is a 3-factor decomposition of a nonnegative data matrix, X ap USVT, where factor matrices, U, S, and V , are restricted to be nonnegative as well. Motivated by the aspect model used for dyadic data analysis as well as in probabilistic latent semantic analysis (PLSA), we present a probabilistic model with two dependent latent variables for NMTF, referred to as probabilistic matrix tri-factorization (PMTF). Each latent variable in the model is associated with the cluster variable for the corresponding object in the dyad, leading the model suited to co-clustering. We develop an EM algorithm to learn the PMTF model, showing its equivalence to multiplicative updates derived by an algebraic approach. We demonstrate the useful behavior of PMTF in a task of document clustering. Moreover, we incorporate the likelihood in the PMTF model into existing information criteria so that the number of clusters can be detected, while the algebraic NMTF cannot.",2379-190X,978-1-4244-2353-8,10.1109/ICASSP.2009.4959893,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959893,Co-clustering;document clustering;probabilistic latent semantic indexing;nonnegative matrix factorization,Matrix decomposition;Data analysis;Clustering algorithms;Frequency;Computer science;Indexing;Face detection;Face recognition;Image recognition;Speech recognition,expectation-maximisation algorithm;learning (artificial intelligence);matrix decomposition;pattern clustering;probability,probabilistic nonnegative matrix tri-factorization;3-factor decomposition;cluster variable;EM algorithm;PMTF model learning;algebraic approach,,12,,17,,26-May-09,,,IEEE,IEEE Conferences
Evaluation of Semantic Similarity Using Vector Space Model Based on Textual Corpus,基於文本語料庫的矢量空間模型語義相似度評估,B. Hssina; B. Bouikhalene; A. Merbouha,"Comput. Sci. Dept., Sultan Moulay Slimane Univ., Beni-Mellal, Morocco; Comput. Sci. Dept., Sultan Moulay Slimane Univ., Beni-Mellal, Morocco; Math. Dept., Sultan Moulay Slimane Univ., Beni-Mellal, Morocco","2016 13th International Conference on Computer Graphics, Imaging and Visualization (CGiV)",12-May-16,2016,,,295,300,"In this work, we have created a semantic similarity calculation system between text documents to contribute to their semantic clustering. Indeed, semantic clustering of documents is a promising field of research, since it guarantees a quick and targeted access to information. The aim of document clustering is to put together similar documents. We used the algebraic model VSM (Vector Space Model) [2] to represent text documents and the WordNet [1] lexical database, in that it groups words together based on their meanings. In this paper, we will present an overview of the static and semantic methods for calculating the similarity measure and the appropriateness of these methods. As our research is focusing on the treatment of text documents on e-learning systems. We worked on a corpus of a set of text documents from the computer science textbook for high school students in Morocco. To evaluate our system, an experiment has been conducted among students who produced text documents. Experimental evaluations using WordNet prove that the system presented in this work improves the accuracy of semantic similarity between the text documents.",,978-1-5090-0811-7,10.1109/CGiV.2016.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467726,Clustering;Corpus;Semantic similarity;text document;WordNet,Semantics;Information retrieval;Indexing;Weight measurement;Euclidean distance;Clustering methods;Computers,computer aided instruction;computer science education;educational institutions;pattern clustering;text analysis,semantic similarity calculation system;text documents;semantic clustering;document clustering;algebraic model VSM;vector space model;WordNet lexical database;static method;semantic method;e-learning systems;computer science textbook;high school students;Morocco,,1,,26,,12-May-16,,,IEEE,IEEE Conferences
Semi-supervised Document Clustering via Active Learning with Pairwise Constraints,通過成對約束的主動學習進行半監督文檔聚類,R. Huang; W. Lam,"Chinese Univ. of Hong Kong, Shatin; Chinese Univ. of Hong Kong, Shatin",Seventh IEEE International Conference on Data Mining (ICDM 2007),12-Mar-08,2007,,,517,522,"This paper investigates a framework that discovers pair-wise constraints for semi-supervised text document clustering. An active learning approach is proposed to select informative document pairs for obtaining user feedbacks. A gain directed document pair selection method that measures how much we can learn by revealing the relationships between pairs of documents is designed. Three different models, namely, uncertainty model, generation error model, and objective function model are proposed. Language modeling is investigated for representing clusters in the semi-supervised document clustering approach.",2374-8486,978-0-7695-3018-5,10.1109/ICDM.2007.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4470283,,Feedback;Gain measurement;Parametric statistics;Machine learning;Data mining;Data engineering;Systems engineering and theory;Research and development management;Probability distribution;Text categorization,learning (artificial intelligence);pattern clustering;text analysis,semi supervised text document clustering;active learning;pairwise constraints;informative document pairs;gain directed document pair selection method;uncertainty model;generation error model;objective function model;language modeling;user feedback,,12,,11,,12-Mar-08,,,IEEE,IEEE Conferences
A novel approach for document extraction based on SVD and FCA,一種基於SVD和FCA的文檔提取新方法,R. C. Jisha; S. Hari; S. Shyba,"Department of Computer Science and Applications, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Amrita University, Amritapuri, Kollam, India; Department of Computer Science and Applications, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Amrita University, Amritapuri, Kollam, India; Department of Computer Science and Applications, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Amrita University, Amritapuri, Kollam, India",2016 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC),8-May-17,2016,,,1,6,"Nowadays Information Retrieval (IR) is difficult because of huge amount of information published on the Internet. So it is very relevant to organize documents based on its content. The proposed work address this issue by generating concepts from the documents and these documents are grouped based on a data mining approach. To generate the concept, keywords are extracted from the documents but the extracted set is very large. So for dimensionality reduction, SVD is applied. This paper proposes a novel approach for document clustering based on Formal Concept Analysis (FCA). Concept generation and dimensionality reduction are the two issues addressed here. FCA approach leads to give a fast searching result based on the domain specific keyword. The test result shows that the dimensionality reduction is attained after applying SVD.",2473-943X,978-1-5090-0612-0,10.1109/ICCIC.2016.7919533,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919533,FCA;Concept lattice;Keyword extraction;SVD;Data Mining;Dimensionality reduction,Lattices;Matrix decomposition;Context;Formal concept analysis;Singular value decomposition;Clustering algorithms;Sparse matrices,data mining;document handling;formal concept analysis;information retrieval;pattern clustering;singular value decomposition,document extraction;SVD;information retrieval;data mining;keyword extraction;document clustering;formal concept analysis;FCA;concept generation;dimensionality reduction;domain specific keyword,,1,,19,,8-May-17,,,IEEE,IEEE Conferences
Local adaptive receptive field dimension selective self-organizing map for multi-view clustering,用於多視圖聚類的局部自適應感受野維選擇性自組織圖,V. O. Antonino; A. F. R. Araujo,"Center of Informatics, Federal University of Pernambuco, Recife, Brazil; Center of Informatics, Federal University of Pernambuco, Recife, Brazil",2016 International Joint Conference on Neural Networks (IJCNN),3-Nov-16,2016,,,698,705,"Images, text, web documents, videos, real-world data are very often high-dimensional. Many researchers or users may need to construct accurate predictive models for a variety of applications, especially those that involve clustering. Handling high dimensional data is a reality in processing task involving areas such as high-throughput genotyping platforms and human genetic clustering in bioinformatics, medical imaging and IMRT segmentation in medicine, market research, social network analysis, and anomaly detection. However, the performance of clustering algorithms usually decreases significantly when the sample dimension grows. Moreover, the big data can be acquired taking into consideration different views of them, characterizing the so-called multi-view clustering. In this paper, we use a subspace clustering approach, a time-varying self-organizing map, to deal with multi-view clustering. The method showed itself promising since it can handle real-world data characterized by high sparsity, high dimensionality nature, and different representations. A number of experiments with the proposed solution showed better performance than a number of other state-of-the-art models built specifically to deal with multi-view data.",2161-4407,978-1-5090-0620-5,10.1109/IJCNN.2016.7727268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727268,High-dimensional data;local receptive field;relevance learning;self-organizing maps (SOMs);subspace clustering;Multi-view Clustering,Clustering algorithms;Feature extraction;Data models;Context;Computer vision;Clustering methods;Self-organizing feature maps,Big Data;pattern clustering;self-organising feature maps,local adaptive receptive field dimension selective self-organizing map;multiview clustering;predictive models;high dimensional data handling;Big Data;subspace clustering approach;time-varying self-organizing map;multiview data,,,,28,,3-Nov-16,,,IEEE,IEEE Conferences
Persian Plagirisim Detection Using CNN s,數據挖掘（DM）的最新趨勢：DM出版物的文檔聚類,S. Lazemi; H. Ebrahimpour-Komleh; N. Noroozi,"Department of Computer Engineering, The University of Kashan, Kashan, Iran; Department of Computer Engineering, The University of Kashan, Kashan, Iran; Faculty of Mathematics, The University of Kashan, Kashan, Iran",2018 8th International Conference on Computer and Knowledge Engineering (ICCKE),9-Dec-18,2018,,,171,175,"The abundant and growing amount of scientific-research works and the ease of access to them has caused some abusive exploits from jobber people and illicit use of them in scientific and academic environments. ?Plagiarism??refers to the use of scientific-research works by others without reference to them correctly. Due to the rapid growth of Persian electronic resources, this paper considers the plagiarism detection in Persian texts. Plagiarism detection consists of two distinct steps: Candidate Retrieval and Text Alignment. The focus of our proposed method is on both steps. In the first step, using a Convolutional Neural Network (CNN), a vector representation is created in document-level and then, the candidate documents are retrieved using the k-means clustering algorithm. In order to align text, the features are extracted at the sentence-level using a CNN. Finally, using the classification algorithms, the copied sentences are detected. Experiments were performed on the prepared corpus in the AAI competition and the prepared corpus in the PAN2015 competition. The achieved precision and recall are 0.843 and 0.806 for the first corpus and 0.833 and 0.826 for the second one respectively.",2375-1304,978-1-5386-9569-2,10.1109/ICCKE.2018.8566340,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8566340,plagiarism detection;convolutional neural network;Persian;clustering algorithm,Plagiarism;Feature extraction;Portable document format;Clustering algorithms;Web pages;Semantics;Support vector machines,convolution;document handling;feature extraction;feedforward neural nets;information retrieval;natural language processing;pattern clustering;security of data;text analysis;vectors,Persian plagirisim detection;CNN;abusive exploits;jobber people;scientific environments;academic environments;Persian electronic resources;Persian texts;document-level;candidate documents;sentence-level;convolutional neural network;candidate retrieval;text alignment;feature extraction;vector representation;k-means clustering algorithm,,,,22,,9-Dec-18,,,IEEE,IEEE Conferences
Content-based classification of graphical document images,使用雲計算，集群和麵向文檔的數據庫進行企業內容管理,A. Khare; P. Jeph; H. Ghosh,"TCS Innovation Labs, Delhi, Tata Consultancy Services Ltd, Plot 249 D&E, Udyog Vihar, Phase IV, Gurgaon, Haryana, India; TCS Innovation Labs, Delhi, Tata Consultancy Services Ltd, Plot 249 D&E, Udyog Vihar, Phase IV, Gurgaon, Haryana, India; TCS Innovation Labs, Delhi, Tata Consultancy Services Ltd, Plot 249 D&E, Udyog Vihar, Phase IV, Gurgaon, Haryana, India",2010 2nd European Workshop on Visual Information Processing (EUVIP),20-Jan-11,2010,,,241,246,"We present a computer vision based approach for classifying graphical document images by matching distinct visual patterns present in them. To accomplish this task the image is first decomposed into congruous segments, some of which contain distinct patterns followed by image matching to identify the presence of a specific pattern in the image. We have used clustering based image segmentation to extract distinctive patterns and PCA-SIFT image features for robust image matching. We have used R-Tree based feature indexing for faster retrieval of images. We have done our experiments on advertisement images which contain company's trademarks and finally classify them based on their advertiser.",,978-1-4244-7289-5,10.1109/EUVIP.2010.5699113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699113,Document image processing;Image segmentation;Tree searching;Pattern classification;Pattern matching,Image segmentation;Trademarks;Feature extraction;Indexing;Image color analysis;Image matching,computer vision;content-based retrieval;document image processing;feature extraction;image classification;image matching;image segmentation;tree searching,content-based classification;computer vision;graphical document image classification;visual pattern matching;congruous segments;image matching;image segmentation;feature extraction;PCA-SIFT image;R-Tree;indexing;advertisement images;trademarks,,,,10,,20-Jan-11,,,IEEE,IEEE Conferences
Speaker role clustering using turn features and maximum inter-cluster distances,具有半監督聚類的圖像註釋,Y. Li; X. Zhang; X. Li; A. Chen; J. Yang; X. Feng; Q. Huang; Z. Chen,"School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; School of Electronic and Information Engineering, South China University of Technology, China; The First Affiliated Hospital of JINAN University, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)",9-Feb-17,2016,,,482,486,"Speaker role clustering is to obtain the number of different roles and to merge the utterances of the same role into one cluster in an unsupervised way, which is important for rich transcription of multi-speaker spoken documents. This paper presents an approach to role clustering using turn features and maximum distances of inter-clusters. The turn features of each speaker are extracted from audio outputs of speaker diarization, and used as the initial clusters. During clustering iteration, the cluster-pair (e.g. CA and CB) with the minimum distance is merged and the cluster number is decreased by one if the distance of the Nc - 1 clusters (after merging CA and CB) is bigger than that of the Nc clusters (not merging CA and CB); otherwise, the clustering iteration is finished. Evaluated on four types of multi-speaker spoken documents, the proposed approach outperforms the previous clustering approach and is close to the supervised approach in terms of K scores.",,978-1-5090-0654-0,10.1109/ICALIP.2016.7846538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846538,multi-speaker spoken document;role classification;turn features,Speech;Feature extraction;Hidden Markov models;TV;Speech recognition;Presses;Merging,audio signal processing;feature extraction;speaker recognition,cluster number;cluster-pair;clustering iteration;speaker diarization;audio outputs;multispeaker spoken documents;maximum intercluster distances;turn features;speaker role clustering,,,,17,,9-Feb-17,,,IEEE,IEEE Conferences
Phrase-based document similarity based on an index graph model,基於新穎性的在線文檔增量文檔聚類,K. M. Hammouda; M. S. Kamel,"Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada; Dept. of Syst. Design Eng., Waterloo Univ., Ont., Canada","2002 IEEE International Conference on Data Mining, 2002. Proceedings.",10-Mar-03,2002,,,203,210,"Document clustering techniques mostly rely on single term analysis of the document data set, such as the vector space model. To better capture the structure of documents, the underlying data model should be able to represent the phrases in the document as well as single terms. We present a novel data model, the document index graph, which indexes web documents based on phrases, rather than single terms only. The semi-structured web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the documents. The document index graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The similarity between documents is based on both single term weights and matching phrases weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, enhances web document clustering quality significantly.",,0-7695-1754-4,10.1109/ICDM.2002.1183904,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183904,,Data mining;Clustering methods;Data models;Web mining;System analysis and design;Data engineering;Design engineering;Systems engineering and theory;Functional analysis;Web sites,text analysis;indexing;directed graphs;Web sites,phrase-based document similarity;document clustering techniques;single term analysis;document data set;vector space model;document index graph;document structure capture;phrase representation;Web documents;single term weights;matching phrases weights,,28,,20,,10-Mar-03,,,IEEE,IEEE Conferences
Research on Text Clustering Based on Concept Weight,聚集層次聚類算法的通用框架,Y. Li; Y. Liu; X. Lv; S. Shi,"Chinese Inf. Process. Res. Center, Beijing Inf. Sci. & Technol. Univ., Beijing, China; Chinese Inf. Process. Res. Center, Beijing Inf. Sci. & Technol. Univ., Beijing, China; Chinese Inf. Process. Res. Center, Beijing Inf. Sci. & Technol. Univ., Beijing, China; Chinese Inf. Process. Res. Center, Beijing Inf. Sci. & Technol. Univ., Beijing, China",2010 Fourth International Conference on Genetic and Evolutionary Computing,17-Feb-11,2010,,,232,235,"Through research on the calculation method of feature words' weight in texts and semantic similarity between words, we proposed a calculation method of feature words' weight based on concept weight for the semantic association phenomenon of text features and the prevalence of high-dimensional problem in a text vector space model. This method reduces the semantic loss of the feature set and the dimension of the text vector, and then makes the text vector space model better and improves the quality of text clustering. Experimental results show the feasibility of the method, and prove that concept-weight-based text clustering increased by 22 percentage points or so than non-concept-weight-based in the final evaluation of the FI index value.",,978-1-4244-8891-9,10.1109/ICGEC.2010.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715412,Concept Frequency;Concept Document Frequency;Concept Weight;Text Clustering,Semantics;Feature extraction;Information processing;Data models;Electronic mail;Information science;Data mining,feature extraction;pattern clustering;set theory;text analysis;word processing,feature word;semantic association phenomenon;text vector space model;feature set;concept weight-based text clustering,,,,12,,17-Feb-11,,,IEEE,IEEE Conferences
A Meaningful Information Extraction System for Interactive Analysis of Documents,自適應神經網絡的超文本聚類方法,J. Maitre; M. M矇nard; G. Chiron; A. Bouju; N. Sid癡re,"L3i, La Rochelle University; L3i, La Rochelle University; L3i, La Rochelle University; L3i, La Rochelle University; L3i, La Rochelle University",2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,92,99,"This paper is related to a project aiming at discovering weak signals from different streams of information, possibly sent by whistleblowers. The study presented in this paper tackles the particular problem of clustering topics at multi-levels from multiple documents, and then extracting meaningful descriptors, such as weighted lists of words for document representations in a multi-dimensions space. In this context, we present a novel idea which combines Latent Dirichlet Allocation and Word2vec (providing a consistency metric regarding the partitioned topics) as potential method for limiting the ""a priori"" number of cluster K usually needed in classical partitioning approaches. We proposed 2 implementations of this idea, respectively able to: (1) finding the best K for LDA in terms of topic consistency; (2) gathering the optimal clusters from different levels of clustering. We also proposed a non-traditional visualization approach based on a multi-agents system which combines both dimension reduction and interactivity.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978125,weak signal;clustering topics;word embedding;multi-agent system;vizualisation,Analytical models;Clustering algorithms;Partitioning algorithms;Text analysis;Resource management;Multi-agent systems;Coherence,data mining;data visualisation;information retrieval;multi-agent systems;pattern clustering;text analysis,document representations;Latent Dirichlet Allocation;Word2vec;consistency metric;classical partitioning approaches;topic consistency;nontraditional visualization approach;multiagents system;dimension reduction;information extraction system,,,,30,,3-Feb-20,,,IEEE,IEEE Conferences
Clustering based rescoring for semantic indexing of multimedia documents,稀疏共現數據的信息理論聚類,A. Hamadi; G. Qu矇not; P. Mulhem,"UJF-Grenoble 1 / UPMF-Grenoble 2 / Grenoble INP / CNRS, LIG UMR 5217, Grenoble, F-38041, France; UJF-Grenoble 1 / UPMF-Grenoble 2 / Grenoble INP / CNRS, LIG UMR 5217, Grenoble, F-38041, France; UJF-Grenoble 1 / UPMF-Grenoble 2 / Grenoble INP / CNRS, LIG UMR 5217, Grenoble, F-38041, France",2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI),8-Aug-13,2013,,,41,46,"This paper describes a new approach for multimedia documents indexing and addresses the problem of automatically detecting a large number of visual concepts. Though using a multi-label approaches are used in some works, concepts detectors are often trained independently. We propose a model that takes into account the detection of not only a target concept but also other ones and regroups in terms of semantics similar samples. The expected benefit from such a combination is to consider the relationships between concepts in order to reclassify the results of an initial indexing system. Experiments on the TRECVID 2012 data are presented and discussed. Our method has significantly improved a quite good baseline system performance up to +6 % on mean average precision.",1949-3991,978-1-4799-0956-8,10.1109/CBMI.2013.6576550,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576550,semantic indexing;multimedia;visual concept detection;re-scoring;re-rankng;clustering;fusion,Multimedia communication;Indexing;Semantics;Visualization;Proposals;Videos;Feature extraction,document handling;indexing;multimedia computing;pattern clustering,clustering based rescoring;semantic indexing;multimedia documents;visual concepts;multilabel approaches;TRECVID 2012;mean average precision;concepts detectors,,,,17,,8-Aug-13,,,IEEE,IEEE Conferences
Annotation-aware web clustering based on topic model and random walks,PDF-TREX：一種從PDF文檔中識別和提取表格的方法,J. Sun; X. Wang; C. Yuan; G. Fang,"Center of Information Science and Technology, Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Center of Information Science and Technology, Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Center of Information Science and Technology, Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Center of Information Science and Technology, Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China",2011 IEEE International Conference on Cloud Computing and Intelligence Systems,13-Oct-11,2011,,,12,16,"Web page clustering based on semantic or topic promises improved search and browsing on the web. Intuitively, tags from social bookmarking websites such as del.icio.us can be used as a complementary source to document thus improving clustering of web pages. In this paper, we present a novel model which employs topic model to associate annotated document with a distribution of topics, and then constructs a graph including tags, document and topics by performing a Random Walks for clustering. We examine the performance of our model on a real-world data set, illustrating that our model provides improved clustering performance than algorithm utilizing page text alone.",2376-595X,978-1-61284-204-2,10.1109/CCIS.2011.6045023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045023,social tagging;topic model;random walks;web clustering,Web pages;Clustering algorithms;Probability;Web search;Measurement;Data models,document handling;Internet;pattern clustering;Web sites,annotation aware Web clustering;random walks;topic model;social bookmarking Websites;document source;Web pages,,1,,22,,13-Oct-11,,,IEEE,IEEE Conferences
Cross-media retrieval by cluster-based correlation analysis,Blogosphere的群集標籤,D. Ma; X. Zhai; Y. Peng,"Institute of Computer Science and Technology, Peking University; Institute of Computer Science and Technology, Peking University; Institute of Computer Science and Technology, Peking University",2013 IEEE International Conference on Image Processing,13-Feb-14,2013,,,3986,3990,"Multimedia content such as images and texts with similar semantic meanings are always used together. Therefore, to utilize the information shared by multi-modal objects, cross-media retrieval is becoming increasingly crucial. This area concerns problems that query and results are of different media types. Existing methods either neglect correlations between entities of different media types, or suffer low performances when adopting correlation analysis and facing queries out of dataset. In this paper, we present cluster-based correlation analysis (CBCA) to exploit the correlation between different types of multimedia objects, and to measure heterogeneous semantic similarities. Based on a collection of multimedia documents (MMD), CBCA first perform clustering on uni-media feature spaces to produce several semantic clusters for each modality. After that, by using the co-occurrence information of semantic clusters of different modalities, CBCA constructs a cross-modal cluster graph (CMCG) to represent the similarities between clusters. Our proposed CBCA exploits semantic meanings of a finer granularity by clustering, mines semantic correlation between clusters instead of multimedia objects. Compared with state-of-art methods, experiments on Sina Weibo dataset show the effectiveness of CBCA.",2381-8549,978-1-4799-2341-0,10.1109/ICIP.2013.6738821,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738821,cross-media retrieval;multimedia;correlation analysis;cluster analysis,,data mining;document handling;graph theory;multimedia systems;pattern clustering;query processing,cross-media retrieval;cluster-based correlation analysis;multimedia content;multimodal objects;query;CBCA;multimedia objects;heterogeneous semantic similarity measurement;multimedia documents;MMD;uni-media feature spaces;semantic cluster co-occurrence information;cross-modal cluster graph;CMCG;semantic correlation mining;Sina Weibo dataset,,5,,18,,13-Feb-14,,,IEEE,IEEE Conferences
Document classification with spherical word vectors,可變距離度量的聚類算法在解決Web挖掘問題中的應用,Y. Pan; C. Xing; D. Wang,"Center for Speech and Language Technology (CSLT) Research Institute of Information Technology, Tsinghua University, Beijing, P.R. China; Center for Speech and Language Technology (CSLT) Research Institute of Information Technology, Tsinghua University, Beijing, P.R. China; Center for Speech and Language Technology (CSLT) Research Institute of Information Technology, Tsinghua University, Beijing, P.R. China",2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),25-Feb-16,2015,,,270,273,"Recent research shows that low-dimensional continuous representations of words (word vectors) can be successfully employed to classify documents, and document vectors derived from semantic clustering work better than those derived from simple average pooling. On the other hand, our recent study demonstrated that embedding words on a hypersphere offers better performance on tasks including semantic relatedness and bilingual translation when compared to the original approach that embeds words in an unconstrained plane space. In this paper, spherical word vectors are applied to the document classification task. The experiments show that spherical word vectors can deliver good performance when combined with semantic clustering based on vMF distributions.",,978-9-8814-7680-7,10.1109/APSIPA.2015.7415518,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415518,,Semantics;Training;Mixture models;Clustering methods;Syntactics;Mathematical model;Data models,document handling;pattern classification;pattern clustering;vectors,document classification;spherical word vectors;low-dimensional continuous word representations;document vectors;semantic clustering;semantic relatedness;bilingual translation;vMF distributions,,1,,20,,25-Feb-16,,,IEEE,IEEE Conferences
All common embedded subtrees for clustering XML documents by structure,使用模糊邏輯來利用HTML標記進行網頁表示,Zhiwei Lin; Hui Wang; S. McClean; Haiying Wang,"Faculty of Computing and Engineering, University of Ulster, Northern Ireland, United Kingdom; Faculty of Computing and Engineering, University of Ulster, Northern Ireland, United Kingdom; Faculty of Computing and Engineering, University of Ulster, Northern Ireland, United Kingdom; Faculty of Computing and Engineering, University of Ulster, Northern Ireland, United Kingdom",2009 International Conference on Machine Learning and Cybernetics,25-Aug-09,2009,1,,13,18,"XML documents are tree-structured, and measuring the similarity of such tree structures plays a key role in XML clustering. In order to maximally capture common information for XML clustering, this paper investigates a novel similarity measurement - counting all common embedded subtrees of two trees, and its use for discovering latent hierarchical information for XML clustering. An efficient dynamic programming algorithm for counting all common embedded subtrees is proposed and also theoretically studied. The all common embedded subtrees similarity is employed in the definition of a dissimilarity measure for XML documents. This dissimilarity measure is evaluated in the standard hierarchical clustering framework on real XML documents. Experimental results show that all common embedded subtrees outperform the tree edit distance in clustering XML documents under the standard performance measures for clustering.",2160-1348,978-1-4244-3702-3,10.1109/ICMLC.2009.5212557,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212557,XML;clustering;tree similarity;tree edit distance;all common embedded subtrees,XML;Machine learning;Cybernetics,dynamic programming;pattern clustering;trees (mathematics);XML,XML documents clustering;embedded subtrees;tree structures;dynamic programming algorithm;tree edit distance,,,,10,,25-Aug-09,,,IEEE,IEEE Conferences
Unsupervised Classification of Structurally Similar Document Images,使用QDSM和余弦相似度比較查詢文檔聚類的有效性,J. Kumar; D. Doermann,"Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA; Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,1225,1229,"In this paper, we present a learning based approach for computing structural similarities among document images for unsupervised exploration in large document collections. The approach is based on multiple levels of content and structure. At a local level, a bag-of-visual words based on SURF features provides an effective way of computing content similarity. The document is then recursively partitioned and a histogram of codewords is computed for each partition. Structural similarity is computed using a random forest classifier trained with these histogram features. We experiment with three diverse datasets of document images varying in size, degree of structural similarity, and types of document images. Our results demonstrate that the proposed approach provides an effective general framework for grouping structurally similar document images.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.248,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628809,Structural similarity;Clustering;Unsupervised classification;Random forest,Radio frequency;Vegetation;Feature extraction;Training;NIST;Optical character recognition software;Accuracy,decision trees;document image processing;image classification;unsupervised learning,histogram features;random forest classifier;codeword histogram;recursive partitioning;content similarity computation;SURF features;bag-of-visual words;large document collections;unsupervised exploration;structural similarity computation;learning-based approach;structurally similar document images;unsupervised classification,,12,,25,,15-Oct-13,,,IEEE,IEEE Conferences
Development and Research of the Text Messages Semantic Clustering Methodology,基於PROV-O的Web內容來源方法,N. Rizun; P. Kap?anski; Y. Taranenko,"Dept. of Appl. Inf. in Manage., Gdansk Univ. of Technol., Gdansk, Poland; Dept. of Appl. Inf. in Manage., Gdansk Univ. of Technol., Gdansk, Poland; Dnipropetrovs'k Dept. of Appl. Linguistics & Methods of Teaching Foreign Languages, Alfred Nobel Univ., Dnipropetrovs'k, Ukraine",2016 Third European Network Intelligence Conference (ENIC),2-Feb-17,2016,,,180,187,"The methodology of semantic clustering analysis of customer's text-opinions collection is developed. The author's version of the mathematical models of formalization and practical realization of short textual messages semantic clustering procedure is proposed, based on the customer's text-opinions collection Latent Semantic Analysis knowledge extracting method. An algorithm for semantic clustering of the text-opinions is developed, the distinctive characteristics of which is the introduction of concepts and methods of identification point of reference in the scale of text-opinions collection closeness determination, instrument of the documents' closeness degree identification, measure of similarity between pairs of documents. The version of quantitative evaluation of the clustering results is developed. The concepts of resolving power of the method of semantic clustering and level of the clustering procedure quality are proposed. Analysis of the specific features and the effectiveness level of various distance measures is conducted.",,978-1-5090-3455-0,10.1109/ENIC.2016.034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838063,semantic clustering;text-opinions;Latent Semantic Analysis;closeness;measure of similarity,Semantics;Clustering algorithms;Measurement;Mathematical model;Symmetric matrices;Electronic mail;Algorithm design and analysis,pattern clustering;text analysis,text message semantic clustering methodology;semantic clustering analysis;customer text-opinion collection;short-textual message semantic clustering procedure;latent semantic analysis knowledge extracting;document closeness degree identification;clustering procedure quality,,4,,21,,2-Feb-17,,,IEEE,IEEE Conferences
HCSF: A hierarchical clustering algorithm based on swarm intelligence and fuzzy logic for ciphertext search,基於圖的技術在單文檔（GTEK）中提取關鍵短語,X. Zhu; C. Chen; X. Tian; J. Hu,"State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; The University of New South Wales, Canberra, Australia",2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA),23-Nov-15,2015,,,290,295,"With the demand of data security, ciphertext search is becoming a popular issue. Especially, the efficiency of ciphertext search is attracting more and more attention as the data volume is experiencing explosive growth. Traditional search methods are not suitable for current situation since all the data stored at the cloud server is encrypted. Therefore, this paper proposes a hierarchical clustering algorithm based on swarm intelligence and fuzzy logic (HCSF) to address the efficiency problem in ciphertext search. The proposed method classifies different documents into different categories by taking the advantage of swarm intelligence and fuzzy logic. Meanwhile, a split mechanism based on the maximum of documents in a cluster is proposed to deal with some clusters containing excessive documents. Furthermore, a hierarchical structure is designed to organize the documents by combining with the split mechanism. An experiment built on the 20 Newsgroups data set is conducted to assess the efficiency of HCSF. The experimental result shows the proposed method is efficient.",,978-1-4799-8389-6,10.1109/ICIEA.2015.7334127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334127,ciphertext search;efficient search;hierarchical clustering;document clustering,Servers;Cryptography;Clustering algorithms;Cloud computing;Indexes;Algorithm design and analysis;Particle swarm optimization,cloud computing;cryptography;fuzzy logic;pattern clustering;statistical analysis;swarm intelligence,hierarchical clustering algorithm;swarm intelligence;fuzzy logic;HCSF;ciphertext search;data security;cloud server,,1,,14,,23-Nov-15,,,IEEE,IEEE Conferences
Latent Dirichlet Co-Clustering,結合了本體學習和基於信息檢索的術語相似性的Web服務集群,M. M. Shafiei; E. E. Milios,"Dalhousie University, Canada; Dalhousie University, Canada",Sixth International Conference on Data Mining (ICDM'06),8-Jan-07,2006,,,542,551,"We present a generative model for simultaneously clustering documents and terms. Our model is a four-level hierarchical Bayesian model, in which each document is modeled as a random mixture of document topics , where each topic is a distribution over some segments of the text. Each of these segments in the document can be modeled as a mixture of word topics where each topic is a distribution over words. We present efficient approximate inference techniques based on Markov Chain Monte Carlo method and a moment-matching algorithm for empirical Bayes parameter estimation. We report results in document modeling, document and term clustering, comparing to other topic models, Clustering and Co-Clustering algorithms including latent Dirichlet allocation (LDA), model-based overlapping clustering (MOC), model-based overlapping co-clustering (MOCC) and information-theoretic co-clustering (ITCC).",2374-8486,978-0-7695-2701-7,10.1109/ICDM.2006.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053080,,Large scale integration;Linear discriminant analysis;Clustering algorithms;Data mining;Inference algorithms;Natural languages;Computer science;Bayesian methods;Parameter estimation;Frequency,Bayes methods;Markov processes;Monte Carlo methods;pattern clustering;text analysis,latent Dirichlet co-clustering;clustering documents;four-level hierarchical Bayesian model;document topics random mixture;text segments;Markov Chain Monte Carlo method;moment-matching algorithm;approximate inference techniques;empirical Bayes parameter estimation;document modeling;latent Dirichlet allocation;model-based overlapping coclustering;information-theoretic coclustering,,28,,22,,8-Jan-07,,,IEEE,IEEE Conferences
Cuteforce Analyzer: A Distributed Bruteforce Attack on PDF Encryption with GPUs and FPGAs,WE-LDA：用於Web服務群集的Word嵌入增強型LDA模型,B. Danczul; J. Fu?; S. Gradinger; B. Greslehner; W. Kastl; F. Wex,"R&D, Univ. of. App. Sc. Upper Austria, Hagenberg, Austria; Secure Inf. Syst., Univ. of. App. Sc. Upper Austria, Hagenberg, Austria; R&D, Univ. of. App. Sc. Upper Austria, Hagenberg, Austria; NA; R&D, Univ. of. App. Sc. Upper Austria, Hagenberg, Austria; R&D, Univ. of. App. Sc. Upper Austria, Hagenberg, Austria","2013 International Conference on Availability, Reliability and Security",7-Nov-13,2013,,,720,725,"Working on cryptanalytic tasks using a heterogeneous cluster with different types of processors (CPU, GPU, FPGA) can be an advantage over classical homogeneous clusters. In this paper we demonstrate that distributing crypt analytics tasks to different types of processors can lead to better performance than can be achieved using a single type of processor. To this end we have built a framework for the management of a heterogeneous cluster and implented a password brute forcer for password protected PDF documents. Our results show that such a framework can be implemented with little overhead in terms of performance.",,978-0-7695-5008-4,10.1109/ARES.2013.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657310,high-performance computing;HPC;cryptanalysis;GPU;FPGA;cluster,Graphics processing units;Field programmable gate arrays;Encryption;Portable document format;Electronic mail,cryptography;distributed processing;document handling;field programmable gate arrays;graphics processing units,distributed bruteforce attack;cryptanalytic task distribution;heterogeneous cluster management;password bruteforcer;PDF document protection;GPU;FPGA;PDF encryption,,2,,8,,7-Nov-13,,,IEEE,IEEE Conferences
Web Multimedia Object Clustering via Information Fusion,使用模糊層次算法對句子層次文本進行聚類,W. Lu; L. Li; T. Li; H. Zhang; J. Guo,"Pattern Recognition & Intell. Syst. Lab., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA; Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA; Pattern Recognition & Intell. Syst. Lab., Beijing Univ. of Posts & Telecommun., Beijing, China; Pattern Recognition & Intell. Syst. Lab., Beijing Univ. of Posts & Telecommun., Beijing, China",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,319,323,"Multimedia information plays an increasingly important role in humans daily activities. Given a set of web multimedia objects (images with corresponding texts), a challenging problem is how to group these images into several clusters using the available information. Previous researches focus on either adopting individual information, or simply combining image and text information together for clustering. In this paper, we propose a novel approach (Dynamic Weighted Clustering) to separate images under the ""supervision"" of text descriptions, Also, we provide a comparative experimental investigation on utilizing text and image information to tackle web image clustering. Empirical experiments on a manually collected web multimedia object (related to the events after disasters) dataset are conducted to demonstrate the efficacy of our proposed method.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065327,Multimedia Object Clustering;Image;Text;Information Fusion;Dynamic Weighting,Feature extraction;Symmetric matrices;Clustering algorithms;Accuracy;Multimedia communication;Semantics;Weight measurement,image fusion;Internet;multimedia computing;pattern clustering;text analysis,Web multimedia object clustering;text information;image information;dynamic weighted clustering;text description;image clustering;multimedia information fusion,,4,,18,,3-Nov-11,,,IEEE,IEEE Conferences
TSM. Topic Selection Method of Web Documents,使用語義分析和聚類的多文檔基於主題的摘要,M. Huang; H. Kong; S. Baek; P. Kim,"Chosun University, Korea; Chosun University, Korea; Chosun University, Korea; Chosun University, Korea",First Asia International Conference on Modelling & Simulation (AMS'07),10-Apr-07,2007,,,369,374,"In this paper, we propose a topic selection method about Web documents. The idea of our approach is to utilize an ontology structure and TF (term frequency) values of each term. For improving the performance of documents clustering, our research is strongly demanded. We process Web documents for keywords acquisition using TF values and relevancy values between terms using relations defined in WordNet. And then, we proposed the topic selection formula as we consider three kinds of cases during the topic selection. In conclusion, we demonstrate that our approach is very useful for the topic selection of documents",,0-7695-2845-7,10.1109/AMS.2007.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148688,,Ontologies;Computer aided software engineering;Computer science;Frequency measurement;Particle measurements;Internet;Natural languages;Asia;Societies,document handling;Internet;ontologies (artificial intelligence),topic selection method;Web documents;ontology structure;term frequency values;documents clustering;WordNet,,1,,9,,10-Apr-07,,,IEEE,IEEE Conferences
Clustering analysis of feature words in news text based on co-occurrence matrix,強大的文檔圖像認證,S. Liu; X. Fan; J. Chai,"School of Information Engineering Communication University of China Beijing, China; School of Information Engineering Communication University of China Beijing, China; School of Information Engineering Communication University of China Beijing, China","2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",26-Feb-18,2017,,,1,5,"In this paper, we use a new method to improve the context of a news. Especially, a new TF-IDF formula is used to calculate the weight of the feature word of News. We set a parameter W to the feature words, which takes into account the factors such as the parts of speech of feature words and inverse document frequency. The parameter is adjusted based on the K-core theory, and therefore to determine the range of feature words. Our work aims to analysis the co-occurrence strength of news feature words in a certain period of time, and obtains the cluster analysis of the co-occurrence intensity distance of news feature words. The simulation results of clustering analysis is significant and the quantitative research methods can be commonly used in bibliometrics in the news field to analyze the news content.",,978-1-5386-1937-7,10.1109/CISP-BMEI.2017.8302144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302144,Co-occurrence matrix;clustering analysis,Feature extraction;Time-frequency analysis;Business;Speech;Simulation;Clustering algorithms;Signal processing algorithms,graph theory;pattern clustering;statistical analysis;text analysis,news feature words;clustering analysis;news text;co-occurrence matrix;TF-IDF formula;parts of speech;bibliometrics;K-core theory,,,,13,,26-Feb-18,,,IEEE,IEEE Conferences
Hierarchical Clustering for Topic Analysis Based on Variable Feature Selection,基於概念的高效挖掘模型，用於增強文本聚類,J. Zeng; L. Gong; Q. Wang; C. Wu,"Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,7,,477,481,"Hierarchical topic structure can express topics in a natural way which is more reasonable for human machine interface. However, the hierarchical topic structure that is extracted by most of the topic analysis algorithms can not present a meaningful description for all subtopics in the hierarchical tree. We propose a new hierarchical clustering algorithm based on variable feature selection for each level in the hierarchical structure. The algorithm employs a top-down strategy to extract subtopics and setups the relation between topics in neighbor levels based on common documents number. The number of the levels in the hierarchical structure is determined by the frequency of the selected word feature. Experiments on a real world dataset which is collected from a news website shows that the proposed algorithm can generate more meaningful topic structure, by comparing to the current hierarchical topic clustering algorithms.",,978-0-7695-3735-1,10.1109/FSKD.2009.205,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360056,Topic structure;Hierarchical clustering;Topic analysis,Clustering algorithms;Algorithm design and analysis;Stock markets;Frequency shift keying;Fuzzy systems;Computer science;Humans;Probability distribution;Blogs;Internet,human computer interaction;pattern clustering;text analysis,topic analysis;variable feature selection;hierarchical topic structure;human machine interface;top-down strategy;subtopic extraction;word feature;hierarchical topic clustering,,1,,7,,28-Dec-09,,,IEEE,IEEE Conferences
Unsupervised Clustering with Smoothing for Detecting Paratext Boundaries in Scanned Documents,基於MATLAB分佈式計算的多目標遺傳算法的文檔聚類,A. Lucic; R. Burke; J. Shanahan,DePaul University Library; University of Colorado; DePaul University,2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL),8-Aug-19,2019,,,53,56,"Digital humanities scholars are developing new techniques of literary study using non-consumptive processing of large collections of scanned text. A crucial step in working with such collections is to separate the main text of a work from the surrounding paratext, the content of which may distort word counts, location references, sentiment scores, and other important outputs. Simple heuristic methods have been devised, but are not accurate for some texts and some methodological needs. This study describes a method for paratext detection based on smoothed unsupervised clustering. We show that this method is more accurate than simple heuristics, especially for non-fiction works, and edited works with larger amounts of paratext. We also show that a more accurate detection of paratext boundaries improves the accuracy of subsequent text processing, as exemplified by a readability metric.",,978-1-7281-1547-4,10.1109/JCDL.2019.00018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8791148,digital libraries;non consumptive analytics;text mining,Libraries;Task analysis;Metadata;Clustering methods;Feature extraction;Urban areas;Bibliographies,document image processing;humanities;pattern clustering;text analysis,paratext boundaries;subsequent text processing;scanned documents;digital humanities scholars;nonconsumptive processing;scanned text;word counts;location references;sentiment scores;paratext detection;smoothed unsupervised clustering;nonfiction works;edited works;heuristic methods,,,,19,,8-Aug-19,,,IEEE,IEEE Conferences
Image restoration of arbitrarily warped documents,Tweets聚類：自適應PSO,M. S. Brown; W. B. Seales,"Dept. of Comput. Sci., Hong Kong Univ. of Sci. & Technol., Kowloon, China; NA",IEEE Transactions on Pattern Analysis and Machine Intelligence,16-Aug-04,2004,26,10,1295,1306,"We present a framework for acquiring and restoring images of warped documents. The purpose of our restoration is to create a planar representation of a once planar document that has undergone an arbitrary and unknown rigid deformation. To accomplish this restoration, our framework acquires and flattens the 3D shape of a warped document to determine a nonlinear image transform that can correct for image distortion caused by the document's shape. Our framework is designed for use in library and museum digitization efforts where very old and badly damaged manuscripts are imaged.",1939-3539,,10.1109/TPAMI.2004.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1323798,Index Terms- Manuscript restoration;document deskewing;image restoration;cultural heritage digitization.,Image restoration;Shape;Optical distortion;Nonlinear distortion;Optical imaging;Cameras;Image processing;Software libraries;Cultural differences;Image enhancement,image restoration;document image processing;image enhancement;finite element analysis;libraries,image restoration;arbitrarily warped documents;planar document;3D warped document shape;nonlinear image transform;image distortion;museum digitization;library digitization,"Algorithms;Artifacts;Artificial Intelligence;Automatic Data Processing;Cluster Analysis;Computer Graphics;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Manuscripts as Topic;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted",82,,28,,16-Aug-04,,,IEEE,IEEE Journals
Mining Documents in a Small Enterprise Using WordStat,阿拉伯文多文檔摘要係統（AMD-SS）的比較研究,E. Udoh; J. Rhoades,"Indiana University - Purdue University; Dept. of Comput. Sci., Indiana Univ., Fort Wayne, IN",Third International Conference on Information Technology: New Generations (ITNG'06),24-Apr-06,2006,,,490,494,"Text mining is growing as an essential method of knowledge discovery from general and business documents. Although, documents viz. press releases, emails, memos, contracts, government reports and news feeds, are considered to be unstructured, they are tapped for information using text analysis techniques like feature extraction, thematic indexing, clustering and summarization. For this project, 30 representative documents from a small enterprise were collected to determine the dominant features in their activities. Based on the analysis of the document profiles generated by extracting the frequencies of certain terms, clustering and filtering on the basis of both repetitive occurrence and co-occurrence, a coherent picture of the functional relationship among large and heterogeneous lists of terms were obtained. It affords investigators an extractive interface to complex text data. This paper shows how these documents were mined using text-based WordStat software as well as the potentials, features and options of the program",,0-7695-2497-4,10.1109/ITNG.2006.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1611640,Text Mining;Documents;Cooccurrence;Clustering and WordStat.,Text analysis;Data mining;Text mining;Contracts;Government;Feeds;Feature extraction;Indexing;Frequency;Filtering,business data processing;data mining;small-to-medium enterprises;text analysis,document mining;small enterprise;WordStat;knowledge discovery;text analysis,,2,,4,,24-Apr-06,,,IEEE,IEEE Conferences
A Term Cluster Query Expansion Model Based on Classification Information in Natural Language Information Retrieval,使用聚類算法對專利文件進行技術匹配,J. W. Kang; H. Kang; M. Ko; H. S. Jeon; J. Nam,"Hankuk Acad. of Foreign Studies, Yongin, South Korea; Konkuk Univ., Seoul, South Korea; Konkuk Univ., Seoul, South Korea; Konkuk Univ., Seoul, South Korea; Konkuk Univ., Seoul, South Korea",2010 International Conference on Artificial Intelligence and Computational Intelligence,3-Dec-10,2010,2,,172,176,"A natural language information retrieval system ranks related documents according to criteria based on user query keywords and document similarities. However, many efforts have been made to make more useful query keywords because users do not use many keywords in their natural language search query when retrieving information on the Web. Because a keyword does not provide much information, however, relevance feedback is generally used to complement the weakness of general retrieval methods. This paper proposes a term cluster query expansion model based on classification information of retrieved documents. This model generates classification information from the upper ranked n documents retrieved by retrieval system. On the basis of the extracted classification information, the term cluster (m) that represents each group is generated, and then the model allows user to select term cluster that corresponds to user information needs. The query keywords are expanded by using a relevance feedback algorithm based on the selected classification information. As a result of the experiments with test collection, the retrieval effectiveness was improved by 13.2% compared to the initial query when the Rocchio method was used.",,978-1-4244-8432-4,10.1109/AICI.2010.159,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655201,Natural Language Information Retrieval;Term Cluster;Query Expansion;Classification Information;Relevance Feedback,Natural languages;History;Information retrieval;Geography;Industries;Clustering algorithms;Classification algorithms,information needs;natural language processing;pattern classification;query processing;relevance feedback,term cluster query expansion model;classification information;natural language information retrieval system;user query keywords;document similarities;natural language search query;relevance feedback algorithm;user information needs;Rocchio method,,2,,12,,3-Dec-10,,,IEEE,IEEE Conferences
Document Retrieval in Pen-Based Media Data,Twitter主題內用戶檢測模式和消息聚類研究,S. Schimke; C. Vielhauer,"Otto-von-Guericke University of Magdeburg, Germany; Otto-von-Guericke University of Magdeburg, Germany",2006 Second International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'06),26-Dec-06,2006,,,186,192,"The rise of devices with pen as the primary input option (personal digital assistants and Tablet PCs) or specially equipped ink pens for capturing scripture on paper will produce large amounts of handwritten or hand drawn documents. In this paper we describe a prototype implementation of a search system for this kind of pen-based documents without involving character recognition. Although in the last years handwriting recognition systems experienced large performance improvements, they lack in situations of drawing recognition and for people, having a very unclear script style. Our algorithm for handwriting retrieval achieves in database tests with 59 documents from twelve users a performance of about 81 % recall and precision rate",,0-7695-2625-X,10.1109/AXMEDIS.2006.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041349,,Information retrieval;Ink;Handwriting recognition;Writing;Personal digital assistants;Personal communication networks;Testing;Clustering algorithms;Sampling methods;Feature extraction,document handling;information retrieval,document retrieval;pen-based documents;handwriting retrieval,,,,10,,26-Dec-06,,,IEEE,IEEE Conferences
Edge-based connected component approach for skew correction of complex document images,自動泰國在線作業評分,J. Kumar; T. Kasar; A. G. Ramakrishnan,"Medical Intelligence and Language Engineering Laboratory, Department of Electrical Engineering, Indian Institute of Science, Bangalore, 560 012 India; Medical Intelligence and Language Engineering Laboratory, Department of Electrical Engineering, Indian Institute of Science, Bangalore, 560 012 India; Medical Intelligence and Language Engineering Laboratory, Department of Electrical Engineering, Indian Institute of Science, Bangalore, 560 012 India",TENCON 2007 - 2007 IEEE Region 10 Conference,14-Jan-08,2007,,,1,4,"Skew correction of complex document images is a difficult task. We propose an edge-based connected component approach for robust skew correction of documents with complex layout and content. The algorithm essentially consists of two steps - an 'initialization' step to determine the image orientation from the centroids of the connected components and a 'search' step to find the actual skew of the image. During initialization, we choose two different sets of points regularly spaced across the the image, one from the left to right and the other from top to bottom. The image orientation is determined from the slope between the two succesive nearest neighbors of each of the points in the chosen set. The search step finds succesive nearest neighbors that satisfy the parameters obtained in the initialization step. The final skew is determined from the slopes obtained in the 'search' step. Unlike other connected component based methods, the proposed method does not require any binarization step that generally precedes connected component analysis. The method works well for scanned documents with complex layout of any skew with a precision of 0.5 degrees.",2159-3450,978-1-4244-1271-6,10.1109/TENCON.2007.4429083,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4429083,,Nearest neighbor searches;Histograms;Robustness;Neural networks;Image edge detection;Biomedical imaging;Biomedical engineering;Laboratories;Text processing;Clustering methods,document image processing;edge detection,edge-based connected component approach;skew correction;complex document images;image orientation;search step;nearest neighbors;initialization step;binarization step;connected component analysis;scanned documents,,4,,11,,14-Jan-08,,,IEEE,IEEE Conferences
Developments in Partitioning XML Documents by Content and Structure Based on Combining Multiple Clusterings,詞語境聚類作為消除詞多義性的一種方法,G. Costa; R. Ortale,"ICAR, Rende, Italy; ICAR, Rende, Italy",2013 IEEE 25th International Conference on Tools with Artificial Intelligence,10-Feb-14,2013,,,477,482,"The combination of multiple clusterings for partitioning XML documents is proposed as a promising method, aimed to decompose the inherently difficult problem of catching structural and content relationships within an XML corpus into a number of simpler subproblems. To verify the validity of such an intuition, a new technique for partitioning XML documents is presented, in which conventional clustering techniques operating on flattened representations of individual aspects of the XML documents (that also include some rare patterns) are used to partition the available XML corpus. The effectiveness of the devised technique is revealed by a comparative empirical evaluation on benchmark XML corpora.",2375-0197,978-1-4799-2972-6,10.1109/ICTAI.2013.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735288,,XML;Encyclopedias;Electronic publishing;Internet;Vectors;Vegetation,document handling;pattern clustering;XML,XML document partitioning;document content;document structure;multiple clustering combination;XML corpus;clustering technique,,9,,33,,10-Feb-14,,,IEEE,IEEE Conferences
Administrative document segmentation based on texture approach and fuzzy clustering,基於搜索的軟件模塊集群技術：評論文章,W. Zaaboub; L. Tlig; M. Sayadi,"University of Tunis, Laboratory SIME, ENSIT, 1008, Tunis, Tunisia; University of Tunis, Laboratory SIME, ENSIT, 1008, Tunis, Tunisia; University of Tunis, Laboratory SIME, ENSIT, 1008, Tunis, Tunisia","2016 International Image Processing, Applications and Systems (IPAS)",20-Mar-17,2016,,,1,6,"The document image segmentation is an indispensable task in the document layout analysis system. This paper presents an accurate segmentation approach based on fuzzy classification for the administrative document image. The texture-based analysis works for this kind of document image are rare. And the research works on specific tasks are limited. Moreover, the texture-based segmentation methods are desired because they do not rely strongly on a priori knowledge surrounding the document. In addition, the robustness of these methods for degraded documents has been proven. For these purposes, the texture is explored in the analysis for our image type, using a fuzzy classification. The Fisher score determinate the most discriminative texture features for our segmentation: mean and variance. Our approach achieves encouraging and promising results for the detection of document zones: text, image and background. Qualitative and quantitative experiments are presented to determinate our approach performance.",,978-1-5090-1645-7,10.1109/IPAS.2016.7880128,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880128,document analysis;texture segmentation;fuzzy c-means;statistical features,Feature extraction;Image segmentation;Classification algorithms;Conferences;Databases;Morphological operations,document image processing;fuzzy set theory;image segmentation;image texture,administrative document image segmentation;fuzzy clustering;fuzzy classification;texture-based segmentation methods;Fisher score,,,,23,,20-Mar-17,,,IEEE,IEEE Conferences
Text Clustering via Term Semantic Units,使用改進的LINGO對Marathi文檔進行文本分類,L. Jing; J. Yun; J. Yu; H. Huang,"Dept. of Comput. Sci., Beijing Jiaotong Univ., Beijing, China; Dept. of Comput. Sci., Beijing Jiaotong Univ., Beijing, China; Dept. of Comput. Sci., Beijing Jiaotong Univ., Beijing, China; Dept. of Comput. Sci., Beijing Jiaotong Univ., Beijing, China",2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,1-Nov-10,2010,1,,417,420,"How best to represent text data is an important problem in text mining tasks including information retrieval, clustering, classification and etc.. In this paper, we proposed a compact document representation with term semantic units which are identified from the implicit and explicit semantic information. Among it, the implicit semantic information is extracted from syntactic content via statistical methods such as latent semantic indexing and information bottleneck. The explicit semantic information is mined from the external semantic resource (Wikipedia). The proposed compact representation model can map a document collection in a low-dimension space (term semantic units which are much less than the number of all unique terms). Experimental results on real data sets have shown that the compact representation efficiently improve the performance of text clustering.",,978-1-4244-8482-9,10.1109/WI-IAT.2010.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5616629,text clustering;compact representation;term semantic units,,pattern clustering;statistical analysis;text analysis,text clustering;term semantic units;information retrieval;compact document representation model;statistical methods;semantic information extraction;explicit semantic information;external semantic resource;Wikipedia,,,,17,,1-Nov-10,,,IEEE,IEEE Conferences
Distributed pattern matching and document analysis in big data using Hadoop MapReduce model,基於文本行提取的古代文獻分析,A. V. Ramya; E. Sivasankar,"Dept. of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, India; Dept. of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, India","2014 International Conference on Parallel, Distributed and Grid Computing",5-Feb-15,2014,,,312,317,"Sequential pattern mining and Document analysis is an important data mining problem in Big Data with broad applications. This paper investigates a specific framework for managing distributed processing in dataset pattern match and document analysis context. MapReduce programming model on a Hadoop cluster is highly scalable and works with commodity machines with integrated mechanisms for fault tolerance. In this paper, we propose a Knuth Morris Pratt based sequential pattern matching in distributed environment with the help of Hadoop Distributed File System as efficient mining of sequential patterns. It also investigates the feasibility of partitioning and clustering of text document datasets for document comparisons. It simplifies the search space and acquires a higher mining efficiency. Data mining tasks has been decomposed to many Map tasks and distributed to many Task trackers. The map tasks find the intermediate results and send to reduce task which consolidates the final result. Both theoretical analysis and experimental result with data as well as cluster of varying size shows the effectiveness of MapReduce model primarily based on time requirements.",,978-1-4799-7683-6,10.1109/PDGC.2014.7030762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030762,Distributed Processing;MapReduce Programming Model;Hadoop Cluster,Niobium;Nickel;Grid computing;Conferences;Artificial intelligence,Big Data;data mining;parallel processing;pattern clustering;pattern matching;text analysis,distributed pattern matching;document analysis;big data;Hadoop MapReduce model;MapReduce programming model;Hadoop cluster;Knuth Morris Pratt based sequential pattern matching;Hadoop Distributed File System;sequential pattern mining;text document dataset clustering;text document dataset partitioning;data mining;map tasks;task trackers,,2,,13,,5-Feb-15,,,IEEE,IEEE Conferences
Finding Core Topics: Topic Extraction with Clustering on Tweet,結合邊緣檢測分析和聚類的實時文本檢測方法,S. Kim; S. Jeon; J. Kim; Y. Park; H. Yu,"Dept. of Comput. Sci. & Eng., POSTECH, Pohang, South Korea; Dept. of Comput. Sci. & Eng., POSTECH, Pohang, South Korea; Dept. of Comput. Sci. & Eng., POSTECH, Pohang, South Korea; Dept. of Multimedia Sci., Sookmyung Womens Univ., Seoul, South Korea; Dept. of Creative IT Excellence Eng., POSTECH, Pohang, South Korea",2012 Second International Conference on Cloud and Green Computing,14-Feb-13,2012,,,777,782,"Twitter is one of the most popular microblogging services that lets users post short text called Tweet. Tweet is distinguished from conventional text data in that it is typically composed of short and informal message, and it makes typical text analysis methods do not work well. Accordingly, extracting meaningful topics from tweets brings up new challenges. In this work, we propose a simple and novel method called Core-Topic-based Clustering (CTC), which extracts topics and cluster tweets simultaneously based on the clustering principles: minimizing the inter-cluster similarity and maximizing the intra-cluster similarity. Experimental results show that our method efficiently extracts meaningful topics, and the clustering performance is better than K-means algorithm.",,978-1-4673-3027-5,10.1109/CGC.2012.120,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382905,social network;document clustering;topic extraction,Clustering algorithms;Vectors;Twitter;Internet;Encyclopedias,pattern clustering;social networking (online),topic extraction;core-topic-based clustering;tweet clustering;microblogging service;text analysis method;clustering principle;intercluster similarity;intracluster similarity;K-means clustering algorithm;Twitter,,8,,18,,14-Feb-13,,,IEEE,IEEE Conferences
Automatic script identification from images using cluster-based templates,基於灰度圖像的手寫和打印文檔的魯棒偏斜估計,J. Hochberg; L. Kerns; P. Kelly; T. Thomas,"Dept. of Comput. Res., Los Alamos Nat. Lab., NM, USA; NA; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,378,381 vol.1,"We describe a system that automatically identifies the script used in documents stored electronically in image form. The system can learn to distinguish any number of scripts. It develops a set of representative symbols (templates) for each script by clustering textual symbols from a set of training documents and representing each cluster by its centroid. ""Textual symbols"" include discrete characters in scripts such as Cyrillic, as well as adjoined characters, character fragments, and whole words in connected scripts such as Arabic. To identify a new document's script, the system compares a subset of symbols from the document to each script's templates, screening out rare or unreliable templates, and choosing the script whose templates provide the best match. Our current system, trained on thirteen scripts, correctly identifies all test documents except those printed in fonts that differ markedly from fonts in the training set.",,0-8186-7128-9,10.1109/ICDAR.1995.599017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599017,,Shape;Testing;Optical character recognition software;Natural languages;Degradation;Laboratories;Indexing;Assembly,optical character recognition;image recognition,automatic script identification;cluster-based templates;image form;representative symbols;templates;textual symbols;training documents;centroid;discrete characters;scripts;Cyrillic;character fragments;whole words;connected scripts;Arabic;fonts;training set,,16,,5,,6-Aug-02,,,IEEE,IEEE Conferences
Performance Evaluation of Semantic Approaches for Automatic Clustering of Similar Web Services,基於層次聚類的論壇主題檢測,G. Vadivelou; E. Ilavarasan,"Dept. of Comput. & Eng., Bharathiar Univ., Coimbatore, India; Pondicherry Eng. Coll., Puducherry, India",2014 World Congress on Computing and Communication Technologies,3-Apr-14,2014,,,237,242,"In current web applications, more businesses are gradually publishing their business as services over the web. This growing number of web services available within an organization and on the Web raises a new and challenging search problem: locating desired web services. Searching for web services with conventional web search engines is insufficient in this context. Automatically clustering Web Service Description Language (WSDL) files on the web into functionally similar homogeneous service groups can be seen as a bootstrapping step for creating a service search engine and, at the same time, reducing the search space for service discovery. In order to overcome some the limitations of pattern-matching approach, the proposed work uses two semantic approaches to cluster similar services. An experimental study based on an information retrieval technique known as latent semantic analysis is applied to the collection of WSDL files and the another semantic approach is based on WordNet which is a lexical database to cluster similar services, as a predecessor step to retrieve the relevant Web services for a user request by search engines. The baseline approach and the two approaches based on semantic is applied on a collection of WSDL documents consisting of to test the quality of clusters formed. As a result, WordNet based approach for clustering shows better cluster quality.",,978-1-4799-2877-4,10.1109/WCCCT.2014.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755149,Web service;pattern-matching;latent semantic analysis;WordNet;WSDL,Web services;Semantics;Feature extraction;Context;Clustering algorithms;Search engines;Standards,data mining;document handling;information retrieval;natural language processing;pattern clustering;pattern matching;search engines;semantic Web;specification languages;Web services,cluster quality;WSDL document collection;baseline approach;user request;lexical database;WordNet;latent semantic analysis;information retrieval technique;similar service clustering;pattern matching approach;service discovery;search space reduction;service search engine;bootstrapping;homogeneous service groups;Web service description language;Web search engine;search problem;organization;business as service;automatic similar Web service clustering;performance evaluation;semantic approach,,2,,24,,3-Apr-14,,,IEEE,IEEE Conferences
Signature Detection and Matching for Document Image Retrieval,基於改進STC的事件檢測算法,G. Zhu; Y. Zheng; D. Doermann; S. Jaeger,"University of Maryland, College Park; Siemens Corporate Research, Princeton; University of Maryland, College Park; CAS-MPG Partner Institute for Computational Biology, Shanghai",IEEE Transactions on Pattern Analysis and Machine Intelligence,22-Sep-09,2009,31,11,2015,2031,"As one of the most pervasive methods of individual identification and document authentication, signatures present convincing evidence and provide an important form of indexing for effective document image processing and retrieval in a broad range of applications. However, detection and segmentation of free-form objects such as signatures from clustered background is currently an open document analysis problem. In this paper, we focus on two fundamental problems in signature-based document image retrieval. First, we propose a novel multiscale approach to jointly detecting and segmenting signatures from document images. Rather than focusing on local features that typically have large variations, our approach captures the structural saliency using a signature production model and computes the dynamic curvature of 2D contour fragments over multiple scales. This detection framework is general and computationally tractable. Second, we treat the problem of signature retrieval in the unconstrained setting of translation, scale, and rotation invariant nonrigid shape matching. We propose two novel measures of shape dissimilarity based on anisotropic scaling and registration residual error and present a supervised learning framework for combining complementary shape information from different dissimilarity metrics using LDA. We quantitatively study state-of-the-art shape representations, shape matching algorithms, measures of dissimilarity, and the use of multiple instances as query in document image retrieval. We further demonstrate our matching techniques in offline signature verification. Extensive experiments using large real-world collections of English and Arabic machine-printed and handwritten documents demonstrate the excellent performance of our approaches.",1939-3539,,10.1109/TPAMI.2008.237,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4633365,Document image analysis and retrieval;signature detection and segmentation;signature matching;structural saliency;deformable shape;measure of shape dissimilarity.;Document image analysis and retrieval;signature detection and segmentation;signature matching;structural saliency;deformable shape;measure of shape dissimilarity,Image retrieval;Shape measurement;Image segmentation;Authentication;Indexing;Document image processing;Object detection;Text analysis;Production;Anisotropic magnetoresistance,handwriting recognition;image matching;image retrieval;image segmentation,signature detection;signature matching;document image retrieval;document authentication;individual identification;image processing,"Algorithms;Artificial Intelligence;Automatic Data Processing;Biometry;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique",41,,66,,26-Sep-08,,,IEEE,IEEE Journals
Unsupervised document summarization using clusters of dependency graph nodes,用於文檔圖像二值化的局部同現和對比度映射,A. El-Kilany; I. Saleh,"Faculty of Computers and Information, Cairo University, Egypt; Faculty of Computers and Information, Cairo University, Egypt",2012 12th International Conference on Intelligent Systems Design and Applications (ISDA),24-Jan-13,2012,,,557,561,"In this paper, we investigate the problem of extractive single document summarization. We propose an unsupervised summarization method that is based on extracting and scoring keywords in a document and using them to find the sentences that best represent its content. Keywords are extracted and scored using clustering and dependency graphs of sentences. We test our method using different corpora including news, events and email corpora. We evaluate our method in the context of news summarization and email summarization tasks and compare the results with previously published ones.",2164-7151,978-1-4673-5119-5,10.1109/ISDA.2012.6416598,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416598,Extractive summarization;Dependency graph;Louvain clustering;Email summarization;ROUGE,Electronic mail;Gold;Feature extraction;USA Councils;Conferences;Context,electronic mail;graph theory;information resources;information retrieval;pattern clustering;text analysis,unsupervised document summarization;dependency graph node clusters;extractive single document summarization problem;unsupervised summarization method;keyword extraction;keyword scoring;events;email corpora;news summarization;email summarization,,3,,24,,24-Jan-13,,,IEEE,IEEE Conferences
A subtractive clustering scheme for text-independent online writer identification,用於可視化文檔集合的Focus + Context技術,G. Singh; S. Sundaram,"Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, 781-039, India; Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, 781-039, India",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,311,315,"This paper proposes a text-independent writer identification framework for online handwritten text. The method utilizes an unsupervised learning scheme termed `subtractive clustering' to discover the unique writing styles of a given author. Subtractive clustering has been adopted in the literature for the problems of image segmentation and speaker identification. To the best of our knowledge, its applicability in the domain of writer identification is yet to be explored. Unlike traditional clustering techniques such as k-means and fuzzy c-means, the subtractive clustering algorithm does not rely on the initial choice of seed points. Instead, it locates the high density regions in the feature space, and this make this scheme an interesting exploration to capture the writing styles of an author (referred to as `prototypes'). The discovered prototypes from the clustering algorithm are subsequently employed to score the authorship of an unknown handwritten text. In addition, inspired from the t f-idf approach used in document retrieval, we propose a modified scoring scheme for identifying the writer. The efficacy of the algorithms are evaluated on the paragraphs from the IAM-Online Handwritten Database.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333774,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333774,,Clustering algorithms,handwritten character recognition;image retrieval;pattern clustering;text detection;unsupervised learning,text-independent online writer identification;subtractive clustering scheme;online handwritten text;unsupervised learning scheme;unique writing styles;image segmentation;speaker identification;subtractive clustering algorithm;high density region location;feature space;unknown handwritten text authorship;document retrieval;lAM-Online Handwritten Database,,11,,14,,23-Nov-15,,,IEEE,IEEE Conferences
A Pixel Labeling Approach for Historical Digitized Books,模糊概念圖及其在Web文檔聚類中的應用,M. Mehri; P. H矇roux; P. Gomez-Kr瓣mer; A. Boucher; R. Mullot,"L3I, Univ. of La Rochelle, La Rochelle, France; LITIS, Univ. of Rouen, St. ?tienne du Rouvray, France; L3I, Univ. of La Rochelle, La Rochelle, France; L3I, Univ. of La Rochelle, La Rochelle, France; L3I, Univ. of La Rochelle, La Rochelle, France",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,817,821,"In the context of historical collection conservation and worldwide diffusion, this paper presents an automatic approach of historical book page layout segmentation. In this article, we propose to search the homogeneous regions from the content of historical digitized books with little a priori knowledge by extracting and analyzing texture features. The novelty of this work lies in the unsupervised clustering of the extracted texture descriptors to find homogeneous regions, i.e. graphic and textual regions, by performing the clustering approach on an entire book instead of processing each page individually. We propose firstly to characterize the content of an entire book by extracting the texture information of each page, as our goal is to compare and index the content of digitized books. The extraction of texture features, computed without any hypothesis on the document structure, is based on two non-parametric tools: the autocorrelation function and multiresolution analysis. Secondly, we perform an unsupervised clustering approach on the extracted features in order to classify automatically the homogeneous regions of book pages. The clustering results are assessed by internal and external accuracy measures. The overall results are quite satisfying. Such analysis would help to construct a computer-aided categorization tool of pages.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.167,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628732,Historical books;texture;autocorrelation;multiresolution;homogeneity;pixel labeling;consensus clustering;clustering accuracy metrics,Correlation;Feature extraction;Image segmentation;Accuracy;Indexes;Layout,document image processing;feature extraction;history;image texture;pattern clustering;publishing;unsupervised learning,pixel labeling approach;historical collection conservation;historical digitized books;historical book page layout segmentation;homogeneous region search;texture feature extraction;texture feature analysis;unsupervised clustering approach;texture descriptors;graphic region;textual region;document structure;autocorrelation function;multiresolution analysis;computer-aided categorization tool,,8,,30,,15-Oct-13,,,IEEE,IEEE Conferences
Advanced state clustering for very large vocabulary HMM-based on-line handwriting recognition,基於自適應頁面聚類的信息檢索加權方法,A. Kosmala; D. Willett; G. Rigoll,"Dept. of Comput. Sci., Mercator Univ. Duisburg, Germany; NA; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,442,445,"The paper presents some novel methods for the introduction of context dependent hidden Markov models (HMM) to online handwriting recognition. The use of these so-called n-graphs can lead to substantially improved modeling accuracy, but requires some intelligent parameter reduction methods (state clustering). This is especially the case for the investigated very large vocabulary system, incorporating an active vocabulary of 200000 words. Switching from context independent models to context dependent models-considering the underlying vocabulary-yields in the worst case to 25000 HMMs and very poor trainability for most of the introduced models. Therefore, the conducted investigations are focused on an appropriate state clustering method which is supported by decision trees and some new self organizing approaches to generate the required trees. The presented comparison takes also the different context dependencies (left, right or both sides) into consideration.",,0-7695-0318-7,10.1109/ICDAR.1999.791819,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791819,,Vocabulary;Hidden Markov models;Handwriting recognition,handwriting recognition;handwritten character recognition;optical character recognition;pattern clustering;hidden Markov models;text analysis;decision trees,advanced state clustering;very large vocabulary HMM based online handwriting recognition;context dependent hidden Markov models;modeling accuracy;intelligent parameter reduction methods;very large vocabulary system;active vocabulary;context independent models;context dependent models;trainability;state clustering method;decision trees;self organizing approaches;context dependencies,,3,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Cluster based human action recognition using latent dirichlet allocation,聯合因素分析和潛在聚類,N. A. Deepak; R. Hariharan; U. N. Sinha,"National Aerospace Laboratories, Bangalore, India; Department of Electronics & Electrical Engineering, M. S. Ramaiah School of Advanced Studies, Bangalore, India; Distinguished Scientist, CSIR-CMMACS, NAL, Bangalore, India","2013 International conference on Circuits, Controls and Communications (CCUBE)",23-Jan-14,2013,,,1,4,"Recognizing human actions in video streams is a challenging task in the field of image processing and surveillance. This is due to variabilities in shapes, articulations of human body, cluttered background scene and occlusions. Conventional human action recognition algorithms generate coarse clusters of input videos, with lesser information regarding the cluster generation. In this paper, a mapping technique has been proposed which transforms the gait sequences into document-word template required for topic models such as Latent Dirichlet Algorithm (LDA). LDA is used to group the input videos into finer clusters. Experiments on KTH dataset [10] suggest that the proposed algorithm is effective method for recognizing human actions from the video streams.",,978-1-4799-1601-6,10.1109/CCUBE.2013.6718561,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718561,Clusters;Gait Sequence;Human Action Recognition;Latent Dirichlet Allocation,Training;Streaming media;Clustering algorithms;Histograms;Tracking;Dictionaries;Accuracy,gait analysis;image motion analysis;image recognition;image sequences;video signal processing;video streaming;video surveillance,cluster based human action recognition;Latent Dirichlet allocation;video streams;image processing;surveillance;cluster generation;mapping technique;gait sequences;document-word template;LDA;KTH dataset,,2,,10,,23-Jan-14,,,IEEE,IEEE Conferences
Post-Processing of Automatic Text Summarization for Domain-Specific Documents,基於規則的科學論文元數據提取框架,Z. Geng; J. Zhang; X. Li; J. Du; Z. Liu,"Beijing Inst. of Fashion Technol., Beijing, China; Beijing Inst. of Fashion Technol., Beijing, China; Beijing Inst. of Fashion Technol., Beijing, China; Beijing Inst. of Fashion Technol., Beijing, China; Beijing Inst. of Fashion Technol., Beijing, China",2010 International Conference on Communications and Mobile Computing,24-May-10,2010,1,,387,391,"For dissolving the imperfection that traditional text summarization method is poor to express domain-specific document meanings, techniques called post-processing to summaries are studied. The post-processing includes: elimination of redundancy; adjustment to coarse summary by clustering document paragraphs; generalization of summary sentences; filling domain-specific knowledge by means of constructing knowledge base. Finally, experiments on clothing field documents are done and software named CTS is developed based on the above theories. Compared with MS Word 2003, the experimental results show that our approaches are effective and efficient. The fruit can be introduced easily into mobile devices to explorer Web pages according its domain.",,978-1-4244-6328-2,10.1109/CMC.2010.143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471449,Automatic text summarization;Post-processing;Knowledge base,Clothing;Statistics;Thesauri;Text processing;Tree data structures;Frequency;Classification tree analysis;Mobile communication;Mobile computing;Filling,pattern clustering;text analysis,automatic text summarization post-processing;domain-specific documents;redundancy elimination;document paragraph clustering;summary sentence generalization;domain-specific knowledge;knowledge base;clothing field document;CTS software,,2,,9,,24-May-10,,,IEEE,IEEE Conferences
Document Vector Extension for Documents Classification,XML和概率XML文檔之間的最小樹編輯距離,S. Guo; N. Yao,"Computer Science and Technology, Dalian University of Technology, 12399 Dalian, Liaoning China (e-mail: guoshun@mail.dlut.edu.cn); Computer Science and Technology, Dalian University of Technology, Dalian, Liaoning China (e-mail: lucos@dlut.edu.cn)",IEEE Transactions on Knowledge and Data Engineering,,2019,PP,99,1,1,"Simple linear models, which usually learn word-level representations that are later combined to form document representations, have recently shown impressive performance. To improve the performance of document-level classification, it is crucial to explore the factors affecting the quality of the document vector. In this paper, we propose the concept of containers and further explore the properties of word containers and document containers by experiments and theoretical demonstrations. We find that the document container has a fixed capacity and that the document vector obtained by a simple average of too many word embeddings undoubtedly cannot be fully loaded by the container and will lose some semantic and syntactic information on very large text datasets. We also propose an efficient approach for document representation, using clustering algorithms to divide a document container into several subcontainers and establishing the relationship between the subcontainers. We additionally report and discuss the properties of two methods of clustering algorithms, DVEM-Kmeans and DVEM-Random, on large text datasets by sentiment analysis and topic classification tasks. Compared to simple linear models, the results show that our models outperform the existing state-of-the-art in generating high-quality document representations for document-level classification relatedness tasks. Our approaches can also be introduced to other models based on neural networks, such as convolutional neural networks, recurrent neural networks and generative adversarial networks, in supervised or semisupervised settings.",1558-2191,,10.1109/TKDE.2019.2961343,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938709,word container;document container;clustering algorithms;DVEM-Kmeans;DVEM-Random,Containers;Task analysis;Windows;Neural networks;Training;Semantics;Clustering algorithms,,,,,,,,23-Dec-19,,,IEEE,IEEE Early Access Articles
FIC WAN frequent itemset clustering of web articles by analyzing the article neighborhood,用於文檔表示的數據挖掘中的多種聚類方法和算法,T. Ku?e?ka; D. Chud獺; P. Sl獺de?ek,"Slovak University of Technology, Faculty of Informatics and Information Technologies, Bratislava, Slovakia; Slovak University of Technology, Faculty of Informatics and Information Technologies, Bratislava, Slovakia; Slovak University of Technology, Faculty of Informatics and Information Technologies, Bratislava, Slovakia",2013 IEEE 14th International Symposium on Computational Intelligence and Informatics (CINTI),9-Jan-14,2013,,,509,514,"Document clustering is a process of organizing text data into clusters where a cluster usually represents a group of topic related documents. Most effective text clustering approaches are based on frequent itemsets. A popular algorithm that uses this approach is FIHC (Frequent Itemset-based Hierarchical Clustering). In recent years, many modifications have been made to this algorithm. In this paper we focus on clustering web articles which represent a special type of text data. They contain hyperlinks through which they are linked with other articles on the web. We propose a FICWAN algorithm which is a modification of FIHC. FICWAN is especially suited for web data. We show that by considering the neighborhood of a web article and its HTML tags and CSS we are able to significantly improve the quality of created clusters. We experimented with our approach on several corpuses and the results clearly outperformed FIHC.",,978-1-4799-0197-5,10.1109/CINTI.2013.6705250,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705250,,Clustering algorithms;HTML;Cascading style sheets;Itemsets;Partitioning algorithms;Web pages;Informatics,data mining;information retrieval;Internet;pattern clustering;text analysis,FICWAN frequent itemset clustering;Web articles clustering;article neighborhood;document clustering;topic related document;FIHC;frequent itemset-based hierarchical clustering;hyperlinks;HTML tags;CSS,,,,11,,9-Jan-14,,,IEEE,IEEE Conferences
Index compression through document reordering,稀疏子空間聚類：算法，理論和應用,D. Blandford; G. Blelloch,"Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA",Proceedings DCC 2002. Data Compression Conference,7-Aug-02,2002,,,342,351,"An important concern in the design of search engines is the construction of an inverted index. An inverted index, also called a concordance, contains a list of documents (or posting list) for every possible search term. These posting lists are usually compressed with difference coding. Difference coding yields the best compression when the lists to be coded have high locality. Coding methods have been designed to specifically take advantage of locality in inverted indices. Here, we describe an algorithm to permute the document numbers so as to create locality in an inverted index. This is done by clustering the documents. Our algorithm, when applied to the TREC ad hoc database (disks 4 and 5), improves the performance of the best difference coding algorithm we found by fourteen percent. The improvement increases as the size of the index increases, so we expect that greater improvements would be possible on larger datasets.",1068-0314,0-7695-1477-4,10.1109/DCC.2002.999972,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999972,,Data compression;Chromium,search engines;data compression;database indexing;document handling,index compression;document reordering;search engines;inverted index;concordance;posting lists;difference coding;locality;document clustering;TREC ad hoc database,,33,,11,,7-Aug-02,,,IEEE,IEEE Conferences
Detection of a new class in a huge corpus of text documents through semi-supervised learning,GDClust：基於圖的文檔聚類技術,D. S. Guru; M. Suhil; H. S. Gowda; L. N. Raju,"Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysore, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysore, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysore, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysore, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",3-Nov-16,2016,,,494,499,This paper poses a new problem of detecting an unknown class present in a text corpus which has huge amount of unlabeled samples but a very small quantity of labeled samples. A simple yet efficient solution has also been proposed by modifying conventional clustering technique to demonstrate the scope of the problem for further research. A novel way to estimate cluster diameter is proposed which in turn has been used as a measure to estimate the degree of dissimilarity between two clusters. The main idea of the model is to arrive at a cluster of unlabeled text samples which is far away from any of the labeled clusters guided by few rules such as diameter of the cluster and dissimilarity between pair of clusters. This work is first of its kind in the literature and has tremendous applications in text mining tasks. In fact the model proposed is a general framework which can be applied onto any application which necessarily involves identification of unseen classes in a semi-supervised learning environment. The model has been studied with extensive empirical analysis on different text datasets created from the benchmarking 20Newsgroups dataset. The results of the experimentation have revealed the capabilities of the proposed approach and the possibilities for future research.,,978-1-5090-2029-4,10.1109/ICACCI.2016.7732094,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732094,Text Categorization;Semi-Supervised Learning;Clustering;Unknown Class Detection;Text Representation;Term Weighting,Text categorization;Clustering algorithms;Informatics;Semisupervised learning;Classification algorithms;Computational modeling;Merging,data mining;learning (artificial intelligence);pattern clustering;text analysis,class detection;text document corpus;semisupervised learning;labeled samples;clustering technique;cluster diameter estimation;dissimilarity degree estimation;unlabeled text sample cluster;labeled clusters;text mining tasks;general framework;20Newsgroups dataset,,,,17,,3-Nov-16,,,IEEE,IEEE Conferences
A term-based algorithm for hierarchical clustering of Web documents,聚類指導的稀疏結構學習的無監督特徵選擇,A. Schenker; M. Last; A. Kandel,"Dept. of Comput. Sci. & Eng., Univ. of South Florida, Tampa, FL, USA; NA; NA",Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),7-Aug-02,2001,,,3076,3081 vol.5,"In this paper we introduce the novel class hierarchy construction algorithm (CHCA) in order to create hierarchical clusterings of Web documents. Unlike most clustering methods, CHCA operates on nominal data (the words occurring in each document) and it differs from other hierarchical clustering techniques in that it uses the object-oriented concept of inheritance to create the parent/child relationship between clusters. A prototype system has been developed using CHCA to create cluster hierarchies from web search results returned by conventional search engines. CHCA, without any guidance, creates term-based clusters from the contents of the retrieved pages and assigns each page to a cluster; the clusters correspond to topics and sub-topics in the investigated domain. The performance of our system is compared with a similar web search clustering system (Vivisimo).",,0-7803-7078-3,10.1109/NAFIPS.2001.943719,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=943719,,Clustering algorithms;Web search;Data mining;Natural language processing;Systems engineering and theory;Search engines;Content based retrieval;Knowledge representation;Information retrieval;Web mining,information resources;pattern clustering;inheritance,term-based algorithm;hierarchical clustering;World Wide Web documents;class hierarchy construction algorithm;CHCA;object-oriented inheritance;parent/child relationship,,3,,24,,7-Aug-02,,,IEEE,IEEE Conferences
Autonomous Document Cleaning?A Generative Approach to Reconstruct Strongly Corrupted Scanned Texts,MMM誘導的k成員共聚用於共現信息的k匿名化,Z. Dai; J. L羹cke,"Department of Computer Science, University of Sheffield, Sheffield, South Yorkshire, United Kingdom; Cluster of Excellence Hearing4all and the School of Medicine and Health Sciences, University of Oldenburg, Oldenburg, Germany",IEEE Transactions on Pattern Analysis and Machine Intelligence,1-Sep-14,2014,36,10,1950,1962,"We study the task of cleaning scanned text documents that are strongly corrupted by dirt such as manual line strokes, spilled ink, etc. We aim at autonomously removing such corruptions from a single letter-size page based only on the information the page contains. Our approach first learns character representations from document patches without supervision. For learning, we use a probabilistic generative model parameterizing pattern features, their planar arrangements and their variances. The model's latent variables describe pattern position and class, and feature occurrences. Model parameters are efficiently inferred using a truncated variational EM approach. Based on the learned representation, a clean document can be recovered by identifying, for each patch, pattern class and position while a quality measure allows for discrimination between character and non-character patterns. For a full Latin alphabet we found that a single page does not contain sufficiently many character examples. However, even if heavily corrupted by dirt, we show that a page containing a lower number of character types can efficiently and autonomously be cleaned solely based on the structural regularity of the characters it contains. In different example applications with different alphabets, we demonstrate and discuss the effectiveness, efficiency and generality of the approach.",1939-3539,,10.1109/TPAMI.2014.2313126,Deutsche Forschungsgemeinschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777544,Probabilistic generative models;document cleaning;scanned text;unsupervised learning;expectation maximization;variational approximation;expectation truncation,Vectors;Computational modeling;Data models;Approximation methods;Probabilistic logic;Visualization;Histograms,document image processing;expectation-maximisation algorithm;feature extraction;image reconstruction;image representation;learning (artificial intelligence);natural language processing;probability;text analysis;variational techniques,autonomous document cleaning;strongly corrupted scanned texts reconstruction;scanned text documents cleaning;manual line strokes;spilled ink;single letter-size page;character representations;document patches;learning;probabilistic generative model;pattern features;planar arrangements;latent variables;pattern position;pattern class;feature occurrences;model parameters;truncated variational EM approach;quality measure;character discrimination;noncharacter patterns;full Latin alphabet;character types;structural regularity,,8,,50,,24-Mar-14,,,IEEE,IEEE Journals
Research and Application of Event Finding based on massive Internet Imagine Tag,使用模糊格神經計算（FLN）在結構化數據域中進行聚類和分類,X. Zhao,"Guidaojiaotong Polytech. Instituteline, Shenyang, China",2014 International Conference on Computational Intelligence and Communication Networks,26-Mar-15,2014,,,652,656,"System mainly studies mass events found images from the Internet, this paper focuses on the data label document Flickr to quantify. This paper also implements single-pass clustering algorithm using traditional text clustering. In this paper, achieve three strategies of single-pass clustering algorithm, and analyze and compare the three strategies. Different document order in the event of the discovery process requires traditional events found single-pass clustering algorithm. This paper implements a hot event with a buffer to find a way.",,978-1-4799-6929-6,10.1109/CICN.2014.145,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065565,Flickr;single-pass clustering algorithm Buffer;Topic Detection,Clustering algorithms;Vectors;Algorithm design and analysis;Cost function;Fault detection;Event detection;Internet,Internet;pattern clustering;social networking (online);text analysis,event finding;massive-Internet imagine tag;mass events;data label document;Flickr;single-pass clustering algorithm;text clustering;document order;event discovery process,,,,7,,26-Mar-15,,,IEEE,IEEE Conferences
A Distributed Calculation Scheme for Contents Categorization,用於軟文檔聚類的改進模糊ART,M. Kohana; H. Sakaji; A. Kobayashi; S. Okamoto,"Dept. of Comput. & Inf. Sci., Seikei Univ., Musahino, Japan; Dept. of Comput. & Inf. Sci., Seikei Univ., Musahino, Japan; Dept. of Comput. Sci. & Eng., Toyohashi Univ. of Technol., Toyohashi, Japan; Dept. of Comput. & Inf. Sci., Seikei Univ., Musahino, Japan",2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA),8-May-17,2017,,,614,620,"This paper describes a distributed calculation scheme for scoring relationship among documents. This scheme categorizes documents by using an algorithm which calculates a score value for the relationship between a category and a word in a document. The longer calculation time becomes when increasing the number of documents. Therefore, our scheme uses multiple machines. A master node divides a document set into several subsets, and it distributes them to each calculation nodes. Using this distributed calculation makes the calculation time short, and also makes the memory usage low.",1550-445X,978-1-5090-6029-0,10.1109/AINA.2017.99,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7920965,Distributed system;Parallel calculation;Text mining;Document clustering;Document categorization,Videos;Estimation;Electronic mail;Clustering algorithms;Computers;Information science;Text mining,classification;data mining;parallel processing;pattern clustering;text analysis,content categorization;distributed calculation scheme;score value calculates;distributed calculation;calculation time;parallel calculation;text mining;document clustering;document categorization,,1,,16,,8-May-17,,,IEEE,IEEE Conferences
Multi-lingual text clustering method using bilingual semantic correspondence analysis,一種新的基於密度的二元數據集聚類算法,L. Yuansheng,"Network Information Management, Jiangxi University of Finance and Economics, Nanchang, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",24-Oct-16,2016,,,1216,1221,"Among most existing methods of multi-lingual text clustering, the dictionary-based approaches did not effectively deal with ambiguity and polysemy problem, the approaches based on machine translation inevitably introduced noise, Cross-lingual latent semantic indexing (CL-LSI) does not fully take into account bilingual semantic relationship. The paper proposes a new method based on Bilingual Semantic Correspondence Analysis (BiSCAN). In the new method, a bilingual document-pair are looked as two views for the same semantic content and a single topic space is created for each language by using partial least squares (PLS) method in order to capture statistical dependencies of bilingual semantic correspondence. The topic spaces are merged for document clustering task. Experimental results on Chinese-English stories collected from the Hong Kong government news website show that performance of the presented method is over or near to mono-lingual clustering in the original feature spaces and significantly better than CL-LSI.",,978-1-5090-4093-3,10.1109/FSKD.2016.7603352,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603352,Multi-lingual text clustering;bilingual semantic corresponding analysis;partial least squares,Semantics;Correlation;Clustering algorithms;Matrix decomposition;Dictionaries;Indexing;Probabilistic logic,language translation;natural language processing;pattern clustering;statistical analysis;text analysis;Web sites,bilingual semantic correspondence analysis;multilingual text clustering method;dictionary-based approach;machine translation;BiSCAN;bilingual document-pair;semantic content;partial least squares method;statistical dependencies;document clustering task;Chinese-English stories;Hong Kong government news Web site,,,,20,,24-Oct-16,,,IEEE,IEEE Conferences
Ontology based fuzzy classification of web documents for semantic information retrieval,通過自組織地圖自動生成文本文檔的類別,K. Joshi; A. Verma; A. Kandpal; S. Garg; R. Chauhan; R. H. Goudar,"Department of Information Technology, GEU Dehradun, India; Department of Computer Science and Engineering, GEU Dehradun, India; Department of Information Technology, GEU Dehradun, India; Department of Information Technology, GEU Dehradun, India; Department of Computer Science and Engineering, GEU Dehradun, India; Department of Computer Science and Engineering, GEU Dehradun, India",2013 Sixth International Conference on Contemporary Computing (IC3),26-Sep-13,2013,,,1,5,Several approaches have been introduced in the field of information retrieval. Although these approaches are effective but sometimes they are not able to provide accurate information to the user. In this paper an ontology based approach of information retrieval has been presented that uses fuzzy set of various documents for a specific domain. An algorithm for fuzzy based classification of web documents is proposed to create semantic index. The proposed algorithm differs from others as: it utilizes K-means clustering algorithm to find semantically similar terms and domain ontology as well. The retrieved results would always be semantic as they are limited to a particular threshold of classified range.,,978-1-4799-0192-0,10.1109/IC3.2013.6612160,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612160,Information Retrieval;Ontology;Semantic search;Query Expansion;K-means clustering;Fuzzy Sets,Semantics;Ontologies;Indexes;Classification algorithms;Clustering algorithms;Crawlers,document handling;fuzzy set theory;ontologies (artificial intelligence);pattern classification;pattern clustering;query processing;semantic Web,ontology based fuzzy classification;Web documents;semantic information retrieval;fuzzy set;semantic index;K-means clustering algorithm;query expansion,,2,,20,,26-Sep-13,,,IEEE,IEEE Conferences
Clustering Algorithms in Biomedical Research: A Review,潛在語義分析在信息檢索中的應用研究,R. Xu; D. C. Wunsch,"Applied Computational Intelligence Laboratory, Department of Electrical & Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA; Applied Computational Intelligence Laboratory, Department of Electrical & Computer Engineering, Missouri University of Science and Technology, Rolla, MO, USA",IEEE Reviews in Biomedical Engineering,6-Dec-10,2010,3,,120,154,"Applications of clustering algorithms in biomedical research are ubiquitous, with typical examples including gene expression data analysis, genomic sequence analysis, biomedical document mining, and MRI image analysis. However, due to the diversity of cluster analysis, the differing terminologies, goals, and assumptions underlying different clustering algorithms can be daunting. Thus, determining the right match between clustering algorithms and biomedical applications has become particularly important. This paper is presented to provide biomedical researchers with an overview of the status quo of clustering algorithms, to illustrate examples of biomedical applications based on cluster analysis, and to help biomedical researchers select the most suitable clustering algorithms for their own applications.",1941-1189,,10.1109/RBME.2010.2083647,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5594620,Biomedical engineering;clustering algorithms;evolutionary computation;neural networks;unsupervised learning,Clustering algorithms;Algorithm design and analysis;Data analysis;Gene expression;Magnetic resonance imaging;Text analysis,biomedical MRI;genomics;medical computing;pattern clustering;statistical analysis;ubiquitous computing,clustering algorithm;biomedical research;gene expression data analysis;genomic sequence analysis;biomedical document mining;MRI image analysis;biomedical applications,Algorithms;Biomedical Research;Cluster Analysis;Computational Biology;Gene Expression Profiling,168,,315,,4-Oct-10,,,IEEE,IEEE Journals
Text Line Detection in Document Images: Towards a Support System for the Blind,基於共現的氣味描述符聚類，用於預測結構與氣味的關係,B. T. Nassu; R. Minetto; L. E. S. d. Oliveira,"Fed. Univ. of Technol. Parana, Curitiba, Brazil; Fed. Univ. of Technol. Parana, Curitiba, Brazil; NA",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,638,642,"We introduce a novel approach for text line detection in document images, keeping in mind the requirements of a portable text recognition system designed to support the blind. Challenges include shadows, cluttered backgrounds, and perspective distortion. Different from previous approaches, the proposed method does not segment the image. A text model is created by clustering SIFT features extracted from positive and negative examples. Text regions are located by matching the features extracted from the input image to the clusters in the text model. Regions around the correspondences are then analyzed, and text lines are identified based on features such as gradients and histogram distribution. Experimental results show that our approach outperforms a state-of-the-art text detector in a text/non-text classification task.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628696,Text Detection;Document Image Processing,Feature extraction;Image segmentation;Text recognition;Histograms;Detectors;Text analysis;Training,document image processing;feature extraction;handicapped aids;image classification;image matching;object detection,text line detection;document image;blind support system;portable text recognition system;shadows;cluttered backgrounds;perspective distortion;SIFT feature clustering;scale-invariant feature transform;feature extraction;feature matching;gradients;histogram distribution;classification task,,2,,9,,15-Oct-13,,,IEEE,IEEE Conferences
Returning Clustered Results for Keyword Search on XML Documents,基於Web的社交網絡搜索系統框架,X. Liu; C. Wan; L. Chen,"Jiangxi University of Finance and Economics, Nanchang; Jiangxi University of Finance and Economics, Nanchang; The Hong Kong University of Science and Technology, Hong Kong",IEEE Transactions on Knowledge and Data Engineering,20-Oct-11,2011,23,12,1811,1825,"Keyword search is an effective paradigm for information discovery and has been introduced recently to query XML documents. In this paper, we address the problem of returning clustered results for keyword search on XML documents. We first propose a novel semantics for answers to an XML keyword query. The core of the semantics is the conceptually related relationship between keyword matches, which is based on the conceptual relationship between nodes in XML trees. Then, we propose a new clustering methodology for XML search results, which clusters results according to the way they match the given query. Two approaches to implement the methodology are discussed. The first approach is a conventional one which does clustering after search results are retrieved; the second one clusters search results actively, which has characteristics of clustering on the fly. The generated clusters are then organized into a cluster hierarchy with different granularities to enable users locate the results of interest easily and precisely. Experimental results demonstrate the meaningfulness of the proposed semantics as well as the efficiency of the proposed methods.",1558-2191,,10.1109/TKDE.2011.183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989812,XML keyword search;search results clustering;cluster hierarchy.,XML;Semantics;Cloud computing;Keyword search;Clustering algorithms;Databases;Pattern matching;information retrieval;Search methods,query processing;XML,keyword search;XML document query;information discovery;XML keyword query;XML trees;eXtensible Markup Language,,15,,31,,18-Aug-11,,,IEEE,IEEE Journals
Chaotic Artificial Bee Colony for Text Clustering,基於共現矩陣的新聞文本聚類分析,K. K. Bharti; P. K. Singh,"Comput. Intell. & DataMining Res. Lab., ABV-Indian Inst. of Inf. Technol. & Manage. Gwalior, Gwalior, India; Comput. Intell. & DataMining Res. Lab., ABV-Indian Inst. of Inf. Technol. & Manage. Gwalior, Gwalior, India",2014 Fourth International Conference of Emerging Applications of Information Technology,2-Mar-15,2014,,,337,343,"Text clustering is widely used for creating clusters of the digital documents. Selection of cluster centers plays an important role in creating clusters of the documents. In this paper, we use artificial bee colony algorithm (hereinafter referred to as ABC) to select an appropriate cluster centers for text documents. The ABC is a swarm intelligence based algorithm inspired by intelligent foraging behavior of real honey bees. The ABC provides good exploration of the search space at a cost of exploitation. To address this issue, we use the chaotic map as a local search paradigm to improve its exploitation capability. The proposed algorithm chaotic artificial bee colony (hereinafter referred to as ChABC) is tested on two benchmark text datasets namely Reuters-21,578 and Classic4, and the obtained results are compared with k-means clustering, ABC, and a recent variant of ABC namely gbest guided ABC (hereinafter referred to as GABC). The comparisons show that the ChABC offers the better clustering quality and faster convergence among all the competitive algorithms in all cases.",,978-1-4799-4272-5,10.1109/EAIT.2014.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052068,Text clustering;artificial bee colony algorithm;chaotic local search,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Particle swarm optimization;Sociology;Statistics;Equations,competitive algorithms;convergence;data mining;pattern clustering;search problems;swarm intelligence;text analysis,"chaotic artificial bee colony;text clustering quality;digital documents;appropriate cluster centers;swarm intelligence-based algorithm;intelligent foraging behavior;search space;chaotic map;local search paradigm;exploitation capability;ChABC;Reuters-21,578;Classic4;k-means clustering;convergence",,6,,20,,2-Mar-15,,,IEEE,IEEE Conferences
Web-Scale Image Clustering Revisited,基於模糊邏輯的表示和自組織映射的網頁聚類,Y. Avrithis; Y. Kalantidis; E. Anagnostopoulos; I. Z. Emiris,NA; NA; NA; NA,2015 IEEE International Conference on Computer Vision (ICCV),18-Feb-16,2015,,,1502,1510,"Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.",2380-7504,978-1-4673-8391-2,10.1109/ICCV.2015.176,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410533,,Quantization (signal);Visualization;Distortion;Artificial neural networks;Search problems;Probabilistic logic;Metadata,approximation theory;data mining;document handling;image processing;minimisation;pattern clustering;search problems,IQ-means;data point quantization;multiindex based inverted search;distortion minimization;dynamic variant design;inverted-quantized k-means;approximate k-means variants;principled clustering methods;seed growing heuristics;hashing;seed detection;document mining;document clustering;duplicate detection;Web-scale image clustering,,12,,39,,18-Feb-16,,,IEEE,IEEE Conferences
Efficient search engine approach for measuring similarity between words: Using page count and snippets,通過對Web搜索信息需求的觀點進行聚類來概述查詢關鍵字的知識,P. Murugesan; K. Malathi,"PG student computer science and engineering, Indian Institute of Information Technology, Srirangam Tiruchirappalli; Deportment of Information Technology, Indian Institute of Information Technology, Srirangam Tiruchirappalli",2015 Online International Conference on Green Engineering and Technologies (IC-GET),19-Apr-16,2015,,,1,5,"Web mining involve activities such as document clustering, community mining etc., to be performed on web. Such tasks need measuring semantic similarity between word. This helps in performing web mining activities easily in many applications. The accurate measures of semantic similarity between any two words is the difficult task. A new approach to measure similarity between words is based on text snippets and page count. These two measures are taken from the results of a search engine like Google. The lexical patterns are extracted from text snippets and word co-occurrence measures are defined using page count. The results of these two are combined. Moreover, the pattern clustering and pattern extraction algorithm are used to find various relationships between any two given words. Support Vector Machines is used to optimize the result. The empirical results reveal that the techniques are finding the best results that can be compared with human ratings and accuracy in web mining activity. Semantic similarity refers to the concept by which a set of document or words within the document are assigned a weight based on their meaning. The accurate measurement of such similarity plays an important role in Natural language Processing.",,978-1-4673-9781-0,10.1109/GET.2015.7453830,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453830,Community mining;pattern clustering;text snippets;page count,Semantics;Search engines;Engines;Web search;Pattern clustering;Clustering algorithms;Mutual information,data mining;information retrieval;natural language processing;pattern clustering;search engines;string matching;support vector machines;word processing,search engine approach;word similarity measurement;document clustering;semantic similarity;Web mining activities;text snippets;page count;search engine;Google;word cooccurrence measures;pattern extraction algorithm;pattern clustering algorithm;support vector machines;natural language processing,,,,11,,19-Apr-16,,,IEEE,IEEE Conferences
Documents Distribution Strategy Based on Queuing Model and Chaotic Searching Algorithm in Web Server Cluster,通過智能分層聚類自動建立文檔索引：一種新穎的方法,Z. Xiong; C. Guo,"Shantou University, China; Wuhan University, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",13-Aug-07,2007,3,,1113,1118,"For a large website adopting Web server cluster, how to organize and distribute web documents is a challenging problem. In this paper, we propose a strategy to distribute web documents in web server cluster, whose aim is to reduce system 's average response time. The strategy uses queuing model to analyze cluster system, and translates the document distribution problem into a 0-1 integer programming problem. Aimed at such kind of 0-1 integer programming problem, we propose a chaotic searching algorithm to solve it. The chaotic searching algorithm lets many isolated chaotic variables search in their tracks, so the corresponding 0-1 distribution matrix built by these variables can experience every possible distribution, thereby it can find the global optimal solution in enough long time. Simulation tests show that the chaotic searching algorithm can find the global optimal solution.",,978-0-7695-2909-7,10.1109/SNPD.2007.419,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288016,,Distribution strategy;Chaos;Clustering algorithms;Web server;Costs;Delay;File systems;Network servers;Linear programming;Throughput,chaos;document handling;integer programming;Internet;matrix algebra;queueing theory,documents distribution strategy;queuing model;chaotic searching algorithm;Web server cluster;average response time;integer programming problem;chaotic variables search;distribution matrix,,1,,9,,13-Aug-07,,,IEEE,IEEE Conferences
Preserving Patterns in Bipartite Graph Partitioning,用於自適應彩色圖像分割的序列化無監督分類器：在數字化古代手稿中的應用,T. Hu; C. Qu; C. L. Tan; S. Y. Sung; W. Zhou,"DongGuan U. of Technology; DongGuan U. of Technology; National U. of Singapore, Singapore; South Texas College, USA; Rutgers University, USA",2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06),19-Dec-06,2006,,,489,496,"This paper describes a new bipartite formulation for word-document co-clustering such that hyperclique patterns, strongly affiliated documents in this case, are guaranteed not to be split into different clusters. Our approach for pattern preserving clustering consists of three steps: mine maximal hyperclique patterns, form the bipartite, and partition it. With hyperclique patterns of documents preserved, the topic of each cluster can be represented by both the top words from that cluster and the documents in the patterns, which are expected to be more compact and representative than those in the standard bipartite formulation. Experiments with real-world datasets show that, with hyperclique patterns as starting points, we can improve the clustering results in terms of various external clustering criteria. Also, the partitioned bipartite with preserved topical sets of documents naturally lends itself to different functions in search engines",2375-0197,0-7695-2728-0,10.1109/ICTAI.2006.97,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031935,,Bipartite graph;Search engines;Clustering algorithms;Partitioning algorithms;Chaos;Educational institutions;Pattern analysis;Joining processes;Artificial intelligence;Computational efficiency,document handling;graph theory;pattern clustering,pattern preservation;bipartite graph partitioning;bipartite formulation;word document coclustering;pattern preserving clustering;maximal document hyperclique pattern;clustering criteria;document topical set;search engine,,2,,33,,19-Dec-06,,,IEEE,IEEE Conferences
Constructing features for document classification by using temporal patterns of term usages,基於Hypergraph的文檔分類：頻繁項集與Hypercliques,H. Abe; S. Tsumoto,"Department of Medical Informatics, Shimane University, School of Medicine, 89-1 Enya-cho Izumo Shimane, 6938501, Japan; Department of Medical Informatics, Shimane University, School of Medicine, 89-1 Enya-cho Izumo Shimane, 6938501, Japan",2011 IEEE International Conference on Granular Computing,5-Jan-12,2011,,,30,36,"In document classification method by using appeared words as features, it is important to determine keywords for the features to characterize each document. However, conventional methods select the keywords based on their frequency or/and particular importance index such as tf-idf, and cut-off the other appeared words by using a threshold value. This omits remaining information such as rare combinations of the appeared words and time dependent differences of their usages. In this paper, we present the availability of the features based on temporal patterns of the overall words and phrases for temporally published documents in one domain. Thus, the documents are characterized by the temporal patterns of one or more importance indices for considering temporal differences of the overall term usages. In the experiment, we compare document classification results of two sets of bibliographical documents on the time dependency by using the two types of the feature set. For an exploratory class labels, we show the availability for obtaining classification rules that mention the relationship between the class and the important temporal patterns for the prediction.",,978-1-4577-0371-3,10.1109/GRC.2011.6122563,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122563,Feature Construction;Document Classification;Temporal Clustering;Text Mining,Indexes;Vectors;Learning systems;Machine learning;Cognition;Frequency conversion;Availability,classification;document handling,document classification;temporal patterns;term usages;appeared words;threshold value;temporally published documents;bibliographical documents;time dependency;classification rules,,,,12,,5-Jan-12,,,IEEE,IEEE Conferences
Ensemble Similarity Measures for Clustering Terms,動態和自適應自組織映射應用於高維大規模文本聚類,A. Ittoo; L. Maruster,"Fac. of Econ. & Bus., Univ. of Groningen, Groningen, Netherlands; Fac. of Econ. & Bus., Univ. of Groningen, Groningen, Netherlands",2009 WRI World Congress on Computer Science and Information Engineering,24-Jul-09,2009,4,,315,319,"Clustering semantically related terms is crucial for many applications such as document categorization, and word sense disambiguation. However, automatically identifying semantically similar terms is challenging. We present a novel approach for automatically determining the degree of relatedness between terms to facilitate their subsequent clustering. Using the analogy of ensemble classifiers in machine learning, we combine multiple techniques like contextual similarity and semantic relatedness to boost the accuracy of our computations. A new method, based on Yarowskypsilas word sense disambiguation approach, to generate high-quality topic signatures for contextual similarity computations, is presented. A technique to measure semantic relatedness between multi-word terms, based on the work of Hirst and St. Onge is also proposed. Experimental evaluation reveals that our method outperforms similar related works. We also investigate the effects of assigning different importance levels to the different similarity measures based on the corpus characteristics.",,978-0-7695-3507-4,10.1109/CSIE.2009.764,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5171010,text clustering;semantic relatetdness;natural language processing,Ontologies;Position measurement;Computer science;Application software;Machine learning;Glass;Pressing;Length measurement;Mutual information,learning (artificial intelligence);pattern classification;pattern clustering;text analysis,ensemble similarity measure;semantically related term clustering;document categorization;word sense disambiguation;ensemble classifier;machine learning;contextual similarity;high-quality topic signature;text analysis,,2,,9,,24-Jul-09,,,IEEE,IEEE Conferences
Multi-document abstractive summarization based on predicate argument structure,基於YOLO的表格檢測方法,S. Alshaina; A. John; A. G. Nath,"Dept. of Computer Science and Engineering, T.K.M College of Engineering, Kollam, India; Dept. of Computer Science and Engineering, T.K.M College of Engineering, Kollam, India; Dept. of Computer Science and Engineering, T.K.M College of Engineering, Kollam, India","2017 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)",2-Nov-17,2017,,,1,6,"The proposed work is based on abstractive summarization which is the division of text summarization. It developed a summary of the multi-document using the semantic relationship between the input documents rather than what we get exactly from the input document. It is very necessary because of the difficulty of generating abstract manually and also a challenging task. In our system, summary is generated based on the predicate argument structure of the sentences. Semantic role labeling is utilized to obtain the predicate argument structure. Main steps involved in the proposed system: Predicate argument structure of the sentences is extracted to represent text semantically as the first step. Next, it group semantically similar predicate argument structure using hybrid approach of K-mean and agglomerative hierarchical clustering by utilizing semantic similarity measures. K-mean is selected due to its run time efficiency and agglomerative hierarchical clustering is selected due to its quality. We extract twelve features of the predicate argument and feature selection is made randomly in the optimization stage. Then top ranked predicate argument structures taken from the optimization phase. Sentences for summary is selected from top ranked predicate argument structure by utilizing language generation. The Proposed study is evaluated by Document Understanding Conference 2002 (DUC 2002). We observed that the proposed work saved the computation time and provides better result than existing systems.",,978-1-5386-3864-4,10.1109/SPICES.2017.8091339,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091339,Text summarization;abstractive summarization;semantic role labeling;predicate argument structure;genetic algorithm;language generation,Semantics;Feature extraction;Ontologies;Optimization;Labeling;Tools;Clustering algorithms,natural language processing;optimisation;pattern clustering;text analysis,text summarization;input documents;semantic role labeling;K-mean clustering;agglomerative hierarchical clustering;optimization stage;language generation;Document Understanding Conference 2002;ranked predicate argument structure;feature selection;group semantically similar predicate argument structure;input document;multidocument abstractive summarization,,,,16,,2-Nov-17,,,IEEE,IEEE Conferences
Text Classification Using Semi-supervised Clustering,概率矩陣三因子分解,W. Zhang; T. Yoshida; X. Tang,"Sch. of Knowledge Sci., Japan Adv. Inst. of Sci. & Technol., Nomi, Japan; Sch. of Knowledge Sci., Japan Adv. Inst. of Sci. & Technol., Nomi, Japan; NA",2009 International Conference on Business Intelligence and Financial Engineering,21-Aug-09,2009,,,197,200,"In this paper, mixture models are used to classify documents. The basic assumption for the documents in a collection is that each class is composed of a number of mixture components. By identifying the components in the document collection, the classes of documents can thereby be identified from each other. A semi-supervised clustering method is proposed to identify the components (clusters), and further, unlabeled data is used to produce more accurate clusters in document collection to correspond the components of document classes. Experimental results show that the proposed method produces better performances than support vector machine (SVM) with linear kernel, and produces comparable performance with Bayesian classifier with expectation maximization (EM) in text classification.",,978-0-7695-3705-4,10.1109/BIFE.2009.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5208902,,Text categorization;Support vector machines;Support vector machine classification;Bayesian methods;Clustering algorithms;Partitioning algorithms;Knowledge engineering;Internet;Clustering methods;Kernel,pattern clustering;text analysis,text classification;semisupervised clustering method;mixture model;support vector machine;Bayesian classifier;expectation maximization;linear kernel,,,,6,,21-Aug-09,,,IEEE,IEEE Conferences
Accelerating Expectation-Maximization Algorithms with Frequent Updates,基於文本語料庫的矢量空間模型語義相似度評估,J. Yin; Y. Zhang; L. Gao,"Univ. of Massachusetts Amherst, Amherst, MA, USA; Northeastern Univ., Shenyang, China; Univ. of Massachusetts Amherst, Amherst, MA, USA",2012 IEEE International Conference on Cluster Computing,25-Oct-12,2012,,,275,283,"Expectation Maximization is a popular approach for parameter estimation in many applications such as image understanding, document classification, or genome data analysis. Despite the popularity of EM algorithms, it is challenging to efficiently implement these algorithms in a distributed environment. In particular, many EM algorithms that frequently update the parameters have been shown to be much more efficient than their concurrent counterparts. Accordingly, we propose two approaches to parallelize such EM algorithms in a distributed environment so as to scale to massive data sets. We prove that both approaches maintain the convergence properties of the EM algorithms. Based on the approaches, we design and implement a distributed framework, FreEM, to support the implementation of frequent updates for the EM algorithms. We show its efficiency through three well-known EM applications: k-means clustering, fuzzy c-means clustering and parameter estimation for the Gaussian Mixture model. We evaluate our framework on both a local cluster of machines and the Amazon EC2 cloud. Our evaluation shows that the EM algorithms with frequent updates implemented on FreEM can run much faster than those implementations with traditional concurrent updates.",2168-9253,978-0-7695-4807-4,10.1109/CLUSTER.2012.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337789,,Clustering algorithms;Algorithm design and analysis;Convergence;Linear programming;Synchronization;Frequency control;Acceleration,convergence;distributed processing;expectation-maximisation algorithm;fuzzy set theory;Gaussian processes;parameter estimation;pattern clustering,expectation-maximization algorithm;frequent updates;parameter estimation;EM algorithm;distributed environment;massive data set;convergence property;FreEM distributed framework;EM application;k-means clustering;fuzzy c-means clustering;Gaussian mixture model;Amazon EC2 cloud,,19,,16,,25-Oct-12,,,IEEE,IEEE Conferences
An improved web information summarization based on SSSC,通過成對約束的主動學習進行半監督文檔聚類,Jun Tang; Xiaojuan Zhao,"Department of Information Engineering, Hunan Urban Construction College, XiangTan, China; Department of Information Engineering, Hunan Urban Construction College, XiangTan, China","2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics (CAR 2010)",29-Apr-10,2010,3,,235,238,"This paper proposed a new method of web news summarization via soft clustering algorithm. It used search engine to extract relevant documents, and mixed query sentence into sentences set which segmented from multi-document set, then this paper adopted efficient soft cluster algorithm SSSC to cluster all the sentences. If the number of cluster which contains the query sentence is larger than or equal to 5, the summary sentence will be extracted by turns from the clusters which query sentence in, or feature fusion will be used to extract summary sentences. Experimental result shows that the proposed summarization method can improve the performance of summary, soft clustering algorithm is efficient.",1948-3422,978-1-4244-5192-0,10.1109/CAR.2010.5456674,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5456674,Web;summarization;soft clustering;sentence similarity,Clustering algorithms;Frequency estimation;Search engines;Robotics and automation;Educational institutions;Clustering methods;Matrix decomposition;Asia;Informatics;Robot control,Internet;pattern clustering;query processing;search engines,Web information summarization;sentence similarity-based soft clustering;Web news summarization;search engine;relevant document extraction;mixed query sentence;feature fusion,,,,12,,29-Apr-10,,,IEEE,IEEE Conferences
A Self-Organising Map Approach for Clustering of XML Documents,一種基於SVD和FCA的文檔提取新方法,F. Trentini; M. Hagenbuchner; A. Sperduti; F. Scarselli; A. C. Tsoi,"Dipartimento di Ingegneria dell'Informazione, Siena University, Italy; NA; NA; NA; NA",The 2006 IEEE International Joint Conference on Neural Network Proceedings,30-Oct-06,2006,,,1805,1812,"The number of XML documents produced and available on the Internet is steadily increasing. It is thus important to devise automatic procedures to extract useful information from them with little or no intervention by a human operator. In this paper, we investigate the efficacy of an unsupervised learning approach, namely self-organising maps (SOMs), for the automatic clustering of XML documents. Specifically, we consider a relatively large corpus of XML formatted data from the INEX initiative and evaluate it using two different self-organising map models. The first model is the classical SOM model, and it requires the XML documents to be represented by real-valued vectors, obtained using a ""bag of words"" (or better a ""bag of tags"") approach. The other model is the SOM for structured data (SOM-SD) approach which is able to cluster structured data, and it is possible to feed the model with tree structured representations of the XML documents, thus explicitly preserving the structural information in the documents. The experimental results show that the SOM model exhibits quite a poor performance on this problem domain which requires the ability to encode structural properties of the data. The SOM-SD model, on the other hand, is able to produce a good clustering and generalization performance.",2161-4407,0-7803-9490-9,10.1109/IJCNN.2006.246898,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1716328,,XML;Machine learning;Internet;Unsupervised learning;HTML;Search engines;Neural networks;Data mining;Humans;Feeds,document handling;self-organising feature maps;XML,self-organising map approach;XML documents clustering;Internet;unsupervised learning approach;cluster structured data,,6,,17,,30-Oct-06,,,IEEE,IEEE Conferences
A hierarchical clustering method for big data oriented ciphertext search,用於多視圖聚類的局部自適應感受野維選擇性自組織圖,C. Chen; X. Zhu; P. Shen; J. Hu,"State Key Laboratory of Information Security, Institute of Information Engineering, CAS Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS Beijing, China; The University of new South Wales, Canberra, Australia",2014 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),8-Jul-14,2014,,,559,564,"Following the wide use of cloud services, the volume of data stored in the data center has experienced a dramatically growth which makes real-time information retrieval much more difficult than before. Furthermore, text information is usually encrypted before being outsourced to data centers in order to protect users' data privacy. Current techniques to search on encrypted data do not perform well within such a massive data environment. In this paper, a hierarchical clustering method for ciphertext search within a big data environment is proposed. The proposed approach clusters the documents based on the minimum similarity threshold, and then partitions the resultant clusters into sub-clusters until the constraint on the maximum size of cluster is reached. In the search phase, this approach can reach a linear computational complexity against exponential size of document collection. In addition, retrieved documents have a better relationship with each other than traditional methods. An experiment has been conducted using the collection set built from the recent ten years' IEEE INFOCOM publications, including about 3000 documents with nearly 5300 keywords. The results have validated our proposed approach.",,978-1-4799-3088-3,10.1109/INFCOMW.2014.6849292,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849292,cloud computing;ciphertext retrieval;multi-keyword ranked search;hierarchical clustering,Servers;Cryptography;Vectors;Indexes;Big data;Equations;Conferences,Big Data;cloud computing;computational complexity;cryptography;data privacy;information retrieval;pattern clustering;text analysis,hierarchical clustering method;Big Data oriented ciphertext search;cloud services;data center;real-time information retrieval;text information encryption;user data privacy protection;encrypted data;document clustering;minimum similarity threshold;subclusters;linear computational complexity;document collection;document retrieval;IEEE INFOCOM publications,,10,,14,,8-Jul-14,,,IEEE,IEEE Conferences
Document categorization and retrieval using semantic microfeatures and growing cell structures,使用語義微特徵和不斷增長的單元結構進行文檔分類和檢索,Wantao Deng; W. Wu,"Sch. of Comput. Sci., Middllesex Univ., London, UK; NA",12th International Workshop on Database and Expert Systems Applications,7-Aug-02,2001,,,270,274,Presents an approach to document categorization and retrieval using growing cell structures. Semantic microfeatures together with other information are adopted for document representation. The main advantage over traditional information retrieval systems which adopt index terms to index and retrieve documents is that the new approach considers the semantic relationships among documents. The paper describes how to construct document microfeature weight vectors and use growing cell structures clustering them. Some experimental results are also presented.,,0-7695-1230-5,10.1109/DEXA.2001.953074,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953074,,Information retrieval;Vocabulary;Data mining;Dictionaries;Organizing;Neural networks;Automatic control;Process control;Clustering algorithms;Sufficient conditions,information retrieval;self-organising feature maps;learning (artificial intelligence),document categorization;document retrieval;growing cell structures;self organizing neural network;semantic microfeatures;document representation;semantic relationships;clustering,,4,,9,,7-Aug-02,,,IEEE,IEEE Conferences
Web Service Recommendation via Combining Doc2Vec-Based Functionality Clustering and DeepFM-Based Score Prediction,通過結合基於Doc2Vec的功能集群和基於DeepFM的分數預測來推薦Web服務,X. Zhang; J. Liu; B. Cao; Q. Xiao; Y. Wen,"Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China; Key Lab. of Knowledge Process. & Networked Manuf., Hunan Univ. of Sci. & Technol., Xiangtan, China","2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",21-Mar-19,2018,,,509,516,"Due to the rapid growth in both the number and diversity of Web services on the Internet, it becomes increasingly difficult for developer to find the desired and appropriate Web services for Mashup creation. Even if the existing approaches show improvements in Web APIs recommendation, it is still challenging to recommend Web APIs with high accuracy and good diversity. Some of them integrate functionality clustering and the quality of service to recommend Web APIs for Mashup creation, but do not consider the high-order composition interaction relationship among functionality information, quality attributes. In this paper, we propose a novel Web APIs recommendation method via integrating the functionality clustering of service and the quality of service. In this method, it firstly obtains the functionality clustering by using Doc2Vec to cluster the description document of Web APIs. Then, the deep factorization machine model is used to extract the multi-dimension quality attributes of service and mine the high-order composition interaction relationship between them. Finally, the comparative experiments are performed on ProgrammableWeb dataset and experimental results show that our method significantly improves the performance of Web API recommendation in term of precision, recall, purity, entropy, DCG and HMD.",,978-1-7281-1141-4,10.1109/BDCloud.2018.00082,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672381,Web API Recommendation;Mashup Creation;Document representation;DeepFM;QoS,Mashups;Quality of service;Feature extraction;Predictive models;Data mining;Clustering algorithms,application program interfaces;document handling;pattern clustering;quality of service;recommender systems;Web services,high-order composition interaction relationship;Web API recommendation;Web service recommendation;Doc2Vec-based functionality clustering;quality of service;functionality information;multidimension quality attributes;programmableWeb dataset;deep factorization machine model;functionality clustering of service;deepFM-based score prediction;mashup creation,,1,,27,,21-Mar-19,,,IEEE,IEEE Conferences
Automatic Detection of Phishing Target from Phishing Webpage,從網絡釣魚網頁中自動檢測網絡釣魚目標,G. Liu; B. Qiu; L. Wenyin,"Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,4153,4156,"An approach to identification of the phishing target of a given (suspicious) webpage is proposed by clustering the webpage set consisting of its all associated webpages and the given webpage itself. We first find its associated webpages, and then explore their relationships to the given webpage as their features for clustering. Such relationships include link relationship, ranking relationship, text similarity, and webpage layout similarity relationship. A DBSCAN clustering method is employed to find if there is a cluster around the given webpage. If such cluster exists, we claim the given webpage is a phishing webpage and then find its phishing target (i.e., the legitimate webpage it is attacking) from this cluster. Otherwise, we identify it as a legitimate webpage. Our test dataset consists of 8745 phishing pages (targeting at 76 well-known websites) selected from Phish Tank and preliminary experiments show that the approach can successfully identify 91.44% of their phishing targets. Another dataset of 1000 legitimate webpages is collected to test our method's false alarm rate, which is 3.40%.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.1010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597725,Phishing;Anti-Phishing;DBSCAN Clustering;Web Document Analysis,Accuracy;Visualization;Layout;Object detection;Data mining;Clustering algorithms;Feature extraction,computer crime;Internet;pattern clustering;text analysis,phishing target automatic detection;phishing Webpage;Webpage set clustering;link relationship;ranking relationship;text similarity;Webpage layout similarity relationship;DBSCAN clustering method;legitimate Webpage,,27,,12,,7-Oct-10,,,IEEE,IEEE Conferences
A Dynamic SOM Algorithm for Clustering Large-Scale Document Collection,一種用於大型文檔集合聚類的動態SOM算法,K. Luo; Y. Liu; X. Wang,NA; NA; NA,Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007),3-Mar-08,2007,,,15,20,"A dynamic SOM algorithm of incremental gradient descent to cluster large-scale document collection is proposed in this paper. In comparison with other SOM algorithms (e.g. GHSOM), the size of output layer in our algorithm can be gradually reduced and dynamically by inserting suitable number of neurons, thus the number of underutilized neurons can be reduced greatly and the training results of this algorithm can fully represent the distribution of topics in document collection. In addition, when using this algorithm to cluster large-scale documents the computation cost can also be shortened remarkably. The overused neurons have been split again to optimize the cluster results further. A good result of cluster can be gained. Experiments results proved the effectiveness of this algorithm.",,978-0-7695-2930-1,10.1109/ALPIT.2007.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4460608,Text clusteringincremental gradient descentdynamic SOMoverused neuronsunderutilized neurons,Clustering algorithms;Heuristic algorithms;Large-scale systems;Neurons;Clustering methods;Navigation;Information technology;Computer science;Computational efficiency;Self organizing feature maps,,,,,,12,,3-Mar-08,,,IEEE,IEEE Conferences
Expert search based knowledge sharing system for collaborative environment,基於專家搜索的協作環境知識共享系統,P. Soor; J. Ghorpade-Aher,"MAEER's MIT COE, Pune, India; MAEER's MIT COE, Pune, India",2017 International Conference on Emerging Trends & Innovation in ICT (ICEI),13-Jul-17,2017,,,62,66,"In collaborative environment (CE) multiple members work together to achieve a common goal. But to find person/source that has most relevant information is a challenging part as the information requirement varies from task to task. Today, people are working in a collaborative environment to explore the ever increasing data and have new insights for knowledge sharing. Analyzing these needs we have proposed a system that finds the most appropriate and relevant documents based on the search query. In this system relevancy of document is supported by expert search method. In expert search, various aspects such as author name, hit count (number of downloads) of document, time span of hit count and last hit to that document are considered and clustering of retrieved results with meaningful cluster labels are identified to find whether a search term is used in the intended sense in that document. In this system, documents uploaded by members are analyzed. After analysis, indexing of documents is done. Further based on search query documents are retrieved and labeled clusters of documents are formed. Finally expert search is applied over mined results. This system helps in getting exact knowledge with quick response and repeating efforts for finding same information can be avoided with efficient time utilization. The performance of proposed system is evaluated with respect to response time of system. Thus, the effective use of knowledge sharing (data/information) is achieved with the Expert Search for time efficient systems.",,978-1-5090-3404-8,10.1109/ETIICT.2017.7977011,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7977011,,Collaboration;Data mining;Search problems;Fertilizers,document handling;groupware;indexing;knowledge based systems;pattern clustering;query processing,expert search based knowledge sharing system;collaborative environment;information requirement;relevant documents;indexing;search query documents;document clusters,,,,19,,13-Jul-17,,,IEEE,IEEE Conferences
Text document clustering based on frequent concepts,基於頻繁概念的文本文檔聚類,R. Baghel; R. Dhir,"Department of Computer Science & Engineering, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India; Department of Computer Science & Engineering, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India","2010 First International Conference On Parallel, Distributed and Grid Computing (PDGC 2010)",6-Jan-11,2010,,,366,371,"This paper presents a novel technique of document clustering based on frequent concepts. The proposed FCDC (Frequent Concepts based Document Clustering), a clustering algorithm works with frequent concepts rather than frequent itemsets used in traditional text mining techniques. Many well known clustering algorithms deal with documents as bag of words while they ignore the important relationship between words like synonym relationship. The proposed algorithm utilizes the semantic relationship between words to create concepts. It exploits the WordNet ontology in turn to create low dimensional feature vector which allows developing a more accurate clustering algorithm.",,978-1-4244-7674-9,10.1109/PDGC.2010.5679969,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679969,,Clustering algorithms;Accuracy;Grid computing;Merging;Databases;Wireless application protocol;Ontologies,data mining;ontologies (artificial intelligence);pattern clustering;text analysis,document clustering;frequent concept;text mining;WordNet;ontology;feature vector,,5,,27,,6-Jan-11,,,IEEE,IEEE Conferences
Automatic cluster assignment for documents,自動為文件分配群集,J. S. Deogun; S. K. Bhatia; V. V. Raghavan,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA",[1991] Proceedings. The Seventh IEEE Conference on Artificial Intelligence Application,6-Aug-02,1991,i,,25,28,"A knowledge-based approach to classification is reported. The proposed methodology uses personal construct theory for interviewing a domain expert to elicit classification knowledge. This interview results in raw data which, on analysis, yields the relationship between different concepts from a user perspective. After finding the relationships, the user is asked to delineate the boundaries which enclose like concepts. With such a grouping of concepts, the authors develop a methodology to establish a relationship between the concepts and the index terms constituting document representations. This relationship is employed to assign a document to the most appropriate cluster. The knowledge elicited from the expert is mapped to system-observable features of documents to develop a classification. The techniques developed are experimentally validated.<>",,0-8186-2135-4,10.1109/CAIA.1991.120840,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=120840,,Information retrieval;Uncertainty;Psychology;Adaptive systems;Computer science;Artificial intelligence;Knowledge based systems;Knowledge acquisition;Indexing;Vocabulary,classification;indexing;information retrieval systems;knowledge acquisition;knowledge based systems,automatic cluster assignment;knowledge-based approach;classification;personal construct theory;interviewing;domain expert;interview;raw data;user perspective;index terms;document representations;system-observable features,,5,,6,,6-Aug-02,,,IEEE,IEEE Conferences
Evaluation of latent dirichlet allocation for document organization in different levels of semantic complexity,不同語義復雜性級別下文檔組織潛在狄利克雷分配的評估,R. A. Sinoara; R. B. Scheicher; S. O. Rezende,"ICMC-USP, Universidade de S瓊o Paulo; ICMC-USP, Universidade de S瓊o Paulo; ICMC-USP, Universidade de S瓊o Paulo",2017 IEEE Symposium Series on Computational Intelligence (SSCI),5-Feb-18,2017,,,1,8,"In the last years, latent Dirichlet allocation (LDA), a state-of-the-art topic modeling method, has been applied in several text mining tasks. LDA solutions can be used as either a clustering solution or a low-dimensional document representation. The low-dimensional space obtained by LDA is normally called semantic space, as alternative forms expressing the same concept or topic are projected to a common representation. In this work, we discuss the problem of document organization in different levels of semantic complexity and evaluate the use of LDA for document organization in a real-world application scenario. Our hypothesis is that LDA achieves good results when documents can be organized based on the vocabulary only, however, LDA semantic space is not semantically rich enough to allow an organization in more complex scenarios. We developed a proof of concept to give evidence to this hypothesis and evaluate the use of LDA in two different approaches: as an exclusive partitional clustering solution and as a dimension reduction method. The solutions were evaluated based on both FScore measure and user's expectations, considering document organization problems with different levels of semantic requirements. The results indicate that LDA reached good FScore if the organization depends mainly on the document vocabulary, but the method was not able to help the discovery of patterns that is semantically more complex.",,978-1-5386-2726-6,10.1109/SSCI.2017.8280939,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280939,,Semantics;Complexity theory;Vocabulary;Companies;Text mining;Task analysis,data mining;document handling;pattern clustering;text analysis,LDA semantic space;complex scenarios;exclusive partitional clustering solution;document organization problems;document vocabulary;latent dirichlet allocation;semantic complexity;state-of-the-art topic;text mining tasks;LDA solutions;low-dimensional document representation,,,,20,,5-Feb-18,,,IEEE,IEEE Conferences
MXML: Implementation of a web-based application for merging XML documents using XML-SIM,MXML：使用XML-SIM合併XML文檔的基於Web的應用程序的實現,W. Viyanon,"Department Mathematics (Computer Science Program), Faculty of Science, Srinakharinwirot University, Bangkok, Thailand",2015 13th International Conference on ICT and Knowledge Engineering (ICT & Knowledge Engineering 2015),4-Jan-16,2015,,,5,10,"This paper presents a design, implementation and evaluation of a web-based application called MergeXML (MXML). MXML was developed to integrate XML documents that are similar in terms of structure and content to complete information which can be used for information retrieval. XML documents are clustered into subtrees representing as instances using leaf-node parents as clustering points. The system finds subtree keys from unique values at leaf-node levels. Subtree keys play an important role for mapping subtrees between two documents. Matched subtrees are merged to complete the information on the base XML document. The result shows that MXML is able to cluster subtrees as proper instances. It can merge additional (different) information to the base XML document. MXML is recommended to run with not too large size of XML documents due to time-out settings on the web server.",2157-099X,978-1-4673-9190-0,10.1109/ICTKE.2015.7368462,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368462,XML Similarity Detection;Similarity Detection;Clustering;Matching;Keys,XML;Semantics;Information retrieval;Integrated circuits;Lead;Servers;Internet,information retrieval;Internet;XML,MXML;Web based application;XML documents;XML-SIM;MergeXML;information retrieval;leaf node parents;clustering points;subtree keys;XML document;Web server;time out settings,,,,9,,4-Jan-16,,,IEEE,IEEE Conferences
Subgraph Spotting through Explicit Graph Embedding: An Application to Content Spotting in Graphic Document Images,通過顯式圖嵌入進行子圖點檢：應用於圖形文檔圖像中的內容點檢,M. M. Luqman; J. Ramel; J. Llados; T. Brouard,"Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France; Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France; Comput. Vision Center, Univ. Autonoma de Barcelona, Barcelona, Spain; Lab. d'Inf., Univ. Francois Rabelais de Tours, Tours, France",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,870,874,"We present a method for spotting a subgraph in a graph repository. Subgraph spotting is a very interesting research problem for various application domains where the use of a relational data structure is mandatory. Our proposed method accomplishes subgraph spotting through graph embedding. We achieve automatic indexation of a graph repository during off-line learning phase, where we (i) break the graphs into 2-node sub graphs (a.k.a. cliques of order 2), which are primitive building-blocks of a graph, (ii) embed the 2-node sub graphs into feature vectors by employing our recently proposed explicit graph embedding technique, (iii) cluster the feature vectors in classes by employing a classic agglomerative clustering technique, (iv) build an index for the graph repository and (v) learn a Bayesian network classifier. The subgraph spotting is achieved during the on-line querying phase, where we (i) break the query graph into 2-node sub graphs, (ii) embed them into feature vectors, (iii) employ the Bayesian network classifier for classifying the query 2-node sub graphs and (iv) retrieve the respective graphs by looking-up in the index of the graph repository. The graphs containing all query 2-node sub graphs form the set of result graphs for the query. Finally, we employ the adjacency matrix of each result graph along with a score function, for spotting the query graph in it. The proposed subgraph spotting method is equally applicable to a wide range of domains, offering ease of query by example (QBE) and granularity of focused retrieval. Experimental results are presented for graphs generated from two repositories of electronic and architectural document images.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.178,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065435,subgraph spotting;explicit graph embedding;graphics recognition;content spotting;focused retrieval,Graphics;Vectors;Bayesian methods;Image edge detection;Indexes;Pattern recognition;Equations,Bayes methods;content-based retrieval;document image processing;graph theory;pattern clustering,subgraph spotting;explicit graph embedding;content spotting;graphic document image;relational data structure;automatic indexation;off-line learning phase;feature vector;agglomerative clustering technique;Bayesian network classifier;query by example;QBE,,6,,12,,3-Nov-11,,,IEEE,IEEE Conferences
A clustering method and radius tuning by end users,最終用戶的聚類方法和半徑調整,H. Takahashi; K. M. Mohiuddin,"Yamato Lab., IBM Japan Ltd., Yamato, Japan; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,698,701 vol.2,"In this paper we describe a top-down clustering method consisting of an intra class step and an inter class step. In the intra class step all the samples for each category are initially divided into a small number of clusters, then the largest cluster is split and its members reallocated. The largest cluster is decided based on a new concept, ""Volume"" of a cluster that is a hybrid of existing two common criteria for splitting: number of members in a cluster, and variance of a cluster. In the inter class step recognition is done for all the training set to assign best radius to each prototype. The radii are used as a normalizing factor in the computation of distance metrics. In our experiments we generated a prototype library by clustering characters written by Americans. When we used another training set written by Japanese only for tuning radii of the American library, the recognition rate of Japanese test set increased from 87.9% to 92.1%. The radii can be tuned even by OCR end users when the application domain is quite different from that of the initial clustering by OCR developers.",,0-8186-7128-9,10.1109/ICDAR.1995.601999,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601999,,Clustering methods;Libraries;Optical character recognition software;Character recognition;Prototypes;Character generation;Laboratories;Optical tuning;Testing;Shape,optical character recognition,clustering method;radius tuning;end users;intra class step;inter class step;training set;best radius;distance metrics;prototype library;Japanese test set;OCR,,,,5,,6-Aug-02,,,IEEE,IEEE Conferences
A new descriptive clustering algorithm based on Nonnegative Matrix Factorization,基於非負矩陣分解的新型描述性聚類算法,Z. Li; H. Peng; X. Wu,"School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science, Hefei University of Technology, China",2008 IEEE International Conference on Granular Computing,31-Oct-08,2008,,,407,412,"Nonnegative matrix factorization (NMF) provides a way for finding a part-based representation of nonnegative data. An important property of NMF is that it can produce a sparse representation of the data; however, in some applications, especially in text clustering, the sparse representation always consists of separated words, which cannot explicitly express the meaning of the basis vector. This paper presents a new descriptive clustering algorithm based on NMF, called DC-NMF that can avoid this separated word problem. In our proposed method, we embrace the phrase-by-document matrix in addition to the commonly used term-by-document matrix. Then, we use conjunct gradient descent to minimize the mean squared error objective function. Finally, we describe each cluster with the highest weighted element corresponding to one particular phrase. Our experimental results indicate that our method can obtain more ldquopurerdquo clusters than other methods.",,978-1-4244-2512-9,10.1109/GRC.2008.4664752,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4664752,,Clustering algorithms;Sparse matrices;Entropy;XML;Computer science;HTML;Dentistry,data mining;document handling;gradient methods;matrix decomposition;mean square error methods;pattern clustering,descriptive clustering algorithm;nonnegative matrix factorization;DC-NMF;phrase-by-document matrix;term-by-document matrix;conjunct gradient descent;mean squared error objective function,,38,,8,,31-Oct-08,,,IEEE,IEEE Conferences
Fitting document representation to specific datasets by adjusting membership functions,通過調整隸屬函數使文檔表示適合特定的數據集,A. P. Garc穩ia-Plaza; V. Fresno; R. Mart穩inez,"NLP & IR Group, UNED, Madrid, Spain; NLP & IR Group, UNED, Madrid, Spain; NLP & IR Group, UNED, Madrid, Spain",2012 IEEE International Conference on Fuzzy Systems,13-Aug-12,2012,,,1,8,"In this work we deal with the problem of web page clustering from the point of view of document representation. Fuzzy ruled-based systems have been successfully used to represent web documents by means of heuristic combinations of criteria. In these systems, rules were established based on the way humans read documents and have been analyzed in previous works. However, membership functions parameters were fixed by default, assuming that any document would follow similar patterns regardless of the rest of documents in the collection. In this work we analyze to what extent collection information could be used to adjust the membership functions in order to improve document representation, and therefore, clustering results. We compare our proposal to the original one in which is based, and to another similar or common approaches. We also perform statistical significance tests to ensure that our modifications have a real effect over the original representation. Results show that adjusting document representation parameters to concrete collections leads to better clustering results.",1098-7584,978-1-4673-1506-7,10.1109/FUZZ-IEEE.2012.6251249,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6251249,Web Page;Representation;Fuzzy Logic;Clustering,Standards;Fuzzy systems;Web pages;Concrete;Tuning;Knowledge based systems;Accuracy,document handling;fuzzy set theory;Internet;pattern clustering;statistical analysis,document representation;membership functions;Web page clustering;fuzzy ruled-based systems;membership functions parameters;statistical significance tests;concrete collections,,1,,25,,13-Aug-12,,,IEEE,IEEE Conferences
Automatic and interactive rule inference without ground truth,自動和交互式規則推理，無需地面真理,C. Carton; A. Lemaitre; B. Co羹asnon,"IRISA - INSA, Universit矇 Europ矇enne de Bretagne, Campus de Beaulieu, 35042 Rennes Cedex, France; IRISA - Universit矇 Rennes 2, Universit矇 Europ矇enne de Bretagne, Campus de Beaulieu, 35042 Cedex, France; IRISA - INSA, Universit矇 Europ矇enne de Bretagne, Campus de Beaulieu, 35042 Rennes Cedex, France",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,696,700,"Dealing with non annotated documents for the design of a document recognition system is not an easy task. In general, statistical methods cannot learn without an annotated ground truth, unlike syntactical methods. However their ability to deal with non annotated data comes from the fact that the description is manually made by a user. The adaptation to a new kind of document is then tedious as the whole manual process of extraction of knowledge has to be redone. In this paper, we propose a method to extract knowledge and generate rules without any ground truth. Using large volume of non annotated documents, it is possible to study redundancies of some extracted elements in the document images. The redundancy is exploited through an automatic clustering algorithm. An interaction with the user brings semantic to the detected clusters. In this work, the extracted elements are some keywords extracted with word spotting. This approach has been applied to old marriage record field detection on the FamilySearch HIP2013 competition database. The results demonstrate that we successfully automatically infer rules from non annotated documents using the redundancy of extracted elements of the documents.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333851,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333851,,Reliability;Learning automata;Niobium;Atmospheric modeling;Manuals,document image processing;inference mechanisms;knowledge acquisition;knowledge based systems;pattern clustering;statistical analysis,interactive rule inference;automatic rule inference;nonannotated documents;document recognition system;statistical methods;syntactical methods;knowledge extraction process;document image extracted elements;automatic clustering algorithm;FamilySearch HIP2013 competition database,,3,,13,,23-Nov-15,,,IEEE,IEEE Conferences
"InLinx for document classification, sharing and recommendation",InLinx用於文檔分類，共享和推薦,C. Bighini; A. Carbonaro; G. Casadei,"Dept. of Comput. Sci., Bologna Univ., Italy; Dept. of Comput. Sci., Bologna Univ., Italy; Dept. of Comput. Sci., Bologna Univ., Italy",Proceedings 3rd IEEE International Conference on Advanced Technologies,28-Jul-03,2003,,,91,95,"We propose a hybrid recommender system, InLinx, that combines content analysis and the development of virtual clusters of students and of didactical sources providing facilities to use the huge amount of digital information according to the student's personal requirements and interests. Novel methods for information management, with special focus on the development of new algorithms and intelligent applications for personalized information sharing, filtering and retrieval is proposed. InLinx helps the student to classify domain specific information found in the Web and saved as bookmarks, to recommend these documents to other students with similar interests and to periodically notify new potentially interesting documents.",,0-7695-1967-9,10.1109/ICALT.2003.1215033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1215033,,Uniform resource locators;Information filtering;Software libraries;Information filters;Recommender systems;Information analysis;Navigation;Computer science;Information management;Clustering algorithms,distance learning;educational technology;information filters;information retrieval;document handling;Internet,hybrid recommender system;InLinx;content analysis;didactical sources;students personal requirements;information management;personalized information sharing;information retrieval;Web;document classification,,4,,19,,28-Jul-03,,,IEEE,IEEE Conferences
Towards modernised and Web-specific stoplists for Web document analysis,建立針對Web文檔分析的現代化和特定於Web的清單,M. P. Sinka; D. W. Corne,"Reading Univ., UK; Reading Univ., UK",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),27-Oct-03,2003,,,396,402,"Research areas such as text classification and document clustering underpin many issues in Web intelligence. A fundamental tool in document clustering is a list of 'stop' words (stoplist) that is used to identify frequent words that are unlikely to assist in classification and is hence removed during pre-processing. Current stoplists are outdated both in light of fluctuations in word usage, and innocent of 'Web-specific' stop words, hence questioning their applicability in Web-based tasks. We explore this by developing new word-entropy based stoplists: one derived from random Web pages, and one derived from the BankSearch dataset. We evaluate these against other stoplists using accuracies obtained from unsupervised clustering experiments. We find that existing stoplists perform well, but are sometimes outperformed by our new stoplists, especially on hard classification tasks.",,0-7695-1932-6,10.1109/WI.2003.1241221,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241221,,Text analysis;Search engines;Internet;Databases;Taxonomy;Text categorization;Fluctuations;Web pages;Uniform resource locators;Indexes,Internet;document handling;pattern clustering;pattern classification;information retrieval;search engines,Web-specific stoplists;Web document analysis;document clustering;Web intelligence;word-entropy based stoplists;Web pages;unsupervised clustering experiments;pattern classification,,8,,13,,27-Oct-03,,,IEEE,IEEE Conferences
An effective method for scene image management,一種有效的場景圖像管理方法,Wang Xiaochi; Xu Jie; Fang Zhigang,"School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, China; School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, China; School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, China",2010 International Conference on Networking and Information Technology,19-Jul-10,2010,,,217,220,"This paper proposes an effective method to manage the huge number of scene images captured by camera users. Here, ?manage??mainly refers to cluster these images into semantically meaningful categories and further try to find some most representative images of each scene to characterize the scene. In this work, our contributions mainly focus on two aspects: (i) during the image clustering process, we propose a voting clustering method based on dense patches extracted from the image; (ii) during the representative image selection process, we propose an adjacency matrix-based method to find the best candidates as the scene's preview or peer-view. Keypoint-based method is used to determine the similarity between the images in each scene category. Finally, to verify the proposed method's performance, a dataset is constructed from the real world image collection. The experimental results show that the proposed method is very effective in managing the huge number of scene images, and also outperforms the conventional method.",2324-8203,978-1-4244-7578-0,10.1109/ICNIT.2010.5508525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508525,scene image management;image clustering;voting-based;adjacency matrix,Layout;Clustering algorithms;Cameras;Voting;Conference management;Technology management;Engineering management;Cities and towns;Educational institutions;Environmental management,document image processing;matrix algebra;pattern clustering,scene image management;camera users;image clustering process;voting clustering method;dense patches;representative image selection process;matrix based method;keypoint based method,,,,11,,19-Jul-10,,,IEEE,IEEE Conferences
An Improved Co-Similarity Measure for Document Clustering,改進的文檔聚類相似度度量,S. F. Hussain; G. Bisson; C. Grimal,"Lab. TIMC-IMAG, Univ. of Grenoble, Grenoble, France; Lab. TIMC-IMAG, Univ. of Grenoble, Grenoble, France; Lab. a""Inf. de Grenoble, Univ. of Grenoble, Grenoble, France",2010 Ninth International Conference on Machine Learning and Applications,4-Feb-11,2010,,,190,197,"Co-clustering has been defined as a way to organize simultaneously subsets of instances and subsets of features in order to improve the clustering of both of them. In previous work, we proposed an efficient co-similarity measure allowing to simultaneously compute two similarity matrices between objects and features, each built on the basis of the other. Here we propose a generalization of this approach by introducing a notion of pseudo-norm and a pruning algorithm. Our experiments show that this new algorithm significantly improves the accuracy of the results when using either supervised or unsupervised feature selection data and that it outperforms other algorithms on various corpora.",,978-1-4244-9211-4,10.1109/ICMLA.2010.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708832,co-clustering;similarity measure;text mining,Strontium;Clustering algorithms;Equations;Complexity theory;Sea measurements;Semantics;Oceans,feature extraction;pattern clustering;text analysis,document clustering;similarity matrices;pseudonorm algorithm;pruning algorithm;feature selection;corpora;cosimilarity measure,,16,,25,,4-Feb-11,,,IEEE,IEEE Conferences
A new approach to the design of knowledge base using XCLS clustering,使用XCLS集群設計知識庫的新方法,J. H. Beevi; N. Deivasigamani,"Department of Computer Science and Information, Technology, Jamal Mohamed College(Autonomous), Trichirappalli, TN, India; Department of Computer Applications, Pavendar Bharathidasan College of Eng. & Tech., Trichirappalli, TN, India","International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)",31-May-12,2012,,,14,19,"A Knowledge Base is a special kind of data base used for storage and retrieval of knowledge. From the perspective of knowledge creators, maintenance and creation of knowledge base is a crucial activity in the life cycle of knowledge management. This paper presents a novel approach to the creation of knowledge base. The main focus of our approach is to extract the knowledge from unstructured web documents and create a knowledge base. Preprocessing techniques such as tokenizing, stemming are performed on the unstructured input web documents. Meanwhile, Similarity and redundancy computation is performed for duplicate knowledge removal. The extracted knowledge is organized and converted to XML documents. XCLS clustering is made on XML documents. Finally, Knowledge base is designed for storing extracted XML documents. A query interface has been developed to retrieve the search knowledge. To test the usefulness and ease of use of our prototype, we used the Technology Acceptance Model (TAM) to evaluate the system. Results are promising.",,978-1-4673-1039-0,10.1109/ICPRIME.2012.6208280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208280,JSON;Google Web Services;XCLS;Clustering;Knowledge Extraction;TAM;Tokenizer,XML;Knowledge based systems;Knowledge engineering;Data mining;Redundancy;Google;Web services,document handling;information retrieval systems;knowledge based systems;knowledge management;pattern clustering;query processing;user interfaces;XML,knowledge base design;XCLS clustering;data base;knowledge retrieval;knowledge storage;knowledge management;unstructured Web documents;tokenizing;stemming;redundancy computation;duplicate knowledge removal;XML documents;query interface;technology acceptance model,,1,,14,,31-May-12,,,IEEE,IEEE Conferences
Attribute based content mining for regional web documents,區域Web文檔的基於屬性的內容挖掘,K. B. Prakash; M. A. D. Rangaswamy; A. R. Raman,"Sathyabama University, Chennai, India; AVIT, Chennai, India; IIT Madras, Chennai - 36, India",IET Chennai Fourth International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2013),11-Jun-15,2013,,,368,373,"The rapid expansion of the Internet has made the WWW a popular place for disseminating and collecting information. Extracting useful information from Web pages thus becomes an important task. Generally, apart from the main content blocks, web pages usually have such blocks as navigation bars, copyright and privacy notices, relevant hyperlinks, and advertisements, which are called noisy blocks. Although such information items are functionally useful for human viewers and necessary for the Web site owners, they often hamper Web page clustering, classification, information retrieval and information extraction. Today, people use the Web for a large variety of activities including travel planning, comparison shopping, entertainment, and research. However, the tools available for collecting, organizing, and sharing Web content have not kept pace with the rapid growth in information. But the major complexity arises when web documents or information is in regional languages. Extracting the content of the document and later communication through oral or text means is quite involved as both syntax and symantics are needed for this. Depending on the form and structure of the web document this task becomes difficult and this is the area the current paper addresses through a novel approach based on the pixel maps and using this how content could be extracted and knowledge is created in the minds of illiterate user. The paper first presents how letters and words which form the basis of text-based communication can be used for content. The objective of this task is to achieve a concept-based term analysis on the sentence and document levels rather than a single-term analysis in the document set only. This paper outlines the use of attributes for content extraction, using basic pixel attributes and pattern matching, statistical model and pattern matching and Artificial Neural Network training.",,978-1-78561-030-1,10.1049/ic.2013.0340,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119727,Media Mining;Multi-Lingual;Statistical Interpretation;ANN,,data mining;document handling;Internet,attribute based content mining;regional Web documents;Internet;WWW;navigation bars;privacy notices;relevant hyperlinks;advertisements;copyright;Web site owners;Web page clustering;information retrieval;information extraction;travel planning;comparison shopping;entertainment;research;Web content;regional languages;pixel maps;content extraction;basic pixel attributes;pattern matching;statistical model;artificial neural network training,,,,,,11-Jun-15,,,IET,IET Conferences
"Robust Text Line, Word And Character Extraction from Telugu Document Image",從泰盧固語文檔圖像中提取可靠的文本行，單詞和字符,V. K. Koppula; N. Atul; U. Garain,"Dept. of CSE, CMR Coll. of Eng. & Tech., Hyderabad, India; Dept. of CIS, Univ. of Hyderabad, Hyderabad, India; Indian Stat. Inst., Kolkata, India",2009 Second International Conference on Emerging Trends in Engineering & Technology,22-Jan-10,2009,,,269,272,"Designing an OCR system for Indian languages in general is more complex than those of European languages due the linguistic complexity. Efforts are on the way for the development of efficient OCR systems for Indian languages, especially for Telugu, a popular South Indian language. In this paper, we proposed a method for reliable extraction of text line, word and character from document images of Telugu scripts. In the text line segmentation, first we establish the relationship between the connected components and then cluster the connected components of a line using vertical spatial relation and nearest neighbor algorithm. In word segmentation, the space between two adjacent characters is computed and clustered into word space and character space. Consonant and vowel modifiers are segregated from the word image and segment the characters.",2157-0485,978-1-4244-5250-7,10.1109/ICETET.2009.196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395511,,Robustness;Image segmentation;Optical character recognition software;Nearest neighbor searches;Clustering algorithms;Carbon capture and storage;Educational institutions;Computational Intelligence Society;Computer vision;Character recognition,document image processing;feature extraction;image segmentation;natural language processing;optical character recognition;text analysis,word extraction;character extraction;Telugu document image;OCR system;Indian languages;linguistic complexity;text line segmentation;vertical spatial relation;nearest neighbor algorithm;word segmentation;text line extraction,,7,,9,,22-Jan-10,,,IEEE,IEEE Conferences
Clustering documents on case vectors represented by predicate-argument structures - applied for eliciting technological problems from patents,以謂詞-自變量結構表示的案例向量的文檔聚類-用於從專利中引出技術問題,H. Yanaka; Y. Ohsawa,"Department of Systems Innovation, Faculty of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Japan; Department of Systems Innovation, Faculty of Engineering, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Japan",2016 Federated Conference on Computer Science and Information Systems (FedCSIS),7-Nov-16,2016,,,175,180,"Patent analysis is useful to understand the trends of technological problems and develop strategies for technologies. Here patent classification is a method to support the analysis. The purpose of this study is to propose a method for patent classification, with the use of hierarchical clustering based on the structural similarity of problems to be solved. The structural similarity can be calculated with case vectors based on predicate-argument structures of the contents of the patents. The interview survey indicated that this classification plays an essential role in analogical problem solving, by allowing visualization of similar technological problems.",,978-8-3608-1090-3,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733232,,Patents;Semantics;Matrix decomposition;Visualization;Technological innovation;Feature extraction;Large scale integration,document handling;patents;pattern classification;pattern clustering,document clustering;predicate-argument structures;patent analysis;patent classification;hierarchical clustering;structural similarity;case vectors;patent content;analogical problem solving,,,,13,,7-Nov-16,,,IEEE,IEEE Conferences
Web documents categorization using fuzzy representation and HAC,使用模糊表示和HAC的Web文檔分類,Jiawei Deng; Lihui Chen,"Sch. of Electr. & Electron. Eng., Nanyang Tech. Univ., Singapore; NA",Proceedings of the First International Conference on Web Information Systems Engineering,6-Aug-02,2000,2,,24,28 vol.2,"Most of the existing techniques for the characterization of Web documents are based on term-frequency analysis. In such models, given a set of documents, the characterization of each document is represented by a feature vector in a vector space. However, as Web documents written in HTML are semi-structured by means of tags, the traditional techniques that assign term weights only by the frequency of occurrence may not be able to provide satisfactory results in representing the content of such documents. Some recent studies have shown that the fuzzy representation (FR) of WWW information based on the significance of HTML tags is an effective alternative for characterizing Web documents. In this paper, the FR is used to generate the feature vector for each Web document and the hierarchical agglomerative clustering (HAC) algorithm is applied to investigate its efficiency and effectiveness for the automatic categorization of Web documents with similar contents. Experiments that have been conducted suggest several benefits of using such an approach.",,0-7695-0577-5,10.1109/WISE.2000.882848,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=882848,,Clustering algorithms;HTML;Frequency;Information retrieval;Navigation;World Wide Web;Internet;Web pages;Natural languages;Probes,information resources;classification;fuzzy set theory;pattern clustering;hypermedia markup languages;vectors,World Wide Web document categorization;fuzzy representation;HAC algorithm;hierarchical agglomerative clustering;term-frequency analysis;document characterization;feature vector;vector space;HTML tags;semi-structured documents;term weights;occurrence frequency;document content representation,,1,,14,,6-Aug-02,,,IEEE,IEEE Conferences
The organisation and visualisation of document corpora: a probabilistic approach,文檔語料庫的組織和可視化：一種概率方法,M. Girolami; A. Vinokourov; A. Kaban,"Dept. of Comput. & Inf. Syst., Paisley Univ., UK; NA; NA",Proceedings 11th International Workshop on Database and Expert Systems Applications,6-Aug-02,2000,,,558,564,"A generic probabilistic framework for the unsupervised organisation and visualisation of document collections is presented. The probabilistic hierarchical clustering of large-scale sparse and high-dimensional data collections is achieved by the development of a family of latent class models which are parameterized using the expectation maximisation algorithm. The framework is based on a hierarchical probabilistic mixture methodology. Two classes of models emerge from the analysis and these have been termed as symmetric and asymmetric models. For text data specifically, both asymmetric and symmetric models based on the multinomial and binomial distributions are most appropriate. The subsequent visualisation of document collections is achieved by exploiting the topographic relations between similar documents. A latent trait model is developed which provides the means of viewing vector space document representations on a 2D grid and thereby visualising the inherent structure of the document collection. A number of experiments are provided to demonstrate the technique and a concluding discussion on the proposed models is given.",1529-4188,0-7695-0680-1,10.1109/DEXA.2000.875081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875081,,Visualization;Databases;Computational intelligence;Information systems;Large-scale systems;Clustering algorithms;Information retrieval;Costs;Parameter estimation;Internet,document handling;data visualisation;probability;optimisation;topology,document corpora visualisation;probabilistic approach;generic probabilistic framework;unsupervised organisation;document collections;probabilistic hierarchical clustering;high-dimensional data collections;latent class models;expectation maximisation algorithm;hierarchical probabilistic mixture methodology;asymmetric models;symmetric models;text data;binomial distributions;multinomial distribution;topographic relations;similar documents;latent trait model;vector space document representations;2D grid,,,,16,,6-Aug-02,,,IEEE,IEEE Conferences
Clustering and topic modeling over tweets: A comparison over a health dataset,推文上的聚類和主題建模：健康數據集的比較,J. A. Lossio-Ventura; J. Morzan; H. Alatrista-Salas; T. Hernandez-Boussard; J. Bian,"Stanford University,Department of Medicine, Biomedical Informatics,USA; School of Engineering, Universidad del Pac穩fico,Lima,Peru; School of Engineering, Universidad del Pac穩fico,Lima,Peru; Stanford University,Department of Medicine, Biomedical Informatics,USA; University of Florida,Health Outcomes & Biomedical Informatics,USA",2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),6-Feb-20,2019,,,1544,1547,"Twitter became the most popular form of social interactions in the healthcare domain. Thus, various teams have evaluated Twitter as an additional source where patients share information about their healthcare with the potential goal to improve their outcomes. Several existing topic modeling and document clustering applications have been adapted to assess tweets showing that the performances of the applications are negatively affected due to the nature and characteristics of tweets. Moreover, Twitter health research has become difficult to measure because of the absence of comparisons between the existing applications. In this paper, we perform an evaluation based on internal indexes of different topic modeling and document clustering applications over two Twitter health-related datasets. Our results show that Online Twitter LDA and Gibbs LDA get a better performance for extracting topics and grouping tweets. We want to provide health practitioners this comparison to select the most suitable application for their tasks.",,978-1-7281-1867-3,10.1109/BIBM47256.2019.8983167,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983167,topic modeling;clustering;internal cluster indexes;natural language processing;Twitter,,document handling;health care;Internet;medical information systems;pattern clustering;social networking (online),social interactions;healthcare domain;document clustering;Twitter health-related datasets;Online Twitter LDA;topic modeling;Gibbs LDA,,,,27,,6-Feb-20,,,IEEE,IEEE Conferences
Generating document clusters using thesauri and neural networks,使用敘詞表和神經網絡生成文檔集群,Farkas,"Centre for Inf. Technol. Innovation, Laval, Que., Canada",1994 Proceedings of Canadian Conference on Electrical and Computer Engineering,6-Aug-02,1994,,,710,713 vol.2,"In this paper, we describe the use of thesauri and neural networks for the classification of lexically similar natural language documents. We discuss the effect of extending the usual keyword representation of documents to a weighted, thesaurally-based, representation using relations among keywords. We present some experimental results obtained from a neural network prototype that uses Kohonen's self-organizing map paradigm.<>",,0-7803-2416-1,10.1109/CCECE.1994.405850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=405850,,Self-organizing feature maps;Information retrieval,thesauri;self-organising feature maps;information retrieval;classification,document clusters generation;thesauri;neural networks;classification;lexically similar natural language documents;keyword representation;Kohonen's self-organizing map paradigm,,11,,11,,6-Aug-02,,,IEEE,IEEE Conferences
A Two-Stage Approach for Reconstruction of Cross-Cut Shredded Text Documents,跨階段切碎文本文檔的兩階段重建方法,Y. Wang; D. Ji,"Dept. of Basic Sci., Taizhou Coll. of Nanjing Univ. of Sci. & Tech., Taizhou, China; Dept. of Comput. Sci., Taizhou Coll. of Nanjing Univ. of Sci. & Tech, Taizhou, China",2014 Tenth International Conference on Computational Intelligence and Security,22-Jan-15,2014,,,12,16,"This paper presents a two-stage approach for reconstruction of cross-cut shredded text documents. Cross-cut shredding is used to mechanically cut a document into rectangular shreds of (almost) identical shapes. After pre-processing shreds with image-based techniques, we defined a cluster quality measure called ""matching proportion"" (MP), with which, shreds in the same rows were found by clustering. Then the shreds in each cluster (row) were aligned and the whole document was reconstructed by aligning all rows. All the alignments were done by a memetic algorithm which was extended from a genetic algorithm by embedding a probabilistic Kruskal based heuristic. Experiments were presented for two different instances. Results show that the two-stage approach is an appropriate reconstruction method which provides good solutions in a reasonable amount of time.",,978-1-4799-7434-4,10.1109/CIS.2014.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016843,document reconstruction;memetic algorithms;genetic algorithms;cluster analysis;Kruskal heuristic,Genetic algorithms;Image reconstruction;Heuristic algorithms;Indexes;Sociology;Statistics;Clustering algorithms,document image processing;genetic algorithms;image matching;image reconstruction;text analysis,two-stage approach;cross-cut shredded text documents reconstruction;image-based techniques;matching proportion;memetic algorithm;genetic algorithm;probabilistic Kruskal based heuristic,,4,,9,,22-Jan-15,,,IEEE,IEEE Conferences
A sequence based dynamic SOM model for text clustering,基於序列的文本聚類動態SOM模型,U. Gunasinghe; S. Matharage; D. Alahakoon,"CCSL, Faculty of IT, Monash University, Australia; CCSL, Faculty of IT, Monash University, Australia; CCSL, Faculty of IT, Monash University, Australia",The 2012 International Joint Conference on Neural Networks (IJCNN),30-Jul-12,2012,,,1,8,"Text clustering can be considered as a four step process consisting of feature extraction, text representation, document clustering and cluster interpretation. Most text clustering models consider text as an unordered collection of words. However the semantics of text would be better captured if word sequences are taken into account. In this paper we propose a sequence based text clustering model where four novel sequence based components are introduced in each of the four steps in the text clustering process. Experiments conducted on the Reuters dataset and Sydney Morning Herald (SMH) news archives demonstrate the advantage of the proposed sequence based model, in terms of capturing context with semantics, accuracy and speed, compared to clustering of documents based on single words and n-gram based models.",2161-4407,978-1-4673-1490-9,10.1109/IJCNN.2012.6252474,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252474,Text clustering;Sequence learning;Growing Self Organizing Map;Text feature selection;Semantics,Feature extraction;Clustering algorithms;Indexes;Semantics;Equations;Mathematical model;Adaptation models,feature extraction;pattern clustering;self-organising feature maps;text analysis,sequence based dynamic SOM model;feature extraction;text representation;document clustering;cluster interpretation;text clustering process;Reuters dataset;Sydney Morning Herald news archives,,4,,22,,30-Jul-12,,,IEEE,IEEE Conferences
Performance Evaluation of Algorithms for Newspaper Article Identification,報紙文章識別算法的性能評估,R. Beretta; L. Laura,"Telpress, S.p.A., Rieti, Italy; Dept. of Comput. & Syst. Sci., Sapienza Univ. of Rome, Rome, Italy",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,394,398,"A typical modern newspaper recognition system operates in distinct phases: i) page segmentation (also called page decomposition or zoning), that is the process of decomposing a page into its structural and logical units (called regions or zones), ii) region (or zone) labeling, where the previously identified units are labeled according to their types (title, text, images, and lines), iii) article identification (or tracking or clustering), in which all the units that belong to a single article are clustered together, and iv) read order identification, in which each item in an article is assigned its reading order inside the article. So far, in the literature, several works appeared describing algorithms and metrics for the first two phases, i.e. page segmentation and region labeling, that indeed play a crucial role in the whole process, however, few results focused on article identification, that is a difficult task mainly due to the rich and complex variety of newspapers layouts. In this paper we propose a methodology to evaluate news-papers article identification algorithms, our approach is based on well-established tools from graph theory: in particular, we reduce the newspaper article clustering problem to a specific graph clustering problem, that is therefore evaluated using the appropriate coverage and performance measures. The advantages of our approach are twofold: on one side, the proposed measures correctly detects that not all the errors are equals, i.e. some errors are worse than others, and the scores are assigned properly. On the other side, we show how to reverse the reduction, in order to exploit the large number of graph clustering algorithm available: indeed, given a graph clustering algorithm, to obtain a full working newspaper article identification algorithm we only need to define a similarity measure between units in the article. We provide some examples, using a specifically designed dataset. Finally, we would like to point out that both our dataset, together with its ground-truth base, and the software tool, that implements the proposed approach, are freely available.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065342,newspaper article identification;performance evaluation;graph clustering,Clustering algorithms;Software algorithms;Algorithm design and analysis;Layout;Software tools;Performance evaluation,document handling;graph theory;image segmentation;matrix algebra;pattern clustering;performance evaluation;publishing;software tools,performance evaluation;newspaper article identification;page segmentation;page decomposition;logical units;structural units;region labeling;metrics;graph theory;newspaper article clustering;graph clustering problem;software tool,,4,,7,,3-Nov-11,,,IEEE,IEEE Conferences
Image annotation by semi-supervised clustering constrained by SIFT orientation information,受SIFT方向信息約束的半監督聚類圖像標註,A. Sayar; F. T. Yarman-Vural,"Scientific & Technological Research Council, Space Technologies Research Institute, ODTU, Ankara, Turkey; Computer Engineering Department, Middle East Technical University, Ankara, Turkey",2008 23rd International Symposium on Computer and Information Sciences,16-Dec-08,2008,,,1,4,"Methods developed for image annotation usually make use of region clustering algorithms. Visual codebooks are generated from the region clusters of low level features. These codebooks are then, matched with the words of the text document related to the image, in various ways. In this paper, we supervise the clustering process by using the orientation information assigned to each interest point of Scale-invariant feature transform (SIFT) features to generate a visual codebook. The orientation information provides a set of constraints in a semi-supervised k-means region clustering algorithm. Consequently, in clustering of regions not only SIFT features are normalized along the dominant orientation, but also orientation information itself is used. Experimental results show that image annotation with added orientation information by semi-supervised clustering is more successful compared to the one that uses SIFT features alone. The proposed algorithm is implemented in a parallel computation environment.",,978-1-4244-2880-9,10.1109/ISCIS.2008.4717882,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4717882,,Clustering algorithms;Vocabulary;Bridges;Concurrent computing;Image databases;Spatial databases;Visual databases;Information retrieval;Image retrieval;Image segmentation,image processing;pattern clustering,image annotation;semisupervised clustering;region clustering algorithms;clustering process;scale-invariant feature transform;semisupervised k-means,,2,,8,,16-Dec-08,,,IEEE,IEEE Conferences
Self-Organizing Map and K-Means for Meteorological Day Type Identification for the Region of Annaba -Algeria-,自組織地圖和K均值用於安納巴-阿爾及利亞地區的氣象日類型識別,S. Khedairia; M. T. Khadir,"Lab. sur La Gestion Electron. du Document, Annaba Univ., Alegria; Lab. sur La Gestion Electron. du Document, Annaba Univ., Alegria",2008 7th Computer Information Systems and Industrial Management Applications,9-Jul-08,2008,,,91,96,"A two level clustering approach has been proposed in this paper in order to perform a classification analysis of meteorological data of Annaba region (North-East of Algeria) using data from 1995 to 1999. The Kohonen self-organizing map (SOM) has been used to group the data and produce the meteorological prototypes. The number of prototypes of SOM is large, to facilitate quantitative analysis of the map and the data similar units need to be grouped (clustered). As a second clustering stage k-means algorithm has been used to cluster the SOM units. Quantitative (using two categories of validity indices) and qualitative criteria were introduced to verify the results of the clustering. The different experiments developed extracted six distinct classes, which were related to typical meteorological conditions in the area.",,978-0-7695-3184-7,10.1109/CISIM.2008.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4557840,Meteorological day type identification;self-organizing map;k-means;clustering;Kohonen maps,Meteorology;Clustering algorithms;Prototypes;Partitioning algorithms;Urban pollution;Principal component analysis;Data mining;Self organizing feature maps;Air pollution;Atmosphere,geophysics computing;meteorology;pattern clustering;self-organising feature maps,self-organizing map;meteorological day type identification;Annaba-Algeria region;two level clustering approach;classification analysis;North-East of Algeria;second clustering stage k-means algorithm;qualitative criteria,,4,,33,,9-Jul-08,,,IEEE,IEEE Conferences
A framework towards efficient and effective sequence clustering,高效序列聚類的框架,Wei Wang; Jiong Yang,"IBM Thomas J. Watson Res. Center, NY, USA; NA",Proceedings 18th International Conference on Data Engineering,7-Aug-02,2002,,,282,,"Analyzing sequence data (particularly in categorical domains) has become increasingly important, partially due to the significant advances in biology and other fields. Examples of sequence data include DNA sequences, unfolded protein sequences, text documents, Web usage data, system traces, etc. Previous work on mining sequence data has mainly focused on frequent pattern discovery. In this project, we focus on the problem of clustering sequence data.",1063-6382,0-7695-1531-2,10.1109/ICDE.2002.994736,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994736,,Probability distribution;Biological information theory;Data mining;Protein sequence;Amino acids;Extraterrestrial measurements;Clustering algorithms;Data analysis;DNA;Tree data structures,sequences;pattern clustering;data analysis,sequence data clustering;DNA sequences;unfolded protein sequences;text documents;Web usage data;system traces;sequence data analysis;categorical domains,,,,,,7-Aug-02,,,IEEE,IEEE Conferences
Patent search and trend analysis,專利檢索和趨勢分析,A. M. Supraja; S. Archana; S. Suvetha; T. V. Geetha,"Department of Computer Science and Engineering, College of Engineering, Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering, Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering, Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering, Guindy, Anna University, Chennai, India",2015 IEEE International Advance Computing Conference (IACC),13-Jul-15,2015,,,501,506,"A patent is an intellectual property document that protects new inventions. It covers how things work, what they do, how they do it, what they are made of and how they are made. The owner of the granted patent application has the ability to take a legal action to stop others from making, using, importing or selling the invention without permission. While applying for a patent, the inventor has issues in identifying similar patents. Citations of related patents, which are referred to as the prior art, should be included while applying for a patent. We propose a system to develop a Patent Search Engine to identify related patents. We also propose a system to predict Business Trends by analyzing the patents. In our proposed system, we carry out a query independent clustering of patent documents to generate topic clusters using LDA. From these clusters, we retrieve query specific patents based on relevance thereby maximizing the query likelihood. Ranking is based on relevancy and recency which can be performed using BM25F algorithm. We analyze the Topic-Company trends and forecast the future of the technology which is based on the Time Series Algorithm - ARIMA. We evaluate the proposed methods on USPTO patent database. The experimental results show that the proposed techniques perform well as compared to the corresponding baseline methods.",,978-1-4799-8047-5,10.1109/IADCC.2015.7154759,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154759,patents;information retrieval;clustering;cluster title generation;technology forecasting,Patents;Market research;Companies;Search problems;Databases;Technological innovation,document handling;patents;pattern clustering;query processing;search engines;technological forecasting;time series,intellectual property document;patent search engine;business trend prediction;patent document query independent clustering;topic clusters;LDA;query specific patent retrieval;BM25F algorithm;topic-company trends;technology forecasting;ARIMA time series algorithm;USPTO patent database,,3,,15,,13-Jul-15,,,IEEE,IEEE Conferences
Web opinions analysis with scalable distance-based clustering,基於可伸縮距離的聚類的Web意見分析,C. C. Yang; T. D. Ng,"College of Information Science and Technology, Drexel University, Philadelphia, USA; Digital Library Laboratory, The Chinese University of Hong Kong, Hong Kong",2009 IEEE International Conference on Intelligence and Security Informatics,26-Jun-09,2009,,,65,70,"Due to the advance of Web 2.0 technologies, a large volume of Web opinions are available in computer-mediated communication sites such as forums and blogs. Many of these Web opinions involve terrorism and crime related issues. For instances, some terrorist groups may use Web forums to propagandize their ideology, some may post threaten messages, and some criminals may recruit members or identify victims through Web social networks. Analyzing and clustering Web opinions are extremely challenging. Unlike regular documents, Web opinions usually appear as short and sparse text messages. Using typical document clustering techniques on Web opinions produce unsatisfying result. In this work, we propose the scalable distance-based clustering technique for Web opinions clustering. We have conducted experiments and benchmarked with the density-based algorithm. It shows that it obtains higher micro and macro accuracy. This Web opinions clustering technique is useful in identifying the themes of discussions in Web social networks and studying their development as well as the interactions of active participants.",,978-1-4244-4171-6,10.1109/ISI.2009.5137273,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137273,content analysis;Web forum analysis;social media analytics;social networks;document clustering,Information analysis;Social network services;Visualization;Clustering algorithms;Educational institutions;Information science;Software libraries;Laboratories;Computer mediated communication;Blogs,computer mediated communication;social networking (online);terrorism;text analysis,Web opinions clustering;scalable distance-based document clustering technique;Web 2.0 technology;computer-mediated communication site;Web forum;Web blog;terrorism issues;crime-related issues;Web social network;text message;density-based clustering algorithm,,5,,15,,26-Jun-09,,,IEEE,IEEE Conferences
A Mechanics-Based Similarity Measure for Text Classification in Machine Learning Paradigm,機器學習範式中基於力學的文本分類相似性度量,V. Kuppili; M. Biswas; D. R. Edla; K. J. R. Prasad; J. S. Suri,"Department of Computer Science and Engineering, National Institute of Technology, Farmagudi, India; Department of Computer Science and Engineering, National Institute of Technology, Farmagudi, India; Department of Computer Science and Engineering, National Institute of Technology, Farmagudi, India; Department of Humanities and Sciences, National Institute of Technology, Farmagudi, India; Global Biomedical Technologies, Inc., Roseville, CA, USA",IEEE Transactions on Emerging Topics in Computational Intelligence,25-Mar-20,2020,4,2,180,200,"Document classification and clustering is emerging as a new challenge in the Big Data era where terabytes of data are generated every second through billions of mobile phones, desktops, servers, and mobile devices such as cameras and watches. The effectiveness of classification and clustering algorithms depends on the similarity measure used between two text documents in the corpus. We have applied Maxwell-Boltzmann distribution to find the similarity between the two documents within a document corpus. In this paper, the document corpus is treated as a large system, individual documents as containers, attributes as subcontainers, and each term as a particle. The proposed similarity measure is named Maxwell-Boltzmann Similarity Measure (MBSM). MBSM is derived from the overall distribution of feature values and total number of nonzero features among the documents. We demonstrate that MBSM satisfies all properties of a document similarity measure. The MBSM is incorporated in single label K-nearest neighbors classification (SLKNN), multi label K-nearest neighbors classification (MLKNN) and K-means clustering. We benchmark MBSM against other similarity measures like Euclidian, Cosine, Jaccard, Pairwise, ITSim, and SMTP. The comparative performance shows that MBSM outperformed all existing similarity measures and increased classification accuracy of SLKNN and MLKNN and clustering accuracy and entropy of K-means algorithm while making them more robust. The highest accuracy obtained from tenfold cross validation for SLKNN is 0.9531 and MLKNN is 0.9373. The MBSM achieved maximum accuracy of 0.6592 and minimum entropy of 0.2426 amongst all similarity measures in the scale of unity for K-means clustering.",2471-285X,,10.1109/TETCI.2018.2863728,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445685,Text classification;text clustering;Maxwell?Boltzmann distribution;multi-label learning,Biomedical measurement;Containers;Clustering algorithms;Big Data;Atmospheric measurements;Particle measurements;Entropy,Big Data;Boltzmann machines;entropy;learning (artificial intelligence);mobile computing;nearest neighbour methods;pattern classification;pattern clustering;text analysis,mechanics;text classification;K-means clustering;text documents;Maxwell-Boltzmann distribution;document corpus;Maxwell-Boltzmann Similarity Measure;MBSM;document similarity measure;single label K-nearest neighbors classification;multi label K-nearest neighbors classification;SLKNN;MLKNN;Big Data;document classification;document clustering;mobile phones;mobile devices;desktops;servers,,2,,41,IEEE,24-Aug-18,,,IEEE,IEEE Journals
A Prior Knowledge Based Approach to Improving Accuracy of Web Services Clustering,基於先驗知識的提高Web服務集群準確性的方法,M. Shi; J. Liu; B. Cao; Y. Wen; X. Zhang,"Sch. of Comput. Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of Comput. Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of Comput. Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of Comput. Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China; Sch. of Comput. Sci. & Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China",2018 IEEE International Conference on Services Computing (SCC),6-Sep-18,2018,,,1,8,"The rapid growth in both the number and diversity of Web services raises new requirement of clustering techniques to facilitate the service discovery, service repository management etc. Existing clustering methods of Web services primarily focus on using the semantic distances between service features, e.g., topic vectors, mined from WSDL documents. However, these quality topic vectors are hard to be obtained due to the lack of abundant textual information in Web service description documents. In practice, prior knowledge from human's trajectory of utilizing Web services could be helpful in improving the accuracy of Web services clustering. With an analysis in the dataset of Web services and Mashups from ProgrammableWeb, we observe that Web services Mashuped together are highly likely to belong to different clusters and Web services being annotated with identical tags tend to be within the same cluster. Based on these observations, this paper proposes an efficient clustering approach for Web services. The approach firstly uses a probabilistic topic model to elicit the latent topic vectors from Web service description documents. It then performs clustering based on the K-means++ algorithm by incorporating parameters representing above mentioned prior knowledge. A comprehensive evaluation is conducted to validate the performance of our proposed approach based on a ground truth dataset crawled from ProgrammableWeb. Experimental comparisons of the approaches with and without these prior knowledge considerations show that our approach has a significant improvement on the clustering accuracy.",2474-2473,978-1-5386-7250-1,10.1109/SCC.2018.00008,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456394,Web services;Clustering;Prior Knowledge;LDA;K-means++,Mashups;Knowledge engineering;Clustering algorithms;Task analysis;Feature extraction;Semantics,data mining;pattern clustering;probability;text analysis;Web services,prior knowledge based approach;Web services clustering accuracy;service repository management;service feature semantic distance;WSDL documents;textual information;ProgrammableWeb;Mashups;probabilistic topic model;latent topic vectors;K-means++ algorithm;Web service description documents;service discovery,,3,,37,,6-Sep-18,,,IEEE,IEEE Conferences
Webpage Recommender System concerning high dimensional and sparse features,具有高維和稀疏特徵的網頁推薦系統,S. Wu; Min Jiang; X. Gao; Guiying Wei,"Dongling School of Economics and Management, University of Science and Technology Beijing, China; Dongling School of Economics and Management, University of Science and Technology Beijing, China; Dongling School of Economics and Management, University of Science and Technology Beijing, China; Dongling School of Economics and Management, University of Science and Technology Beijing, China",2012 8th International Conference on Information Science and Digital Content Technology (ICIDT2012),16-Aug-12,2012,1,,109,112,"In this paper, we design a Webpage Recommender System which clusters users based on users' browsing history to implement collaborative filtering and prepares webpages that may arouse their interest. In order to solve the problem of high-dimensionality and sparsity in collaborative filtering, the proposed system clusters users using CABOSFV, an efficient algorithm for high-dimensional sparse data clustering of binary attributes. And it uses an automatic webpage classifier to solve problems of cold-start and exhausting of recommendations. When online users send requests to the system, it responses them with those well prepared recommendations.",,978-8-9886-7870-1,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6269237,recommender system;collaborative filtering;document classification;CABOSFV algorithm,Algorithm design and analysis;History;Filtering,classification;collaborative filtering;document handling;online front-ends;pattern clustering;recommender systems;Web sites,Webpage recommender system;high dimensional features;user browsing history;collaborative filtering;CABOSFV;high-dimensional sparse data clustering;binary attribute;automatic Webpage classifier;online user;user request;document classification,,,,10,,16-Aug-12,,,IEEE,IEEE Conferences
An Efficient Framework for Searching Text in Noisy Document Images,在嘈雜的文檔圖像中搜索文本的有效框架,I. Z. Yalniz; R. Manmatha,"Dept. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA; Dept. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA",2012 10th IAPR International Workshop on Document Analysis Systems,7-May-12,2012,,,48,52,"An efficient word spotting framework is proposed to search text in scanned books. The proposed method allows one to search for words when optical character recognition (OCR) fails due to noise or for languages where there is no OCR. Given a query word image, the aim is to retrieve matching words in the book sorted by the similarity. In the offline stage, SIFT descriptors are extracted over the corner points of each word image. Those features are quantized into visual terms (visterms) using hierarchical K-Means algorithm and indexed using an inverted file. In the query resolution stage, the candidate matches are efficiently identified using the inverted index. These word images are then forwarded to the next stage where the configuration of visterms on the image plane are tested. Configuration matching is efficiently performed by projecting the visterms on the horizontal axis and searching for the Longest Common Subsequence (LCS) between the sequences of visterms. The proposed framework is tested on one English and two Telugu books. It is shown that the proposed method resolves a typical user query under 10 milliseconds providing very high retrieval accuracy (Mean Average Precision 0.93). The search accuracy for the English book is comparable to searching text in the high accuracy output of a commercial OCR engine.",,978-0-7695-4661-2,10.1109/DAS.2012.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6195333,document image search;image retrieval;word spotting,Optical character recognition software;Feature extraction;Vocabulary;Accuracy;Image resolution;Noise;Detectors,document image processing;image matching;image retrieval;natural language processing;optical character recognition;pattern clustering;text analysis;transforms;word processing,word spotting framework;scanned books;optical character recognition;query word image;SIFT descriptors;feature quantization;visual terms;hierarchical k-means algorithm;indexing;inverted file;query resolution stage;visterm configuration matching;image plane;word searching;longest common subsequence;Telugu books;user query;matching word retrieval accuracy;mean average precision;English book;text searching;commercial OCR engine;noisy document image,,22,,15,,7-May-12,,,IEEE,IEEE Conferences
Automatic Bengali news documents summarization by introducing sentence frequency and clustering,通過引入句子頻率和聚類來自動進行孟加拉語新聞文件摘要,M. M. Haque; S. Pervin; Z. Begum,"Department of Computer Science & Engineering, University of Dhaka, Dhaka-1000, Bangladesh; Department of Computer Science & Engineering, University of Dhaka, Dhaka-1000, Bangladesh; Institute of Information Technology, University of Dhaka, Dhaka-1000, Bangladesh",2015 18th International Conference on Computer and Information Technology (ICCIT),9-Jun-16,2015,,,156,160,"A method has been proposed in this paper for Bengali news documents summarization which extracts significant sentences using the four major steps (a) preprocessing, (b) sentence ranking, (c) sentence clustering, and (d) summary generation. The noticeable feature of this method is the incorporation of the sentence frequency where redundancy elimination is a consequence. Another one remarkable aspect is sentence clustering on the basis of similarity ratio among sentences. The summary sentence selection is done from all the clusters so that there will be maximum coverage of information in summary even if information is found scattered in input document. Two sets of human generated summary have been utilized where one is to train the system and another is for performance evaluation. The proposed method has been found better while turning comparison with the latest state-of-the art method of Bengali news documents summarization. The results of performance evaluation show that the average Precision, Recall and F-measure values are 0.608, 0.664 and 0.632 respectively.",,978-1-4673-9930-2,10.1109/ICCITechn.2015.7488060,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488060,Documents summarization;sentence clustering;sentence frequency;redundancy elimination;similarity ratio,Computer science;Information technology;Electronic mail;Redundancy;Performance evaluation;Art;Internet,information resources;word processing,automatic Bengali news documents summarization;sentence frequency;preprocessing;sentence ranking;sentence clustering;summary generation;redundancy elimination;similarity ratio;summary sentence selection;F-measure values,,4,,25,,9-Jun-16,,,IEEE,IEEE Conferences
Community structure of the Chinese document network based on content similarity,基於內容相似度的中文文檔網絡社區結構,X. Pan; J. Liu; G. Deng,"Institute of Systems Engineering, Dalian University of Technology, 116023, China; Research Centre of Complex Systems Science, University of Shanghai for Technology and Science, 200093, China; Institute of Systems Engineering, Dalian University of Technology, 116023, China",2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery,9-Sep-10,2010,4,,1515,1519,"Based on the complex network theory, we proposed a clustering algorithm based on content similarity. Firstly, the Chinese documents are represented by the vector-space model, and the content similarity between any two documents is computed by the cosine similarity. Consequently, the network node is defined as a document, and the edge weight is defined as the similarity obtained by the cosine similarity definition. The document connectivity network can be constructed based on the document-to-document similarity graph. If the edge weight between any two nodes is smaller than a constant value, then it's set as zero. Using the edge betweenness of the network, we reconstructed the hierarchical structure of the funding proposal network. Computing the edge betweenness, and remove the edge with largest betweenness; Repeat the above process until all edges are removed. Using an open dataset proposed by Fudan University, we experimentally compared the performance of the partition clustering algorithm and other algorithms, such as K-means and Bisecting K-means. The numerical results indicate that our algorithm is more efficient than K-means and Bisecting K-means algorithms. In addition, the numerical results are robustness to different constant. Finally, the algorithm is implemented on the proposal network, the community structure based on the content similarity is detected.",,978-1-4244-5934-6,10.1109/FSKD.2010.5569332,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569332,component;community structure;content similarity;complex network,Clustering algorithms;Communities;Partitioning algorithms;Algorithm design and analysis;Complex networks;Proposals;Data mining,document handling;pattern clustering;text analysis,community structure;Chinese document network;content similarity;complex network theory;clustering algorithm;vector-space model;K-means,,1,,36,,9-Sep-10,,,IEEE,IEEE Conferences
Mathematical Symbol Indexing Using Topologically Ordered Clusters of Shape Contexts,使用形狀上下文的拓撲排序簇的數學符號索引,S. Marinai; B. Miotti; G. Soda,"Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Firenze, Italy",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,1041,1045,"This paper addresses the indexing and retrieval of mathematical symbols from digitized documents. The proposed approach exploits Shape Contexts (SC) to describe the shape of mathematical symbols. Starting from the vector space method, that is based on SC clustering, we explore the use of topological ordered clusters to improve the retrieval performance. The clustering is computed by means of Self-Organizing Maps that organize the clusters in two dimensional topologically ordered feature maps. The retrieval performance are compared with those obtained using the K-means clustering on a large collection of mathematical symbols gathered from the widely used INFTY database.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.120,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277514,,Indexing;Shape;Software libraries;Text analysis;Information retrieval;Image retrieval;Vector quantization;Spatial databases;Mathematics;Image analysis,indexing;information retrieval;pattern clustering;symbol manipulation,mathematical symbol indexing;topologically ordered clusters;shape contexts;mathematical symbols;digitized documents;vector space method;SC clustering;topological ordered clusters;retrieval performance;self-organizing maps;2D topologically ordered feature maps;K-means clustering;INFTY database,,6,,14,,2-Oct-09,,,IEEE,IEEE Conferences
Survey on Extractive Text Summarization Methods with Multi-Document Datasets,多文檔數據集提取文本摘要方法的調查,P. N. Varalakshmi K; J. S. Kallimani,"Department of Computer Science and Engineering, M S Ramaiah Institute of Technology; Department of Computer Science and Engineering, M S Ramaiah Institute of Technology","2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",2-Dec-18,2018,,,2113,2119,"Text summarization has been one of the key research areas in Natural Language Processing (NLP) for a while. The various methods to summarize one or more documents can be broadly classified into extractive and abstractive text summarization where the former involves selecting key parts in the document and embedding into the summary while balancing between salience and redundancy. The latter involves creating new sentences to provide a summary of the documents. Extractive summarization can further be done in a supervised manner with humans or an unsupervised manner without any human intervention. This paper provides the knowledge a few of the current methods to perform extractive text summarization where the input would be multi document sets. Multi document summarization can consider two types of document sets; a homogeneous set of documents which have a common topic or theme and a heterogeneous set where the main topic for the documents are unrelated but they contain some form information that is related to the summary. The first method uses sentence regression where they consider performing sentence ranking along with sentence relations followed by greedy selection process. The second is an unsupervised paragraph embedding method utilizing a density peaks clustering method. The third method proposes document-level reconstruction using a neural document model. The fourth method is a query focused, joint neural network based model with an attention mechanism. The fifth method concentrates on coherence by providing a graph-based model which does not require discourse analysis as a prerequisite. We also see a way to create a heterogeneous multi-documentcorpus along with the limitations of each of these methods.",,978-1-5386-5314-2,10.1109/ICACCI.2018.8554768,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554768,Multi-Document Summarization;Maximum Marginal Relevance;ROUGE;tf-idf;ROUGE-1 and ROUGE-2,Redundancy;Training;Neural networks;Decoding;Semantics;Computer science;Natural language processing,graph theory;natural language processing;neural nets;pattern clustering;query processing;regression analysis;text analysis,extractive text summarization methods;multidocument datasets;abstractive text summarization;extractive summarization;current methods;multidocument sets;multidocument summarization;document sets;document-level reconstruction;neural document model;fourth method;fifth method;natural language processing;unsupervised manner;graph-based model;multidocument corpus;attention mechanism;sentence regression;clustering method,,,,7,,2-Dec-18,,,IEEE,IEEE Conferences
Unsupervised feature selection technique based on genetic algorithm for improving the Text Clustering,基於遺傳算法的無監督特徵選擇技術改進文本聚類,L. M. Abualigah; A. T. Khader; M. A. Al-Betar,"School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; Department of information technology, Al-Huson University College, Irbid-Jordan",2016 7th International Conference on Computer Science and Information Technology (CSIT),25-Aug-16,2016,,,1,6,"The increasing amount of text documents in digital forms affect the text analysis techniques. Text clustering (TC) is one of the important techniques used for showing a massive amount of text documents by clusters. Hence, the main problem that affects the text clustering technique is the presence sparse and uninformative features on the text documents. The feature selection (FS) is an essential unsupervised learning technique. This technique is used to select informative features to improve the performance of text clustering algorithm. Recently, the meta-heuristic algorithms are successfully applied to solve several hard optimization problems. In this paper, we proposed the genetic algorithm (GA) to solve the unsupervised feature selection problem, namely, (FSGATC). This method is used to create a new subset of informative features in order to obtain more accurate clusters. Experiments were conducted using four benchmark text datasets with variant characteristics. The results showed that the proposed FSGATC is improved the performance of the text clustering algorithm and got better results compared with k-mean clustering standalone. Finally, the proposed method ?FSGATC??evaluated by F-measure and Accuracy, which are common measures used in the domain of text clustering.",,978-1-4673-8914-3,10.1109/CSIT.2016.7549453,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549453,Unsupervised Feature Selection;Genetic Algorithm;K-mean Text Clustering;Informative features;Sparse features,Genetic algorithms;Clustering algorithms;Biological cells;Information technology;Optimization;Linear programming;Sociology,genetic algorithms;learning (artificial intelligence);pattern clustering;text analysis,genetic algorithm;text documents;digital forms;text analysis;unsupervised learning;text clustering algorithm;metaheuristic algorithms;unsupervised feature selection problem;FSGATC;text datasets;k-mean clustering;F-measure,,23,,14,,25-Aug-16,,,IEEE,IEEE Conferences
Text clustering based on term weights automatic partition,基於術語權重自動劃分的文本聚類,Yu Yonghong; Bai Wenyang,"Department of Computer Science, Anhui University of Finance & Economics, Bengbu, China; Department of Computer Science, Nanjing University, China",2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE),19-Apr-10,2010,3,,373,377,"Text clustering is becoming more and more popular due to the increasing of texts on Web and the requirements in real application. This paper introduces a novel automatic text clustering method, in which the genetic algorithm is first applied to the global optimal and high searching efficient term selection to achieve dimensionality reduction, and then appropriate number of partitions of document set are created according to the different combinations of term weights, and each document partition is clustered into an initial clusters based on dynamic programming technique, and last all initial clusters are clustered using the same method to final text clusters. It also provides analysis and theorem proof that the algorithm can provide higher performance in computational complexity, clustering effect and high dimensional data clustering.",,978-1-4244-5586-7,10.1109/ICCAE.2010.5451390,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5451390,text clustering;genetic algorithm;term selection;term weight partition,Clustering algorithms;Computer science;Information retrieval;Machine learning;Clustering methods;Genetic algorithms;Partitioning algorithms;Data mining;Information analysis;Finance,computational complexity;dynamic programming;genetic algorithms;pattern clustering;text analysis;theorem proving,weights automatic partition;automatic text clustering method;dynamic programming technique;computational complexity;theorem proof;genetic algorithm;data clustering;global optimal searching,,3,,20,,19-Apr-10,,,IEEE,IEEE Conferences
Clustering of Javanese News in Krama Alus Level with Javanese Stemming,使用Javanese詞幹在Krama Alus級別聚集Javanese新聞,D. E. Cahyani; L. Merta Tri Utami; H. Setiadi,"Universitas Sebelas Maret,Department of Informatics,Surakarta,Indonesia; Universitas Sebelas Maret,Department of Informatics,Surakarta,Indonesia; Universitas Sebelas Maret,Department of Informatics,Surakarta,Indonesia",2019 International Conference on Information and Communications Technology (ICOIACT),23-Dec-19,2019,,,462,467,"News is generally published by news providers. News providers in publishing news are sometimes not grouped into several news categories. This allows readers difficulty in finding news because the news is not grouped. This study group Krama Alus Javanese news for readers can search for news according to categories with the Hierarchical and K-Means Algorithms. Research data uses news documents from the JOGJATV online news portal. In this study, the data is processed through text preprocessing. Text preprocessing consists of five processes, one of which is stemming. Stemming is used to adapt the Nazief and Adriani algorithms which are adjusted to the rules in Javanese. The method used to process the results of text preprocessing is hierarchical clustering combined with k-means. Hierarchical clustering is used to determine the number of clusters and centroids of each cluster. The results showed that the stemming process required the addition of basic words and stemming rules. While for the clustering results there are 18 clusters. The evaluation of the overall cluster structure with Average Silhouette Width (ASW) shows a value of 0.9142. This value indicates that a cluster has different characteristics from other clusters so that the news documents are in the right group. The clustering results are also validated by Experts with good results, namely 11 clusters that can be labeled while 7 clusters with labels are not specific.",,978-1-7281-1655-6,10.1109/ICOIACT46704.2019.8938438,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938438,News;Javanese Stemming;Hierarchical Clustering;K-Means Clustering;Average Silhouette Width (ASW),,data mining;electronic publishing;information retrieval;pattern clustering;portals;text analysis,k-means algorithms;Javanese new clustering;Krama Alus Javanese news;stemming process;hierarchical clustering;text preprocessing;JOGJATV online news portal;news categories;publishing news;Javanese stemming;Krama Alus level;news documents,,,,15,,23-Dec-19,,,IEEE,IEEE Conferences
Review on Abstractive Text Summarization Techniques (ATST) for single and multi documents,審查單文檔和多文檔的抽象文本摘要技術（ATST）,S. Modi; R. Oza,"Computer Engineering Department, SCET, Surat, India; Computer Engineering Department, SCET, Surat, India","2018 International Conference on Computing, Power and Communication Technologies (GUCON)",28-Mar-19,2018,,,1173,1176,"In recent times, there is an enormous amount of data available on the internet. It is laborious for users to encapsulate large amount of data manually. Automatic text summarization can solve this problem by generating summary automatically. It can be categorized into extractive and abstractive text summarization techniques. Existing techniques of extractive text summarization extract important sentences from original document and generate summary without any modification of actual data. This technique may not present conflicting information properly. Abstractive text summarization can solve this problem by representing the extracted sentences into another understandable semantic form. This paper discusses abstractive text summarization techniques and highlights the parametric evaluation of these techniques.",,978-1-5386-4491-1,10.1109/GUCON.2018.8674894,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674894,abstractive summary;text summarization;single document;multi documents;summary,Semantics;Clustering algorithms;Data mining;Genetics;Redundancy;Merging;Labeling,abstracting;data analysis;natural language processing;text analysis,abstractive text summarization techniques;automatic text summarization;ATST;single documents;multidocuments;Internet,,3,,8,,28-Mar-19,,,IEEE,IEEE Conferences
CiteRivers: Visual Analytics of Citation Patterns,CiteRivers：引文模式的可視化分析,F. Heimerl; Q. Han; S. Koch; T. Ertl,Institute for Visualization and Interactive Systems (VIS); Institute for Visualization and Interactive Systems (VIS); Institute for Visualization and Interactive Systems (VIS); Institute for Visualization and Interactive Systems (VIS),IEEE Transactions on Visualization and Computer Graphics,27-Oct-15,2016,22,1,190,199,"The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.",1941-0506,,10.1109/TVCG.2015.2467621,DFG priority program ?Scalable Visual Analytics?? EC in context of the ?iPatDoc??project; ?ePoetics??project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192685,scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering;scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering,Visualization;Market research;Metadata;Tag clouds;Data mining;Color;Joining processes,citation analysis;data visualisation;document handling;scientific information systems,CiteRivers;visual analytics;citation patterns;scientific literature collections exploration;scientific literature collections analysis;knowledge management;document sets;textual content;document metadata;citation analysis;structure visualization;citation networks;interactive visual analysis;scientific documents;user-steered aggregation;citing publications;interactive visualization approach;dataset exploration;expert user feedback,,42,,53,,12-Aug-15,,,IEEE,IEEE Journals
Clustering method using hypergraph models based on Set Pair Analysis,集對分析的超圖模型聚類方法,G. Lin; S. Li,"Department of Math. & Information Science, Zhangzhou Normal University, 363000, China; China Department of Cognitive Science, Xiamen University, 361005, China",2009 IEEE International Symposium on IT in Medicine & Education,15-Sep-09,2009,1,,1194,1197,"Text clustering methods can be used to structure large sets of text or hypertext documents. However, a lot of well-known methods for text clustering do not really address the special problems of text clustering: very high dimensionality of the data and understandability of the cluster description. In this paper, we introduce a novel approach which is based on the hypergraph model of text clustering by using Set Pair Analysis (SPA) that is a new methodology to describe and process system uncertainty. In this method, we define a new measure for text similarity by the identical, different, and contrary of Set Pair. After setting up the hypergraph model, a hypergraph partitioning algorithm will be used to find clusters. The new method can eliminate disadvantageous factors and decreases the textual dimension of text and enhances the speed and accuracy of the text clustering. The experiment demonstrates that our approach is applicable and effective in high dimensional textual datasets.",,978-1-4244-3928-7,10.1109/ITIME.2009.5236279,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5236279,,Clustering methods;Clustering algorithms;Partitioning algorithms;Web sites;Spatial databases;Information science;Cognitive science;Text mining;Information systems;Information analysis,graph theory;hypermedia;text analysis,clustering method;hypergraph models;set pair analysis;text clustering methods;hypertext documents;cluster description;process system uncertainty;hypergraph partitioning algorithm;high dimensional textual datasets,,,,7,,15-Sep-09,,,IEEE,IEEE Conferences
Using density peaks sentence clustering for update summary generation,使用密度峰值句子聚類生成更新摘要,W. Guohua; G. Yutian,"Key Laboratory of Complex Systems Modeling and Simulation, Ministry of Education, School of Computer Science and Technology, Hangzhou Dianzi University, China; Key Laboratory of Complex Systems Modeling and Simulation, Ministry of Education, School of Computer Science and Technology, Hangzhou Dianzi University, China",2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),3-Nov-16,2016,,,1,5,"Update summarization can be a daunting task in automatic text summarization. It aims to distill evolved messages from a collection of new articles, under the assumption that the reader has already browsed the previous articles. In this paper, a number of state-of-the-art approaches were reviewed for extracting update summarization and then a method called DPC-TS was proposed. This approach is derived from the DPC-based multi-documents summarization method, which was developed in 2015. In our study, a model was established by using the topic signature algorithm, to evaluate the novelty of sentences in update documents set. In addition, the scheme of calculating similarities between sentences and sentence selection approach was improved to ensure there are diversity topics in the summary. DPC-TS update documents summarization method is a simple and effective approach, which can be reproduced and deployed in real environment.",,978-1-4673-8721-7,10.1109/CCECE.2016.7726719,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726719,update summarization;DPC;topic signature,Computational modeling;Clustering algorithms;Complex systems;Computer science;Electronic mail;Mathematical model;Redundancy,pattern clustering;text analysis,density peak sentence clustering;update summary generation;update summarization;automatic text summarization;DPC-TS;DPC-based multidocument summarization;topic signature algorithm;update documents set;sentence selection,,,,15,,3-Nov-16,,,IEEE,IEEE Conferences
Genre Classification for Musical Documents Based on Extracted Melodic Patterns and Clustering,基於旋律模式提取和聚類的音樂文獻體裁分類,B. Lin; T. Chen,"Dept. of Inf. Manage., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan; Dept. of Inf. Manage., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan",2012 Conference on Technologies and Applications of Artificial Intelligence,10-Jan-13,2012,,,39,43,"Genre classification for musical documents is conventionally based on keywords, statistical features or low-level acoustic features. Such features are either lack of in-depth information of music content or incomprehensible for music professionals. This paper proposed a classification scheme based on the correlation analysis of the melodic patterns extracted from music documents. The extracted patterns can be further clustered, and smoothing techniques for the statistics of the patterns can be utilized to improve the performance effectively. The accuracy of 70.67% for classifying five types of genre, including jazz, lyric, rock, classical and others, can be achieved, which outperforms an ANN-based classifier using statistical features significantly. The patterns can be converted into symbolic forms such that the classification results are meaningful and comprehensible for most music workers.",2376-6824,978-1-4673-4976-5,10.1109/TAAI.2012.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395003,Genre Classification;Automatic Tagging;N-gram Melodies;Repeated Melodic Patterns;Music Information Retrieval,Smoothing methods;Feature extraction;Correlation;Accuracy;Rocks;Filtering;Artificial neural networks,document handling;music;pattern classification;pattern clustering;statistical analysis,genre classification;musical documents;extracted melodic patterns;extracted melodic clustering;statistical features;music content;music professionals,,1,,11,,10-Jan-13,,,IEEE,IEEE Conferences
Transparent management of replicated WWW document clusters,透明管理複製的WWW文檔集群,H. Pagnia; O. Theel; H. Schupp,"Tech. Hochschule Darmstadt, Germany; NA; NA",Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,6-Aug-02,2000,,,263,268,"It is commonly agreed that many problems of today's Internet stem from the fact that its communication links are permanently overloaded. We propose a seamlessly integratable architecture which naturally extends the currently existing World Wide Web (WWW) infra-structure by allowing users to transparently access nearby copies of replicated WWW documents. The underlying basic idea is to aggregate closely-related WWW documents into document clusters. These clusters serve as the unit for replication, meaning that all documents of a cluster are identically managed and retrieved. The benefits of this approach are reduced response times for document retrieval as well as network-wide lowered bandwidth requirements for servicing such retrieval requests. The paper describes the overall architecture of our approach and the functioning of the individual components. Additionally, details of our prototype implementation are presented together with a preliminary performance assessment based on experimental measurements.",,0-7695-0571-6,10.1109/PADSW.2000.884578,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884578,,World Wide Web;Delay;Web sites;Bandwidth;Web and internet services;Service oriented architecture;Prototypes;IP networks;Telecommunication traffic;Statistical analysis,replicated databases;information retrieval;information resources,replicated WWW document clusters;Internet;seamlessly integratable architecture;document clusters;replication;document retrieval,,,,21,,6-Aug-02,,,IEEE,IEEE Conferences
Fast Integral MeanShift: Application to Color Segmentation of Document Images,快速積分MeanShift：在文檔圖像顏色分割中的應用,F. Lebourgeois; F. Drira; D. Gaceb; J. Duong,"INSA-Lyon, Univ. of Lyon, Lyon, France; ENI-SFAX, Univ. of Sfax, Sfax, Tunisia; INSA-Lyon, Univ. of Lyon, Lyon, France; INSA-Lyon, Univ. of Lyon, Lyon, France",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,52,56,"Global Mean Shift algorithm is an unsupervised clustering technique already applied for color document image segmentation. Nevertheless, its important computational cost limits its application for document images. The complexity of the global approach is explained by the intensive search of colors samples in the Parzen window to compute the vector oriented toward the mean. For making it more flexible, several attempts have tried to decrease the algorithm complexity mainly by adding spatial information or by reducing the number of colors to shift or even by selecting a reduced number of colors to estimate the means of density function. This paper presents a fast optimized Mean Shift with a much reduced computational cost. This algorithm uses both the discretisation of the shift and the integral image which allow the computation of means into the Parzen windows with a reduced and fixed number of operations. With the discretisation of the color space, the fast optimised MeanShift also memorizes all existing paths to avoid shifting again colors along similar path. Despite the square shape of the Parzen windows and the uniform kernel used, the results are very similar to those obtained by the global Mean Shift algorithm. The proposed algorithm is compared to the different existing implementation of similar algorithms found in the literature.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628584,MeanShift;image segmentation;integral volume,Image color analysis;Image segmentation;Complexity theory;Computational efficiency;Kernel;Vectors;Clustering algorithms,computational complexity;document image processing;image colour analysis;image segmentation,fast integral MeanShift;unsupervised clustering technique;global MeanShift algorithm;color document image segmentation;Parzen window;vector computation;algorithm complexity reduction;spatial information;fast optimized MeanShift;computational cost reduction;color space discretisation,,4,,20,,15-Oct-13,,,IEEE,IEEE Conferences
Automated scalable detection of location-specific Santa Ana conditions from weather data using unsupervised learning,使用無監督學習從天氣數據自動可擴展地檢測特定地點的聖安娜天氣情況,M. H. Nguyen; D. Crawl; J. Li; D. Uys; I. Altintas,"San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA U.S.A.; San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA U.S.A.; San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA U.S.A.; San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA U.S.A.; San Diego Supercomputer Center, University of California, San Diego, La Jolla, CA U.S.A.",2017 IEEE International Conference on Big Data (Big Data),15-Jan-18,2017,,,1203,1212,"Southern California's dry climate and fire-prone vegetation make the area vulnerable to extreme wildfire conditions. These conditions are exacerbated by Santa Ana weather patterns, which are characterized by very low humidity and gusty winds blowing in from the deserts. We present an approach using unsupervised learning to model and detect Santa Ana conditions based on sensor measurements from weather stations. Our approach uses cluster analysis to capture weather patterns specific to the region surrounding each weather station. A method is provided to automatically determine the Santa Ana cluster for each cluster model using dynamic, data-driven criteria. The resulting cluster models are applied to real-time sensor measurements to provide location-specific and time-specific detection of Santa Ana conditions. The Spark distributed platform is leveraged to scale the system to large datasets from multiple weather stations, and the Kepler workflow system is used to provide a GUI-based, easy-to-use interface to the underlying system. Results of testing our approach on an existing network of weather stations are presented. Our scalability experiment shows that the approach can process up to one million live sensor measurements in less than one minute on one machine. The proposed system can be used to aid in wildfire management and prevention by focusing firefighting efforts on regions with increased wildfire risks.",,978-1-5386-2715-0,10.1109/BigData.2017.8258046,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258046,wildfire management;unsupervised learning;cluster analysis;sensor processing;distributed computing;workflows,Portable document format,graphical user interfaces;pattern clustering;sensors;statistical analysis;unsupervised learning;vegetation;weather forecasting;wildfires;wind,automated scalable detection;location-specific Santa Ana conditions;weather data;unsupervised learning;fire-prone vegetation;extreme wildfire conditions;Santa Ana weather patterns;weather station;cluster analysis;Santa Ana cluster;cluster model;dynamic data-driven criteria;resulting cluster models;real-time sensor measurements;time-specific detection;multiple weather stations;live sensor measurements;wildfire risks;wildfire management;scalability experiment;Kepler workflow system;Spark distributed platform,,,,,,15-Jan-18,,,IEEE,IEEE Conferences
Web Snippets Clustering Based on an Improved Suffix Tree Algorithm,基於改進後綴樹算法的網頁摘要聚類,H. Wen; N. -F. Xiao; Q. Chen,"Sch. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,1,,542,547,"Web search results clustering is the navigator for users to find relevant results quickly. Through combining the advantages of vector space model (VSM) and suffix tree clustering (STC) document models, this paper puts forward a more effective Web snippets clustering algorithm. It can take into account the semantic information of candidate label phrases, and offer descriptive, readable and conceptual topic labels for the final documents groups. Evaluation of results demonstrates that clustering Web snippets based on the improved suffix tree algorithm has better performance in making search engine results easy to browse and helping users quickly find Web pages that they are interested in.",,978-0-7695-3735-1,10.1109/FSKD.2009.718,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358514,Suffix tree clustering;base clusters;singular value decomposition,Clustering algorithms;Search engines;Internet;Web search;Web pages;Fuzzy systems;Computer science;Knowledge engineering;Clustering methods;Navigation,Internet;pattern clustering;search engines,vector space model;suffix tree algorithm;Web snippets clustering;search engine;Web pages,,3,,11,,28-Dec-09,,,IEEE,IEEE Conferences
A Framework Based on Semi-Supervised Clustering for Discovering Unique Writing Styles,基於半監督聚類的框架，用於發現獨特的寫作風格,A. Bharath; S. Madhvanath,"Hewlett-Packard Labs., Bangalore, India; Hewlett-Packard Labs., Bangalore, India",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,891,895,"An online multi-stroke character is often written in many ways. While some vary in the number of strokes they contain, others differ in the ordering of strokes. It is important for a writer-independent recognition system to learn these different styles of writing the character during the training phase in order to better model the training data. Typically, the samples of a character are clustered in an unsupervised manner and each cluster is modeled individually. In this paper, we describe an approach based on dasiasemi-supervised clusteringpsila where basic domain knowledge can be incorporated for better clustering of strokes present across all the characters.Experimental results show improved recognition accuracy when compared to the baseline system.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277542,writing style identification;semi-supervised stroke clustering;online handwriting recognition;online Devanagari character recognition,Writing;Hidden Markov models;Character recognition;Clustering algorithms;Nearest neighbor searches;Text analysis;Laboratories;Training data;Feature extraction;Ink,handwritten character recognition;pattern clustering,semisupervised clustering;unique writing style discovery;online multistroke character;writer-independent recognition system;domain knowledge;stroke clustering,,2,,15,,2-Oct-09,,,IEEE,IEEE Conferences
Multilingual Web Documents: the system Hyperling,使用語義微特徵和不斷增長的單元結構進行文檔分類和檢索,Tuan-Dang Nguyen; K. Zreik,"GREYC, University of Caen, France. tnguyen@info.unicaen.fr; GREYC, Caen Univ.",2006 2nd International Conference on Information & Communication Technologies,16-Oct-06,2006,1,,578,582,"Hyperling is a formal, language independent, system dealing with hyperdocuments (Web sites). It observes that links structure and context embed crucial information for both hyperdocument retrieving and hyperdocument mining process. For this we suggest a clustering Hyperling that deals with multilingual hyperdocuments (Web sites). In order to determine the number and frontiers between the different used languages, we adopt a distributional approach to pre process the hyperdocument structure before clustering it. Our main hypothesis considers links related to the same language be regrouped together in a cluster. From this we can conclude that the more important generated clusters represent the dominant languages",,0-7803-9521-2,10.1109/ICTTA.2006.1684435,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1684435,,Information retrieval;Clustering algorithms;Research and development;Statistics;Frequency;Laboratories;Magnetohydrodynamics;Text analysis;Machine learning;Data mining,document handling;natural languages;Web sites,multilingual Web documents;Hyperling;Web sites;hyperdocuments,,,,17,,16-Oct-06,,,IEEE,IEEE Conferences
Providing the integrity and availability in the process of data transfer in the electronic documents management systems of transport-logistical clusters,通過結合基於Doc2Vec的功能集群和基於DeepFM的分數預測來推薦Web服務,A. Nyrkov; S. Sokolov; S. Chernyi; A. Chernyakov; A. Karpina,"Department of Complex Information Security, Admiral Makarov State University of Maritime and Inland Shipping, Saint-Petersburg, Russia; Department of Complex Information Security, Admiral Makarov State University of Maritime and Inland Shipping, Saint-Petersburg, Russia; Department of Complex Information Security, Admiral Makarov State University of Maritime and Inland Shipping, Saint-Petersburg, Russia; Department of Complex Information Security, Admiral Makarov State University of Maritime and Inland Shipping, Saint-Petersburg, Russia; Department of Complex Information Security, Admiral Makarov State University of Maritime and Inland Shipping, Saint-Petersburg, Russia","2016 2nd International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",27-Apr-17,2016,,,1,4,"Information is one of the main resources that every modern organization is based on. A lot of companies introduce electronic document management. At the same time, in order to raise the quality of transport and logistics services, the unions of such organizations become more and more popular. In this article, we consider the main instruments of providing information integrity and availability in electronic documents management of such unions. The platform for software examination within the system of decision making support on the platform of client-server architecture has been developed for electronic documents. Technologies of the WebSockets, standard RFC6455 support, JSON in JavaScript format have been analyzed and studied. The model of the formation of an expert group for decision-making support has been proposed. The process of interacting of software modules is described from the perspective of the processes of coordination of the system.",,978-1-5090-1322-7,10.1109/ICIEAM.2016.7910915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7910915,logistics;transportation enterprises;protection of electronic document circulation;software;digital signature,Servers;Companies;Consumer electronics;Protocols;Software;Libraries,client-server systems;decision support systems;document handling;information management;logistics data processing;transportation,data transfer;electronic documents management systems;transport-logistical clusters;transport services;logistics services;information integrity;information availability;decision making support;client-server architecture;WebSockets technology;RFC6455 support;JSON;JavaScript format,,2,,11,,27-Apr-17,,,IEEE,IEEE Conferences
Automated Extraction and Clustering of Requirements Glossary Terms,從網絡釣魚網頁中自動檢測網絡釣魚目標,C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer,"SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SES Techcom, Betzdorf, Luxembourg",IEEE Transactions on Software Engineering,13-Oct-17,2017,43,10,918,945,"A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.",1939-3520,,10.1109/TSE.2016.2635134,Luxembourg?s National Research Fund; European Research Council; European Union?s Horizon 2020 research and innovation program; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765062,Requirements glossaries;term extraction;natural language processing;clustering;case study research,Terminology;Servers;Pipelines;Natural languages;Monitoring;Software;Clustering algorithms,natural language processing;pattern clustering;text analysis,natural language requirements documents;mathematically-based procedure;user-specified parameters;candidate glossary terms;technical terms;clustering algorithm;automated approach;software requirements document;requirements glossary terms;glossary construction;generic term extraction tools,,8,,100,Traditional,2-Dec-16,,,IEEE,IEEE Journals
Multiple Learned Dictionaries Based Clustered Sparse Coding for the Super-Resolution of Single Text Image,一種用於大型文檔集合聚類的動態SOM算法,R. Walha; F. Drira; F. Lebourgeois; C. Garcia; A. M. Alimi,"ENI-Sfax, Univ. of Sfax, Sfax, Tunisia; ENI-Sfax, Univ. of Sfax, Sfax, Tunisia; INSA-Lyon, Univ. of Lyon, Lyon, France; INSA-Lyon, Univ. of Lyon, Lyon, France; ENI-Sfax, Univ. of Sfax, Sfax, Tunisia",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,484,488,"This paper addresses the problem of generating a super-resolved version of a low-resolution textual image by using Sparse Coding (SC) which suggests that image patches can be sparsely represented from a suitable dictionary. In order to enhance the learning performance and improve the reconstruction ability, we propose in this paper a multiple learned dictionaries based clustered SC approach for single text image super resolution. For instance, a large High-Resolution/Low-Resolution (HR/LR) patch pair database is collected from a set of high quality character images and then partitioned into several clusters by performing an intelligent clustering algorithm. Two coupled HR/LR dictionaries are learned from each cluster. Based on SC principle, local patch of a LR image is represented from each LR dictionary generating multiple sparse representations of the same patch. The representation that minimizes the reconstruction error is retained and applied to generate a local HR patch from the corresponding HR dictionary. The performance of the proposed approach is evaluated and compared visually and quantitatively to other existing methods applied to text images. In addition, experimental results on character recognition illustrate that the proposed method outperforms the other methods, involved in this study, by providing better recognition rates.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628668,,Dictionaries;Databases;Image reconstruction;Training;Image resolution;Interpolation;Optical character recognition software,character recognition;dictionaries;document image processing;image coding;image representation;image resolution;learning (artificial intelligence);pattern clustering;text detection;visual databases,multiple learned dictionaries based clustered sparse coding;single text image superresolution;low-resolution textual image;multiple learned dictionaries based clustered SC approach;learning performance;reconstruction ability;high-resolution-low-resolution patch pair database;HR-LR patch pair database;intelligent clustering algorithm;HR-LR dictionary learning;LR image local patch representation;local HR patch;character recognition,,15,,26,,15-Oct-13,,,IEEE,IEEE Conferences
Markov Random Field Based Text Identification from Annotated Machine Printed Documents,基於專家搜索的協作環境知識共享系統,X. Peng; S. Setlur; V. Govindaraju; R. Sitaram; K. Bhuvanagiri,"Dept. of Comput. Sci. & Eng., SUNY at Buffalo, Amherst, NY, USA; Dept. of Comput. Sci. & Eng., SUNY at Buffalo, Amherst, NY, USA; Dept. of Comput. Sci. & Eng., SUNY at Buffalo, Amherst, NY, USA; HP Labs. India, Bangalore, India; HP Labs. India, Bangalore, India",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,431,435,"In this paper, we describe an approach to segment handwritten text, machine printed text and noise from annotated machine printed documents. Three categories of word level features are extracted. We use a modified K-Means clustering algorithm for classification followed by a relabeling procedure using Markov Random Field(MRF) based on a concept of neighboring patches and Belief Propagation(BP) rules. Experimental results on an imbalanced data set show that our approach achieves an overall recall of 96.33%.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.237,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277639,,Markov random fields;Feature extraction;Image segmentation;Gabor filters;Handwriting recognition;Classification algorithms;Optical character recognition software;Hidden Markov models;Text analysis;Text recognition,document image processing;feature extraction;image classification;image segmentation;Markov processes;pattern clustering;random processes;text analysis,Markov random field;text identification;annotated machine printed document;segment handwritten text;machine printed text;feature extraction;k-mean clustering algorithm;belief propagation,,19,,16,,2-Oct-09,,,IEEE,IEEE Conferences
Locating text in color document images,基於頻繁概念的文本文檔聚類,E. Orta癟a?; B. Sankur; K. Sayood,"Department of Electrical and Electronic Engineering, Bo?azi癟i University, Istanbul, Turkey; Department of Electrical and Electronic Engineering, Bo?azi癟i University, Istanbul, Turkey; Department of Electrical Engineering, University of Nebraska at Lincoln, USA",9th European Signal Processing Conference (EUSIPCO 1998),23-Apr-15,1998,,,1,4,"A novel text extraction algorithm from cluttered color document images is developed and tested. The algorithm consists of a color segmentation stage followed by rule-based filtering of non-text regions. Extraction of text segments algorithm uses the measurement of geometrical properties as well as characterness properties and a set of heuristic rules. The algorithm includes a fusion cycle of three different segmentation maps, and a restitution cycle to restore any deleted characters and/or their diacritical marks. The proposed method, proven successful in extraction of texts from many color document images, has applications in color image indexing and retrieval.",,978-960-7620-06-4,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7089601,,Image color analysis;Image segmentation;Octrees;Color;Image restoration;Quantization (signal);Clustering algorithms,document image processing;image colour analysis;image filtering;image retrieval;image segmentation;text analysis,text location;cluttered color document images;color segmentation;rule-based filtering;nontext regions;text segment extraction;heuristic rules;color image indexing;image retrieval,,,,,,23-Apr-15,,,IEEE,IEEE Conferences
Document Image Binarization Based on NFCM,自動為文件分配群集,L. Tong; K. Chen; Y. Zhang; X. Fu; J. Duan,"Multimedia Technol. Lab., North China Univ. of Technol., Beijing, China; Multimedia Technol. Lab., North China Univ. of Technol., Beijing, China; Multimedia Technol. Lab., North China Univ. of Technol., Beijing, China; Multimedia Technol. Lab., North China Univ. of Technol., Beijing, China; Multimedia Technol. Lab., North China Univ. of Technol., Beijing, China",2009 2nd International Congress on Image and Signal Processing,30-Oct-09,2009,,,1,5,"Document image binarization plays an important role in image segmentation and its effect directly impacts on the quality of the OCR recognition system. However, binarization is difficult for camera-based document images with poor contrast or illumination. In this paper, we propose a binarization algorithm, called NFCM, for camera-based document image. NFCM, a local threshold method, is a combination of Niblack algorithm and FCM (Fuzzy C-Means) algorithm. It is good at not only preserving the character stokes, but also alleviating the ghost artifacts. Comparative experiments show that NFCM can obtain favorable results with respect to the OCR performance.",,978-1-4244-4129-7,10.1109/CISP.2009.5305330,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305330,,Clustering algorithms;Optical character recognition software;Histograms;Cameras;Image segmentation;Image recognition;Lighting;Brightness;Character recognition;Lenses,cameras;document image processing;fuzzy set theory;image segmentation;optical character recognition,camera-based document image binarization algorithm;NFCM;image segmentation;OCR recognition system quality;poor contrast;poor illumination;local threshold method;Niblack algorithm;fuzzy c-means algorithm,,8,,13,,30-Oct-09,,,IEEE,IEEE Conferences
"Competitive learning mechanisms for scalable, incremental and balanced clustering of streaming texts",不同語義復雜性級別下文檔組織潛在狄利克雷分配的評估,A. Banerjee; J. Ghosh,"Dept. of Electr. & Comput. Eng., Texas Univ., Austin, TX, USA; Dept. of Electr. & Comput. Eng., Texas Univ., Austin, TX, USA","Proceedings of the International Joint Conference on Neural Networks, 2003.",26-Aug-03,2003,4,,2697,2702 vol.4,"Automated clustering of text documents such as Web pages is becoming increasingly important for organizing the vast amounts of information available over the Internet. This problem is also very challenging since typically text is represented by very high dimensional (> 1000), normalized (unit length) vectors. Moreover documents are continually being created and their statistics also change with time because of changing new-stories etc, so one needs incremental learning algorithms that can adapt to non-stationary environments. We model high-dimensional, normalized data using a mixture of von Mises-Fisher distributions, and then modify this generative model in a principled way to yield frequency sensitive competitive learning mechanisms that are applicable to streaming data, and produce balanced clusters. Experimental results on clustering of high-dimensional text data sets are provided to show the effectiveness and applicability of the proposed techniques.",1098-7576,0-7803-7898-9,10.1109/IJCNN.2003.1223993,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223993,,Learning systems;Clustering algorithms;Web pages;Organizing;Internet;Frequency;Vocabulary;Sparse matrices;Navigation,unsupervised learning;pattern clustering;Internet;text analysis,automated clustering text documents;Web pages;competitive learning mechanisms;balanced clustering;scalable clustering;statistics;documents;nonstationary environments;high-dimensional normalized data;von Mises-Fisher distributions;frequency sensitive competitive learning mechanisms;incremental clustering;Internet,,1,,19,,26-Aug-03,,,IEEE,IEEE Conferences
Efficient author community generation on Nlp based relevance feature detection,MXML：使用XML-SIM合併XML文檔的基於Web的應用程序的實現,M. Revathy; M. L. Madhavu,"Dept. of Computer Science and Engineering, Sree Buddha College of Engineering, Alappuzha, Kerala, India; Dept. of Computer Science and Engineering, Sree Buddha College of Engineering, Alappuzha, Kerala, India","2017 International Conference on Circuit ,Power and Computing Technologies (ICCPCT)",19-Oct-17,2017,,,1,5,"Many researchers provided different feature discovery techniques, but that shows high time complexity according to the LDA process. It does not provided any user preference for document search history, to avoid this problem a NLP techniques is used. NLP provide maximum pattern for search input so it is generate maximum pattern output. Ranking of the each document is according to the new patterns that generated from NLP. Community generation of documents will be done based on the cluster information; It will help document users to find other documents in the domain. Cluster the whole documents using the technique and applying the advanced ranking. So that the advanced ranking of the relevant feature produce the author community generation process. so that authors can communicate each other.",,978-1-5090-4967-7,10.1109/ICCPCT.2017.8074241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8074241,Feature extraction;Feature selection;NLP;Community generation,Feature extraction;Natural language processing;Classification algorithms;Data models;Data mining;Clustering algorithms;Computers,computational complexity;data mining;document handling;feature extraction;information retrieval;natural language processing;pattern clustering;Web sites,Nlp based relevance feature detection;high time complexity;user preference;document search history;search input;maximum pattern output;document users;advanced ranking;author community generation process;feature discovery techniques;author community generation;LDA process,,1,,16,,19-Oct-17,,,IEEE,IEEE Conferences
A method of Web search result clustering based on rough sets,通過顯式圖嵌入進行子圖點檢：應用於圖形文檔圖像中的內容點檢,Chi Lang Ngo; Hung Son Nguyen,"Inst. of Math., Warsaw Univ., Poland; Inst. of Math., Warsaw Univ., Poland",The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),17-Oct-05,2005,,,673,679,"Due to the enormous size of the Web and low precision of user queries, finding the right information from the Web can be difficult if not impossible. One approach that tries to solve this problem is using clustering techniques for grouping similar document together in order to facilitate presentation of results in more compact form and enable thematic browsing of the results set. The main problem of many Web search result (snippet) clustering algorithm is based on the poor vector representation of snippets. In this paper, we present a method of snippet representation enrichment using tolerance rough set model. We applied the proposed method to construct a rough set based search result clustering algorithm and compared it with other recent methods.",,0-7695-2415-X,10.1109/WI.2005.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517931,rough sets;snippet;clustering,Web search;Rough sets;Clustering algorithms;Search engines;Mathematics;Web pages;Navigation;Algorithm design and analysis;Scalability;Set theory,information retrieval;Internet;document handling;rough set theory;pattern clustering,Web search;document clustering;vector representation;snippet representation enrichment;tolerance rough set model,,2,,15,,17-Oct-05,,,IEEE,IEEE Conferences
Pseudo-Supervised Approach for Text Clustering Based on Consensus Analysis,最終用戶的聚類方法和半徑調整,P. Chen; W. Guo; L. Dai; Z. Ling,"National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",13-Sep-18,2018,,,6184,6188,"In recent years, neural networks (NN) have achieved remarkable performance improvement in text classification due to their powerful ability to encode discriminative features by incorporating label information into model training. Inspired by the success of NN in text classification, we propose a pseudo-supervised neural network approach for text clustering. The neural network is trained in a supervised fashion with pseudo-labels, which are provided by the cluster labels of pre-clustering on unsupervised document representations. To enhance the quality of pseudo-labels, a consensus analysis is employed to select training samples for the neural network. The experimental results demonstrate that the proposed approach can improve the clustering performance significantly.",2379-190X,978-1-5386-4658-8,10.1109/ICASSP.2018.8462376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462376,Text clustering;pseudo-supervised;consensus analysis,Training;Semantics;Clustering algorithms;Artificial neural networks;Analytical models;Task analysis,learning (artificial intelligence);neural nets;pattern clustering;text analysis,text clustering;consensus analysis;text classification;label information;model training;cluster labels;pre-clustering;clustering performance;performance improvement;neural network approach;pseudo supervised approach;unsupervised document representations,,1,,29,,13-Sep-18,,,IEEE,IEEE Conferences
Topic Detection Based on Group Average Hierarchical Clustering,基於非負矩陣分解的新型描述性聚類算法,N. Gao; L. Gao; Y. He; H. Wang; Q. Sun,"Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China; Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China; Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China; Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China; Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China",2013 International Conference on Advanced Cloud and Big Data,5-Jun-14,2013,,,88,92,"Via analyzing characters of vast disaster news on the Internet, a new topic detection algorithm based on Group Average Hierarchical Clustering (GAHC), which is suitable for the processing of big data on the network, is proposed in this paper. The core idea of GAHC is to divide big data into smaller groups, and then cluster groups hierarchically to generate final topics. During the process of clustering, vector space modal is used to represent news documents, and a similarity calculation model based on weights of time and place is proposed. The new algorithm can automatically organize similar disaster news materials, generate news topics, furthermore provide personalized service for users and form the topic detection system for disaster news. Experimental results demonstrate that the performance of the algorithm is good.",,978-1-4799-3261-0,10.1109/CBD.2013.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824578,topic detection;clustering;big data;GAHC,Clustering algorithms;Vectors;Algorithm design and analysis;Heuristic algorithms;Rocks;Internet,Big Data;disasters;document handling;Internet;pattern clustering,group average hierarchical clustering;disaster news character analysis;Internet;GAHC;Big Data processing;vector space modal;news document representation;similarity calculation model;time weights;place weights;automatic similar disaster news material organization;news topic generation;personalized service;topic detection system,,1,,10,,5-Jun-14,,,IEEE,IEEE Conferences
Utilizing semantic information from linked open data in web service clustering,通過調整隸屬函數使文檔表示適合特定的數據集,Y. Gu; H. Cai; C. Xie; L. Jiang; Y. Gu; A. Liu,"School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China; School of Software, Shanghai Jiao Tong University, Shanghai, China; Department of Information, Shanghai Waigaoqiao Shipbuilding Co., Ltd, Shanghai, China",2016 International Conference on Progress in Informatics and Computing (PIC),19-Jun-17,2016,,,654,658,"With web services growing and accumulating in application area, service discovery has become a hot issue for service composition and service management. Service clustering provides a promising way to split the whole searching space into small regions so as to minimize the discovery time effectively. However, semantic information is a critical element during the whole disposing process. Current industrialized Web Service Description Language (WSDL) does not contain enough information for service description. Therefore, a service clustering method has been proposed, which enhances original WSDL document with semantic information by means of Linked Open Data (LOD). Experiment based on real service data has been performed, and comparison with similar methods has also been provided to demonstrate the effectiveness of the method. It is shown that utilizing semantic information from LOD enhances the precision of service clustering. And it forms a sound base for further comprehensive processing with semantic information.",,978-1-5090-3484-0,10.1109/PIC.2016.7949580,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949580,web service;service clustering;Linked Open Data,Semantics;XML;Ontologies;Grain size;Clustering methods;Service-oriented architecture,document handling;pattern clustering;Web services,semantic information;linked open data;Web service clustering;service composition;service management;discovery time minimization;industrialized Web service description language;WSDL document;LOD;real service data,,,,15,,19-Jun-17,,,IEEE,IEEE Conferences
Visual inter-word relations and their use in OCR postprocessing,自動和交互式規則推理，無需地面真理,T. Hong; J. J. Hull,"CEDAR, State Univ. of New York, Buffalo, NY, USA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,442,445 vol.1,A technique is presented that uses visual relationships between word images in a document to improve the recognition of the text it contains. This technique takes advantage of the visual relationships between word images that are usually lost in most conventional optical character recognition (OCR) techniques. The visual relations are defined to be the equivalence that exists between images of the same word or portions of word images. An algorithm is presented that calculates these relationships in a document. The resulting clusters are integrated with the recognition results provided by an OCR system. Inconsistencies in OCR results between equivalent images are identified and used to improve recognition performance. Experimental results are presented in which the input is provided directly from a commercial OCR system.,,0-8186-7128-9,10.1109/ICDAR.1995.599031,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599031,,Optical character recognition software;Text recognition;Image recognition;Character recognition;Clustering algorithms;Text analysis;Image analysis;Marine vehicles;Digital images;Image segmentation,optical character recognition;document image processing,inter-word relations;OCR postprocessing;character recognition;recognition performance;equivalent images;word images;document,,9,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Constrained Text Coclustering with Supervised and Unsupervised Constraints,InLinx用於文檔分類，共享和推薦,Y. Song; S. Pan; S. Liu; F. Wei; M. X. Zhou; W. Qian,"Microsoft Research Asia, Beijing; IBM Research - T. J. Watson Center, Hawthorne; Microsoft Research Asia, Beijing; Microsoft Research Asia, Beijing; IBM Research - Almaden Center, San Jose; IBM Research-China, Beijing",IEEE Transactions on Knowledge and Data Engineering,19-Apr-13,2013,25,6,1227,1239,"In this paper, we propose a novel constrained coclustering method to achieve two goals. First, we combine information-theoretic coclustering and constrained clustering to improve clustering performance. Second, we adopt both supervised and unsupervised constraints to demonstrate the effectiveness of our algorithm. The unsupervised constraints are automatically derived from existing knowledge sources, thus saving the effort and cost of using manually labeled constraints. To achieve our first goal, we develop a two-sided hidden Markov random field (HMRF) model to represent both document and word constraints. We then use an alternating expectation maximization (EM) algorithm to optimize the model. We also propose two novel methods to automatically construct and incorporate document and word constraints to support unsupervised constrained clustering: 1) automatically construct document constraints based on overlapping named entities (NE) extracted by an NE extractor; 2) automatically construct word constraints based on their semantic distance inferred from WordNet. The results of our evaluation over two benchmark data sets demonstrate the superiority of our approaches against a number of existing approaches.",1558-2191,,10.1109/TKDE.2012.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165284,Constrained clustering;coclustering;unsupervised constraints;text clustering,Clustering algorithms;Semantics;Hidden Markov models;Sparse matrices;Clustering methods;Humans;Computational modeling,constraint handling;document handling;expectation-maximisation algorithm;hidden Markov models;pattern clustering,constrained text coclustering;supervised constraints;unsupervised constraints;constrained coclustering method;information-theoretic coclustering;knowledge sources;manually labeled constraints;two-sided hidden Markov random field model;HMRF;expectation maximization algorithm;EM;document constraints;overlapping named entities;NE extractor;word constraints;semantic distance;WordNet,,12,,34,,6-Mar-12,,,IEEE,IEEE Journals
Learning the kernel matrix for XML document clustering,建立針對Web文檔分析的現代化和特定於Web的清單,Jianwu Yang; W. K. Cheung; Xiaoou Chen,"Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; NA; NA","2005 IEEE International Conference on e-Technology, e-Commerce and e-Service",11-Apr-05,2005,,,353,358,"The rapid growth of XML adoption has urged for the need of a proper representation for semi-structured documents, where the document structural information has to be taken into account so as to support more precise document analysis. In this paper, an XML document representation named ""structured link vector model"" is adopted, with a kernel matrix included for modeling the similarity between XML elements. Our formulation allows individual XML elements to have their own weighted contribution to the overall document similarity while at the same time allows the between-element similarity to be captured. An iterative algorithm is derived to learn the kernel matrix. For performance evaluation, the ACM SIGMOD record dataset as well as the CEDE dataset have been tested. Our proposed method outperforms significantly the traditional vector space model and the edit-distance based methods. In addition, the kernel matrix obtained as a by-product provides knowledge about the conceptual relationship between the XML elements.",,0-7695-2274-2,10.1109/EEE.2005.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1402321,,Kernel;XML;Computer science;Text analysis;Information analysis;Iterative algorithms;Testing;Fourier transforms;Training data,XML;pattern clustering;learning (artificial intelligence);data mining,XML document clustering;kernel matrix;semistructured documents;document structural information;structured link vector model;iterative algorithm;performance evaluation;ACM SIGMOD record dataset;CEDE dataset;edit-distance based method,,9,,14,,11-Apr-05,,,IEEE,IEEE Conferences
Word segmentation of printed text lines based on gap clustering and special symbol detection,一種有效的場景圖像管理方法,S. H. Kim; C. B. Jeong; H. K. Kwag; C. Y. Suen,"Dept. of Comput. Sci., Chonnam Nat. Univ., Kwangju, South Korea; Dept. of Comput. Sci., Chonnam Nat. Univ., Kwangju, South Korea; NA; NA",Object recognition supported by user interaction for service robots,10-Dec-02,2002,2,,320,323 vol.2,"This paper proposes a word segmentation method for machine-printed text lines. It utilizes gaps and special symbols as delimiters between words. A gap clustering technique is used to identify the gaps between words regardless of the gap-size variations among different document images. Next a special symbol detection technique is applied to find two types of special symbols lying between words. An experiment with 1,675 text lines in 100 different English and Korean documents shows that the proposed method achieves a high accuracy of word segmentation.",1051-4651,0-7695-1695-X,10.1109/ICPR.2002.1048304,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048304,,Image segmentation;Optical character recognition software;White spaces;Size measurement;Computer science;Artificial intelligence;Pattern recognition;Machine intelligence;Optical devices;Document image processing,character recognition;image segmentation,word segmentation;printed text lines;word segmentation method;machine-printed text lines;delimiters;gap clustering technique;symbol detection technique;Korean documents;English documents;gap clustering;symbol detection,,19,,7,,10-Dec-02,,,IEEE,IEEE Conferences
Organizing Hidden-Web Databases by Clustering Visible Web Documents,改進的文檔聚類相似度度量,L. Barbosa; J. Freire; A. Silva,"University of Utah, lbarbosa@cs.utah.edu; University of Utah, juliana@cs.utah.edu; Universidade Federal do Amazonas, alti@dcc.ufam.edu.br",2007 IEEE 23rd International Conference on Data Engineering,4-Jun-07,2007,,,326,335,"In this paper we address the problem of organizing hidden-Web databases. Given a heterogeneous set of Web forms that serve as entry points to hidden-Web databases, our goal is to cluster the forms according to the database domains to which they belong. We propose a new clustering approach that models Web forms as a set of hyperlinked objects and considers visible information in the form context - both within and in the neighborhood of forms - as the basis for similarity comparison. Since the clustering is performed over features that can be automatically extracted, the process is scalable. In addition, because it uses a rich set of metadata, our approach is able to handle a wide range of forms, including content-rich forms that contain multiple attributes, as well as simple keyword-based search interfaces. An experimental evaluation over real Web data shows that our strategy generates high-quality clusters - measured both in terms of entropy and F-measure. This indicates that our approach provides an effective and general solution to the problem of organizing hidden-Web databases.",2375-026X,1-4244-0802-4,10.1109/ICDE.2007.367878,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221681,,Organizing;Crawlers;Information retrieval;Spatial databases;Context modeling;Data mining;Entropy;Large-scale systems;Humans;Probes,database management systems;document handling;Internet;pattern clustering,hidden-Web database;Web document clustering;Web forms;hyperlinked objects,,26,,40,,4-Jun-07,,,IEEE,IEEE Conferences
Document categorization in multi-agent environment with enhanced machine learning classifier,使用XCLS集群設計知識庫的新方法,S. Singh; C. Prakash,"Dept. of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Dept. of Computer Science and Engineering, Lovely Professional University, Phagwara, India",2014 Seventh International Conference on Contemporary Computing (IC3),15-Sep-14,2014,,,589,594,"Text categorization task have gained the attention of researchers in last 10 years with the increase in web-based contents of documents. For searching a particular document from the web or any large document collection text or document categorization is most useful task. We demand some better system and enhanced machine learning classifiers to accomplish task of document categorization. We designed a multi-agent based system that consists of some software hybrid agents that obtains the category of a document and interact with each other to take final decision about the category and then data is fed to a machine learning classifier in order to enhance the performance. We analyzed the results of the system in form of performance measures such as accuracy, recall, precision and true negative rate. We analyzed the result in two scenarios: one is with decision of agents alone and another is with application of reinforcement clustering technique neural network. We observed that in first scenario the system's accuracy, precision and true negative rate is very good and recall measure is significantly good. After the application of reinforcement clustering technique there is no significant change in system's performance instead the recall is degraded. But still the system is producing good results in both scenarios.",,978-1-4799-5173-4,10.1109/IC3.2014.6897239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6897239,text categorization;machine learning;neural network;agent,Neural networks;Text categorization;Accuracy;Feature extraction;Probability;Equations;Mathematical model,document handling;Internet;learning (artificial intelligence);multi-agent systems;neural nets;pattern classification;pattern clustering,document categorization;multiagent system;machine learning classifier;Web documents;reinforcement clustering;neural network,,,,10,,15-Sep-14,,,IEEE,IEEE Conferences
A time-aware approach for boosting medical records search,區域Web文檔的基於屬性的內容挖掘,J. Zhang; W. Xu; J. Guo,"Lab for Pattern Recognition and Intelligent Systems, School of Information and Communications Engineering, Beijing University of Posts and Telecommunications, China; Lab for Pattern Recognition and Intelligent Systems, School of Information and Communications Engineering, Beijing University of Posts and Telecommunications, China; Lab for Pattern Recognition and Intelligent Systems, School of Information and Communications Engineering, Beijing University of Posts and Telecommunications, China",2016 Digital Media Industry & Academic Forum (DMIAF),26-Sep-16,2016,,,99,102,"Medical records are collections of documents recording a patient's changing conditions, exhibiting temporal characteristic. Yet previous works on medical records search did not pay attention to it. We propose to model the medical records as sequential data, and utilize the temporal similarity between them to improve the performance of medical records search. In this paper, we propose a Temporal Bag-of-Words model to represent medical records as document sequence. In which framework, we adopt Dynamic Time Warping algorithm to calculate the temporal similarity between sequences. Then a clustering-based combination method is proposed for re-ranking. Experiments on TREC Medical Track data shows the effectiveness of the proposed framework for boosting medical records search.",,978-1-5090-1000-4,10.1109/DMIAF.2016.7574910,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574910,,Heuristic algorithms;Clustering algorithms;Data models;Boosting;Algorithm design and analysis;Bioinformatics;Biological system modeling,document handling;electronic health records;information retrieval;pattern clustering,time-aware approach;medical records search;sequential data;temporal similarity;temporal bag-of-words model;document sequence;dynamic time warping algorithm;clustering-based combination method;TREC Medical Track data,,,,15,,26-Sep-16,,,IEEE,IEEE Conferences
Cloud-based clustering of text documents using the GHSOM algorithm on the GridGain platform,從泰盧固語文檔圖像中提取可靠的文本行，單詞和字符,M. Sarnovsky; Z. Ulbrik,"Department of cybernetics and artificial intelligence, Faculty of electrical engineering and informatics, Technical University in Kosice, Letna 9/A, 042 00 Kosice, Slovak Republic; Department of cybernetics and artificial intelligence, Faculty of electrical engineering and informatics, Technical University in Kosice, Letna 9/A, 042 00 Kosice, Slovak Republic",2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI),26-Sep-13,2013,,,309,313,"This paper provides an overview of our research activities aimed on efficient use of distributed computing concepts for text-mining tasks. Work presented within this paper describes the GHSOM (Growing Hierarchical Self-Organizing Maps) algorithm for clustering of text documents and proposes the design and implementation of distributed version of this approach. Proposed implementation is based on JBOWL framework as a base for text mining. For distribution we used MapReduce paradigm implemented within the GridGain framework, which was used as a cloud application platform. Experiments were performed on standard Reuters dataset and for testing purposes we decided to use a simple private cloud infrastructure.",,978-1-4673-6400-3,10.1109/SACI.2013.6608988,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608988,,Neurons;Vectors;Clustering algorithms;Algorithm design and analysis;Classification algorithms;Java;Informatics,cloud computing;data mining;parallel programming;pattern clustering;self-organising feature maps;text analysis,cloud-based clustering;text documents clustering;GHSOM algorithm;GridGain platform;growing hierarchical self-organizing maps algorithm;JBOWL framework;text mining;MapReduce paradigm;cloud application platform;Reuters dataset;private cloud infrastructure;Java bag-of-words library,,13,,12,,26-Sep-13,,,IEEE,IEEE Conferences
Marrying K-means with Evidence Accumulation in Clustering Analysis,以謂詞-自變量結構表示的案例向量的文檔聚類-用於從專利中引出技術問題,H. Zhang; X. Guo; L. Ye; S. Li,"School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China",2018 IEEE 4th International Conference on Computer and Communications (ICCC),1-Aug-19,2018,,,2050,2056,"Text clustering is becoming increasingly important to text mining and to the development of commercial applications. Previous research mainly focused on single clustering on documents. Compared with cluster ensembles, segmentations obtained from single clustering runs are less convincing in terms of accuracy and consistency. In this paper, we propose an approach based on evidence accumulation clustering (EAC) with k-means for text clustering problems. Our goal is to obtain a consistent, stable, and credible clustering scheme. First, we ran the k-means algorithm multiple times while the number of clusters ranges in an optimum area. Then, we constructed a matrix called co-association matrix by integrating all the derived clustering partitions. Finally, we obtained consistent clusters by performing hierarchical cluster algorithm on the co-association matrix. The linkage criterion used was a single link. The above process is equivalent to the process of finding a minimum spanning tree (MST) for a completed graph determined by a co-association matrix. The algorithm was tested on four text data sets. Experimental results showed that our method improves the accuracy of the final results.",,978-1-5386-8339-2,10.1109/CompComm.2018.8780791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8780791,K-means;evidence accumulation clustering;hierar-chical clustering,Clustering algorithms;Partitioning algorithms;Optimization;Distortion;Mathematical model;Covariance matrices;Text mining,data mining;matrix algebra;optimisation;pattern clustering;text analysis;trees (mathematics),text mining;evidence accumulation clustering;text clustering problems;co-association matrix;k-means algorithm;EAC;optimisation;minimum spanning tree;MST;graph,,,,20,,1-Aug-19,,,IEEE,IEEE Conferences
Improving classification performance by extending documents terms,使用模糊表示和HAC的Web文檔分類,Widodo; W. C. Wibowo,"Faculty of Computer Science, University of Indonesia, Jakarta, Indonesia; Faculty of Computer Science, University of Indonesia, Jakarta, Indonesia",2014 International Conference on Data and Software Engineering (ICODSE),19-Mar-15,2014,,,1,5,"Classification is a technique in data mining for categorizing objects. Text Classification is re-challenged for classifying very short documents or text as shown in social media collection. This paper proposes a method to improve the performance of classification on short documents. In this work, we expand words in every document before the documents are classified We use TFIDF model, Hidden Markov Model k-means clustering, and Latent Semantic Indexing (LSI) for expanding documents. The results show that extending document term by just 1 word will increase its accuracy, while extending by 2,4, and 8 words tend to give stable results.",,978-1-4799-7996-7,10.1109/ICODSE.2014.7062657,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062657,text classification;TFIDF model;Hidden Markov Model k-means;Latent Semantic Indexing;extend words,Accuracy;Hidden Markov models;Bayes methods;Bagging;Text categorization;Semantics,category theory;classification;data mining;hidden Markov models;indexing;pattern clustering;text analysis,documents terms;data mining;object categorization;text classification;TFIDF model;hidden Markov model;k-means clustering;latent semantic indexing;LSI,,1,,15,,19-Mar-15,,,IEEE,IEEE Conferences
On dynamic data clustering and visualization using swarm intelligence,文檔語料庫的組織和可視化：一種概率方法,E. Saka; O. Nasraoui,"Knowledge Discovery and Web Mining Lab, University of Louisville, KY, USA; Knowledge Discovery and Web Mining Lab, University of Louisville, KY, USA",2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010),22-Apr-10,2010,,,337,340,"Clustering and visualizing high-dimensional sparse data simultaneously is a very attractive goal, yet it is also a challenging problem. Our previous studies using a special type of swarms, known as flocks of agents, provided some promising approaches to this challenging problem on several limited size UCI machine learning data sets and Web usage sessions (from web access logs). However, dynamic domains, such as practically any data generated on the Web, may require frequent costly updates of the clusters (and the visualization), whenever new data records are added to the dataset. The new coming data may be due to new user activity on a website (clickstreams) or a search engine (queries), or new Web pages in the case of document clustering, etc. Additionally, data records may result in a change of clustering in time. Therefore, clusters may need to be updated, thus leading to the need to mine dynamic clusters. This paper summarizes our initial studies in designing a simultaneous clustering and visualization algorithm and proposes the Dynamic-FClust Algorithm, which is based on flocks of agents as a biological metaphor. This algorithm falls within the swarm-based clustering family, which is unique compared to other approaches, because its model is an ongoing swarm of agents that socially interact with each other, and is therefore inherently dynamic.",,978-1-4244-6523-1,10.1109/ICDEW.2010.5452721,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5452721,,Data visualization;Particle swarm optimization;Clustering algorithms;Image segmentation;Iterative algorithms;Web mining;Machine learning;Search engines;Web pages;Algorithm design and analysis,data visualisation;learning (artificial intelligence);multi-agent systems;pattern clustering,dynamic data clustering;data visualization;swarm intelligence;high-dimensional sparse data;flocks-of-agents;UCI machine learning data set;Web usage session;data record;dynamic-FClust algorithm;biological metaphor;swarm-based clustering,,2,,12,,22-Apr-10,,,IEEE,IEEE Conferences
Refining the extraction of relevant documents from biomedical literature to create a corpus for pathway text mining,推文上的聚類和主題建模：健康數據集的比較,H. Harte; Y. Lu; S. Osborn; D. Dehoney; D. Chin,"PPD Discovery, Inc., Mento Park, CA, USA; PPD Discovery, Inc., Mento Park, CA, USA; PPD Discovery, Inc., Mento Park, CA, USA; PPD Discovery, Inc., Mento Park, CA, USA; PPD Discovery, Inc., Mento Park, CA, USA",Computational Systems Bioinformatics. CSB2003. Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003,8-Sep-03,2003,,,644,645,"For biologists to keep up with developments in their field or related fields, automation is desirable to more efficiently read and interpret a rapidly growing literature. Identification of proteins or genes and their interactions can facilitate the mapping of canonical or evolving pathways from the literature. In order to mine such data, we developed procedures and tools to pre-qualify documents for further analysis. Initially, a corpus of documents for proteins of interest was built using alternate symbols from Locuslink and the Stanford SOURCE as MEDLINE search terms. The query was refined using the optimum keywords together with MeSH terms combined in a Boolean query to minimize false positives. The document space was examined using a strategy employing; latent semantic indexing (LSI), which uses Entrez's ""related papers"" utility for MEDLINE. Documents' relationships were visualized using an undirected graph and scored by their relatedness. Distinct document clusters, formed by the most highly connected related papers, are mostly composed of abstracts relating to one aspect of research. This feature was used to filter irrelevant abstracts, which resulted in a reduction in corpus size of 10% to 30% depending on the domain. The excluded documents were examined to confirm their lack of relevance. Corpora consisted of the most relevant documents thus reducing the number of false positives and irrelevant examples in the training set for pathway mapping. Documents were tagged, using a modified version of GATE2, with terms based on GO for rule induction using RAPIER.",,0-7695-2000-6,10.1109/CSB.2003.1227432,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227432,,Text mining;Proteins;Abstracts;Automation;Data mining;Text analysis;Indexing;Large scale integration;Visualization;Filters,proteins;molecular biophysics;data mining;genetics;biology computing,relevant documents extraction;biomedical literature;corpus creation;pathway text mining;biologists;automation;proteins identification;genes identification;canonical mapping;data mining;pre-qualify documents;symbols usage;LocusLink;Stanford SOURCE;MEDLINE search terms;optimum keywords;MeSH terms;Boolean query;latent semantic indexing;Entrez related paper;documents relationship visualization;undirected graph;document clusters;corpora;pathway mapping;GATE2;genetic ontology;rule induction;RAPIER,,1,,6,,8-Sep-03,,,IEEE,IEEE Conferences
Pecos: A Scalable Solution for Analyzing and Managing Qualitative Data,使用敘詞表和神經網絡生成文檔集群,R. Arora; T. N. Ba; T. A. Connors,"Texas Adv. Comput. Center, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Texas State Univ., San Marcos, TX, USA",2016 Seventh International Workshop on Data-Intensive Computing in the Clouds (DataCloud),9-Feb-17,2016,,,17,23,"Large, heterogeneous, and complex data collections can be difficult to analyze and manage manually. There is a need for scalable and user-friendly approaches that can automate the analysis and management of such collections in a timely and efficient manner. To meet the aforementioned need, we are developing a system named Pecos which combines (1) an android application, (2) cloud computing middleware and resources, and (3) High Performance Computing (HPC) platform and software, for performing large-scale data analysis through an easy-to-use interface. Currently, Pecos can be used to analyze and manage data collections residing on Google drive or on remote Linux systems that are accessible via SSH connection. Some of the steps in the data analysis and management process that are already enabled through Pecos are (1) content-based classification and clustering of documents, (2) filtering or searching the documents on the basis of a text pattern, (3) performing checksum analysis and metadata extraction, (4) supporting format conversion of documents, and, (5) developing visualizations for analyzing the collection. Work is under progress to (1) optimize the algorithms in Pecos for document analysis, (2) develop a recommendation system, and (3) integrate the functionality for analyzing data from social media.",,978-1-5090-6158-7,10.1109/DataCloud.2016.006,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845277,Document analysis;HPC;data management;android application;topic modeling;data mining;information search and retrieval,Data collection;Data analysis;Metadata;Androids;Humanoid robots;Web servers;Data visualization,Android (operating system);cloud computing;data analysis;document handling;meta data;middleware;parallel processing;pattern classification;pattern clustering,Pecos;qualitative data management;qualitative data analysis;complex data collections;user-friendly approaches;cloud computing middleware;cloud computing resources;high performance computing platform;HPC platform;large-scale data analysis;Google drive;remote Linux systems;SSH connection;content-based document classification;content-based document clustering;document searching;document filtering;checksum analysis;metadata extraction;document format conversion;social media data analysis,,1,,19,,9-Feb-17,,,IEEE,IEEE Conferences
A Distributed Multi-exemplar Affinity Propagation Clustering Algorithm Based on MapReduce,跨階段切碎文本文檔的兩階段重建方法,Y. Yang; C. Wang; J. Lai,"Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China; Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China; Sch. of Data & Comput. Sci., Sun Yat-sen Univ., Guangzhou, China",2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService),12-Jun-17,2017,,,191,197,"Clustering algorithm is one of the fundamental techniques in data mining, which plays a crucial role in various applications, such as pattern recognition, document retrieval, and computer vision. As so far, many effective algorithms have been proposed. Affinity Propagation is an algorithm requires no parameter indicating the number of clusters, which is the most distinguishing advantage compared to the k-means clustering algorithm. Multi-Exemplar Affinity Propagation (MEAP) extends the single-exemplar model to the multi-exemplar model, which could describe the dataset with more complex structure. With the amount of data increasing rapidly, the growing size of dataset makes the clustering problem become more and more challenging. To solve this problem, the parallel computing framework is widely used, such as MapReduce. However, for the MEAP algorithm, it is not a straightforward task to implement the updating of MEAP messages in MapReduce, which without proper design would be time-consuming. In this paper, we propose to utilize the stability of data distribution to apply the MEAP algorithm on the MapReduce platform and develop an efficient Distributed Multi-Exemplar Affinity Propagation (DisMEAP) clustering algorithm by using three MapReduce stages. The experiment results demonstrate that our algorithm can perform well in processing large-scale datasets and could achieve the same accuracy as the original MEAP algorithm.",,978-1-5090-6318-5,10.1109/BigDataService.2017.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944938,Clustering;Multi-exemplar;Affinity propagation;Parallel system;MapReduce,Clustering algorithms;Algorithm design and analysis;Distributed databases;Computational modeling;Clustering methods;Sparks;Couplings,data mining;distributed algorithms;parallel processing;pattern clustering,clustering algorithm;data mining;single-exemplar model;parallel computing framework;data distribution stability;MapReduce platform;DisMEAP clustering algorithm;distributed multiexemplar affinity propagation clustering algorithm,,,,20,,12-Jun-17,,,IEEE,IEEE Conferences
Text detection in color scene images based on unsupervised clustering of multi-channel wavelet features,基於序列的文本聚類動態SOM模型,T. Saoi; H. Goto; H. Kobayashi,"Graduate Sch. of Inf. Sci., Tohoku Univ., Sendai, Japan; NA; NA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,690,694 Vol. 2,"Texts in natural scenes provide us with much useful information. In order to use such information automatically, it is necessary to make computers detect text regions in the images. Gllavata et. al. proposed a method based on unsupervised classification of high frequency wavelet coefficients for text detection in video frames [Gllavata et. al. (2004)]. Although the method is very accurate, it does not work so well with some color images, since it lacks the ability of discriminating color difference. This paper proposes an enhanced version of the method. We develop a new unsupervised clustering technique for the classification of multi-channel wavelet features to deal with color images. Experimental results show that the new method yields better results for color scene images.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.227,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575633,,Layout;Color;Gray-scale;Frequency;Wavelet coefficients;Intelligent robots;Data mining;Robotics and automation;Text recognition;Robot vision systems,image colour analysis;pattern clustering;character recognition;wavelet transforms;text analysis,text detection;color scene images;unsupervised clustering;multichannel wavelet features;unsupervised classification,,10,,10,,16-Jan-06,,,IEEE,IEEE Conferences
Postal envelope segmentation by 2-D histogram clustering through watershed transform,報紙文章識別算法的性能評估,E. Akira Yonekura; J. Facon,"Pontificia Univ. Catolica do Parana, Curitiba, Brazil; Pontificia Univ. Catolica do Parana, Curitiba, Brazil","Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,338,342 vol.1,"In this paper we present a new postal envelope segmentation method based on 2-D histogram clustering and watershed transform. Segmentation task consists in detecting the modes associated with homogeneous regions in envelope images such as handwritten address block, postmarks, stamps and background. The homogeneous modes in 2-D histogram are segmented through the morphological watershed transform. Our approach is applied to complex Brazilian postal envelopes. Very little a priori knowledge of the envelope images is required. The advantages of this approach will be described and illustrated with tests carried out on 300 different images where there are no fixed position for the handwritten address block, postmarks and stamps.",,0-7695-1960-1,10.1109/ICDAR.2003.1227685,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227685,,Histograms;Image segmentation;Postal services;Sorting;Testing;Envelope detectors;Automation;Graphics;Gray-scale;Image reconstruction,postal services;image recognition;wavelet transforms;handwritten character recognition;pattern clustering,postal envelope segmentation;2D histogram clustering;watershed transform;modal detection;homogeneous regions;envelope images;handwritten address block;postmarks;stamps;envelope background;Brazilian postal envelopes,,3,,12,,8-Sep-03,,,IEEE,IEEE Conferences
Clustering Web Documents Based on Correlation of Hyperlinks,受SIFT方向信息約束的半監督聚類圖像標註,Kou Takahashi; T. Miura; I. Shioya,Hosei University; NA; NA,21st International Conference on Data Engineering Workshops (ICDEW'05),5-Jul-06,2005,,,1225,1225,"In this investigation, we propose a new Web clustering approach based on combining cluster results.",,0-7695-2657-8,10.1109/ICDE.2005.204,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1647842,Web Mining;Web Clustering;Summarization,Web pages;Data mining;Frequency;Search engines;XML;Engineering management;Informatics;Web mining;Floods;Usability,,Web Mining;Web Clustering;Summarization,,1,,8,,5-Jul-06,,,IEEE,IEEE Conferences
Recognizing Patterns in Text Data through Effective Initialization of Spherical K-means,自組織地圖和K均值用於安納巴-阿爾及利亞地區的氣象日類型識別,I. Sharma; H. Sharma,"Career Point University, Kota, India; Dept. of CSE, Rajasthan Technical University, Kota, India","2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)",30-Sep-18,2018,,,327,331,Spherical k-means is a fast and effective method for clustering text documents. But its performance is greatly affected by the initial centroids or seeds. It has been established that D2-sampling based methods like kmeans++ are very effective initialization methods for k-means to produce well separated seeds. A similar adaptation for spherical k-means is not possible due to huge dimensionality of text corpus. Pairwise distance among all text documents is impractical. We propose two seeding methods for spherical k-means that performs equally well but are computationally affordable. Separation among output clusters and cohesiveness are used to evaluate performance of the initialization methods over popular text corpus.,,978-1-5386-0965-1,10.1109/ICECA.2018.8474766,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474766,text clustering;document clutsreing;spherical km-means;kmeans++;initialization;seeding;text corpus,Conferences;Memory management;Time complexity;Aerospace electronics;Clustering methods;Clustering algorithms,pattern clustering;sampling methods;text analysis,initial centroids;D2-sampling based methods;pairwise distance;seeding methods;output clusters;text data;spherical k-means;text document clustering;pattern recognition;text corpus,,,,17,,30-Sep-18,,,IEEE,IEEE Conferences
Document Retrieval Based on Logo Spotting Using Key-Point Matching,高效序列聚類的框架,V. P. Le; N. Nayef; M. Visani; J. -M. Ogier; C. De Tran,"Lab. L3I, La Rochelle Univ., La Rochelle, France; Lab. L3I, La Rochelle Univ., La Rochelle, France; Lab. L3I, La Rochelle Univ., La Rochelle, France; Lab. L3I, La Rochelle Univ., La Rochelle, France; Coll. of Inf. & Commun. Technol., Can Tho Univ., Can Tho, Vietnam",2014 22nd International Conference on Pattern Recognition,6-Dec-14,2014,,,3056,3061,"In this paper, we present an approach to retrieve documents based on logo spotting and recognition. A document retrieval system is proposed inspired from our previous method for logo spotting and recognition. First, the key-points from both the query logo images and a given set of document images are extracted and described by SIFT descriptor, and are matched in the SIFT feature space. They are filtered by the nearest neighbor matching rule based on the two nearest neighbors and are then post-filtered with BRIEF descriptor. Secondly, logo segmentation is performed using spatial density-based clustering, and homography is used to filter the matched key-points as a post processing. Finally, for ranking, we use two measures which are calculated based on the number of matched key-points. Tested on a well-known benchmark database of real world documents containing logos Tobacco-800, our approach achieves better performance than the state-of-the-art methods.",1051-4651,978-1-4799-5209-0,10.1109/ICPR.2014.527,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977239,document retrieval;logo spotting;logo recognition;key-point matching,Matched filters;Accuracy;Feature extraction;Databases;Shape;Materials requirements planning;Optical filters,document image processing;feature extraction;filtering theory;image segmentation;information retrieval;pattern clustering;transforms,logo spotting;document retrieval system;logo recognition;key-point matching;query logo images;SIFT descriptor;SIFT feature space;nearest neighbor matching rule;BRIEF descriptor;logo segmentation;spatial density-based clustering;filtering method,,13,,20,,6-Dec-14,,,IEEE,IEEE Conferences
An improved sIB algorithm for document clustering using combination weighting measures,專利檢索和趨勢分析,Bo Ji; Y. Ye,"School of Information Engineering, Zhengzhou University, 450052, China; School of Information Engineering, Zhengzhou University, 450052, China",2011 IEEE International Conference on Computer Science and Automation Engineering,14-Jul-11,2011,3,,110,114,"This paper presents an improved sIB algorithm (CW-sIB) for high dimension document clustering using combination weighting. Traditionally, feature weighting researches on clustering devote themselves to search one single effective weighting scheme. However, how to choose a proper weighting scheme is a generally acknowledged devilish problem. To address this issue, we propose the linear combination weighting method derived from the idea of combination evaluation for multiple attribute decision making problem. The application of combination weighting can overcome the limitations of using single weighting scheme. It will help to reflect the essential characteristics of the document data better. The experiments on real document data have shown that the proposed CW-sIB algorithm is superior to the sIB algorithm. Meanwhile, we report results as to which combination of weighting scheme elements show merit in the decomposition of datasets.",,978-1-4244-8728-8,10.1109/CSAE.2011.5952644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952644,,Clustering algorithms;Partitioning algorithms;Indexes;Weight measurement;Accuracy;Complexity theory;Text categorization,decision making;pattern clustering;text analysis,improved sIB algorithm;CW-sIB algorithm;document clustering;combination weighting measures;feature weighting;linear combination weighting method;multiple attribute decision making problem;text categorization;text mining methods;information retrieval;machine learning,,,,15,,14-Jul-11,,,IEEE,IEEE Conferences
An Optimized K-Means Algorithm Based on FSTVM,基於可伸縮距離的聚類的Web意見分析,Y. Chen; P. Sun,"Dept. of Comput. Sci., Dalian Neusoft Inst. of Inf., Dalian, China; Dept. of Comput. Sci., Dalian Neusoft Inst. of Inf., Dalian, China",2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS),11-Nov-18,2018,,,363,366,"Aiming at the text document similarity and initial center point problems of K-means algorithm, a new model is proposed, in which used a new method of Frequency-Sorted Term Vector Model(FSTVM) to represent a document, and for reducing the dimension of a document designed an automatic method to filter commonly used words, and redesigned the similarity calculation formula and optimization algorithm for the initial center. Compared with the traditional algorithms, the new proposed algorithm can get initial centers with higher quality and steadier cluster results, Experimental results prove this clustering algorithm is simple and effective.",,978-1-5386-8031-5,10.1109/ICVRIS.2018.00095,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531422,"K-Means Algorithm, Frequency-Sorted Term Vector Model, Document Similarity, Initial Center Point",Clustering algorithms;Feature extraction;Solid modeling;Vocabulary;Computational modeling;Partitioning algorithms;Filtering,optimisation;pattern clustering;sorting;text analysis;vectors,similarity calculation formula;optimization algorithm;clustering algorithm;optimized K-means algorithm;text document similarity;initial center point problems;frequency-sorted term vector model;FSTVM,,,,10,,11-Nov-18,,,IEEE,IEEE Conferences
Request distribution-aware caching in cluster-based Web servers,機器學習範式中基於力學的文本分類相似性度量,V. Olaru; W. F. Tichy,"Comput. Sci. Dept., Karlsruhe Univ., Germany; Comput. Sci. Dept., Karlsruhe Univ., Germany","Third IEEE International Symposium on Network Computing and Applications, 2004. (NCA 2004). Proceedings.",8-Nov-04,2004,,,311,316,"This work presents a performance analysis of request distribution-aware caching in cluster-based Web servers. We use the Zipf-like request distribution curve to guide static Web document caching. A combination of cooperative caching and exclusive caching provides for a cluster-wide caching system that avoids document replication accross the cluster. We explore the benefits of cooperative caching algorithms that use request distribution information to steer their behavior over general purpose cooperative caching algorithms. Exclusive caching exercises a fine-grained control over replication of data blocks across the cluster. The performance of the system has been assessed by using the WebStone benchmark. Our cluster-based server employs Linux kernel-level implementations of cooperative caching and exclusive caching. Current results show that request distribution-aware caching outperforms general-purpose caching algorithms, makes up for the performance loss of non-replicated data solutions and compares favorably to fully-replicated solutions.",,0-7695-2242-4,10.1109/NCA.2004.1347792,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1347792,,Web server;Cooperative caching;Clustering algorithms;Kernel;Control systems;Network servers;Computer science;Performance analysis;Linux;Performance loss,Internet;file servers;cache storage;workstation clusters,request distribution-aware caching;cluster-based Web servers;performance analysis;Zipf-like request distribution curve;static Web document caching;exclusive caching;cluster-wide caching system;document replication;request distribution information;data blocks;WebStone benchmark;cluster-based server;Linux kernel-level implementation;general-purpose caching algorithm;nonreplicated data solution;cooperative caching,,1,,20,,8-Nov-04,,,IEEE,IEEE Conferences
Secure Rank-Ordered Search of Multi-keyword Trapdoor over Encrypted Cloud Data,基於先驗知識的提高Web服務集群準確性的方法,A. Ibrahim; H. Jin; A. A. Yassin; D. Zou,"Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China",2012 IEEE Asia-Pacific Services Computing Conference,14-Mar-13,2012,,,263,270,"Advances in cloud computing and Internet technologies have pushed more and more data owners to outsource their data to remote cloud servers to enjoy with huge data management services in an efficient cost. However, despite its technical advances, cloud computing introduces many new security challenges that need to be addressed well. This is because, data owners, under such new setting, loss the control over their sensitive data. To keep the confidentiality of their sensitive data, data owners usually outsource the encrypted format of their data to the untrusted cloud servers. Several approaches have been provided to enable searching the encrypted data. However, the majority of these approaches are limited to handle either a single keyword search or a Boolean search but not a multikeyword ranked search, a more efficient model to retrieve the top documents corresponding to the provided keywords. In this paper, we propose a secure multi-keyword ranked search scheme over the encrypted cloud data. Such scheme allows an authorized user to retrieve the most relevant documents in a descending order, while preserving the privacy of his search request and the contents of documents he retrieved. To do so, data owner builds his searchable index, and associates with each term document with a relevance score, which facilitates document ranking. The proposed scheme uses two distinct cloud servers, one for storing the secure index, while the other is used to store the encrypted document collection. Such new setting prevents leaking the search result, i.e. the document identifiers, to the adversary cloud servers. We have conducted several empirical analyses on a real dataset to demonstrate the performance of our proposed scheme.",,978-1-4673-4825-6,10.1109/APSCC.2012.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478225,Cloud computing;searchable encryption;information retrieval;Paillier encryption;Bloom filter,Servers;Indexes;Encryption;Privacy;Cloud computing,authorisation;cloud computing;cryptography;data privacy;document handling,secure rank-ordered search;multikeyword trapdoor;encrypted cloud data;cloud computing;Internet technology;data management service;data owner;sensitive data control;sensitive data confidentiality;single keyword search;Boolean search;multikeyword ranked search;user authorization;search request;privacy preservation;document ranking;document identifier;empirical analysis,,17,,18,,14-Mar-13,,,IEEE,IEEE Conferences
Distributed Clustering of Text Collections,具有高維和稀疏特徵的網頁推薦系統,J. Zamora; H. Allende-Cid; M. Mendoza,"Instituto de Estad穩stica, Pontificia Universidad Cat籀lica de Valpara穩so, Valpara穩so, Chile; Escuela de Ingenier穩a Inform獺tica, Pontificia Universidad Cat籀lica de Valpara穩so, Valpara穩so, Chile; Centro Cient穩fico y Tecnol籀gico de Valpara穩so, Universidad T矇cnica Federico Santa Mar穩a, Valpara穩so, Chile",IEEE Access,1-Nov-19,2019,7,,155671,155685,"Current data processing tasks require efficient approaches capable of dealing with large databases. A promising strategy consists in distributing the data along with several computers that partially solve the undertaken problem. Finally, these partial answers are integrated to obtain a final solution. We introduce distributed shared nearest neighbors (D-SNN), a novel clustering algorithm that work with disjoint partitions of data. Our algorithm produces a global clustering solution that achieves a competitive performance regarding centralized approaches. The algorithm works effectively with high dimensional data, being advisable for document clustering tasks. Experimental results over five data sets show that our proposal is competitive in terms of quality performance measures when compared to state of the art methods.",2169-3536,,10.1109/ACCESS.2019.2949455,"Postdoctoral Project; CONICYT-FONDECYT, Government of Chile; Fondo de Fomento al Desarrollo Cient穩fico y Tecnol籀gico; Consejo Nacional de Innovaci籀n, Ciencia y Tecnolog穩a; Millennium Institute for Foundational Research on Data, Government of Chile; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882328,Distributed algorithms;distributed text clustering;high dimensional data,Clustering algorithms;Distributed databases;Partitioning algorithms;Task analysis;Computers;Parallel processing;Approximation algorithms,nearest neighbour methods;pattern clustering;text analysis,D-SNN;distributed shared nearest neighbors;quality performance measures;data sets;document clustering tasks;high dimensional data;centralized approaches;competitive performance;global clustering solution;disjoint partitions;novel clustering algorithm;partial answers;undertaken problem;large databases;current data processing tasks;text collections;distributed clustering,,,,40,CCBY,24-Oct-19,,,IEEE,IEEE Journals
Text Categorization with Considering Temporal Patterns of Term Usages,在嘈雜的文檔圖像中搜索文本的有效框架,H. Abe; S. Tsumoto,"Dept. of Med. Inf., Shimane Univ., Izumo, Japan; Dept. of Med. Inf., Shimane Univ., Izumo, Japan",2010 IEEE International Conference on Data Mining Workshops,20-Jan-11,2010,,,800,807,"In document categorization method by using similarity measures based on word vectors, it is important to determine key words to characterize each document. However, conventional methods select the key words based on their frequency or/and particular importance index such as tf-idf. In this paper, we propose a method to characterize each document by using temporal clusters of technical term usages. The method obtains document clusters based on the similarity between the document that are characterized by the temporal patterns of an importance index for considering temporal differences of the term usages In the experiment, we compare document categorization results based on document clustering by using the two types of feature sets about two sets of bibliographical documents. By regarding to the experimental results, we discuss the usefulness of the temporal patterns of term usages to characterize the documents.",2375-9259,978-1-4244-9244-2,10.1109/ICDMW.2010.186,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693378,Text Mining;Document Clustering;Temporal Patterns;Term Usage Index,Indexes;Feature extraction;Frequency conversion;Text categorization;Artificial neural networks;Clustering algorithms,pattern classification;pattern clustering;text analysis,text categorization;temporal pattern;document categorization method;similarity measure;word vector;keyword determination;temporal cluster;technical term usage;bibliographical document,,4,,16,,20-Jan-11,,,IEEE,IEEE Conferences
Character Line Segmentation Based on Feature Clustering,通過引入句子頻率和聚類來自動進行孟加拉語新聞文件摘要,Y. Xi; Y. Chen; Q. Liao; L. Winghong; F. Shunming; D. Jiangwen,"Shenzhen,Tsinghua University; Shenzhen,Tsinghua University; Shenzhen,Tsinghua University; ASM Assembly Automation Ltd.,4/F,Watson; ASM Assembly Automation Ltd.,4/F,Watson; ASM Assembly Automation Ltd.,4/F,Watson",Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),12-Nov-07,2007,1,,402,406,"A novel character line segmentation method for degraded binary images based on feature clustering is proposed in this paper. The application of this research is to segment character lines on images of IC chip surfaces. First, several cutting lines are detected and every two neighbor cutting lines define a candidate character line (CCL). Second, the feature of each CCL is extracted and clustering is used to choose real character lines (RCL) from CCLs. Third, a postprocessing step is applied to confirm or refine the character line segmentation results. Experiments have demonstrated that this novel method can find all the character lines in degraded document images of IC chip surfaces quickly and accurately and is robust to images with background noise and logo.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4378740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378740,,Image segmentation;Integrated circuit noise;Background noise;Degradation;Application specific integrated circuits;Automation;Optical character recognition software;Character recognition;Surface emitting lasers;Noise robustness,feature extraction;image segmentation;optical character recognition,character line segmentation;feature clustering;degraded binary images;IC chip surfaces;cutting lines;candidate character line;real character lines;document images,,,,4,,12-Nov-07,,,IEEE,IEEE Conferences
Scalable distributed query and update service implementations for XML document elements,基於內容相似度的中文文檔網絡社區結構,T. Grabs; K. Bohm; H. -. Schek,"Swiss Federal Inst. of Technol., Zurich, Switzerland; NA; NA",Proceedings Eleventh International Workshop on Research Issues in Data Engineering. Document Management for Data Intensive Business and Scientific Applications. RIDE 2001,7-Aug-02,2001,,,35,42,"Materialization and indexing of XML views is an important issue, and a number of 'XML-to-RDBMS' mappings have been proposed. To build a scalable system for XML document storage where those views are up-to-date, our approach is to provide efficient implementations of basic services for query processing and updates. As a first contribution we define such services, and show that they support many 'XML-to-RDBMS' mappings. As a second contribution, we describe implementations of such services in a database cluster. We evaluate two implementations, one based on a two-level logging and isolation mechanism and one using Windows 2000 COM+ transactions. The main results are as follows: the first approach scales linearly for updates, and query response times are interactive. For example, with high workloads of 100 concurrent clients and 8 cluster components, query response times are 3 seconds on average. The alternative approach is weaker both regarding query response times (10 seconds on average for the above scenario) and scalability.",,0-7695-0957-6,10.1109/RIDE.2001.916489,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916489,,XML;Delay;Transaction databases;Scalability;Information retrieval;Distributed databases;Indexing;Query processing;Contracts;Relational databases,query processing;hypermedia markup languages;multimedia databases;document handling;database indexing;relational databases,scalable distributed query;update service;document database;indexing;XML document storage;query processing;database cluster;two-level logging;Windows 2000;COM;query response times;scalability;relational databases,,6,,23,,7-Aug-02,,,IEEE,IEEE Conferences
Substitution deciphering based on HMMs with applications to compressed document processing,使用形狀上下文的拓撲排序簇的數學符號索引,Dar-Shyang Lee,"Ricoh Innovations Inc., Menlo Park, CA, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,6-Jan-03,2002,24,12,1661,1666,"It has been shown that simple substitution ciphers can be solved using statistical methods such as probabilistic relaxation. However, the utility of such solutions has been limited by their inability to cope with noise encountered in practical applications. We propose a new solution to substitution deciphering based on hidden Markov models. We show that our algorithm is more accurate than relaxation and much more robust in the presence of noise, making it useful for applications in compressed document processing. Recovering character interpretations from the sequence of cluster identifiers in a symbolically compressed document can be treated as a cipher problem. Although a significant amount of noise is present in the cluster sequence, enough information can be recovered with a robust deciphering algorithm to accomplish certain document analysis tasks. The feasibility of this approach is demonstrated in a multilingual document duplicate detection system.",1939-3539,,10.1109/TPAMI.2002.1114860,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1114860,,Hidden Markov models;Noise robustness;Image coding;Statistics;Clustering algorithms;Relaxation methods;Noise shaping;Shape;Acoustic noise;Statistical analysis,document image processing;data compression;image coding;hidden Markov models;decoding,substitution deciphering;HMM;compressed document processing;statistical methods;probabilistic relaxation;noise;hidden Markov models;cluster sequence;substitution cipher;image compression;document analysis;multilingual document duplicate detection system,,13,,15,,6-Jan-03,,,IEEE,IEEE Journals
Semi-supervised Non-negative Patch Alignment Framework,多文檔數據集提取文本摘要方法的調查,L. Lan; X. Huang; N. Guan; Z. Luo; X. Zhang,"Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China",2012 11th International Conference on Machine Learning and Applications,10-Jan-13,2012,1,,174,178,"Non-negative matrix factorization (NMF) learns the latent semantic space more direct and reliable than the latent semantic indexing (LSI) and the spectral clustering methods, thus performs well in document clustering. Recently, semi-supervised NMF such as N2S2L, CNMF and unsupervised method such as GNMF significantly improve the face recognition performance, but they are designed for classification. In this paper, we combine both geometric structure and label information with NMF under the non-negative patch alignment framework (NPAF) to form SS-NPAF. Due to this combination, it greatly improves the clustering performance. To optimize SS-NPAF, we apply the well-known projected gradient method to overcome the slow convergence problem of the mostly used multiplicative update rule. Experimental results on two popular document datasets, i.e., Reuters21578 and TDT-2, show that SS-NPAF outperforms the representative SS-NMF algorithms.",,978-1-4673-4651-1,10.1109/ICMLA.2012.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406608,Non-negative matrix factorization;semi-supervised learning;document clustering,Matrix decomposition;Semantics;Conferences;Accuracy;Gradient methods;Clustering algorithms,convergence;document handling;gradient methods;matrix decomposition;optimisation;pattern clustering;unsupervised learning,semisupervised nonnegative patch alignment framework;latent semantic space;document clustering performance improvement;semisupervised NMF learning;N2S2L;CNMF;unsupervised method;GNMF face recognition performance improvement;geometric structure;label information;SS-NPAF optimization;projected gradient method;convergence,,1,,25,,10-Jan-13,,,IEEE,IEEE Conferences
Clustering-Based Query Routing in Cooperative Semi-Structured Peer to Peer Networks,基於遺傳算法的無監督特徵選擇技術改進文本聚類,R. S. Alkhawaldeh; J. M. Jose; P. Deepak,"Sch. of Comput. Sci., Univ. of Glasgow, Glasgow, UK; Sch. of Comput. Sci., Univ. of Glasgow, Glasgow, UK; Sch. of Comput. Sci., Queen's Univ., Belfast, UK",2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),16-Jan-17,2016,,,378,382,"We consider the problem of resource selection in clustered Peer-to-Peer Information Retrieval (P2P IR) networks with cooperative peers. The clustered P2P IR framework presents a significant departure from general P2P IR architectures by employing clustering to ensure content coherence between resources at the resource selection layer, without disturbing document allocation. We propose that such a property could be leveraged in resource selection by adapting well-studied and popular inverted lists for centralized document retrieval. Accordingly, we propose the Inverted PeerCluster Index (IPI), an approach that adapts the inverted lists, in a straightforward manner, for resource selection in clustered P2P IR. IPI also encompasses a strikingly simple peer-specific scoring mechanism that exploits the said index for resource selection. Through an extensive empirical analysis on P2P IR testbeds, we establish that IPI competes well with the sophisticated state-of-the-art methods in virtually every parameter of interest for the resource selection task, in the context of clustered P2P IR.",2375-0197,978-1-5090-4459-7,10.1109/ICTAI.2016.0064,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814624,Clustering Peers;Semi-structured;Peer to Peer;Information Retrieval;Query Routing;Evaluation,Peer-to-peer computing;Indexes;Computer architecture;Information retrieval;Clustering algorithms;Query processing;Coherence,information networks;pattern clustering;peer-to-peer computing;query processing;resource allocation,IPI;inverted PeerCluster index;centralized document retrieval;document allocation;resource selection layer;P2P IR architectures;P2P IR networks;peer-to-peer information retrieval networks;resource selection;cooperative semistructured peer to peer networks;clustering-based query routing,,1,,13,,16-Jan-17,,,IEEE,IEEE Conferences
A study on the causes and evolution mechanism of network structure risk of industrial clusters,基於術語權重自動劃分的文本聚類,Weidong Wang,"Institute of Humanities and Social Science, China JiLiang University, Hangzhou, China",2011 2nd IEEE International Conference on Emergency Management and Management Sciences,8-Sep-11,2011,,,205,207,"In this paper, systems theory, modules organization theory and social network analysis methods were used to analyze the causes and evolution mechanism of network structure risk of industrial clusters. It pointed out that a cluster was a self-organizing dissipative structure system, the modular of a cluster network organization not only formed competitive advantage, but also produced potential risk and the network structure risk of a industrial cluster had different characteristics and evolution at different stages of cluster life cycle. The following revelation can be obtained from it that the network structure of industrial cluster was the internal basis to maintain the integrity and function, and the root causes of breeding the risk of industrial clusters. Industrial clusters can improve the ability to prevent risk by restructuring their network structure. The conclusions of this study could build the theoretical basis for the early warning mechanism of network structure risk of industrial clusters. This electronic document is a ?live??template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.",,978-1-4244-9666-2,10.1109/ICEMMS.2011.6015656,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015656,industrial cluster;network structure;risk;cause;evolution,,complex networks;industrial economics;network theory (graphs);risk management,industrial clusters;network structure risk;evolution mechanism;early warning mechanism;electronic document;self-organizing dissipative structure;cluster network organization;cluster life cycle,,,,6,,8-Sep-11,,,IEEE,IEEE Conferences
Topic clustering and topic evolution based on temporal parameters,使用Javanese詞幹在Krama Alus級別聚集Javanese新聞,M. Jayashri; P. Chitra,"Department of Computer Science & Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science & Engineering, Thiagarajar College of Engineering, Madurai, India",2012 International Conference on Recent Trends in Information Technology,31-May-12,2012,,,559,564,"Historical documents contains extreme amount of information about past events, often in unstructured form. Once dates and document names are identified that can differ by genre, we examine collections to detect events. Temporal text mining (TTM) is used to ascertain the temporal patterns in text information collected over time. Trend analysis from the stream of text documents generally uses an approach based on topic detection and tracking (TDT). The task of topic detection is used to detect topics that are previously unknown to the system. Tracking generates the evolution of each topic over the period of arrival time. In this work the TDT task has been formulated as a clustering problem in a class of self-organizing neural networks, called the Adaptive Resonance Theory (ART) networks. We also propose that our algorithm has been able to detect hot topics automatically and track them with good accuracy. From our experimental studies we prove this by comparing the effectiveness of the different validity indices of simple k-means clustering method. We also show the benchmarking results of different kinds of datasets.",,978-1-4673-1601-9,10.1109/ICRTIT.2012.6206816,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6206816,temporal text mining;topic detection;topic tracking;trend analysis;topic clustering,Subspace constraints;Support vector machine classification;Clustering algorithms;Vectors;Accuracy;Prototypes;Algorithm design and analysis,data mining;pattern clustering;self-organising feature maps;text analysis,topic clustering;topic evolution;temporal parameter;historical document;temporal text mining;event detection;text information;trend analysis;topic detection;topic tracking;clustering problem;self-organizing neural network;adaptive resonance theory network;k-means clustering method,,1,,29,,31-May-12,,,IEEE,IEEE Conferences
Key Factors' Clustering for Records with Mixed Data,審查單文檔和多文檔的抽象文本摘要技術（ATST）,H. Nie; J. Zhou,"Zhejiang Normal University, Math Physics and Information Engineering College,Jinhua,China; Zhejiang Normal University, Xingzhi College,Jinhua,China","2019 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)",27-Dec-19,2019,,,536,542,"A table usually contains numerical and symbolic data, so a method is proposed for clustering the records of the table. First, by using the improved function (TF-IWF) of the term frequency-inverse document frequency (TF-IDF), the symbols are transformed into the corresponding numerical values, and the remaining numerical values remain unchanged. On the basis of data transformation, the principal factor iteration method and maximum variance rotation method are used to discover the key factors that affect the clustering of the records. And the scores of the key factor are obtained by the least square method. Then k-means algorithm is used to cluster the scores, and finally the clustering results of the records are obtained. experimental results show that the evaluation indexes of the proposed method are better than those of other clustering methods, and the proposed method is helpful to dimensionality reduction and extract the latent important features of different clusters, so that each cluster can be labeled precisely.",,978-1-7281-3720-9,10.1109/ICPICS47731.2019.8942572,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8942572,Symbol;data transformation;key factor;clustering,Clustering algorithms;Partitioning algorithms;Least mean squares methods;Clustering methods;Transforms;Feature extraction;Correlation,iterative methods;least squares approximations;pattern classification;pattern clustering;text analysis,TF-IWF;term frequency-inverse document frequency;TF-IDF;data transformation;principal factor iteration method;maximum variance rotation method;key factors clustering;k-means algorithm;least square method,,,,31,,27-Dec-19,,,IEEE,IEEE Conferences
Assigning Web News to Clusters,CiteRivers：引文模式的可視化分析,C. Bouras; V. Tsogkas,"Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece; Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece",2010 Fifth International Conference on Internet and Web Applications and Services,3-Jun-10,2010,,,1,6,"The Web is overcrowded with news articles, an overwhelming information source both with its amount and diversity. Assigning news articles to similar groups, on the other hand, provides a very powerful data mining and manipulation technique for topic discovery from text documents. In this paper, we are investigating the application of a great spectrum of clustering algorithms, as well as similarity measures, to news articles that originate from the Web and compare their efficiency for use in an online Web news service application. We also examine the effect of preprocessing on clustering. Our experimentation showed that k-means, despite its simplicity, accompanied with preliminary steps for data cleaning and normalizing, gives better aggregate results when it comes to efficiency.",,978-1-4244-6729-7,10.1109/ICIW.2010.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476825,Web News Articles;Document Clustering;k-means;k-means++;Hierarchical Clustering,Clustering algorithms;Partitioning algorithms;Application software;Informatics;Information retrieval;Web and internet services;Power engineering computing;Data mining;Cleaning;Aggregates,data mining;information resources;pattern clustering;Web services,news articles;data mining;topic discovery;clustering algorithms;online Web news service application;data cleaning,,8,,16,,3-Jun-10,,,IEEE,IEEE Conferences
Evaluating fuzzy clustering for relevance-based information access,集對分析的超圖模型聚類方法,M. E. S. Mendes; L. Sacks,"Dept. of Electron. & Electr. Eng., Univ. Coll. London, UK; Dept. of Electron. & Electr. Eng., Univ. Coll. London, UK","The 12th IEEE International Conference on Fuzzy Systems, 2003. FUZZ '03.",9-Jul-03,2003,1,,648,653 vol.1,"This paper analyzes the suitability of fuzzy clustering methods for the discovery of relevant document relationships, motivated by the need for enhanced relevance-based navigation of Web-accessible resources. The performance evaluation of a modified Fuzzy c-Means algorithm is carried out, and a comparison with a traditional hard clustering technique is presented. Clustering precision and recall are defined and applied as quantitative evaluation measures of the clustering results. The experiments with various test document sets have shown that in most cases fuzzy clustering performs better than the hard k-Means algorithm and that the fuzzy membership values can be used to determine document relevance and to control the amount of information retrieved to the user.",,0-7803-7810-5,10.1109/FUZZ.2003.1209440,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209440,,Ontologies;Clustering algorithms;Information retrieval;Clustering methods;Navigation;Fuzzy control;Fuzzy sets;Educational institutions;Testing;Performance evaluation,relevance feedback;data mining;unsupervised learning;pattern clustering;fuzzy set theory,fuzzy clustering;relevance-based information access;document relationships discovery;enhanced relevance-based navigation;Web-accessible resources;modified fuzzy c-means algorithm;information retrieval;hierarchical clustering algorithms;distance function;e-learning content,,22,,22,,9-Jul-03,,,IEEE,IEEE Conferences
Affine Transformation based Nonnegative Matrix Factorization,使用密度峰值句子聚類生成更新摘要,R. Zhang; H. Imai; Y. Hirose,"Graduate School of Information, Science and Technology, Hokkaido University,Sapporo,Japan; Faculty of Information Science and Technology, and Global Station for Big Data and Cybersecurity Hokkaido University,Sapporo,Japan; Faculty of Information Science and Technology, and Global Station for Big Data and Cybersecurity Hokkaido University,Sapporo,Japan",2019 International Conference on Fuzzy Theory and Its Applications (iFUZZY),16-Apr-20,2019,,,343,346,"A methodology for document clustering based on the nonnegative matrix factorization (NMF) in affine space is presented. Considering the decompostion of V = H, we proposed some algorithms in which the column sums of H is restricted to be one, then the factorization is affine invariant, thus extending the applicable range of NMF methods. Multiplicative estimation algorithms are provided for computing the new decomposition, as well as the supporting theoretical analysis. Moreover, numerical experimental results explore the properties of the new decomposition method.",2377-5831,978-1-7281-0840-7,10.1109/iFUZZY46984.2019.9066262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066262,,Clustering algorithms;Linear programming;Matrix decomposition;Machine learning;Optimization;Information science;Big Data,affine transforms;document handling;matrix decomposition;pattern clustering;statistical analysis,affine transformation;nonnegative matrix factorization;NMF methods;multiplicative estimation algorithms;decomposition method;document clustering,,,,13,,16-Apr-20,,,IEEE,IEEE Conferences
Mining Topically Coherent Patterns for Unsupervised Extractive Multi-document Summarization,基於旋律模式提取和聚類的音樂文獻體裁分類,Y. Wu; Y. Li; Y. Xu; W. Huang,"Fac. of Sci. & Eng., Queensland Univ. of Technol., Brisbane, QLD, Australia; Fac. of Sci. & Eng., Queensland Univ. of Technol., Brisbane, QLD, Australia; Fac. of Sci. & Eng., Queensland Univ. of Technol., Brisbane, QLD, Australia; Sch. of Econ. & Manage., Hubei Univ. of Technol., Wuhan, China",2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI),16-Jan-17,2016,,,129,136,"Addressing the problem of information overload, automatic multi-document summarization (MDS) has been widely utilized in the various real-world applications. Most of existing approaches adopt term-based representation for documents which limit the performance of MDS systems. In this paper, we proposed a novel unsupervised pattern-enhanced topic model (PETMSum) for the MDS task. PETMSum combining pattern mining techniques with LDA topic modelling could generate discriminative and semantic rich representations for topics and documents so that the most representative, non-redundant, and topically coherent sentences can be selected automatically to form a succinct and informative summary. Extensive experiments are conducted on the data of document understanding conference (DUC) 2006 and 2007. The results prove the effectiveness and efficiency of our proposed approach.",,978-1-5090-4470-2,10.1109/WI.2016.0028,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817045,multi-document summarization;pattern mining;topic modelling,Semantics;Data mining;Adaptation models;Context modeling;Correlation;Buildings;Bayes methods,data mining;document handling;pattern clustering;statistics;unsupervised learning,topically coherent pattern mining;unsupervised extractive multidocument summarization;unsupervised extractive MDS;information overload;term-based document representation;unsupervised pattern-enhanced topic model;PETMSum;LDA topic modelling;semantic rich representations;topically coherent sentence selection;document understanding conference data;DUC data,,1,,38,,16-Jan-17,,,IEEE,IEEE Conferences
Semi-automated OCR database generation for Nabataean scripts,透明管理複製的WWW文檔集群,A. Ul-Hasan; S. S. Bukhari; S. F. Rashid; F. Shafait; T. M. Breuel,"Technical University of Kaiserslautern, Germany; Technical University of Kaiserslautern, Germany; Technical University of Kaiserslautern, Germany; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Technical University of Kaiserslautern, Germany",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,1667,1670,"A large amount of real-world data is required to train and benchmark any character recognition algorithm. Developing a page-level ground-truth database for this purpose is overwhelmingly laborious, as it involves a lot of manual efforts to produce a reasonable database that covers all possible words of a language. Moreover, generating such a database for historical (degraded) documents or for a cursive script like Urdu1 is even more complex and grueling. The presented work attempts to solve this problem by proposing a semi-automated technique for generating ground-truth database. It is believed that the proposed automation will greatly reduce the manual efforts for developing any OCR database. The basic idea is to apply ligature-clustering prior to manual labeling. Two prototype datasets for Urdu script have been developed using the proposed technique and the results are also presented.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460468,,Databases;Labeling;Manuals;Clustering algorithms;Shape;Optical character recognition software;Accuracy,database management systems;document image processing;natural language processing;optical character recognition;pattern clustering,semiautomated OCR database generation;Nabataean scripts;real-world data;character recognition algorithm;page-level ground-truth database;historical degraded documents;cursive script;ligature clustering;manual labeling;Urdu script,,2,,13,,14-Feb-13,,,IEEE,IEEE Conferences
A study of Chinese text summarization using adaptive clustering of paragraphs,快速積分MeanShift：在文檔圖像顏色分割中的應用,Po Hu; Tingting He; Donghong Ji; Meng Wang,"Dept. of Comput. Sci., Central China Normal Univ., Wuhan, China; Dept. of Comput. Sci., Central China Normal Univ., Wuhan, China; NA; NA","The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.",30-Nov-04,2004,,,1159,1164,"Automatic summarization is an important research issue in natural language processing. This paper presents a special summarization method to generate single-document summary with maximum topic completeness and minimum redundancy. It initially implements the semantic-class-based vector representations of various kinds of linguistic units in a document by means of HowNet (an existing ontology), which can improve the representation quality of traditional term-based vector space model in a certain degree. Then, by adopting K-means clustering algorithm as well as a clustering analysis algorithm, we can capture the number of different latent topic regions in a document adoptively. Finally, topic representative sentences are selected from each topic region to form the final summary. In order to evaluate the effectiveness of the proposed summarization method, a novel metric which is known as representation entropy is used for summarization redundancy evaluation. Preliminary experimental results show that the proposed method outperforms the conventional basic summarization method under the evaluation scheme when dealing with diverse genres of Chinese documents with free writing style and flexible topic distribution.",,0-7695-2216-5,10.1109/CIT.2004.1357351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1357351,,Clustering algorithms;Natural language processing;Algorithm design and analysis;Partitioning algorithms;Helium;Computer science;Ontologies;Entropy;Writing;Internet,text analysis;natural languages;pattern clustering;linguistics;ontologies (artificial intelligence);computational linguistics;abstracting,Chinese text summarization;adaptive paragraph clustering;automatic summarization;natural language processing;single-document summary;maximum topic completeness;minimum redundancy;semantic-class-based vector representations;linguistic units;HowNet ontology;term-based vector space model;K-means clustering algorithm;clustering analysis algorithm;latent topic regions;topic representative sentences;final summary;representation entropy;summarization redundancy evaluation;Chinese document;free writing style;flexible topic distribution,,9,,11,,30-Nov-04,,,IEEE,IEEE Conferences
On the clustering of large-scale data: A matrix-based approach,使用無監督學習從天氣數據自動可擴展地檢測特定地點的聖安娜天氣情況,L. Wang; M. Dong,"Department of Computer Science, Wayne State University, Detroit, MI 48202, USA; Department of Computer Science, Wayne State University, Detroit, MI 48202, USA",The 2011 International Joint Conference on Neural Networks,3-Oct-11,2011,,,139,144,"Nowadays, the analysis of large amounts of digital documents become a hot research topic since the libraries and database are converted electronically, such as PUBMED and IEEE publications. The ubiquitous phenomenon of massive data and sparse information imposes considerable challenges in data mining research. In this paper, we propose a theoretical framework, Exemplar-based Low-rank sparse Matrix Decomposition (ELMD), to cluster large-scale datasets. Specifically, given a data matrix, ELMD first computes a representative data subspace and a near-optimal low-rank approximation. Then, the cluster centroids and indicators are obtained through matrix decomposition, in which we require that the cluster centroids lie within the representative data subspace. From a theoretical perspective, we show the correctness and convergence of the ELMD algorithm, and provide detailed analysis on its efficiency. Through extensive experiments performed on both synthetic and real datasets, we demonstrate the superior performance of ELMD for clustering large-scale data.",2161-4407,978-1-4244-9637-2,10.1109/IJCNN.2011.6033212,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033212,,Approximation methods;Matrix decomposition;Clustering algorithms;Approximation algorithms;Noise;Accuracy;Sparse matrices,approximation theory;data mining;data structures;document handling;matrix decomposition;pattern clustering;set theory;ubiquitous computing,large scale data set clustering;matrix-based approach;digital document;PUBMED publication;IEEE publication;digital library;digital database;ubiquitous phenomenon;sparse information;data mining;exemplar-based low rank sparse matrix decomposition;data matrix;data subspace;near optimal low rank approximation;cluster centroids;ELMD algorithm;synthetic dataset;real dataset,,3,,18,,3-Oct-11,,,IEEE,IEEE Conferences
Content-Based Clustering for Tag Cloud Visualization,基於改進後綴樹算法的網頁摘要聚類,A. Zubiaga; A. P. Garc穩a-Plaza; V. Fresno; R. Mart穩nez,"Dpto. Lenguajes y Sist. Informaticos, Univ. Nat. de Educ. a Distancia, Madrid, Spain; Dpto. Lenguajes y Sist. Informaticos, Univ. Nat. de Educ. a Distancia, Madrid, Spain; Dpto. Lenguajes y Sist. Informaticos, Univ. Nat. de Educ. a Distancia, Madrid, Spain; Dpto. Lenguajes y Sist. Informaticos, Univ. Nat. de Educ. a Distancia, Madrid, Spain",2009 International Conference on Advances in Social Network Analysis and Mining,4-Sep-09,2009,,,316,319,"Social tagging systems are becoming an interesting way to retrieve web information from previously annotated data. These sites present a tag cloud made up by the most popular tags, where neither tag grouping nor their corresponding content is considered. We present a methodology to obtain and visualize a cloud of related tags based on the use of self-organizing maps, and where the relations among tags are established taking into account the textual content of tagged documents. Each map unit can be represented by the most relevant terms of the tags it contains, so that it is possible to study and analyze the groups as well as to visualize and navigate through the relevant terms and tags.",,978-0-7695-3689-7,10.1109/ASONAM.2009.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231851,social-tagging;clustering;information access;visualization,Tag clouds;Tagging;Navigation;Ontologies;Semantic Web;Data visualization;Self organizing feature maps;Feeds;Subscriptions;Social network services,data visualisation;information retrieval;Internet;pattern clustering;self-organising feature maps;social networking (online),content-based clustering;tag cloud visualisation;social tagging;self-organizing maps;web information retrieval;textual content;tagged documents,,18,,10,,4-Sep-09,,,IEEE,IEEE Conferences
Automatic extractive text summarization using K-means clustering,基於半監督聚類的框架，用於發現獨特的寫作風格,K. Shetty; J. S. Kallimani,"Department of Computer Science and Engineering, M S Ramaiah Institute of Technology, Bangalore, India; Department of Computer Science and Engineering, M S Ramaiah Institute of Technology, Bangalore, India","2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)",8-Feb-18,2017,,,1,9,"The rise in the dimension of the World Wide Web has made an explosion of the amount of accessible information. As the textual data involves several instances of redundancy, omission of part of sentences or entire sentences is possible without altering the meaning of the document. Summarization of the text can informally be defined as the act of condensing the document from its original size without significantly compromising the semantics. For the purpose of generating an appropriate summary, the raw text is first pre-processed which involves - removing nonASCII characters and stop-words, tokenizing and stemming. Appropriate features are extracted from the data, tf-idf values for each word are computed and the entire pre-processed data is then transformed into a tf-idf matrix. Every sentence of the document will be represented as a vector in the dimensional space of the document's vocabulary. To obtain a concise summary, sentences are appropriately clustered based on the degree of separation of vectors in the Euclidean place. Association of sentences to a cluster using K-means method is totally based on cosine similarity. The count of the clusters is to be formed is predefined. As the number of clusters increase the accuracy of the summary increases. From each of the clusters the sentences which are informative are picked to form the final summary. Using recall and precision measures, the effectiveness of the summary is verified.",,978-1-5386-2361-9,10.1109/ICEECCOT.2017.8284627,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284627,Finite State Machine;Vector Space Model;Clustering;Lemmatization;Cosine Similarity;Sentence Extraction;Summary Generation,Feature extraction;Hidden Markov models;Data mining;Vocabulary;Semantics;Automata;Neural networks,document handling;feature extraction;information retrieval;Internet;pattern clustering;text analysis,appropriate feature extraction;entire pre-processed data;k-means clustering;concise summary;tf-idf matrix;tf-idf values;tokenizing;raw text;appropriate summary;document;entire sentences;sentence;textual data;World Wide Web;automatic extractive text summarization,,1,,30,,8-Feb-18,,,IEEE,IEEE Conferences
Signature Based Document Retrieval Using GHT of Background Information,使用背景信息的GHT進行基於簽名的文檔檢索,P. P. Roy; S. Bhowmick; U. Pal; J. Y. Ramel,"LI, Univ. Francois Rabelais, Tours, France; CVPR Unit, Indian Stat. Inst., Kolkata, India; CVPR Unit, Indian Stat. Inst., Kolkata, India; LI, Univ. Francois Rabelais, Tours, France",2012 International Conference on Frontiers in Handwriting Recognition,31-Jan-13,2012,,,225,230,"This paper deals with signature based document retrieval from documents with cluttered background. Here, a signature object is characterized by spatial features computed from recognition result of background blobs. The background blobs are computed by analyzing character holes and water reservoir zones in different directions. For the indexation purpose, a codebook of the background blobs is created using a set of training data. Zernike Moment feature is extracted from each blob and a K-Mean clustering algorithm is used to create the codebook of blobs. During retrieval, Generalized Hough Transform (GHT) is used to detect the query signature and a voting is casted to find possible location of the query signature in a document. The spatial features computed from background blobs found in the target document are used for GHT. The peak of votes in GHT accumulator validates the hypothesis of the query signature. The proposed method is tested on a collection of mixed documents (handwritten/printed) of various scripts and we have obtained encouraging results from the experiments.",,978-1-4673-2262-1,10.1109/ICFHR.2012.270,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424396,Signature Spotting;Document Retrieval;Generalized Hough Transform;Background Information,Reservoirs;Feature extraction;Handwriting recognition;Transforms;Cavity resonators;Shape,digital signatures;feature extraction;handwriting recognition;Hough transforms;image coding;pattern clustering;query processing,signature based document retrieval;GHT;cluttered background;signature object;spatial feature;background blob;character hole;water reservoir zone;indexation purpose;codebook;Zernike moment feature extraction;K-mean clustering algorithm;generalized Hough transform;query signature;handwritten document;printed document,,6,,13,,31-Jan-13,,,IEEE,IEEE Conferences
Multi-document summarization using sentence fusion for Indonesian news articles,印度尼西亞新聞文章使用句子融合進行多文檔摘要,F. Christie; M. L. Khodra,"School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2016 International Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA)",2-Jan-17,2016,,,1,6,"This paper describes work on summarizing several input documents into one coherent and readable document. The steps that are discussed in this report are tokenization, POS-Tagging, building clusters of similar sentences, joining sentence by sentence fusion with word graph, extracting the result from a word graph, and in the end, sentence selection using the algorithm integer linear programming. Testing of the created system will be done manually and involves anonymous questionnaire regarding the readablility and credibility of the resulting document.",,978-1-5090-1636-5,10.1109/ICAICTA.2016.7803134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803134,multi-document summarization;sentence fusion;word graph,Semantics;Integer linear programming;Testing;Natural language processing;Feature extraction;Capacitance-voltage characteristics,document handling;integer programming;linear programming;natural language processing;publishing,document credibility;document readablility;algorithm integer linear programming;sentence selection;word graph;similar sentence cluster;POS-tagging;tokenization;input document summarization;Indonesian news articles;sentence fusion;multidocument summarization,,1,,23,,2-Jan-17,,,IEEE,IEEE Conferences
Automatic Personalized Marathi Content Generation,自動個性化Marathi內容生成,S. R. Vispute; S. Kanthekar; A. Kadam; C. Kunte; P. Kadam,"Department of Computer Engineering, PCCOE, Nigdi, Pune, India; Department of Computer Engineering, PCCOE, Nigdi, Pune, India; Department of Computer Engineering, PCCOE, Nigdi, Pune, India; Department of Computer Engineering, PCCOE, Nigdi, Pune, India; Department of Computer Engineering, PCCOE, Nigdi, Pune, India","2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)",19-Jun-14,2014,,,294,299,"The purpose of the present work is to create a system to retrieve personalized documents in Marathi Language. The system mainly focuses on providing personalized documents to the end user by analyzing the browsing history and user profile of the user in Marathi language. The system also provides manual bookmark facility to the end user as per the user interest. This paper provides personalization of Marathi text documents by using Label Induction Grouping [LINGO] Algorithm based on Vector Space Model [VSM]. This paper presents the automatic personalization of Marathi documents and literature survey of the related work done in automatic categorization of Marathi text documents. Several learning techniques exist for the classification of text documents like Decision Trees, Support Vector Machine, Na簿ve Bayes, etc. Several clustering techniques are available for text categorization namely K-means, Suffix Tree Clustering, Label Induction Grouping Algorithm, etc. With the help of literature survey, it is found that Vector Space Model [VSM] gives better accuracy than other models.",,978-1-4799-2494-3,10.1109/CSCITA.2014.6839275,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839275,Categorization;Clustering;Automatic Personalization;Browsing History;Bookmark Facility,Clustering algorithms;Matrix decomposition;Classification algorithms;Search engines;Information technology;History;Text categorization,information retrieval;learning (artificial intelligence);natural language processing;pattern classification;pattern clustering;text analysis,automatic personalized Marathi content generation;personalized document retrieval;Marathi language;user profile;browsing history analysis;bookmark facility;Marathi text document personalization;label induction grouping algorithm;LINGO algorithm;vector space model;VSM;automatic categorization;learning techniques;text document classification;decision trees;support vector machine;Na簿ve Bayes classification;clustering techniques;text categorization;k-means clustering;suffix tree clustering,,2,,14,,19-Jun-14,,,IEEE,IEEE Conferences
Search results optimization method combined with multi-features,結合多種功能的搜索結果優化方法,Y. Qin; D. Zheng; B. Xu,"MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, China",2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-Sep-11,2011,2,,1167,1171,"The optimization of search results has always been the research hotspot in the area of search engine. More concretely, topic partition by clustering proved to be a good way. However, the clusters, some of which still contain a lot of documents, implicitly limit the users' retrieval speed. Meanwhile we find that the information of documents' features have good effects on the document ranking. To address the issue, we try to apply the multi-features to search results after the process of clustering. Statistic and semantic information of the multi-features are fully used to re-rank the documents. Related experiments show that our approach outperforms that of single clustering much. The evaluation indicators' rising shows that the Top N results satisfy the users' need more.",,978-1-61284-181-6,10.1109/FSKD.2011.6019649,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019649,multi-features;statistic;semantic;clustering;re-ranking;Hownet,Semantics;Clustering algorithms;Algorithm design and analysis;Feature extraction;Search engines;Indexes;Optimization,document handling;pattern clustering;search engines,search results optimization method;search engine;topic partition;clustering process;document feature information;document ranking;multifeature information,,,,14,,15-Sep-11,,,IEEE,IEEE Conferences
Complex document image segmentation using localized histogram analysis with multi-layer matching and clustering,使用帶有多層匹配和聚類的局部直方圖分析進行複雜的文檔圖像分割,Yen-Lin Chen; Chung-Cheng Chiu; Bing-Fei Wu,"Dept. of Electr. & Control Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Electr. & Control Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Electr. & Control Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",7-Mar-05,2004,4,,3063,3070 vol.4,"This paper proposes a new segmentation method to separate the text from various complex document images. An automatic multilevel thresholding method, based on discriminant analysis, is utilized to recursively segment a specified block region into several layered image sub-blocks. Then the multi-layer region based clustering method is performed to process the layered image sub-blocks to form several object layers. Hence character strings with different illuminations, nontext objects and background components are segmented into separate object layers. After performed text extraction process, the text objects with different sizes, styles and illuminations are properly extracted. Experimental results on the extraction of text strings from complex document images demonstrate the effectiveness of the proposed region-based segmentation method.",1062-922X,0-7803-8566-7,10.1109/ICSMC.2004.1400809,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1400809,,Image segmentation;Histograms;Image analysis;Lighting;Text analysis;Data mining;Control engineering;Clustering methods;Image color analysis;Image texture analysis,document image processing;image segmentation;image matching;statistical analysis;text analysis;feature extraction,complex document image segmentation;localized histogram analysis;multi-layer matching;multi-layer clustering;automatic multilevel thresholding;discriminant analysis;text extraction process;text strings,,2,,9,,7-Mar-05,,,IEEE,IEEE Conferences
Generating true color paper textures of historical documents,生成歷史文檔的真彩色紙紋理,C. S. V. C. Cavalcanti; C. A. B. Mello; M. P. S. Rodrigues,"Escola Politecnica de Pernambuco, Univ. de Pernambuco, Brazil; Escola Politecnica de Pernambuco, Univ. de Pernambuco, Brazil; Escola Politecnica de Pernambuco, Univ. de Pernambuco, Brazil","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.",2-May-05,2005,2,,ii/545,ii/548 Vol. 2,"This paper presents two methods for automatic generation of true color paper textures of historical documents. One uses a 256-gray level image of the texture, some features inherent to the original paper itself, and neural networks to colorize. The other generates a synthetic version of the texture based on the rebuilding of its histogram. Both methods produce images which are perceptually close to the originals by visual inspection and quantitatively by the use of variance analysis. With the colorizing process, it is possible to create a cluster of textures and use it to generate different textures from just one sample. The synthesis process allows the complete generation of the image of the document achieving high compression rates.",2379-190X,0-7803-8874-7,10.1109/ICASSP.2005.1415462,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415462,,Histograms;Analysis of variance;Entropy;Image generation;Algorithm design and analysis;Image color analysis;Gray-scale;Image storage;Neural networks;Inspection,document image processing;image colour analysis;image texture;neural nets,variance analysis;true color paper texture generation;historical document texture generation;automatic texture generation;gray level image;colorizing neural networks;synthetic texture;histogram rebuilding;texture cluster creation;image compression rate,,,,11,,2-May-05,,,IEEE,IEEE Conferences
Social role clustering with topic model,具有主題模型的社會角色聚類,J. Bai; L. Li; D. Zeng; J. Lin,"The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, China",2016 IEEE Conference on Intelligence and Security Informatics (ISI),17-Nov-16,2016,,,211,213,"In this paper, we propose a new role analyzing paradigm for social networks enlightened by topic modeling, which can be adopted as a primitive building block in various security related tasks, such as hidden community finding, important person recognizing and so on. We first present the social network under analyzing as a heterogeneous network constructed by both the users and the subjects discussed among them. We then view this network in a Bag-of-Users schema, which mimics its classical Bag-of-Words counterpart. In this schema, the subjects discussed are treated as ?documents??while the users are treated as ?words??which construct the ?documents?? Based on this novel presentation, we finally apply topic modeling technology to perform the social role clustering. Experiments on a practical security-related social network dataset prove the effectiveness of our approach.",,978-1-5090-3865-7,10.1109/ISI.2016.7745473,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745473,social role;social network;topic model;network structure mining;hidden community,Social network services;Heterogeneous networks;Analytical models;Computational modeling;Finance;Security;Data models,document handling;pattern clustering;security of data;social networking (online),social role clustering;topic modeling;heterogeneous network;Bag-of-Users schema;security-related social network dataset,,1,,5,,17-Nov-16,,,IEEE,IEEE Conferences
Bigradient learning algorithm for dimension reduction of text document space,文本文檔空間降維的Bigradient學習算法,L. Skovajsov獺; I. Mokri禳,"Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovak Republic; Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovak Republic",2009 IEEE International Conference on Computational Cybernetics (ICCC),22-Jan-10,2009,,,125,128,"This paper shows text document dimension reduction and clustering technique which is called the bigradient learning algorithm. This algorithm is based on the two learning parameters. The results show, that bigradient learning algorithm, used with proper selected values, does almost the same clustering as the other arbitrary PCA learning method by neural network. At the end, the three linear PCA methods for document clustering are compared. They are: linear Hebbian neural network with Oja learning rule, and neural network with bigradient learning algorithm. The results are concluded.",,978-1-4244-5310-8,10.1109/ICCCYB.2009.5393948,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393948,,Neural networks;Clustering algorithms;Singular value decomposition;Principal component analysis;Vectors;Neurons;Cybernetics;Informatics;Learning systems;Matrix decomposition,gradient methods;information retrieval;neural nets;principal component analysis,bigradient learning algorithm;dimension reduction;text document space;clustering technique;PCA learning method;linear Hebbian neural network;document retrieval,,,,16,,22-Jan-10,,,IEEE,IEEE Conferences
Efficient Semi-supervised Spectral Co-clustering with Constraints,約束條件下的高效半監督譜共聚,X. Shi; W. Fan; P. S. Yu,"Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA; NA; Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA",2010 IEEE International Conference on Data Mining,20-Jan-11,2010,,,1043,1048,"Co-clustering was proposed to simultaneously cluster objects and features to explore inter-correlated patterns. For example, by analyzing the blog click-through data, one finds the group of users who are interested in a specific group of blogs in order to perform applications such as recommendations. However, it is usually very difficult to achieve good co-clustering quality by just analyzing the object-feature correlation data due to the sparsity of the data and the noise. Meanwhile, one may have some prior knowledge that indicates the internal structure of the co-clusters. For instance, one may find user cluster information from the social network system, and the blog-blog similarity from the social tags or contents. This prior information provides some supervision toward the co-cluster structures, and may help reduce the effect of sparsity and noise. However, most co-clustering algorithms do not use this information and may produce unmeaningful results. In this paper we study the problem of finding the optimal co-clusters when some objects and features are believed to be in the same cluster a priori. A matrix decomposition based approach is proposed to formulate as a trace minimization problem, and solve it efficiently with the selected eigenvectors. The asymptotic complexity of the proposed approach is the same as co-clustering without constraints. Experiments include graph-pattern co-clustering and document-word co-clustering. For instance, in graph-pattern data set, the proposed model can improve the normalized mutual information by as much as 5.5 times and 10 times faster than two naive solutions that expand the edges and vertices in the graphs.",2374-8486,978-1-4244-9131-5,10.1109/ICDM.2010.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694082,Spectral;Semi-supervised Learning;Co-clustering,Bipartite graph;Minimization;Complexity theory;Clustering algorithms;Noise;Optimization;Internet,constraint handling;correlation methods;eigenvalues and eigenfunctions;graph theory;matrix decomposition;pattern clustering;social networking (online),semisupervised spectral coclustering algorithm;object feature analysis;data sparsity;social network system;matrix decomposition based approach;trace minimization problem;asymptotic complexity;graph-pattern co-clustering;document-word co-clustering;normalized mutual information,,17,,7,,20-Jan-11,,,IEEE,IEEE Conferences
Query revision during cluster based search on large unstructured corpora,大型非結構化語料庫的基於集群的搜索過程中的查詢修訂,V. Deolalikar,"Hewlett-Packard Research, Sunnyvale, CA 94089",2014 IEEE International Conference on Big Data (Big Data),8-Jan-15,2014,,,845,853,"We investigate a frequently occurring issue in search (retrieval) in the age of big unstructured data. Searches conducted on large unstructured corpora result in long results lists. Such results lists are often clustered and reranked for ease of navigation. Should a query be revised during time-critical examinations of such long cluster based reranked lists? This question arises naturally during early stages of commercially important applications of IR such as eDiscovery, but has not yet been given any research attention. Four factors compound the difficulty of this question in the setting of eDiscovery: (a) the query sources (the technical experts) are different from the legal staff that are actually executing the query and using the retrieval system, (b) the retrieved lists for each query tend to be very long, and (c) the user might be accessing these retrieved results through a clustering interface, and (c) all decisions must be transparent and easy to explain due to the litigious nature of the application. Analogous difficulties arise in other applications involving search over large unstructured corpora. We introduce a framework to help users make the decision of ?whether to revise.??Our framework consists of two components. First, we introduce a ?limited view??which is a summary of a long cluster-based reranked list. This is the first input to the user. This provides the user a summary of the long cluster-based list. Second, we construct query predictors for this limited view, and provide their prediction as a second input to the user. This prediction is used to corroborate the inspection of the summary limited view. The proposed combination of a limited view and query performance prediction can assist search staff in determining whether to pursue an expensive query revision or not, as well as save precious time by precluding inspections of lists with very few relevant documents during the early stages of commercially important applications such as eDiscovery.",,978-1-4799-5666-1,10.1109/BigData.2014.7004314,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004314,,Law;Standards;Clustering algorithms;Search problems;Measurement;Vectors,document handling;pattern clustering;query processing,cluster based search;big unstructured data;results list clustering;results list re-ranking;time-critical examinations;long-cluster-based reranked lists;IR;eDiscovery;query sources;technical experts;legal staff;retrieval system;retrieved result access;clustering interface;large unstructured corpora;query predictors;summary limited view inspection;query performance prediction;search staff;query revision,,,,43,,8-Jan-15,,,IEEE,IEEE Conferences
An Enhanced Method for Topic Modeling using Concept-Latent,一種使用概念潛在的主題建模的增強方法,C. A; M. G. G,"Faculty of Computing, Sathyabama Institute of Science and Technology, Chennai, India; Faculty of Computing, Sathyabama Institute of Science and Technology, Chennai, India","2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",11-Oct-19,2019,,,485,490,"In this digital era, Topic Modeling plays a vital role in clustering of documents. In topic modeling, each document is represented as a collection of words. Labeling appropriate word distributions to the defined topic is a challenging task. Topic modeling using unsupervised algorithm does not align topics with those as it exists in the real world metaphor. This problem of aligning topic with real world objects can be carried out by two different methods-either to use a supervised learning algorithm that assigns individual topic with a label acquired from existing data or to use a predefined set of topics and to match them with the distribution of words. The above said methods have some drawbacks as the former method may match a label which does not convey a semantic meaning with the word distributions whereas the latter lacks the knowledge required to detect the topic from the closed set of topics. In this paper an algorithm called Concept-Latent Dirichlet Allocation (Concept-LDA) is proposed which incorporates reinforcement learning for topic modeling which inturn improves the accuracy of resulting topics and of the topic labeling. Experiments were conducted using LDA and Concept-LDA for topic modeling and it is shown that Concept-LDA showing better accuracy than LDA.",,978-1-7281-0211-5,10.1109/COMITCon.2019.8862179,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862179,Documents;Reinforcement learning;Latent Dirichlet Allocation;Gamma distribution;Topic modeling;etc.,Feature extraction;Clustering algorithms;Reinforcement learning;Computational modeling;Resource management;Text processing,natural language processing;pattern clustering;supervised learning;word processing,topic modeling;topic labeling;documents clustering;word distributions;supervised learning algorithm;concept-latent Dirichlet allocation;Concept-LDA;reinforcement learning,,,,15,,11-Oct-19,,,IEEE,IEEE Conferences
A SOM mapping technique for visualizing documents in a database,SOM映射技術，用於可視化數據庫中的文檔,S. A. Morris; Z. Wu; G. Yen,"Sch. of Electr. & Comput. Eng., Oklahoma State Univ., Stillwater, OK, USA; NA; NA",IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),7-Aug-02,2001,3,,1914,1919 vol.3,"A method is introduced for mapping documents, based on document citations, on a two dimensional map for clustering and visualization for the application of technology forecasting. The citation data is used to build an adjacency matrix which describes the document set as an undirected graph. The dimensionality of the adjacency matrix is reduced using principal components analysis. The reduced dimension data is used to train a small rectangular self organizing map (SOM). After training, each document's input vector is premultiplied by the SOM weight matrix to find a spatial response across the SOM and the centroid of this response is used to map the document. The ordination method is demonstrated on a synthetic data set with good results. Further encouraging results using an actual 118 polymer document dataset are also shown.",1098-7576,0-7803-7044-9,10.1109/IJCNN.2001.938456,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938456,,Visual databases;Technology forecasting;Frequency;Histograms;Space technology;Data engineering;Data visualization;Application software;Principal component analysis;Organizing,principal component analysis;self-organising feature maps;learning (artificial intelligence);information retrieval,SOM mapping technique;documents visualisation;document database;document citations;two dimensional map;clustering;technology forecasting;adjacency matrix;document set;undirected graph;principal components analysis;rectangular self organizing map;spatial response;ordination method;polymer document dataset,,6,,9,,7-Aug-02,,,IEEE,IEEE Conferences
Semi-Supervised Collective Matrix Factorization for Topic Detection and Document Clustering,用於主題檢測和文檔聚類的半監督集體矩陣分解,Y. Wang; Y. Zhang; B. Zhou; Y. Jia,"Centre for Appl. Inf., Victoria Univ. Melbourne, Melbourne, VIC, Australia; Centre for Appl. Inf., Victoria Univ. Melbourne, Melbourne, VIC, Australia; Coll. of Comput., Nat. Univ. of Defense Techonlogy, Changsha, China; Coll. of Comput., Nat. Univ. of Defense Techonlogy, Changsha, China",2017 IEEE Second International Conference on Data Science in Cyberspace (DSC),10-Aug-17,2017,,,88,97,"Topic detection and tracking (TDT) under modern media circumstances has been dramatically innovated with the ever-changing social network and inconspicuous connections among participants in the internet communities. Apart from the inherent word features of analysing materials, such as news articles and personal or professional comments, incidental information attracts increasing attention from the research community. Meanwhile, numerous interrelations hiding in the propagated articles and network participants also promote the transfer and evolvement of topics, not only apparent connections, for example having the same tags and belonging to the same party, but also weak connections which are complicated and with little causal relations. Therefore, answering the question how to exploit and use this hidden information in the social network will extend the landscape of research on TDT. In this paper, we employ the followers' groups extracted from Twitter as the social context that accompanied the corresponding news articles and explore the interior links among them to develop a non-negative factorization methods with semi-supervised information derived from the original data. Furthermore, experiments are conducted on real and semi-synthetic data sets to test the performance of topic detection and documents clustering. The results demonstrate that the proposed method outperforms several state-of-the-art methods.",,978-1-5386-1600-0,10.1109/DSC.2017.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005460,Non-negative matrix;constraint;propagation,Matrix decomposition;Social network services;Media;Dynamics;Probabilistic logic;Linear programming;Semantics,document handling;Internet;matrix decomposition;pattern clustering;social networking (online),semi-supervised collective matrix factorization;topic detection and tracking;TDT;document clustering;social network;Internet communities;news articles;personal comments;professional comments;incidental information;social context;nonnegative factorization methods;interior links;real data sets;semi-synthetic data sets,,,,38,,10-Aug-17,,,IEEE,IEEE Conferences
A novel document distance based on concept vector space,基於概念向量空間的新型文檔距離,L. Li; H. Li,"Department of foreign languages and culture, Anhui University, Hefei, P. R. China; Department of EEIS, University of Science and Technology of China, Hefei, P. R. China",2017 IEEE 17th International Conference on Communication Technology (ICCT),17-May-18,2017,,,2014,2017,"A novel metric to measure the distance between documents is proposed in this paper. By utilizing the recent results in word embeddings which can present semantical information between words by real-value vectors, we model a document as a concept vector space, where the concepts are a series of key words extracted based on the text by dependency parsing and linguistic knowledge. A new document distance is defined on the concept vector space to measure the relatedness or similarity between two documents, which can be used in many natural language processing (NLP) task such as document classification, news clustering, etc. The proposed metric has no hyperparameters to tuning and is easily to compute. Further we give a demonstration on a few real world document classification datasets based on k-nearest neighbor (kNN) algorithm. The experiment results show that the new document distance can lead to an impressive quality improvement on document classification.",2576-7828,978-1-5090-3944-9,10.1109/ICCT.2017.8359982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359982,document distance;document model;vector space;word embeddings;kNN,Semantics;Cats;Extraterrestrial measurements;Classification algorithms;Linguistics,linguistics;natural language processing;text analysis;vectors,concept vector space;word embeddings;document classification datasets;semantical information;keyword extraction;parsing;linguistic knowledge;natural language processing;NLP,,,,12,,17-May-18,,,IEEE,IEEE Conferences
A block based segmentation of JPEG compressed document images,JPEG壓縮文檔圖像的基於塊的分割,N. H. V. L. P. Kumar; V. K. Srivastava,"Department of Electronics and Communication Engineering, Motilal Nehru National Institute of Technology-211004, India; Department of Electronics and Communication Engineering, Motilal Nehru National Institute of Technology-211004, India",2012 Students Conference on Engineering and Systems,14-May-12,2012,,,1,4,"Block based image and video coding systems are used extensively in practice. In low bit rate applications they suffer with undesirable compression artifacts, especially for document images. Existing methods can reduce theses artifacts by using post processing methods without changing the encoding process. Some of these post processing methods requires classification of the encoded blocks into different categories. In this paper we proposed a block based segmentation of JPEG coded images.",,978-1-4673-0455-9,10.1109/SCES.2012.6199093,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199093,AC energy;DCT coefficient;JPEG;K-means clustering;Threshold value,Transform coding;Quantization;Image segmentation;Image coding;Encoding;Image color analysis;Classification algorithms,document image processing;image coding;image segmentation,block based segmentation;JPEG compressed document images;video coding systems;post processing methods;encoding process,,,,12,,14-May-12,,,IEEE,IEEE Conferences
A Noise Rejection Mechanism for pLSA-induced Fuzzy Co-clustering,pLSA引起的模糊共聚的噪聲抑制機制,K. Honda; K. Hoshii; S. Ubukata; A. Notsuy,"Osaka Prefecture University,Graduate School of Engineering,Osaka,Japan,599-8531; Osaka Prefecture University,Graduate School of Engineering,Osaka,Japan,599-8531; Osaka Prefecture University,Graduate School of Engineering,Osaka,Japan,599-8531; Osaka Prefecture University,Graduate School of Humanities and Sustainable System Sciences,Osaka,Japan,599-8531",2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),26-Aug-20,2020,,,1,8,"Noise fuzzy clustering is a useful scheme for analyzing intrinsic data structures through robust estimation of fuzzy c-partition. In this paper, a novel noise rejection scheme for improving fuzzy co-clustering is proposed, which is useful in such cooccurrence information analysis as document classification, where multi-topic co-cluster structure extraction by probabilistic latent semantic analysis (pLSA) is achieved through rejection of the influences of noise objects. Supported by the uniform noise distribution concept in noise fuzzy clustering, a noise cluster having uniform item occurrence probabilities is newly introduced into the pLSA-induced fuzzy co-clustering model. Several numerical experiments demonstrate the advantage of tuning the noise sensitivity of the pLSA-induced objective function.",1558-4739,978-1-7281-6932-3,10.1109/FUZZ48607.2020.9177597,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9177597,,Linear programming;Probabilistic logic;Robustness;Mixture models;Analytical models;Numerical models;Entropy,data structures;document handling;fuzzy set theory;information analysis;pattern classification;pattern clustering;probability,uniform item occurrence probabilities;multitopic co-cluster structure extraction;document classification;cooccurrence information analysis;fuzzy c-partition;pLSA-induced fuzzy co-clustering;pLSA-induced objective function;noise sensitivity;noise cluster;uniform noise distribution;noise objects;probabilistic latent semantic analysis;noise rejection;intrinsic data structures;noise fuzzy clustering,,,,26,,26-Aug-20,,,IEEE,IEEE Conferences
Implementation of Intelligent Searching Using Self-Organizing Map for Webmining Used in Document Containing Information in Relation to Cyber Terrorism,使用自組織地圖進行Webmining智能搜索的實現，該Webmining用於包含與網絡恐怖主義有關的信息的文檔,Endy; C. lim; K. I. Eng; A. S. Nugroho,"Dept. of Inf. Technol., Swiss German Univ., Tangerang, Indonesia; Dept. of Inf. Technol., Swiss German Univ., Tangerang, Indonesia; Dept. of Inf. Technol., Swiss German Univ., Tangerang, Indonesia; Center for Inf. & Commun. Technol., Agency for the Assessment & Applic. of Technol. (BPPT), Jakarta, Indonesia","2010 Second International Conference on Advances in Computing, Control, and Telecommunication Technologies",23-Dec-10,2010,,,195,197,"The terrorism activities are not only in real world as development of technology, but also in cyber world. Terrorism activities in cyber world are called cyber terrorism. One of methodology for cyber terrorism detection is by applying data mining algorithm to textual content of terrorism related web pages. Web mining is technology applied to extract information from the web. By using web mining, cyber terrorism information will be collected from internet. This research aims to use text cluster technique, by which the web documents are clustered using Self-Organizing Map algorithm based on number of occurrences of the certain words in documents that have relevance to cyber terrorism. The result shows mapping of the clustered documents that have performance 6.1 and 22.75 in term of vector quantization error (VQE). According this result, we concluded that Self-Organizing Map (SOM) is able to visualizethe topology of the data, by converting statistical relationship among the data into simple geometrical relationship of their image points in 2-dimensional grid.",,978-1-4244-8746-2,10.1109/ACT.2010.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5675810,cyber terrorism;intelligent searching;Self-Organizing Map,Terrorism;Computers;Internet;Neurons;Government;Web mining;Vector quantization,computer crime;data mining;information retrieval;Internet;pattern clustering;self-organising feature maps;terrorism;vector quantisation,intelligent searching;Web mining;cyber terrorism detection;data mining algorithm;Web pages;information extraction;internet;text cluster technique;Web documents;selforganizing map algorithm;vector quantization error,,2,,4,,23-Dec-10,,,IEEE,IEEE Conferences
Extracting Partitional Clusters from Heterogeneous Datasets using Mutual Entropy,使用互熵從異構數據集中提取分區聚類,M. Hossain; S. Bridges; Y. Wang; J. Hodges,"Department of Computer Science, Fairmont State University, WV. mhossain@fairmontstate.edu; Department of Computer Science and Engineering, Mississippi State University, MS. bridges@cse.msstate.edu; Department of Computer Science and Engineering, Mississippi State University, MS. ywang@cse.msstate.edu; Department of Computer Science and Engineering, Mississippi State University, MS. hodges@cse.msstate.edu",2007 IEEE International Conference on Information Reuse and Integration,4-Sep-07,2007,,,447,454,"Clustering has traditionally been used for partitioning the objects of a single dataset. Some applications may require the clustering of multiple related heterogeneous datasets where it may not be easy to compute a useful and effective integrated feature space. In this paper, we present an algorithm called CEMENT (Cluster Ensemble using Mutual ENTropy) to address the problem of clustering two related datasets where the datasets represent the same or overlapping sets of objects but use different feature sets. The algorithm takes the partitional clusters generated from two datasets as input and uses a constraint-based approach to generate a single set of clusters. CEMENT is an EM (expectation maximization) approach where the objective function is the mutual entropy between the two sets of clusters. The algorithm was applied to the problem of clustering a document collection consisting of journal abstracts from ten different Library of Congress categories. These documents were pre-processed using several NLP (natural language processing) steps to extract syntactic and semantic feature sets. We present empirical results and statistical tests showing that CEMENT yields higher quality clusters with this dataset than several baseline clustering approaches.",,1-4244-1499-7,10.1109/IRI.2007.4296661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296661,,Entropy;Clustering algorithms;Partitioning algorithms;Computer science;Bridges;Data engineering;Application software;Abstracts;Libraries;Natural language processing,computational linguistics;document handling;expectation-maximisation algorithm;maximum entropy methods;natural language processing;pattern clustering,heterogeneous dataset clustering;mutual entropy;expectation maximization approach;constraint-based approach;objective function;document collection;natural language processing;semantic feature set extraction;syntactic feature set extraction;statistical test,,,,13,,4-Sep-07,,,IEEE,IEEE Conferences
Requirements guided dynamic software clustering,需求指導的動態軟件集群,Wei Zhao; Lu Zhang; Hong Mei; Jiasu Sun,"Inst. of Software, Peking Univ., Beijing, China; Inst. of Software, Peking Univ., Beijing, China; Inst. of Software, Peking Univ., Beijing, China; Inst. of Software, Peking Univ., Beijing, China",21st IEEE International Conference on Software Maintenance (ICSM'05),21-Nov-05,2005,,,605,608,"In this paper, we propose a requirements guided dynamic approach to address software clustering -which aims at providing the logically meaningful and high-level decompositions of large and complex systems. In our approach, the hierarchical structure of functional requirements are constructed by a text document clustering technique named hierarchical agglomerative clustering (HAC) as a high-level skeleton to facilitate the further decomposition of source code through dynamic analysis. We also perform an experimental study based on a GNU system and present the quantitative and qualitative analysis of the experimental results.",1063-6773,0-7695-2368-4,10.1109/ICSM.2005.76,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510155,software clustering;hierarchical agglomerative clustering;program comprehension,Software maintenance;Skeleton;Sun;Computer science;Performance analysis;Software systems;Collaboration;Software performance;Merging;Software measurement,text analysis;formal specification,requirements guided dynamic software clustering;text document clustering technique;hierarchical agglomerative clustering;source code;dynamic analysis;quantitative analysis;qualitative analysis,,,,19,,21-Nov-05,,,IEEE,IEEE Conferences
Multilayer SOM With Tree-Structured Data for Efficient Document Retrieval and Plagiarism Detection,具有樹狀結構數據的多層SOM，可進行有效的文檔檢索和抄襲檢測,T. W. S. Chow; M. K. M. Rahman,"Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China; Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China",IEEE Transactions on Neural Networks,1-Sep-09,2009,20,9,1385,1402,"This paper proposes a new document retrieval (DR) and plagiarism detection (PD) system using multilayer self-organizing map (MLSOM). A document is modeled by a rich tree-structured representation, and a SOM-based system is used as a computationally effective solution. Instead of relying on keywords/lines, the proposed scheme compares a full document as a query for performing retrieval and PD. The tree-structured representation hierarchically includes document features as document, pages, and paragraphs. Thus, it can reflect underlying context that is difficult to acquire from the currently used word-frequency information. We show that the tree-structured data is effective for DR and PD. To handle tree-structured representation in an efficient way, we use an MLSOM algorithm, which was previously developed by the authors for the application of image retrieval. In this study, it serves as an effective clustering algorithm. Using the MLSOM, local matching techniques are developed for comparing text documents. Two novel MLSOM-based PD methods are proposed. Detailed simulations are conducted and the experimental results corroborate that the proposed approach is computationally efficient and accurate for DR and PD.",1941-0093,,10.1109/TNN.2009.2023394,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5175377,Document retrieval (DR);multilayer self-organizing map (MLSOM);plagiarism detection (PD);tree-structured representation,Nonhomogeneous media;Information retrieval;Plagiarism;Support vector machines;Support vector machine classification;Clustering algorithms;Search engines;Image retrieval;Computational modeling,image retrieval;query processing;self-organising feature maps;tree data structures,document retrieval system;plagiarism detection system;multilayer selforganizing map;tree-structured representation;word-frequency information;MLSOM algorithm;image retrieval;clustering algorithm;local matching techniques;simulation,"Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Humans;Information Storage and Retrieval;Internet;Neural Networks (Computer);Neurons;Pattern Recognition, Automated;Plagiarism;Time Factors",44,,44,,28-Jul-09,,,IEEE,IEEE Journals
A Novel Chinese Multi-Document Summarization Using Clustering Based Sentence Extraction,基於聚類的句子抽取的中文多文檔摘要,D. Liu; Y. He; D. Ji; H. Yang,"School of Physics, Xiangfan University, Xiangfan 441053, P. R. China; School of Computer, Wuhan University, Wuhan 430079, P. R. China; Center for Study of Language and Information, Wuhan University, Wuhan 430079, P. R. China. E-MAIL: dexiliu@gmail.com; School of Computer, Wuhan University, Wuhan 430079, P. R. China; Center for Study of Language and Information, Wuhan University, Wuhan 430079, P. R. China; Center for Study of Language and Information, Wuhan University, Wuhan 430079, P. R. China; Institute for Infocomm Research, Heng Mui Keng Terrace 119613, Singapore; School of Computer, Wuhan University, Wuhan 430079, P. R. China; Center for Study of Language and Information, Wuhan University, Wuhan 430079, P. R. China",2006 International Conference on Machine Learning and Cybernetics,4-Mar-09,2006,,,2592,2597,"This paper proposes a strategy for Chinese multi-document summarization based on clustering and sentence extraction. It adopts the term vector to represent the linguistic unit in Chinese document, which obtains higher representation quality than traditional word-based vector space model in a certain extent. As for clustering, we propose two heuristics to automatically detect the proper number of clusters: the first one makes full use of the summary length fixed by the user; the second is a stability method, which has been applied to other unsupervised learning problems. We also discuss a global searching method for sentence selection from the clusters. To evaluate our summarization strategy, an extrinsic evaluation method based on classification task is adopted. Experimental results on news document set show that the new strategy can significantly enhance the performance of Chinese multi-document summarization",2160-1348,1-4244-0061-9,10.1109/ICMLC.2006.258855,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4028501,Chinese Multi-document summarization;term vector space;stability method;global searching method;extrinsic evaluation,Data mining;Stability;Helium;Physics computing;Natural languages;Unsupervised learning;Information retrieval;Mathematical model;Instruments;Ontologies,natural languages;pattern classification;pattern clustering;text analysis;unsupervised learning,Chinese multidocument summarization;pattern clustering;sentence extraction;linguistic unit;heuristics;stability method;unsupervised learning;global searching method;extrinsic evaluation method;pattern classification;term vector space,,5,,14,,4-Mar-09,,,IEEE,IEEE Conferences
Search Results Clustering Based on Suffix Array and VSM,基於後綴數組和VSM的搜索結果聚類,S. Bai; W. Zhu; B. Zhang; J. Ma,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China; Fac. of Comput. & Inf. Sci., Hosei Univ., Tokyo, Japan","2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing",7-Mar-11,2010,,,852,857,"With the rapid growth of web pages, search engines will usually present a long ranked list of documents. The users must sift through the list with ""title"" and ""snippet"" (a short description of the document) to find the desired document. This method may be good for some simple and specific tasks but less effective and efficient for ambiguous queries such as ""apple"" or ""jaguar"". To improve the effect and efficiency of information retrieval, an alternative method is to automatically organize retrieval results into clusters. This paper presents an improved Lingo algorithm named Suffix Array Similarity Clustering (SASC) for clustering web search results. This method creates the clusters by adopting improved suffix array, which ignores the redundant suffixes, and computing document similarity based on the title and short document snippets returned by Web search engines. Experiments show that the SASC algorithm has not only a better performance in time-consuming than Lingo but also in cluster description quality and precision than Suffix Tree Clustering.",,978-1-4244-9779-9,10.1109/GreenCom-CPSCom.2010.107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724930,Suffix Array;Suffix Tree;STC;Lingo;search results clustering,Arrays;Clustering algorithms;Information retrieval;Algorithm design and analysis;Matrix decomposition;Software algorithms;Software,information retrieval;Internet;pattern clustering;search engines,suffix array;VSM;search results clustering;ambiguous queries;information retrieval;suffix array similarity clustering;Lingo algorithm;SASC;clustering web search;Web search engines,,1,,8,,7-Mar-11,,,IEEE,IEEE Conferences
Joint Local and Global Consistency on Interdocument and Interword Relationships for Co-Clustering,關於文檔間和單詞間關係的局部和全局聯合一致性,B. Bao; W. Min; T. Li; C. Xu,"Chinese Academy of Sciences, National Laboratory of Pattern Recognition, Beijing, China; Chinese Academy of Sciences, National Laboratory of Pattern Recognition, Beijing, China; College of Electrical Engineering and Automation, Anhui University, Hefei, China; Chinese Academy of Sciences, National Laboratory of Pattern Recognition, Beijing, China",IEEE Transactions on Cybernetics,20-May-17,2015,45,1,15,28,"Co-clustering has recently received a lot of attention due to its effectiveness in simultaneously partitioning words and documents by exploiting the relationships between them. However, most of the existing co-clustering methods neglect or only partially reveal the interword and interdocument relationships. To fully utilize those relationships, the local and global consistencies on both word and document spaces need to be considered, respectively. Local consistency indicates that the label of a word/document can be predicted from its neighbors, while global consistency enforces a smoothness constraint on words/documents labels over the whole data manifold. In this paper, we propose a novel co-clustering method, called co-clustering via local and global consistency, to not only make use of the relationship between word and document, but also jointly explore the local and global consistency on both word and document spaces, respectively. The proposed method has the following characteristics: 1) the word-document relationships is modeled by following information-theoretic co-clustering (ITCC); 2) the local consistency on both interword and interdocument relationships is revealed by a local predictor; and 3) the global consistency on both interword and interdocument relationships is explored by a global smoothness regularization. All the fitting errors from these three-folds are finally integrated together to formulate an objective function, which is iteratively optimized by a convergence provable updating procedure. The extensive experiments on two benchmark document datasets validate the effectiveness of the proposed co-clustering method.",2168-2275,,10.1109/TCYB.2014.2317514,National Program on Key Basic Research Project 973 Program; National Natural Science Foundation of China; China Post-Doctoral Science Foundation; Open Project Program of the National Laboratory of Pattern Recognition (NLPR); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844822,Co-clustering;information theory;local and global learning;Co-clustering;information theory;local and global learning,Random variables;Manifolds;Mutual information;Kernel;Matrix decomposition;Vectors;Joints,iterative methods;optimisation;pattern clustering;word processing,information-theoretic coclustering;ITCC;word partitioning;document partitioning;word-document relationships;smoothness regularization;objective function;iterative optimization,,7,,33,,26-Jun-14,,,IEEE,IEEE Journals
Evaluation of Clustering Techniques for Efficient Searching in JXTA-based P2P Systems,基於JXTA的P2P系統中有效搜索的聚類技術評估,F. Xhafa; E. J. Villoldo; T. Daradoumis; L. Barolli,"Dept. of Languages & Inf. Syst., Polytech. Univ. of Catalonia, Barcelona; NA; NA; NA","2008 International Conference on Complex, Intelligent and Software Intensive Systems",22-Aug-08,2008,,,466,471,"The efficient file searching is an essential feature in P2P systems. While many current approaches use brute force techniques to search files by meta information (file names, extensions or user-provided tags), the interest is in implementing techniques that allow content-based search in P2P systems. Recently, clustering techniques have been used for searching text documents to increase the efficiency of document discovery and retrieval. Integrating such techniques into P2P systems is important toenhance searching in P2P file sharing systems. While some effort has been done for content-based searching for text documents in P2P systems, there has been few research work for applying these techniques for multimedia content in P2P systems. In this paper we introduce two P2P content-based clustering techniques for multimedia documents. These techniques are an adaptation of the existing Class-based Semantic Search (CSS) algorithm for text documents. The proposed algorithms have been integrated into a JXTA-based overlay P2P platform, and some initial evaluation results are provided. The JXTA-Overlay together with the considered clustering techniques is thus very useful for developing P2P multimedia applications requiring efficient searching of multimedia contents in peer nodes.",,978-0-7695-3109-0,10.1109/CISIS.2008.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4606721,,Peer to peer computing;Indexes;Clustering algorithms;Time measurement;Middleware;Classification algorithms;Algorithm design and analysis,content-based retrieval;document handling;Java;multimedia computing;peer-to-peer computing,clustering technique evaluation;JXTA-based P2P systems;file searching;meta information;content-based search;text document searching;document discovery;document retrieval;multimedia content;class-based semantic search algorithm,,,,14,,22-Aug-08,,,IEEE,IEEE Conferences
Incremental clustering for dynamic document databases,動態文檔數據庫的增量聚類,F. Can; N. D. Drochak,"Miami Univ., FL, USA; Miami Univ., FL, USA",Proceedings of the 1990 Symposium on Applied Computing,6-Aug-02,1990,,,61,67,"Clustering of very large document databases is a necessity to reduce the space time complexity of retrieval operations. Continuous update of clusters is required owing to the dynamic nature of document databases. An algorithm for incremental clustering is introduced. Its complexity analysis and cost comparison with reclustering are provided. It is shown through empirical testing that the incremental clustering is as effective as the reclustering by producing clusters that are very similar. This similarity is then shown not to be by chance. In the experiments it is shown that the information retrieval effectiveness of the algorithm is compatible with the reclustering algorithm, which is known to have good retrieval performance.<>",,0-8186-2031-5,10.1109/SOAC.1990.82141,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=82141,,Databases;Information retrieval;Clustering algorithms;Costs;Testing;Machine assisted indexing,computational complexity;database management systems;information retrieval systems,dynamic document databases;very large document databases;space time complexity;retrieval operations;incremental clustering;complexity analysis;cost comparison;reclustering;empirical testing;information retrieval effectiveness;retrieval performance,,2,,11,,6-Aug-02,,,IEEE,IEEE Conferences
Algorithms for postprocessing OCR results with visual inter-word constraints,具有視覺詞間約束的OCR結果後處理算法,T. Hong; J. J. Hull,"Center of Excellence for Document Anal. & Recognition, State Univ. of New York, Buffalo, NY, USA; NA","Proceedings., International Conference on Image Processing",6-Aug-02,1995,3,,312,315 vol.3,Algorithms are presented that determine the visual relationships between word images in a document. These include instances of common word images and common substrings that occur often in English language text images. This information is then used to improve the performance of a commercial optical character recognition (OCR) algorithm. The algorithms presented calculate clusters of equivalent word images as well as common initial and final substrings. Experimental results are presented that show a 40% reduction in word level error rate is achieved on a test set of documents degraded by uniform noise.,,0-8186-7310-9,10.1109/ICIP.1995.537638,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537638,,Optical character recognition software;Optical noise;Clustering algorithms;Natural languages;Character recognition;Error analysis;Testing;Degradation;Noise reduction;Noise level,document image processing;optical character recognition,OCR results;postprocessing algorithms;visual interword constraints;English language text images;performance;optical character recognition algorithm;OCR algorithm;word images;substrings;experimental results;word level error rate;uniform noise;text document,,1,,3,,6-Aug-02,,,IEEE,IEEE Conferences
A Service Clustering Method Based on Wisdom of Crowds,基於人群智慧的服務聚類方法,H. Gao; K. K. Dluzniak; H. Xia; W. Jie; Y. Chen; W. Xing; X. Wang; Z. Wang,Xi?an University of Posts and Telecommunications; University of West London; Xi?an University of Posts and Telecommunications; University of West London; Xi?an University of Posts and Telecommunications; Xi?an University of Posts and Telecommunications; Xi?an University of Posts and Telecommunications; Xi?an University of Posts and Telecommunications,2019 IEEE International Congress on Big Data (BigDataCongress),29-Aug-19,2019,,,98,105,"As the number and variety of services increase, it is becoming difficult and time-consuming to locate services that satisfy users' need. Service clustering is efficacious method to prune the query space, to narrow the searching space, and improve the accuracy of locating services that satisfied users' needs. At present, clustering method of web services adopted single or traditional clustering algorithms. However, accuracy and stability of single or traditional clustering algorithms is poor. In the paper, we proposed SWOC a service clustering method based on wisdom of crowd. Firstly, by using SWOC we calculated document similarity. Secondly, we implemented a mapping algorithm that reduces the correlation of web services and improve accuracy of method. And then, we applyed different number of clusters using different individual clustering methods that increase the number of partitions so as to enhance the robustness of SWOC. Lastly, the diversity algorithm evaluates and selects the partitions to extract interesting information for the final aggregation with the weight of each individual result. Experiments were performed on the real web service dataset crawled from ProgrammableWeb which prove the accuracy, recall, F-value and stability of proposed method.",2642-7273,978-1-7281-2772-9,10.1109/BigDataCongress.2019.00026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818221,cluster;service clustering;wisdom of crowds;clustering analysis;clustering ensemble,,crowdsourcing;pattern clustering;query processing;service-oriented architecture;Web services,service oriented computing;crowd wisdom;SWOC;searching space;query space;clustering algorithms;Web services;service clustering,,,,32,,29-Aug-19,,,IEEE,IEEE Conferences
An EM Clustering Algorithm which Produces a Dual Representation,產生雙重表示的EM聚類算法,S. Kim; W. J. Wilbur,"Nat. Center for Biotechnol. Inf., Nat. Inst. of Health, Bethesda, MD, USA; Nat. Center for Biotechnol. Inf., Nat. Inst. of Health, Bethesda, MD, USA",2011 10th International Conference on Machine Learning and Applications and Workshops,9-Feb-12,2011,2,,90,95,"Clustering text documents is an important step in mining useful information on the Web or other text-based resources. The common task in text clustering is to handle text in a multi-dimensional space, and to partition documents into groups, where each group contains documents that are similar to each other. However, this strategy lacks a comprehensive view for humans since it cannot explain the subject of each cluster. Utilizing semantic information such as an ontology can solve this problem, but it needs a well-defined database or pre-labeled gold standard set. In this paper, we present a theme-based clustering algorithm for text documents. Given text, subject terms are extracted and used for clustering documents in a probabilistic framework. An EM approach is used to ensure documents are assigned to correct themes, hence it converges to an optimal solution. The proposed method is distinctive because its results are sufficiently explanatory for human understanding as well as efficient for usual clustering performance. The experimental results show that the proposed method provides competitive performance compared to other state-of-the-art approaches. In addition, the extracted themes represent well the topics of clusters on the MEDLINE dataset.",,978-1-4577-2134-2,10.1109/ICMLA.2011.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147054,,Clustering algorithms;Parkinson's disease;Algorithm design and analysis;Probabilistic logic;Vectors;Humans,data mining;Internet;medical administrative data processing;medical computing;pattern clustering;probability;text analysis,EM clustering algorithm;dual representation;text document clustering;information mining;text based resource;Web resource;multidimensional space;semantic information;prelabeled gold standard set;theme based clustering algorithm;probabilistic framework;MEDLINE簧 dataset,,,,22,,9-Feb-12,,,IEEE,IEEE Conferences
Improving arabic broadcast transcription using automatic topic clustering,使用自動主題聚類改善阿拉伯語廣播轉錄,S. M. Chu; L. Mangu,"IBM T. J. Watson Research Center, Yorktown Heights, New York, 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, New York, 10598, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",30-Aug-12,2012,,,4449,4452,"Latent Dirichlet Allocation (LDA) has been shown to be an effective model to augment n-gram language models in speech recognition applications. In this work, we aim to take advantage of the superior unsupervised learning ability of the framework, and use it to uncover topic structure embedded in the corpora in an entirely data-driven fashion. In addition, we describe a bi-level inference and classification method that allows topic clustering at the utterance level while preserving the document-level topic structures. We demonstrate the effectiveness of the proposed topic clustering pipeline in a state-of-the-art Arabic broadcast transcription system. Experiments show that optimizing LM in the LDA topic space leads to 5% reduction in language model perplexity. It is further shown that topic clustering and adaptation is able to attain 0.4% absolute word error rate reduction on the GALE Arabic task.",2379-190X,978-1-4673-0046-9,10.1109/ICASSP.2012.6288907,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288907,Arabic ASR;topic clustering;language modeling,Hidden Markov models;Adaptation models;Semantics;Optimization;Training;Interpolation;Speech,broadcasting;document handling;inference mechanisms;natural language processing;pattern classification;pattern clustering;speech recognition;unsupervised learning,automatic topic clustering;latent Dirichlet allocation;augment n-gram language models;speech recognition applications;superior unsupervised learning ability;corpora;data-driven fashion;bi-level inference;classification method;utterance level;document-level topic structures;topic clustering pipeline;state-of-the-art Arabic broadcast transcription system;LDA topic space;language model perplexity;word error rate reduction;GALE Arabic task,,1,,16,,30-Aug-12,,,IEEE,IEEE Conferences
Semantic Oriented Text Clustering Based on RDF,基於RDF的面向語義的文本聚類,S. Fatimi; C. El Saili; L. Alaoui,"International University of Rabat,TIC Lab,Sala Al Jadida,Morocco; International University of Rabat,TIC Lab,Sala Al Jadida,Morocco; International University of Rabat,TIC Lab,Sala Al Jadida,Morocco",2020 International Conference on Intelligent Systems and Computer Vision (ISCV),23-Sep-20,2020,,,1,8,"Text clustering is the discipline that purports to find related groups in a collection of documents. Based on text clustering the use of documents can be more salubrious. Researchers have used various methods to implement text clustering either agglomerative, divisive, or itemsets-based clustering. Most of these proposed approaches do not take into account the semantic relationships between words, in this case, the documents are considered only as bags of unrelated words. Our work aims to consider the semantics of the text phrases in the clustering task, and to get full usage and exploitation of documents. The semantic web concept is overloaded with valuable techniques allowing the significant use of documents. Our goal is to take full advantage of these techniques. Using the Resource Description Framework (RDF) to represent textual data as triplets. They provide a semantic representation of data on which the clustering process will be based, to provide a more efficient clustering system. On the other hand, and based on the clustering process, we opt on incorporating other techniques such as ontology representation using RDF, RDF Schemas (RDFS), and Web Ontology Language (OWL) to manipulate and extract meaningful information. In this paper, we propose a framework of semantic oriented text clustering based on RDF by the means of a semantic similarity measure, and we highlight the benefits of using semantic web techniques in clustering, topic modeling, and information extraction based on questioning, reasoning and inferencing processes.",,978-1-7281-8041-0,10.1109/ISCV49265.2020.9204133,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204133,text clustering;similarity measure;ontology;semantic web;RDF;RDFS;OWL;reasoning;inferencing rules;SPARQL;topic modeling;summarization.,,data mining;feature extraction;inference mechanisms;information retrieval;knowledge representation languages;Linked Data;ontologies (artificial intelligence);pattern clustering;semantic Web;text analysis,RDF;semantic oriented text clustering;semantic similarity measure;text phrases;itemset based clustering;semantic Web;documents clustering;resource description framework;data representation;Web ontology language;information extraction,,,,49,,23-Sep-20,,,IEEE,IEEE Conferences
Get what you want from Internet using fuzzy k-means clustering algorithm,使用模糊k均值聚類算法從Internet獲得您想要的東西,D. -S. Zhu; M. -Q. Zhou,"College of Physical Science and Technology, Yangtze University, Jinzhou, China, 434000; College of Horticulture and Garden, Yangtze University, Jinzhou, China, 434000",2009 IEEE International Conference on Granular Computing,22-Sep-09,2009,,,814,817,"This paper proposed a new method of getting what you want from Internet using fuzzy k-means clustering algorithm. It used search engine to obtain relevant documents content, then adopted efficient Fuzzy k-means clustering algorithm to cluster all the sentences. The summary sentences were extracted by turns from the clusters. Experimental result shows that the proposed method can improve the performance of summary.",,978-1-4244-4830-2,10.1109/GRC.2009.5255009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255009,,Internet;Clustering algorithms;Frequency estimation;Search engines;Educational institutions;Data mining;Clustering methods;Prototypes;Information retrieval;Electronic mail,fuzzy set theory;Internet;pattern clustering;search engines,Internet;fuzzy k-means Clustering Algorithm;search engine,,,,12,,22-Sep-09,,,IEEE,IEEE Conferences
A contrast independent algorithm for adaptive binarization of degraded document images,對比度獨立算法，用於降級文檔圖像的自適應二值化,M. Valizadeh; M. Komeili; N. Armanfard; E. Kabir,"Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran; Department of Electrical Engineering, tarbiat modares university, Tehran, Iran",2009 14th International CSI Computer Conference,8-Dec-09,2009,,,127,133,"This paper presents an efficient algorithm for adaptive binarization of degraded document images. Document binarization algorithms suffer from poor and variable contrast in document images. We propose a contrast independent binarization algorithm that does not require any parameter setting by user. Therefore, it can handle various types of degraded document images. The proposed algorithm involves two consecutive stages. At the first stage, independent of contrast between foreground and background, some parts of each character are extracted and in the second stage, the gray level of foreground and background are locally estimated. For each pixel, the average of estimated foreground and background gray levels is defined as threshold. After extensive experiments, the proposed binarization algorithm demonstrate superior performance against four well-know binarization algorithms on a set of degraded document images captured with camera.",,978-1-4244-4261-4,10.1109/CSICC.2009.5349339,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349339,degraded document;binarization;water flow model;text extraction;contrast independent,Degradation;Text analysis;Image analysis;Performance analysis;Clustering algorithms;Data mining;Cameras;Image processing;Pattern recognition;Image segmentation,document image processing,contrast independent algorithm;adaptive binarization;degraded document images;document binarization algorithms,,,,14,,8-Dec-09,,,IEEE,IEEE Conferences
A tripartite graph-based fuzzy co-similarities for document retrieval,基於三方圖的文檔的模糊相似度,S. Alouane-Ksouri; M. S. Hidri; K. Barkaoui,"Universit矇 de Tunis El Manar, Ecole Nationale d'Ing矇nieurs de Tunis, BP. 37, Le Belv矇d癡re 1002, Tunis, Tunisia; Universit矇 de Tunis El Manar, Ecole Nationale d'Ing矇nieurs de Tunis, BP. 37, Le Belv矇d癡re 1002, Tunis, Tunisia; CEDRIC-CNAM, Rue Saint-Martin Paris 75003, France",2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),11-Dec-14,2014,,,575,580,"This work exploits the use of a fuzzy many-valued concept analysis for document retrieval to efficiently answer users' queries. For the purpose of visualization, we propose to deal with fuzzy many-valued formal contexts in a tripartite graph with respect to fuzzy triadic document co-similarities. The proposed model aims at describing the documents according to three hierarchical levels. It is based on a prototyping of feature vectors applied to documents, sentences and words. The recourse to fuzzy conceptual scaling has enabled us to visualize many-valued concepts through the construction of fuzzy nested lattices. Once this built, we proceed to document retrieval using building process and lattice incremental path such a simple information retrieval.",,978-1-4799-5148-2,10.1109/FSKD.2014.6980898,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980898,Fuzzy conceptual scaling;Document retrieval;Triadic co-similarities;Fuzzy nested Lattice,Context;Lattices;Computational modeling;Clustering algorithms;Cascading style sheets;Vectors;Navigation,document handling;fuzzy set theory;graph theory;query processing,tripartite graph-based fuzzy cosimilarities;document retrieval;fuzzy many-valued concept analysis;user query answering;fuzzy triadic document cosimilarities;fuzzy conceptual scaling;fuzzy nested lattices;information retrieval,,,,13,,11-Dec-14,,,IEEE,IEEE Conferences
Cross-domain text classification using semantic based approach,使用基於語義的方法進行跨域文本分類,B. U. Anu Barathi,"Rajalakshmi Engineering College, Chennai, India",International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011),2-Feb-12,2011,,,820,825,"Internet is a huge repository of disparate information growing at an exponential rate. Efficient and effective document retrieval and classification systems are required to turn the massive amount of data into useful information, and eventually into knowledge. A traditional approach to document classification requires labelled data in order to construct reliable and accurate classifiers. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, extend the idea underlying this approach by making the latent semantic relation ship between the two domains explicit. The Semantic based cross domain classification by providing the algorithm in the extended vector space model of in-domain and out-of-domain documents. Se mantic information was embedded within the document representation, and proved via experimentation that improved classification accuracy can be achieved. The concepts form individual features, with undergoing stemming, or splitting of multi-word expressions.",,978-9-38043-000-3,10.1049/cp.2011.0479,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143428,co-clustering;in-domain;out-of-domain;stemming;splitting,,pattern classification;pattern clustering;text analysis,cross-domain text classification;semantic based approach;Internet;document retrieval system;document classification system;co-clustering based classification algorithm;in-domain document;out-of-domain document;document representation;multiword expression,,1,,,,2-Feb-12,,,IET,IET Conferences
Extracting text from WWW images,從WWW圖片中提取文字,Jiangying Zhou; D. Lopresti,"Panasonic Inf. & Networking Technol. Lab., Princeton, NJ, USA; NA",Proceedings of the Fourth International Conference on Document Analysis and Recognition,6-Aug-02,1997,1,,248,252 vol.1,"The authors examine the problem of locating and extracting text from images on the World Wide Web. They describe a text detection algorithm which is based on color clustering and connected component analysis. The algorithm first quantizes the color space of the input image into a number of color classes using a parameter-free clustering procedure. It then identifies text-like connected components in each color class based on their shapes. Finally, a post-processing procedure aligns text-like components into text lines. Experimental results suggest this approach is promising despite the challenging nature of the input data.",,0-8186-7898-4,10.1109/ICDAR.1997.619850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619850,,World Wide Web;Clustering algorithms;Web sites;Shape;Data mining;Image color analysis;Face detection;Laboratories;Detection algorithms;Algorithm design and analysis,document image processing;feature extraction;image colour analysis;Internet,text extraction;WWW images;text location;World Wide Web;text detection algorithm;color clustering;connected component analysis;color space quantization;input image;color classes;parameter-free clustering procedure;text-like connected components;shapes;post-processing procedure;text-like component alignment;text lines,,23,,11,,6-Aug-02,,,IEEE,IEEE Conferences
Clustering techniques for rule extraction from unstructured text fragments,從非結構化文本片段中提取規則的聚類技術,A. Clark; D. Filev,"Adv. Manuf. Technol. Dev., Ford Motor Co., Dearborn, MI, USA; Adv. Manuf. Technol. Dev., Ford Motor Co., Dearborn, MI, USA",NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society,5-Dec-05,2005,,,793,798,This paper focuses on techniques for clustering unstructured text fragments which are generated from a rule extraction agent. The text fragments represent paragraphs of text containing potential rules. The latent semantic indexing method is applied to map the unstructured text into a linear vector space. Similar text fragments are identified based on the similarity between their vector representations. The problem of clustering based on general similarity measures that are different than the conventional distance based measures is discussed. A new version of the mountain clustering method is developed to address the problem of identifying groups of similar vectors that correspond to documents with analogous content. Several clustering algorithms are compared in their ability to satisfactorily cluster these text fragments into sets of related concepts. An intelligent agent algorithm for extraction of rules from text documents is proposed and demonstrated.,,0-7803-9187-X,10.1109/NAFIPS.2005.1548641,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1548641,,Vectors;Clustering algorithms;Knowledge based systems;Manufacturing processes;Indexing;Rain;Technology planning;Clustering methods;Intelligent agent;Information retrieval,text analysis;pattern clustering;software agents;programming language semantics,rule extraction;unstructured text fragment clustering;latent semantic indexing;linear vector space;mountain clustering;intelligent agent;text document,,7,,13,,5-Dec-05,,,IEEE,IEEE Conferences
Embracing Information Explosion without Choking: Clustering and Labeling in Microblogging,擁抱信息爆炸而不會窒息：微博中的聚類和標籤,X. Hu; L. Tang; H. Liu,"Department of Computer Science and Engineering, Texas A&M University, College Station, TX; Clari Inc., Mountain View, CA; Computer Science and Engineering, Arizona State University, Tempe, AZ",IEEE Transactions on Big Data,20-May-17,2015,1,1,35,46,"The explosive popularity of microblogging services produce a large volume of microblogging messages. It presents great difficulties for a user to quickly gauge his/her followees' opinions when the user interface is overwhelmed by a large number of messages. Useful information is buried in disorganized, incomplete, and unstructured text messages. We propose to organize the large amount of messages into clusters with meaningful cluster labels, thus provide an overview of the content to fulfill users' information needs. Clustering and labeling of microblogging messages are challenging because that the length of the messages are much shorter than conventional text documents. They usually cannot provide sufficient term co-occurrence information for capturing their semantic associations. As a result, traditional text representation models tend to yield unsatisfactory performance. In this paper, we present a text representation framework by harnessing the power of semantic knowledge bases, i.e., Wikipedia and Wordnet. The originally uncorrelated texts are connected with the semantic representation, thus it enhances the performance of short text clustering and labeling. The experimental results on Twitter and Facebook datasets demonstrate the superior performance of our framework in handling noisy and short microblogging messages.",2332-7790,,10.1109/TBDATA.2015.2451635,National Science Foundation (NSF); Office of Naval Research (ONR); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153539,Microblogging;Clustering;Labeling;Semantic Knowledge;Microblogging;clustering;labeling;semantic knowledge,Semantics;Internet;Electronic publishing;Syntactics;Labeling;Encyclopedias,information needs;knowledge representation;pattern clustering;social networking (online);text analysis,information explosion;cluster labels;microblogging message labeling;microblogging message clustering;text documents;semantic associations;text representation models;semantic knowledge bases;Wikipedia;Wordnet;short text clustering;short text labeling;Facebook datasets;Twitter datasets;user interface;user information needs,,6,,48,,9-Jul-15,,,IEEE,IEEE Journals
Document image defect models and their uses,文檔圖像缺陷模型及其用途,H. S. Baird,"AT&T Bell Lab., Murray Hill, NJ, USA",Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93),6-Aug-02,1993,,,62,67,"The accuracy of today's document recognition algorithms falls abruptly when image quality degrades even slightly. In an effort to surmount this barrier, researchers have in recent years intensified their study of explicit, quantitative, parameterized models of the image defects that occur during printing and scanning. The author reviews the recent literature and discusses the form these models might take. A preview of a large public-domain database of character images, labeled with ground-truth including all defect model parameters, is given. The use of massive pseudo-randomly generated training sets for the construction of high-performance decision trees for preclassification is described. In a more theoretical vein, the author reports preliminary results in the estimation of the intrinsic error of precisely-specified text recognition problems. Finally, the author calls attention to some open problems.<>",,0-8186-4960-7,10.1109/ICDAR.1993.395781,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395781,,Image quality;Degradation;Printing;Image recognition;Hardware;NIST;Clustering algorithms;Physics;Ink;Pixel,document image processing;optical character recognition;visual databases;model-based reasoning,document image defect models;image quality degradation;ground truth labelling;intrinsic error estimation;accuracy;document recognition algorithms;explicit;quantitative;parameterized models;printing;scanning;public-domain database;character images;defect model parameters;pseudo-randomly generated training sets;high-performance decision trees;preclassification;precisely-specified text recognition problems,,38,,18,,6-Aug-02,,,IEEE,IEEE Conferences
Semantic Frame-Based Document Representation for Comparable Corpora,可比語料庫的基於語義框架的文檔表示,H. Kim; X. Ren; Y. Sun; C. Wang; J. Han,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA",2013 IEEE 13th International Conference on Data Mining,3-Feb-14,2013,,,350,359,"Document representation is a fundamental problem for text mining. Many efforts have been done to generate concise yet semantic representation, such as bag-of-words, phrase, sentence and topic-level descriptions. Nevertheless, most existing techniques counter difficulties in handling monolingual comparable corpus, which is a collection of monolingual documents conveying the same topic. In this paper, we propose the use of frame, a high-level semantic unit, and construct frame-based representations to semantically describe documents by bags of frames, using an information network approach. One major challenge in this representation is that semantically similar frames may be of different forms. For example, ""radiation leaked"" in one news article can appear as ""the level of radiation increased"" in another article. To tackle the problem, a text-based information network is constructed among frames and words, and a link-based similarity measure called SynRank is proposed to calculate similarity between frames. As a result, different variations of the semantically similar frames are merged into a single descriptive frame using clustering, and a document can then be represented as a bag of representative frames. It turns out that frame-based document representation not only is more interpretable, but also can facilitate other text analysis tasks such as event tracking effectively. We conduct both qualitative and quantitative experiments on three comparable news corpora, to study the effectiveness of frame-based document representation and the similarity measure SynRank, respectively, and demonstrate that the superior performance of frame-based document representation on different real-world applications.",2374-8486,978-0-7695-5108-1,10.1109/ICDM.2013.99,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729519,Document Representation;Clustering;Graph Similarity,Semantics;Context;Tsunami;Equations;Earthquakes;Data mining;Labeling,data mining;data structures;pattern clustering;text analysis,semantic frame-based document representation;text mining;comparable corpora;bag-of-words;topic-level descriptions;monolingual comparable corpus handling;high-level semantic unit;text-based information network approach;link-based similarity measure;SynRank;single descriptive frame;pattern clustering;text analysis tasks;event tracking,,3,,34,,3-Feb-14,,,IEEE,IEEE Conferences
Distributed noun attribute based on its first appearance for text document clustering,基於首次出現的分佈式名詞屬性，用於文本文檔聚類,S. Vijayalakshmi; D. Manimegalai,"Department of Computer Application, NMSSVN College, Madurai; Department of Information Technology, National Engineering College, Kovilpatty",2014 IEEE International Conference on Computational Intelligence and Computing Research,7-Sep-15,2014,,,1,5,"Selection of attributes plays a vital role to improve the quality of clustering. We present a comparative study on three attribute selection techniques and it reveals unattempt combinations, and provides guidelines in selecting attributes. It is occasionally studied in unsupervised learning; however it has been extensively explored in supervised learning. The suggested framework is primarily concerned with the problem of determining and selecting key distributional noun attributes, which are nominated by ranking the attributes according to the importance measure scores from the original noun attributes without class information. Experimental results on Reuter, 20 Newsgroup, WebKB and SCJC (Specific Crime Judgment Corpus) datasets indicate that algorithm with different scores in the context are able to identify the important attributes.",,978-1-4799-3975-6,10.1109/ICCIC.2014.7238544,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238544,Distributed Noun Attributes;Flocking Algorithm;HCL-K MeanAlgorithm;FirstAppearance,DNA;Clustering algorithms;Entropy;Training;Algorithm design and analysis;Arrays;Conferences,pattern clustering;text analysis;unsupervised learning,distributed noun attribute;text document clustering;clustering quality improvement;attribute selection techniques;unsupervised learning;supervised learning;attribute ranking;importance measure scores;Reuter dataset;20 Newsgroup dataset;WebKB dataset;SCJC dataset;Specific Crime Judgment Corpus dataset,,,,18,,7-Sep-15,,,IEEE,IEEE Conferences
Handwritten document image binarization: An adaptive K-means based approach,手寫文檔圖像二值化：一種基於自適應K均值的方法,P. Jana; S. Ghosh; S. K. Bera; R. Sarkar,"Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, India",2017 IEEE Calcutta Conference (CALCON),5-Feb-18,2017,,,226,230,"Degraded historical document images face many challenges in the process of optical character recognizing or word spotting, even after applying the traditional binarization techniques. In this paper, we propose a K-means based clustering technique for adaptive binarization of degraded document images. For validation of test results, we have used the recent dataset of Handwritten counterpart of Document Image Binarization Contest (H-DIBCO'16) comprising of highly degraded handwritten document images and computed detailed results of each image. In order to corroborate verification and validation, the experimental results are compared with three top winning ones in the contest and other prominent techniques in the literature. Experimental results reveal outstanding performance in the four evaluation measures compared with the top winners of the competition, claiming its effectiveness and validity conformance.",,978-1-5386-3744-9,10.1109/CALCON.2017.8280729,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280729,document image binarization;background estimation;global and adaptive thresholding;K-means;H-DIBCO,Lighting;Gray-scale;Histograms;Estimation;Image edge detection;Conferences;Optical character recognition software,document image processing;handwritten character recognition;image segmentation;optical character recognition,Handwritten document image binarization;degraded historical document images;optical character;word spotting;traditional binarization techniques;adaptive binarization;Handwritten counterpart;handwritten document images;document image binarization contest,,2,,21,,5-Feb-18,,,IEEE,IEEE Conferences
A comparative study of clustering methods using word embeddings,詞嵌入聚類方法的比較研究,N. Bastas; G. Kalpakis; T. Tsikrika; S. Vrochidis; I. Kompatsiaris,"Information Technologies Institute, Centre for Research and Technology Hellas,Thessaloniki,Greece; Information Technologies Institute, Centre for Research and Technology Hellas,Thessaloniki,Greece; Information Technologies Institute, Centre for Research and Technology Hellas,Thessaloniki,Greece; Information Technologies Institute, Centre for Research and Technology Hellas,Thessaloniki,Greece; Information Technologies Institute, Centre for Research and Technology Hellas,Thessaloniki,Greece",2019 European Intelligence and Security Informatics Conference (EISIC),5-Jun-20,2019,,,54,61,"Grouping large amounts of data is critical for various tasks, including the identification of content on a specific topic of interest (such as terrorism-related content) within a collection of material gathered from online sources. Various existing approaches typically extract relevant features using topic distributions and/or embedding methods, and subsequently apply clustering techniques in the derived representation space. In this work, we present a comparative study using Latent Dirichlet Allocation (LDA), Paragraph-Vector Distributed Bag-of-Words (PV-DBOW), and Paragraph-Vector Distributed Memory (PV-DM) models as representation methods, in conjunction with five traditional clustering algorithms, namely k-means, spherical k-means, possibilistic fuzzy c-means, agglomerative clustering and NMF, on two publicly available and one proprietary datasets. Fifteen combinations are formed which are assessed using external clustering validity measures, such as Adjusted Mutual Information (AMI) and Adjusted Rand Index (ARI) against available ground-truth. Our results indicate that using PV-DBOW leads in general to better clustering performance in all datasets.",,978-1-7281-6735-0,10.1109/EISIC49498.2019.9108898,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108898,Clustering algorithms;text mining;topic modeling;document embeddings;terrorism-related content,,fuzzy set theory;pattern clustering;terrorism;vectors,clustering methods;word embeddings;terrorism-related content;online sources;topic distributions;embedding methods;derived representation space;latent Dirichlet allocation;PV-DM;representation methods;traditional clustering algorithms;agglomerative clustering;publicly available datasets;one proprietary datasets;external clustering validity measures;PV-DBOW;clustering performance;paragraph-vector distributed bag-of-words;paragraph-vector distributed memory models;clustering algorithms;spherical k-means;possibilistic fuzzy c-means;adjusted mutual information;AMI;adjusted rand index;ARI,,,,42,,5-Jun-20,,,IEEE,IEEE Conferences
Clustering: Algorithms and Applications,聚類：算法和應用,H. Frigui,"CECS Dept., University of Louisville, USA. e-mail: h.frigui@louisville.edu","2008 First Workshops on Image Processing Theory, Tools and Applications",9-Jan-09,2008,,,1,11,"In this paper, we describe algorithms that perform fuzzy clustering and feature weighting simultaneously and in an unsupervised manner. These algorithms are conceptually and computationally simple, and learn a different set of feature weights for each identified cluster. The cluster dependent feature weights offer two advantages. First, they guide the clustering process to partition the data into more meaningful clusters. Second, they can be used in the subsequent steps of a learning system to improve its learning behavior. An extension of the algorithm to deal with an unknown number of clusters is also presented. The extension is based on competitive agglomeration, whereby the number of clusters is over-specified, and adjacent clusters are allowed to compete for data points in a manner that causes clusters which lose in the competition to gradually become depleted and vanish. We illustrate the performance of the proposed approach by using it to segment color images, categorize text document collections, and build a multi-modal thesaurus and use it to annotate image regions.",2154-512X,978-1-4244-3321-6,10.1109/IPTA.2008.4743793,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4743793,fuzzy clustering;Feature weighting;competitive agglomeration;image segmentation;content-based image retrieval;image annotation;multimedia data mining,Clustering algorithms;Partitioning algorithms;Image segmentation;Data mining;Shape measurement;Image processing;Learning systems;Color;Thesauri;Information retrieval,fuzzy set theory;image colour analysis;image segmentation;pattern clustering,fuzzy clustering;feature weighting;cluster dependent feature weights;data partition;learning system;competitive agglomeration;color image segmentation;categorize text document collections;multimodal thesaurus;annotate image regions,,1,,51,,9-Jan-09,,,IEEE,IEEE Conferences
Privacy Protection Based Retrieval on WBAN Big Data,基於隱私保護的WBAN大數據檢索,L. Yao; J. Gu; M. Tang,"Sch. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Sch. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China; Sch. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China",2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS),26-Jan-17,2016,,,876,882,"Information privacy for Wireless body area network (WBAN) includes the user's physiological parameters. When a user of WBAN system intends to query medical data, the security issue is involved. Ciphertext retrieval technology is an effective way to ensure this confidentiality. However, efficiency and accuracy for data retrieval are challenges under the consideration of the aggregation of medical data is so large that it has to be stored in a cloud server. This paper proposes the similarity search tree structure to enhance the hit rate of multi-keyword ranking search. We also propose dynamic interval clustering algorithm DIK-MEDOIDS under the environment of cloud storage, while documents clustering initialization, the differences between the maximum and minimum document vector are divided into k slots, and the size of each slot equals to hypersphere diameter. Then the document vector which is closest to the middle value of the range is set as the hypersphere center. The size of each slot depends on the number of total documents amount. Therefore the clustering process dynamically changes with the increase of amount of documents, and the initialization time complexity is o(l). This algorithm works well for Big Data environment of ciphertext retrieval scenarios. The experimental results prove that the time consumption is linear function the document amount being at a low level. It shows that DIK-MEDOIDS algorithm has larger ascension than traditional DK-MEDOIDS algorithm in initialization.",,978-1-5090-4297-5,10.1109/HPCC-SmartCity-DSS.2016.0126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828467,WBAN;Big Data;data retrieval;styling;styling hypersphere,Heuristic algorithms;Clustering algorithms;Cloud computing;Servers;Wireless communication;Body area networks;Indexes,Big Data;body area networks;cloud computing;computational complexity;cryptography;data protection;document handling;pattern clustering;query processing;storage management,wireless body area network;WBAN system;medical data querying;ciphertext retrieval;data confidentiality;medical data aggregation;medical data retrieval;cloud server;multikeyword ranking search;dynamic interval clustering;DIK-MEDOIDS;cloud storage;document clustering initialization;document vector;hypersphere diameter;initialization time complexity;WBAN Big Data;privacy protection based retrieval;linear function,,,,18,,26-Jan-17,,,IEEE,IEEE Conferences
Weighted hybrid features to resolve mixed entities,加權混合特徵可解決混合實體,I. Lee; B. On,"Troy University, Troy, AL. USA; Advanced Institute of Convergence Technology, Suwon, Korea",2011 Sixth International Conference on Digital Information Management,1-Dec-11,2011,,,67,72,"With the popularity of Internet, tremendous amount of unstructured document information is available to access. Extracting related information from huge unstructured documents is a very difficult task. Especially, confusion can occur by synonym and polysemy, miss spelling, abbreviation, etc. To resolve those confusion is known as an Entity Resolution problem. Clustering algorithms have been popularly used to resolve mixed entities. However, most researches focus on one feature of an entity such as co-author lists or paper titles. In this paper, we are proposing a weighted hybrid feature scheme to distinguish mixed entities among unstructured documents. Experimental results show that weighted hybrid approach improves the accuracy and efficiency.",,978-1-4577-1539-6,10.1109/ICDIM.2011.6093351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093351,mixed entity resolution;data mining;web document clustering;feature selections,Educational institutions;Clustering algorithms;Terminology;Vectors;Cadaver;Accuracy;Matrix converters,document handling;Internet;pattern clustering,weighted hybrid features;mixed entities;Internet;unstructured document information;synonym;polysemy;miss spelling;abbreviation;entity resolution problem;clustering algorithms,,,,27,,1-Dec-11,,,IEEE,IEEE Conferences
Collaborative Information Retrieval Model Based on Fuzzy Clustering,基於模糊聚類的協同信息檢索模型,F. Naouar; L. Hlaoua; M. N. Omri,"MARS Res. Lab. Univ. of Sousse, Tunisia; MARS Res. Lab. Univ. of Sousse, Tunisia; MARS Res. Lab. Univ. of Sousse, Tunisia",2017 International Conference on High Performance Computing & Simulation (HPCS),14-Sep-17,2017,,,495,502,"The collaborative approach has shown interest in several fields of application, particularly in information retrieval to satisfy a need for shared information. Despite this collaboration, the search for relevant information is always a tedious task as long as the mass of information continues to increase, part of which is a source, while other parties represent comments on these sources. It is obvious that nowadays we witness an explosion of multimedia documents so that multimedia information retrieval techniques remain insufficient to satisfy the needs of the user despite the collaborative framework: multimedia-type documents cannot be rich in information and more specifically the video documents. We consider, therefore, annotations as a new source of information. In addition to their relevance, we notice that annotations express generally brief ideas using some words that they cannot be comprehensible independently of his context. To use them, a classification is considered necessary. The emergence of new annotations should be considered and therefore the classification should be extended. A centroid is determined in a virtual way to represent each annotation class. From where, the interest to use the fuzzy classification to know which elements can belong to several clusters. It consists, in a calculation of the center of gravity of all the existing classes. This is the reason why; we proposed a fuzzy clustering-based annotation. In the experiments, we tried to consider a relevance feedback system based on confidence network considering new relevant classified annotations as a source of information. To validate this model, we have carried out a set of experiments and we have obtained encouraging results.",,978-1-5386-3250-5,10.1109/HPCS.2017.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8035119,Fuzzy Clustering;Relevant Annotation;Collaborative Retrieval;Relevance feedback,Collaboration;Clustering algorithms;Semantics;Information retrieval;Multimedia communication;Probabilistic logic;Partitioning algorithms,fuzzy set theory;pattern clustering;relevance feedback,annotation class;fuzzy classification;fuzzy clustering;relevance feedback system;collaborative information retrieval model;multimedia documents;multimedia information retrieval techniques;multimedia-type documents;confidence network,,,,34,,14-Sep-17,,,IEEE,IEEE Conferences
Agglomerative algorithm to discover semantics from unstructured big data,聚集算法從非結構化大數據中發現語義,I. Chiang,"College of Business Administration, Taipei Medical University, Taipei, Taiwan 110",2015 IEEE International Conference on Big Data (Big Data),28-Dec-15,2015,,,1556,1563,"The paper presents a graph model and an agglomerative algorithm for text document clustering. Given a set of documents, the associations among frequently co-occurring terms in any of the documents naturally form a graph, which can be decomposed into connected components at various levels. Each connected component represents a concept in the collection. These concepts can categorize documents into different semantic classes. The experiments on three different data sets from news, Web, and medical literatures have shown our algorithm is significantly better than traditional clustering algorithms, such as k-means, principal direction division partitioning, AutoClass and hierarchical clustering.",,978-1-4799-9926-2,10.1109/BigData.2015.7363920,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363920,agglomerative document categorization/clustering;association rules;hypergraph;hierarchical clustering,Itemsets;Association rules;Clustering algorithms;Semantics;Feature extraction;Partitioning algorithms;Big data,Big Data;graph theory;pattern clustering;text analysis,agglomerative algorithm;semantics discovery;unstructured Big Data;graph model;text document clustering;frequently co-occurring term associations,,1,,33,,28-Dec-15,,,IEEE,IEEE Conferences
Recognition-Based Approach of Numeral Extraction in Handwritten Chemistry Documents Using Contextual Knowledge,基於上下文知識的手寫化學文檔數字提取中基於識別的方法,N. Ghanmi; A. Bela簿d,"LORIA, Nancy, France; LORIA, Univ. de Lorraine, Nancy, France",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,251,256,"This paper presents a complete procedure that uses contextual and syntactic information to identify and recognize amount fields in the table regions of chemistry documents. The proposed method is composed of two main modules. Firstly, a structural analysis based on connected component (CC) dimensions and positions identifies some special symbols and clusters other CCs into three groups: fragment of characters, isolated characters or connected characters. Then, a specific processing is performed on each group of CCs. The fragment of characters are merged with the nearest character or string using geometric relationship based rules. The characters are sent to a recognition module to identify the numeral components. For the connected characters, the final decision on the string nature (numeric or non-numeric) is made based on a global score computed on the full string using the height regularity property and the recognition probabilities of its segmented fragments. Finally, a simple syntactic verification at table row level is conducted in order to correct eventual errors. The experimental tests are carried out on real-world chemistry documents provided by our industrial partner eNovalys. The obtained results show the effectiveness of the proposed system in extracting amount fields.",,978-1-5090-1792-8,10.1109/DAS.2016.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490126,numeral extraction;digit recognition;numeric string segmentation;structural features;syntactic verification,Syntactics;Feature extraction;Chemistry;Support vector machines;Reservoirs;Shape,document image processing;feature extraction;image segmentation,recognition-based approach;numeral extraction;handwritten chemistry documents;contextual knowledge;chemistry documents;connected component dimensions;CC;isolated characters;connected characters;character fragments;geometric relationship based rules;recognition module;global score;height regularity property;recognition probabilities;segmented fragments;syntactic verification;table row level;eventual errors;real-world chemistry documents;eNovalys,,1,,13,,13-Jun-16,,,IEEE,IEEE Conferences
Enhancing Paper Documents with Direct Access to Multimedia for More Intelligent Support of Reading,通過直接訪問多媒體增強紙質文檔，以更智能地支持閱讀,K. Kanev; T. Orr,"University of Aizu, kanev@u-aizu.ac.jp; University of Aizu, t-orr@u-aizu.ac.jp",2006 IEEE International Professional Communication Conference,26-Feb-07,2006,,,84,91,"This paper presents a brief overview of recent technologies that connect multimedia to paper and then identifies features and functionalities desirable for the support of reading. An innovative cluster pattern interface (CLUSPIreg) is described, along with its potential for addressing the specific needs of readers of paper documents. We argue that by identifying and properly addressing the specific information needs of readers of paper documents based on reader requirements found in research literature, advanced levels of support can be accomplished with innovative technological achievements, which can provide a more intelligent way to improve a reader's engagement with paper text",2158-1002,0-7803-9777-0,10.1109/IPCC.2006.320393,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4114186,multimedia enhanced documents;paper text;reading comprehension;links;intelligent support;CLUSPI簧 technology,Paper technology;Couplings;Joining processes;Multimedia systems;Decoding;Acoustical engineering;Proposals;Employment;Internet;Cameras,document handling;multimedia computing;pattern clustering,paper document enhancement;direct multimedia access;intelligent reading support;cluster pattern interface;reader requirements,,2,,30,,26-Feb-07,,,IEEE,IEEE Conferences
Localized forgery detection in hyperspectral document images,高光譜文檔圖像中的局部偽造檢測,Z. Luo; F. Shafait; A. Mian,"University of Science and Technology of China, Hefei, China; National University of Sciences and Technology (NUST), Islamabad, Pakistan; The University of Western Australia, Perth, Australia",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,496,500,"Hyperspectral imaging is emerging as a promising technology to discover patterns that are otherwise hard to identify with regular cameras. Recent research has shown the potential of hyperspectral image analysis to automatically distinguish visually similar inks. However, a major limitation of prior work is that automatic distinction only works when the number of inks to be distinguished is known a priori and their relative proportions in the inspected image are roughly equal. This research work aims at addressing these two problems. We show how anomaly detection combined with unsupervised clustering can be used to handle cases where the proportions of pixels belonging to the two inks are highly unbalanced. We have performed experiments on the publicly available UWA Hyperspectral Documents dataset. Our results show that INFLO anomaly detection algorithm is able to best distinguish inks for highly unbalanced ink proportions.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333811,,Support vector machines;Subspace constraints,digital forensics;document image processing;hyperspectral imaging,INFLO anomaly detection algorithm;UWA hyperspectral document dataset;unsupervised clustering;hyperspectral image analysis;cameras;hyperspectral document image;localized forgery detection,,13,,25,,23-Nov-15,,,IEEE,IEEE Conferences
SUMMARY: efficiently summarizing transactions for clustering,簡介：有效地匯總事務以進行集群,Jianyong Wang; G. Karypis,"Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA",Fourth IEEE International Conference on Data Mining (ICDM'04),4-Apr-05,2004,,,241,248,"Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to the transaction (or document) classification and clustering. However, most of the frequent-itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring some properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results have shown that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm, it is very effective in clustering the categorical data and summarizing the dense transaction databases.",,0-7695-2142-8,10.1109/ICDM.2004.10105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410290,,Itemsets;Data mining;Transaction databases;Clustering algorithms;Algorithm design and analysis;Computer science;Association rules;Application software;Design methodology;High performance computing,data mining;pattern clustering;transaction processing,SUMMARY;summarizing transactions;frequent itemset mining;association rule mining;transaction classification;transaction clustering;document classification;document clustering;clustering algorithm;transaction database;categorical data;search space pruning,,5,,35,,4-Apr-05,,,IEEE,IEEE Conferences
A Novel Ensemble Representation Learning method for Document Classification,一種新的文檔分類集成表示學習方法,P. Sharmila; S. Venkatesh; C. Deisy; S. Parthasarathy; S. Parasuraman,"Thiagarajar College of Engineering,Department of Information Technology,Madurai,India; Thiagarajar College of Engineering,Department of Information Technology,Madurai,India; Thiagarajar College of Engineering,Department of Information Technology,Madurai,India; Thiagarajar College of Engineering,Department of Computer Applications,Madurai,India; Monash University,Department of Mechanical Engineering,Malaysia",2018 IEEE 4th International Symposium in Robotics and Manufacturing Automation (ROMA),10-Feb-20,2018,,,1,4,"Representation learning is the central role for all natural language processing task. Bag of Words method lacks in semantics and word order, hence word embedding model Word2Vec is used to capture the word semantics. But for morphological rich language, the vector representation would be noisy due to polysemy. To address these problems, Bag of Concepts is introduced to capture association between the words in the documents and forms concept cluster. Sometimes, Bag of Concepts representations may ignore the syntax for large amount of data. Hence a novel ensemble representation learning method for document classification is proposed by combining the word2vec and Bag of Concepts model to tackle the above mentioned problems. Extensive results on the Reuter datasets, show that the proposed model for document classification outperforms the baseline model in terms of F1 score.",,978-1-7281-0374-7,10.1109/ROMA46407.2018.8986705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986705,Representation learning;document classification;word2vec;bag-of-concepts,,learning (artificial intelligence);natural language processing;pattern classification;vectors;word processing,Word2Vec;word embedding model;word semantics;morphological rich language;vector representation;document classification;natural language processing task;bag of words method;ensemble representation learning method;concept cluster,,,,22,,10-Feb-20,,,IEEE,IEEE Conferences
I-Cluster: the execution sandbox,I-Cluster：執行沙箱,B. Richard,"Hewlett-Packard Labs., Grenoble, France",Proceedings. IEEE International Conference on Cluster Computing,6-Jan-03,2002,,,93,98,"I-Cluster is an HP Laboratories Grenoble initiative in collaboration with the ID-IMAG laboratory of INRIA Rhone-Alpes. The aim of this research programme is to develop a framework of tools that transparently take advantage of unused network resources and federate them to crystallize into specific virtual functions such as supercomputing. To be more precise, I-Cluster enables automatic real-time analysis of the availability, configuration and workload of machines on an intranet. When instantiation of a supercomputing function is carried out by a user, I-Cluster determines the most appropriate set of machines for carrying out this function, allocates the machines into a virtual cluster and then starts execution of the function. To obtain this result, I-Cluster possesses a ""sandbox"" on each machine on the intranet, which is transparent to the user, and that enables use of the computing resources of these machines during their idle periods while securely protecting user data and jobs. This document presents the sandbox developed for I-Cluster and its features.",,0-7695-1745-5,10.1109/CLUSTR.2002.1137733,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137733,,Distributed computing;Personal communication networks;Operating systems;Laboratories;Computer architecture;Protection;Supercomputers;Collaboration;Crystallization;Availability,workstation clusters;intranets;network operating systems,I-Cluster;unused network resources;virtual functions;supercomputing;automatic real-time availability analysis;automatic real-time configuration analysis;automatic real-time workload analysis;intranet;virtual cluster;idle periods;execution sandbox,,1,,31,,6-Jan-03,,,IEEE,IEEE Conferences
Clustering Images by Unmasking ??A New Baseline,通過揭露新基線對圖像進行聚類,M. Georgescu; R. T. Ionescu,"Faculty of Mathematics and Computer Science, University of Bucharest, Romania; Faculty of Mathematics and Computer Science, University of Bucharest, Romania",2019 IEEE International Conference on Image Processing (ICIP),26-Aug-19,2019,,,1580,1584,"We propose a novel agglomerative clustering method based on unmasking, a technique that was previously used for authorship verification of text documents and for abnormal event detection in videos. In order to join two clusters, we alternate between (i) training a binary classifier to distinguish between the samples from one cluster and the samples from the other cluster, and (ii) removing at each step the most discriminant features. The faster-decreasing accuracy rates of the intermediately-obtained classifiers indicate that the two clusters should be joined. To the best of our knowledge, this is the first work to apply unmasking in order to cluster images. We compare our method with k-means as well as a recent state-of-the-art clustering method. The empirical results indicate that our approach is able to improve performance for various (deep and shallow) feature representations and different tasks, such as handwritten digit recognition, texture classification and fine-grained object recognition.",2381-8549,978-1-5386-6249-6,10.1109/ICIP.2019.8803097,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803097,Clustering;unmasking;unsupervised learning;agglomerative clustering,Training;Clustering algorithms;Clustering methods;Testing;Support vector machines;Task analysis;Visualization,image classification;image representation;image texture,authorship verification;text documents;abnormal event detection;binary classifier;faster-decreasing accuracy rates;feature representations;agglomerative clustering method;handwritten digit recognition;texture classification;fine-grained object recognition,,,,32,,26-Aug-19,,,IEEE,IEEE Conferences
Font clustering and classification in document images,文檔圖像中的字體聚類和分類,S. ?zt羹rk; B. Sankur; A. T. Abak,"Bo?azi癟i University, Department of Electrical-Electronic Engineering, Bebek, Istanbul, Turkey; Bo?azi癟i University, Department of Electrical-Electronic Engineering, Bebek, Istanbul, Turkey; Marmara Research Center, Information Technologies Research Institute, Gebze, Kocaeli, Turkey",2000 10th European Signal Processing Conference,2-Apr-15,2000,,,1,4,Clustering and identification of fonts in document images impacts on the performance of optical character recognition (OCR). Therefore font features and their clustering tendency are investigated. Font clustering is implemented both from shape similarity and from OCR performance points of view. A font recognition algorithm is developed to identify the font group with which a given text was created.,,978-952-1504-43-3,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7075268,,Optical character recognition software;Clustering algorithms;Vectors;Text recognition;Discrete cosine transforms;Character recognition;Feature extraction,,,,,,,,2-Apr-15,,,IEEE,IEEE Conferences
Using OSCAR to Win the Cluster Challenge,使用OSCAR贏得集群挑戰,P. Greidanus; G. W. Klok,"Center of Excellence in Integrated Nanotools, Univ. of Alberta, Edmonton, AB; NA",2008 22nd International Symposium on High Performance Computing Systems and Applications,2-Jul-08,2008,,,47,51,"This paper discusses using OSCAR (open source cluster administration resource) to compete and win, in the first ever cluster challenge competition at supercomputing 2007. The cluster challenge invited teams of undergraduate students to assemble a cluster and compete for 44 hours on the conference floor. The teams competed in two main areas; (i) achieving the best results on the high performance computing challenge benchmark and (ii) who could complete the most of a selection of data sets for several common scientific applications. In the this document we explain why Team Alberta chose OSCAR, how we feel it contributed to our success and our experiences using it.",2378-2099,978-0-7695-3250-9,10.1109/HPCS.2008.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4556072,OSCAR;Cluster;HPC;System Administration,High performance computing;Packaging;Open source software;Application software;Software packages;Assembly;Hardware;Chemistry;Oceans;Distributed computing,open systems;public domain software;workstation clusters,cluster challenge;open source cluster administration resource;high performance computing,,,,12,,2-Jul-08,,,IEEE,IEEE Conferences
Character segmentation of color images from digital camera,數碼相機彩色圖像的字符分割,Kongqiao Wang; J. A. Kangas; Wenwen Li,"Human Interface Technol., Nokia (China) Res. Center, Beijing, China; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,210,214,"Because of the lack of prior knowledge about the color of characters and the disturbance caused by lighting conditions and noise, character segmentation from scene images is a very difficult issue. In this paper, a novel character segmentation method for color images from a digital camera is presented, combining edge detection, a watershed transform and clustering. Since the characters extracted from color images are output in a binary format, they can be input directly to an OCR system for recognition. Experimental results have proven the method's effectiveness.",,0-7695-1263-1,10.1109/ICDAR.2001.953785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953785,,Image segmentation;Color;Digital cameras;Colored noise;Image edge detection;Layout;Anisotropic magnetoresistance;Optical character recognition software;Character recognition;Optical noise,image colour analysis;optical character recognition;image segmentation;document image processing;lighting;edge detection;transforms;pattern clustering,character segmentation;color images;digital camera;character color;lighting;noise;scene images;edge detection;watershed transform;color clustering;character extraction;binary format;OCR system;optical character recognition,,9,,10,,7-Aug-02,,,IEEE,IEEE Conferences
Text document classification using swarm intelligence,使用群體智能對文本文檔進行分類,A. L. Vizine; L. N. de Castro; R. R. Gudwin,"Santos Catholic Univ., Brazil; Santos Catholic Univ., Brazil; NA","International Conference on Integration of Knowledge Intensive Multi-Agent Systems, 2005.",9-May-05,2005,,,134,139,"This paper presents an algorithm for the automatic grouping of PDF documents, and with potential application for Web document classification. The algorithm developed is based on an ant-clustering algorithm, which was inspired by the behavior of some ant species in the organization their nests. To apply the ant-clustering algorithm for text document classification, two modifications had to be introduced in the standard algorithm: 1) the use of a metric to evaluate the similarity degree of text data, instead of numeric data; and 2) the proposal of a cooling schedule for a user-defined parameter so as to improve the convergence properties of the algorithm. To illustrate the behavior of the modified algorithm, it was applied to sets of real-world documents taken from the IEEE WCCI -1998 CD.",,0-7803-9013-X,10.1109/KIMAS.2005.1427067,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1427067,,Particle swarm optimization;Information retrieval;Information resources;Scheduling algorithm;Software libraries;Proposals;Clustering algorithms;Classification algorithms;Cooling;Convergence of numerical methods,classification;text analysis;pattern clustering;evolutionary computation,text document classification;swarm intelligence;PDF document;Web document classification;ant-clustering algorithm;organizational behavior;text data similarity,,8,,22,,9-May-05,,,IEEE,IEEE Conferences
Pyramidal Stochastic Graphlet Embedding for Document Pattern Classification,金字塔形隨機小圖嵌入用於文檔模式分類,A. Dutta; P. Riba; J. Llad籀s; A. Forn矇s,"Comput. Sci. Dept., Univ. Aut`onoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Aut`onoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Aut`onoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Aut`onoma de Barcelona, Barcelona, Spain",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,33,38,"Document pattern classification methods using graphs have received a lot of attention because of its robust representation paradigm and rich theoretical background. However, the way of preserving and the process for delineating documents with graphs introduce noise in the rendition of underlying data, which creates instability in the graph representation. To deal with such unreliability in representation, in this paper, we propose Pyramidal Stochastic Graphlet Embedding (PSGE). Given a graph representing a document pattern, our method first computes a graph pyramid by successively reducing the base graph. Once the graph pyramid is computed, we apply Stochastic Graphlet Embedding (SGE) for each level of the pyramid and combine their embedded representation to obtain a global delineation of the original graph. The consideration of pyramid of graphs rather than just a base graph extends the representational power of the graph embedding, which reduces the instability caused due to noise and distortion. When plugged with support vector machine, our proposed PSGE has outperformed the state-of-the-art results in recognition of handwritten words as well as graphical symbols.",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8269946,graph embedding;hierarchical graph representation;graph clustering;stochastic graphlet embedding;graph classification,Clustering algorithms;Task analysis;Distortion;Image edge detection;Stochastic processes;Visualization;Pattern recognition,graph theory;graphs;pattern classification;support vector machines,graphs;robust representation paradigm;documents;graph representation;Pyramidal Stochastic Graphlet Embedding;graph pyramid;base graph;embedded representation;original graph;representational power;graph embedding;document pattern classification methods,,2,,29,,29-Jan-18,,,IEEE,IEEE Conferences
Comparison of semantic and single term similarity measures for clustering turkish documents,土耳其語文檔聚類的語義和單項相似度度量的比較,B. Yucesoy; S. G. Oguducu,"Istanbul Tech. Univ., Istanbul; Istanbul Tech. Univ., Istanbul",Sixth International Conference on Machine Learning and Applications (ICMLA 2007),25-Feb-08,2007,,,393,398,"With the rapid growth of the World Wide Web (www), it becomes a critical issue to design and organize the vast amounts of on-line documents on the web according to their topic. Even for the search engines it is very important to group similar documents in order to improve their performance when a query is submitted to the system. Clusterng is useful for taxonomy design and similarity search of documents on such a domain. Similarity is fundamental to many clustering applications on hypertext. In this paper, we will study how measures of similarity are used to cluster a collection of documents on a web site. Most of the document clustering techniques rely on single term analysis of text, such as vector space model. To better group of related documents we propose a new semantic similarity measure. We compare our measure with Wu-Palmer similarity and cosine similarity. Experimental results show that cosine similarity perform better than the semantic similarities. We demonstrate our results on Turkish documents. This is a first study that considers the semantic similarities between Turkish documents.",,978-0-7695-3069-7,10.1109/ICMLA.2007.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4457262,,Taxonomy;Design engineering;Web sites;World Wide Web;Search engines;Functional analysis;Frequency;Thesauri;Machine learning;Application software,natural language processing;pattern clustering;text analysis;Web sites,Turkish document clustering;World Wide Web;search engine;single term semantic similarity measure;text analysis;vector space model;Web site,,4,,20,,25-Feb-08,,,IEEE,IEEE Conferences
Document Content Extraction Using Automatically Discovered Features,使用自動發現的功能提取文檔內容,S. Wang; H. Baird; C. An,"Comput. Sci. & Eng. Dept., Lehigh Univ., Bethlehem, PA, USA; Comput. Sci. & Eng. Dept., Lehigh Univ., Bethlehem, PA, USA; Comput. Sci. & Eng. Dept., Lehigh Univ., Bethlehem, PA, USA",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,1076,1080,"We report an automatic feature discovery method that achieves results comparable to a manually chosen, larger feature set on a document image content extraction problem: the location and segmentation of regions containing handwriting and machine-printed text in documents images. This approach is a greedy forward selection algorithm that iteratively constructs one linear feature at a time. The algorithm finds error clusters in the current feature space, then projects one tight cluster into the null space of the feature mapping, where a new feature that helps to classify these errors can be discovered. We conducted experiments on 87 diverse test images. Four manually chosen linear features with an error rate of 16.2% were given to the algorithm; the algorithm then found an additional ten features; the composite 14 features achieve an error rate of 13.8%. This outperforms a feature set of size 14 chosen by principal component analysis (PCA) with an error rate of 15.4%. It also nearly matches the error rate of 13.6% achieved by twice as many manually chosen features. Thus our algorithm appears to compete with both the widely used PCA method and tedious and expensive trial-and-error manual exploration.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277493,,Error analysis;Principal component analysis;Clustering algorithms;Null space;Testing;Text analysis;Iterative algorithms;Filters;Image analysis;Handwriting recognition,document image processing;feature extraction;greedy algorithms;handwritten character recognition;image classification;image segmentation;iterative methods;principal component analysis;text analysis,automatic feature discovery;document image content extraction;region location;region segmentation;handwriting-machine-printed text;greedy forward selection algorithm;iterative algorithm;feature mapping;image classification;principal component analysis;PCA,,5,,19,,2-Oct-09,,,IEEE,IEEE Conferences
An Optimized Distributed Clustering Algorithm in Advanced 3-Layer Peer-to-Peer Network,高級三層對等網絡中的一種優化的分佈式聚類算法,Z. Feng; Z. Liu,"Sch. of Comput. Sci. & Technol., Xidian Univ., Xi'an; Sch. of Comput. Sci. & Technol., Xidian Univ., Xi'an",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,2,,405,409,"In this paper, an advanced 3-layer peer-to-peer architecture and algorithm is introduced. The core of it is activated-peer of middle layer that is oriented on special P2P applications, such as large-scale network clustering and cooperation work. Our architecture is based on a multi-layer network of peer neighborhoods. Many activated-peers which act as management center of its lower neighborhoods are grouped to form upper level cluster. The level of activated-peer is dynamic which depends on neighborhood threshold set by root peer. Using this model, the clustering problem can be partitioned in an iterative way like solving each part individually in bottom peer which gets primitive data, then combine clustering up to upper activated-peer and repeat this method to the root peer of network. The matching algorithm is applied to a distributed document clustering problem of our architecture. It acts satisfied efficiency with comparable clustering quality to the centralized approach in the experiments.",,978-0-7695-3305-6,10.1109/FSKD.2008.279,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666148,,Clustering algorithms;Peer to peer computing;Data mining;Distributed decision making;Large-scale systems;Computer architecture;Traffic control;Telecommunication traffic;Fuzzy systems;Computer science,data mining;iterative methods;peer-to-peer computing,optimized distributed clustering algorithm;3-layer peer-to-peer network;large-scale network clustering;cooperation work;iterative method;matching algorithm;distributed document clustering problem,,,,4,,5-Nov-08,,,IEEE,IEEE Conferences
Combining a double clustering approach with sentence simplification to produce highly informative multi-document summaries,將雙聚類方法與句子簡化相結合，以生成內容豐富的多文檔摘要,S. B. Silveira; A. Branco,"University of Lisbon, Edif穩cio C6, Departamento de Inform獺tica, Faculdade de Ci礙ncias, Universidade de Lisboa, Campo Grande, 1749-016, Portugal; University of Lisbon, Edif穩cio C6, Departamento de Inform獺tica, Faculdade de Ci礙ncias, Universidade de Lisboa, Campo Grande, 1749-016, Portugal",2012 IEEE 13th International Conference on Information Reuse & Integration (IRI),17-Sep-12,2012,,,482,489,"This paper presents a method for extractive multi-document summarization that explores a two-phase clustering approach that, combined with a sentence simplification procedure, aims to generate more useful summaries. First, sentences are clustered by similarity, and one sentence per cluster is selected, to reduce redundancy. Then, in order to group them according to topics, those sentences are clustered considering the collection of keywords. Finally, the summarization process includes a sentence simplification step, which aims not only to create simpler and more incisive sentences, but also to make room for the inclusion of further relevant content in the summary. Evaluation reveals that the approach pursued produces highly informative summaries, containing relevant data and no repeated information.",,978-1-4673-2284-3,10.1109/IRI.2012.6303047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303047,,Redundancy;Clustering algorithms;Pragmatics;Measurement;Abstracts;Organizations;Humans,pattern clustering;text analysis,double clustering approach;sentence simplification;multidocument summarization;two-phase clustering approach,,3,,22,,17-Sep-12,,,IEEE,IEEE Conferences
Automatic Tagging Web Services Using Machine Learning Techniques,使用機器學習技術自動標記Web服務,M. Lin; D. W. Cheung,"Dept. of Comput. Sci., Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., Univ. of Hong Kong, Hong Kong, China",2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),20-Oct-14,2014,2,,258,265,"Web services have become popular and increasingly important in e-business and e-commerce applications especially in large scale distributed systems. As a result, increasing number of web services has been developed. However, this abundance creates a vast collection of web services which makes the task of locating a suitable one more challenging and more difficult. Automatic clustering of web services groups together web services with similar functions. Clustering could greatly boost the power of web service search engines and generate tags to improve the search accuracy of tag-based service recommendation. In this paper, we propose a web service clustering technique based on Carrot search clustering and K-means to group similar services together to generate tags and we use naive bayes algorithm to classify web services. We also develop a tag-based service recommendation for WSDL documents. We demonstrate that the proposed clustering approach is effective for web service discovery.",,978-1-4799-4143-8,10.1109/WI-IAT.2014.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927633,Web Service;Clustering,Web services;Vectors;Clustering algorithms;Feature extraction;Tagging;Search engines;XML,Bayes methods;document handling;learning (artificial intelligence);pattern classification;pattern clustering;recommender systems;Web services,Web service discovery;WSDL documents;tag-based service recommendation;Web service classification;naive Bayes algorithm;similar services group;K-means clustering;Carrot search clustering;Web service clustering technique;machine learning techniques;automatic Web service tagging,,9,,19,,20-Oct-14,,,IEEE,IEEE Conferences
Semi-Supervised Soft K-Means Clustering of Life Insurance Questionnaire Responses,人壽保險問卷響應的半監督軟K均值聚類,R. Biddle; S. Liu; G. Xu,"University of Technology Sydney., Advanced Analytics Institute, Sydney, Australia; University of Technology Sydney., Advanced Analytics Institute, Sydney, Australia; University of Technology Sydney., Advanced Analytics Institute, Sydney, Australia","2018 5th International Conference on Behavioral, Economic, and Socio-Cultural Computing (BESC)",25-Apr-19,2018,,,30,31,"The life insurance questionnaire is a large document containing responses in a mixture of structured and unstructured data. The unstructured data poses issues for the user, in the form of extra input effort, and the insurance company, in the form of interpretation and analysis. In this work, we aim to address these problems by proposing a semi-supervised framework for clustering responses into categories using vector space embedding of responses and soft k-means clustering. Our experiments show that our method achieves adequate results. The resulting category clusters from our method can be used for analysis and to replace free text input questions with structured questions in the questionnaire.",,978-1-7281-0207-8,10.1109/BESC.2018.8697227,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697227,,Insurance;Natural language processing;Clustering algorithms;Machine learning;Australia;Task analysis;Computational modeling,graph theory;insurance;learning (artificial intelligence);pattern clustering;text analysis,insurance company;semisupervised framework;vector space embedding;resulting category clusters;free text input questions;structured questions;semisupervised soft k-means clustering;life insurance questionnaire responses;unstructured data;extra input effort,,1,,5,,25-Apr-19,,,IEEE,IEEE Conferences
Hierarchical Text Clustering and Categorisation Using a Semi-Supervised Framework,使用半監督框架的分層文本聚類和分類,M. Mahyoub; J. Hind; D. Woods; C. Wong; A. Hussain; D. Aljumeily,Liverpool John Moores University; Liverpool John Moores University; Liverpool John Moores University; Liverpool John Moores University; Liverpool John Moores University; Liverpool John Moores University,2019 12th International Conference on Developments in eSystems Engineering (DeSE),23-Apr-20,2019,,,153,159,"Several steps need to be considered when conducting a data mining study on a text dataset, this will also involve the use of multiple techniques before achieving any results or extracting any hidden knowledge. Hence, the complexity of working with text data. One of the main steps would be the pre-processing of the text dataset, and this might include multiple techniques such as tokenisation, word stemming, stop words removal, and text vectorisation. To extract knowledge from the text data after preprocessing, depending on the use case or end goal, the application steps might include clustering of text documents, classification of the text documents, and the extraction of document topics and entities. For each of these steps there are several methods and techniques being presented in different research studies. In this paper we present a framework that would categorise, cluster and classify a corpus of unclassified documents. The framework extracts named entities and uses a linked data Knowledge Graph to assign several topics and categories to each document. Then automatically cluster the documents into groups using the K-Mean model with the Elbow and Silhouette methods. Each cluster then gets assigned to a readable name from the extracted linked data based on word frequency in each cluster.",2161-1351,978-1-7281-3021-7,10.1109/DeSE.2019.00037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9073160,Text Mining;Data Mining;Clustering;Categorisation;Machine Learning;Classification,Text mining;Linked data;Data models;Machine learning;Clustering algorithms;Mathematical model,data mining;Linked Data;pattern classification;pattern clustering;text analysis,semisupervised framework;data mining;text documents;hierarchical text clustering;linked data knowledge graph;K-Mean model;elbow and silhouette methods,,,,17,,23-Apr-20,,,IEEE,IEEE Conferences
Annotations on Documents for Information Retrieval,信息檢索文件批註,V. A. Patil; P. Khambre,"Computer Engineering GHRIET Pune, India; Computer Engineering GHRIET, Pune, India",2016 International Conference on Computing Communication Control and automation (ICCUBEA),23-Feb-17,2016,,,1,6,"A huge range of corporations these days generate and proportionate textual descriptions of their products, services, and moves. Such collections of textual records contain widespread quantity of structured facts, which stays buried inside the unstructured text. Whilst records extraction algorithms facilitate the extraction of structured relations, they're often costly and faulty. In particular, while operating on top of textual content that doesn't include any instances of the centered structured data. In this paper, we present an unique alternative approach that facilitates the technology of the established metadata via identifying documents that are probable to comprise the records and this fact in the end will be beneficial for querying the database. Our technique is predicated on the idea that human beings those are more likely to feature the essential metadata all through creation time, if we bring it on by using an interface; or that it is a whole lot simpler for humans (and/or algorithms) to pick out the metadata while such statistics certainly exists in the documents. As a primary contribution of this paper, we are approaching the algorithms, those become aware of established attributes which can be probably to seem inside the report, by collectively utilizing the content material of the textual content and the query workload. Our experimental evaluation suggests that our method generates advanced effects compared to techniques that rely best on the textual content or best on the query workload, to pick out attributes of hobby.",,978-1-5090-3291-4,10.1109/ICCUBEA.2016.7860005,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860005,Document annotation;Adaptive forms;Collaborative platforms,Information retrieval;Clustering algorithms;Collaboration;Algorithm design and analysis;Google;Data integration;Data mining,meta data;query languages;query processing;text analysis,document annotation;information retrieval;database querying;metadata;textual content;query workload,,,,12,,23-Feb-17,,,IEEE,IEEE Conferences
A New Effective Neural Variational Model with Mixture-of-Gaussians Prior for Text Clustering,具有混合高斯先驗的文本聚類的新有效神經變分模型,M. Li; H. Tang; B. Jin; C. Zong,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),13-Feb-20,2019,,,1390,1395,"Text clustering is one of the fundamental tasks in natural language processing and text data mining. It remains challenging because texts have complex internal structure besides the sparsity in the high-dimensional representation. In the paper, we propose a new Neural Variational model with mixture-of-Gaussians prior for Text Clustering (abbr. NVTC) to reveal the underlying textual manifold structure and cluster documents effectively. NVTC is a deep latent variable model built on the basis of the neural variational inference. In NVTC, the stochastic latent variable, which is modeled as one obeying a Gaussian mixture distribution, plays an important role in establishing the association of documents and document labels. On the other hand, by joint learning, NVTC simultaneously learns text encoded representations and cluster assignments. Experimental results demonstrate that NVTC is able to learn clustering-friendly representations of texts. It significantly outperforms several baselines including VAE+GMM, VaDE, LCK-NFC, GSDPMM and LDA on four benchmark text datasets in terms of ACC, NMI, and AMI. Furthermore, NVTC learns effective latent embeddings of texts which are interpretable by topics of texts, where each dimension of latent embeddings corresponds to a specific topic.",2375-0197,978-1-7281-3798-8,10.1109/ICTAI.2019.00195,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995289,Text clustering;Deep generative model;Neural variational inference;Latent variable model,,data mining;Gaussian processes;learning (artificial intelligence);mixture models;natural language processing;pattern clustering;text analysis,mixture-of-Gaussians;text clustering;cluster documents;deep latent variable model;neural variational inference;Gaussian mixture distribution;text encoded representations;cluster assignments;clustering-friendly representations;benchmark text datasets;latent embeddings;natural language processing;text data mining;complex internal structure;high-dimensional representation;abbr. NVTC;neural variational model,,,,17,,13-Feb-20,,,IEEE,IEEE Conferences
Page Segmentation for Historical Document Images Based on Superpixel Classification with Unsupervised Feature Learning,基於超像素分類和無監督特徵學習的歷史文獻圖像頁面分割,K. Chen; C. Liu; M. Seuret; M. Liwicki; J. Hennebert; R. Ingold,"DIVA, Univ. of Fribourg, Fribourg, Switzerland; NLPR, Inst. of Autom., Beijing, China; DIVA, Univ. of Fribourg, Fribourg, Switzerland; DIVA, Univ. of Fribourg, Fribourg, Switzerland; DIVA, Univ. of Fribourg, Fribourg, Switzerland; DIVA, Univ. of Fribourg, Fribourg, Switzerland",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,299,304,"In this paper, we present an efficient page segmentation method for historical document images. Many existing methods either rely on hand-crafted features or perform rather slow as they treat the problem as a pixel-level assignment problem. In order to create a feasible method for real applications, we propose to use superpixels as basic units of segmentation, and features are learned directly from pixels. An image is first oversegmented into superpixels with the simple linear iterative clustering (SLIC) algorithm. Then, each superpixel is represented by the features of its central pixel. The features are learned from pixel intensity values with stacked convolutional autoencoders in an unsupervised manner. A support vector machine (SVM) classifier is used to classify superpixels into four classes: periphery, background, text block, and decoration. Finally, the segmentation results are refined by a connected component based smoothing procedure. Experiments on three public datasets demonstrate that compared to our previous method, the proposed method is much faster and achieves comparable segmentation results. Additionally, much fewer pixels are used for classifier training.",,978-1-5090-1792-8,10.1109/DAS.2016.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490134,page segmentation;layout analysis;historical document image;superpixel;SLIC;autoencoder,Image segmentation;Training;Feature extraction;Support vector machines;Clustering algorithms;Classification algorithms;Labeling,feature extraction;history;image classification;image segmentation;support vector machines,page segmentation method;historical document images;superpixel classification;unsupervised feature learning;hand-crafted features;pixel-level assignment problem;simple linear iterative clustering algorithm;SLIC;central pixel;pixel intensity values;stacked convolutional autoencoders;support vector machine classifier;SVM;smoothing procedure;public datasets;classifier training,,15,,19,,13-Jun-16,,,IEEE,IEEE Conferences
Short Text Clustering Based on Word Semantic Graph with Word Embedding Model,詞嵌入模型的基於詞語義圖的短文本聚類,S. Jinarat; B. Manaskasemsak; A. Rungsawang,Kasetsart University; Kasetsart University; Kasetsart University,2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS),16-May-19,2018,,,1427,1432,"Nowadays, a number of short messages or short text contents created on the Internet are rapidly increasing. Tasks to manipulate, analyze, and extract knowledge from them lead mining techniques such as text clustering to become more important. However, applying traditional text clustering algorithms which consider only common words or phrases to group short texts is inefficient due to the problem of sparsity. In this paper, we propose a new clustering technique, called word semantic graph clustering, based on the use of text concepts. We apply the word embedding model from Word2Vec to capture the semantic meaning of words and later construct semantic subgraphs in which those words represented as vertices are connected by some high semantic similarities. Finally, short text documents will be assigned to the same cluster if they contain at least one word belonging to the same semantic subgraph. Experimental results conducted on two real datasets show that the proposed approach outperforms the state-of-the-art text clustering algorithms. In addition, it can also produce more appropriate label for each cluster than the comparative algorithms do.",,978-1-5386-2633-7,10.1109/SCIS-ISIS.2018.00223,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716128,word semantic;graph clustering;short text;word embedding,Semantics;Clustering algorithms;Image edge detection;Internet;Data mining;Intelligent systems;Knowledge engineering,data mining;graph theory;knowledge representation;pattern clustering;text analysis,short text clustering;word embedding model;Word2Vec;short text documents;mining techniques;word semantic graph clustering;Internet,,2,,10,,16-May-19,,,IEEE,IEEE Conferences
Pairwise Constrained Clustering with Group Similarity-Based Patterns,基於組相似度模式的成對約束聚類,T. Hu; C. Liu; J. Sun; S. Y. Sung; P. A. Ng,NA; NA; NA; NA; NA,2010 Ninth International Conference on Machine Learning and Applications,4-Feb-11,2010,,,260,265,"Conventional k-means only considers pair wise similarity during cluster assignment, which aims to minimizing the distance of points to their nearest cluster centroids. In high dimensional space like document datasets, however, two points may be nearest neighbors without belonging to the same class. Thus pair wise similarity alone is often insufficient for class prediction in such space. To that end, in this paper, we propose to augment k-means with pair wise constraints generated from group similarity-based hyper clique patterns, which consist of strongly affiliated objects and serve as more reliable seeds for classification. Experiments with real-world datasets show that, with such constraints from quality hyper clique patterns, we can improve the clustering results in terms of various external criteria. Also, our experiments indicate that even if few constraints are violated in the original result of k-means, imposing many quality constraints may still bring gain of performance.",,978-1-4244-9211-4,10.1109/ICMLA.2010.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708842,pairwise constraints;hyperclique patterns;k-means;constrained clustering,Nearest neighbor searches;Entropy;Itemsets;Clustering algorithms;Wireless application protocol;Artificial neural networks;Data mining,pattern classification;pattern clustering,pairwise constrained clustering;k-means;pairwise similarity;cluster assignment;nearest cluster centroid;document dataset;nearest neighbor;class prediction;group similarity-based hyper clique pattern;classification;quality constraint,,,,17,,4-Feb-11,,,IEEE,IEEE Conferences
An approach to identify unique styles in online handwriting recognition,在線手寫識別中識別獨特樣式的方法,A. Bharath; V. Deepu; S. Madhvanath,"Hewlett-Packard Labs India, Bangalore, India; Hewlett-Packard Labs India, Bangalore, India; Hewlett-Packard Labs India, Bangalore, India",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,775,778 Vol. 2,We describe a method for identifying different writing styles of online handwritten characters based on clustering. The motivation of this experiment is to develop automatic characterization of different writing styles that arise due to variation in stroke number or stroke ordering. An efficient agglomerative hierarchical clustering technique with the nearest neighbor approach was implemented to cluster strokes. The results obtained from our experiment indicate that the resulting prototypes are unique and essentially capture different writing styles.,2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575650,,Handwriting recognition;Writing;Prototypes;Smoothing methods;Character recognition;Clustering algorithms;Nearest neighbor searches;Algorithm design and analysis;Clustering methods;Noise reduction,handwriting recognition;handwritten character recognition;pattern clustering,online handwriting recognition;writing style;online handwritten character;stroke number;stroke ordering;agglomerative hierarchical clustering;nearest neighbor,,9,,8,,16-Jan-06,,,IEEE,IEEE Conferences
An Efficient Token-based Approach for Web-Snippet Clustering,Web片段聚類的基於令牌的高效方法,J. L. Jianchao Li; T. Y. Tianfang Yao,"Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China","2006 Semantics, Knowledge and Grid, Second International Conference on",20-Aug-12,2006,,,13,13,"Online clustering of the results returned by search engines becomes prevailing in recent times. It addresses the problem of too many records returned by current search engines, which renders the manual search of actually desired information difficult, especially if the query encompasses several subtopics. Clustering is a useful technique to group records to clusters and thereby make it more convenient to retrieve information of interest. We first propose an innovative approach by using tokens as basic units for clustering, which avoids segmentation for oriental languages and can be applied to any language. Second, we introduce a Directed Probability Graph (DPG) model that identifies meaningful phrases as cluster labels using statistical methods without any external knowledge. The clustering procedure is performed without calculating the similarity between pair-wise documents. As shown by our experiments, our clustering algorithm is very efficient and suitable for online Web-snippet clustering.",,0-7695-2673-X,10.1109/SKG.2006.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5727650,,,directed graphs;document handling;pattern clustering;probability;query processing;search engines;token networks;Web services,search engine;manual search rendering;query processing;group record;information retrieval;innovative approach;directed probability graph;DPG;cluster label;statistical method;pairwise document;clustering algorithm;online Web snippet clustering,,,,13,,20-Aug-12,,,IEEE,IEEE Conferences
Documents Clustering Based on Optimized Compressibility Vector Space,基於優化壓縮性向量空間的文檔聚類,N. Zhang; T. Watanabe,"Grad. Sch. of Inf. Syst., Univ. of Electro-Commun., Chofu, Japan; Grad. Sch. of Inf. Syst., Univ. of Electro-Commun., Chofu, Japan",2009 International Conference on Computational Intelligence and Software Engineering,28-Dec-09,2009,,,1,5,"To access and store large-scale electrical documents becomes possible due to the high performance of computer hardware and broadband accessible network. In order to handle these increasing number of documents properly, a efficient document representation model is as important as the classification algorithms. Several text representation methods, such as bag-of-words and N-gram models, have been widely used. Another representation approach named pattern representation scheme using data compression (PRDC) has been proposed lately. It does not only independently process data of linguistic text, but also processes multimedia data effectively. In this study, we will propose a method to improve PRDC approach and compare it with the two aforementioned methods. The performances will be compared in terms of clustering ability. Experiment results will show that the proposed method can provide better performance than that of the other two methods and also the PRDC.",,978-1-4244-4507-3,10.1109/CISE.2009.5363976,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5363976,,Information retrieval;Data compression;Text categorization;Information systems;Large-scale systems;Computer networks;High performance computing;Hardware;Classification algorithms;Information management,computational linguistics;data compression;multimedia systems;pattern classification;pattern clustering;text analysis,documents clustering;optimized compressibility vector space;electrical documents;computer hardware;broadband accessible network;classification algorithms;text representation;pattern representation;data compression;linguistic text;multimedia data,,,,10,,28-Dec-09,,,IEEE,IEEE Conferences
A Genetic System for Cluster Analysis for Hypertext Documents,用於超文本文檔的聚類分析的遺傳系統,L. M. di Carlantonio; R. M. E. da Costa,"Univ. do Estado do Rio de Janeiro - UERJ, Rio de Janeiro; Univ. do Estado do Rio de Janeiro - UERJ, Rio de Janeiro",Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007),27-Nov-07,2007,,,771,776,"Due to the increase in the number of WWW home pages, we are facing new challenges for information retrieval and indexing. Some ""intelligent"" techniques, including neural networks, symbolic learning, and genetic algorithms have been used to group different classes of data in an efficient way. This article describes a system for cluster analysis of hypertext documents based on genetic algorithms. The effectiveness of the system in getting groups with similar documents is evidenced by the experimental results.",2164-7151,978-0-7695-2976-9,10.1109/ISDA.2007.126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389701,,Text analysis;Genetic algorithms;Algorithm design and analysis;Clustering algorithms;Information analysis;HTML;Data mining;Vectors;Intelligent systems;System analysis and design,genetic algorithms;hypermedia;information retrieval;Internet;neural nets;pattern clustering;statistical analysis;Web sites,genetic system;cluster analysis;hypertext documents;WWW home pages;information retrieval;neural networks;symbolic learning;genetic algorithms,,,,14,,27-Nov-07,,,IEEE,IEEE Conferences
Using semantic and structural similarities for indexing and searching scientific papers,使用語義和結構相似性對科學論文進行索引和搜索,S. R. Ali Rizvi; Shawn Xiong Wang,"Department of Computer Science, California State University, Fullerton, USA; Department of Computer Science, California State University, Fullerton, USA",2011 IEEE International Conference on Computer Science and Automation Engineering,14-Jul-11,2011,4,,133,137,"Finding relevant scientific documents from a huge set of academic papers is a challenging task and with the tremendous growth in electronic publication, locating the most relevant and related scientific documents when going through a new research paper is becoming even more challenging. In this paper, we present a new way of indexing and searching the scientific documents to assist researchers in finding relevant documents when coming across a new research document. In particular, we explored how DT-Tree (DocumentTerm-Tree) - a new structure for the representation of scientific documents - can be used to create an index of scientific documents. We used MVP-Tree to create index using DT-Tree representation of the documents. We then performed search experiments, using new scientific documents as queries, to show that relevant documents are retrieved when DT-Tree structures are used to create MVP-Tree.",,978-1-4244-8728-8,10.1109/CSAE.2011.5952818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952818,Semantic Analysis;Similarity Measure;Document clustering;dimension reduction;key term extraction;sparsity;k-means;index structures;search;text mining,Indexing;Silicon;Physics;Algorithm design and analysis;Arrays;Semantics,document handling;electronic publishing;indexing;query processing;scientific information systems;tree data structures,semantic similarities;structural similarities;scientific paper searching;scientific paper indexing;academic paper;electronic publication;research paper;DocumentTerm-tree;scientific document representation;DT-Tree representation;MVP-Tree;query processing,,,,21,,14-Jul-11,,,IEEE,IEEE Conferences
Image annotation with semi-supervised clustering,具有半監督聚類的圖像註釋,A. Sayar; F. T. Y. Vural,"TUBITAK UZAY, Middle East Technical University, Ankara, Turkey; Computer Engineering Department, Middle East Technical University, Ankara, Turkey",2009 24th International Symposium on Computer and Information Sciences,23-Oct-09,2009,,,12,17,"Methods developed for image annotation usually make use of region clustering algorithms. Visual codebooks are generated from the region clusters of low level features. These codebooks are then, matched with the words of the text document related to the image, in various ways. In this paper, we supervise the clustering process by using three types of side information. The first one is the topic probability information obtained from the text document associated with the image. The second is the orientation and the third one is the color information around each interest point. The side information provides a set of constraints in a semi-supervised k-means region clustering algorithm. Consequently, in clustering of regions not only low level features, but also this extra information is used. Experimental results show that image annotation with semi-supervision of side information is more successful compared to the one that uses low level features alone. Moreover, a speedup is obtained in the modified k-means algorithm because of the constraints. The proposed algorithm is implemented in a high performance parallel computation environment.",,978-1-4244-5021-3,10.1109/ISCIS.2009.5291929,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291929,,Clustering algorithms;Vocabulary;Concurrent computing;High performance computing;Image databases;Spatial databases;Visual databases;Information retrieval;Image retrieval;Image segmentation,document image processing;image colour analysis;image matching;pattern clustering;probability;text analysis,image annotation;semisupervised clustering;visual codebook;image matching;text document;probability information;color image analysis,,6,,13,,23-Oct-09,,,IEEE,IEEE Conferences
An EM Based Algorithm for Skew Detection,基於EM的偏斜檢測算法,A. Egozi; I. Dinstein,Ben-Gurion University of the Negev; Ben-Gurion University of the Negev,Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),12-Nov-07,2007,1,,277,281,"We present a a statistical approach to skew detection, where the textual features of a document image are modeled as a mixture of straight lines in Gaussian noise. The EM algorithm is used to estimate the parameters of the mixture model and the skew angle estimate is extracted from the estimated parameters. Experiments prove that our method has some advantages over other existing methods in terms of accuracy and efficiency.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4378719,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378719,,Gaussian noise;Parameter estimation;Detection algorithms;Algorithm design and analysis;Digital images;Character recognition;Nearest neighbor searches;Least squares methods;Clustering algorithms;Maximum likelihood estimation,document image processing;expectation-maximisation algorithm;Gaussian noise;parameter estimation,expectation maximization algorithm;skew detection;document image textual feature;Gaussian noise;parameter estimation;skew angle estimate,,6,,16,,12-Nov-07,,,IEEE,IEEE Conferences
Automatic table ground truth generation and a background-analysis-based table structure extraction method,自動表地面真相生成和基於背景分析的表結構提取方法,Yalin Wangt; I. T. Phillipst; R. Haralick,"Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,528,532,"We first describe an automatic table ground truth generation system which can efficiently generate a large amount of accurate table ground truth suitable for the development of table detection algorithms. Then a novel background analysis-based, coarse-to-fine table identification algorithm and an X-Y cut table decomposition algorithm are described. We discuss an experimental protocol to evaluate the table detection algorithms. For a total of 1,125 document pages having 518 table entities and a total of 10,941 cell entities, our table detection algorithm takes line, word segmentation results as input and obtains around 90% cell correct detection rates.",,0-7695-1263-1,10.1109/ICDAR.2001.953845,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953845,,Detection algorithms;Text analysis;Data mining;Partitioning algorithms;Clustering algorithms;Computer science;Educational institutions;Protocols;Image segmentation;Image analysis,document image processing;image segmentation,table structure extraction method;table ground truth generation system;table detection algorithms;background analysis-based identification;X-Y cut table decomposition algorithm;line segmentation;word segmentation;document layout analysis;experimental results,,3,,18,,7-Aug-02,,,IEEE,IEEE Conferences
One-Shot Template Matching for Automatic Document Data Capture,一鍵式模板匹配，可自動捕獲文檔數據,P. Dhakal; M. Munikar; B. Dahal,Docsumo; Docsumo; Docsumo,2019 Artificial Intelligence for Transforming Business and Society (AITB),2-Jan-20,2019,1,,1,6,The following topics are dealt with: learning (artificial intelligence); pattern clustering; distributed databases; computer vision; pattern classification; educational institutions; time series; Internet; text analysis; image enhancement.,,978-1-7281-4220-3,10.1109/AITB48515.2019.8947440,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947440,document processing;automatic data capture;template matching;one-shot learning,Optical character recognition software;Annotations;Proposals;Feature extraction;Data mining;Visualization;Machine learning,computer vision;distributed databases;learning (artificial intelligence);pattern classification;pattern clustering,learning (artificial intelligence);pattern clustering;distributed databases;computer vision;pattern classification;educational institutions;time series;Internet;text analysis;image enhancement,,,,20,,2-Jan-20,,,IEEE,IEEE Conferences
Unsupervised Word Clustering Using Deep Features,使用深度功能的無監督詞聚類,M. Kulkarni; S. S. Karande; S. Lodha,"TCS Innovation Labs., Pune, India; TCS Innovation Labs., Pune, India; TCS Innovation Labs., Pune, India",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,263,268,"Digitization is crucial especially in the Indian context. OCR engines fail on Indian scripts mainly because character segmentation is non-trivial. Even word based recognition approaches suffer from the issues such as time degradations, word segmentation errors, font style/size variations. In this paper, we propose a deep learning architecture based approach for unsupervised word clustering. An edge responsive untrained Convolutional Neural Network (CNN) is used as a feature extractor. Graph connected component analysis is applied on the similarity graph computed from the word features. Our approach inherently detects similar shape patterns at word level and hence, it is language agnostic. We validated our approach against multiple state of art word matching techniques. Experimental results show that our approach significantly outperforms all of them on variety of data sets. In addition, the approach is observed to be robust to word segmentation errors, font style/size variations.",,978-1-5090-1792-8,10.1109/DAS.2016.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490128,Deep learning;word clustering,Feature extraction;Convolution;Machine learning;Computer architecture;Neural networks;Object recognition;Training,convolution;document image processing;feature extraction;graph theory;image segmentation;learning (artificial intelligence);natural language processing;neural nets;optical character recognition;pattern clustering,unsupervised word clustering;deep features;digitization;OCR engines;Indian scripts;character segmentation;word based recognition;word segmentation errors;font style variations;font size variations;deep learning architecture;edge responsive untrained convolutional neural network;feature extraction;graph connected component analysis;similarity graph,,,,21,,13-Jun-16,,,IEEE,IEEE Conferences
Query Based Clustering Method in Structured P2P Overlay Networks,結構化P2P覆蓋網絡中基於查詢的聚類方法。,Q. Ma; K. Zhao; X. Wang,"Key Lab. of Universal Wireless Commun., Beijing Univ. of Posts & Telecommun., Beijing, China; Key Lab. of Universal Wireless Commun., Beijing Univ. of Posts & Telecommun., Beijing, China; Key Lab. of Universal Wireless Commun., Beijing Univ. of Posts & Telecommun., Beijing, China",2010 7th IEEE Consumer Communications and Networking Conference,25-Feb-10,2010,,,1,5,"In structured peer-to-peer (P2P) overlay networks, similar documents are randomly distributed over peers with their data identifiers consistently hashed, which makes complex search challenging. Current state-of-the-art complex query approaches in structured P2P systems are mainly based on inverted list intersection. When the identifiers are distributed among peers, a complex query may involve many peers and cause a large amount of network traffic. One solution of implementing efficient complex query is to organize documents on each peer using clustering. In this paper, we propose a clustering method, QBC, which is composed of pull mode and push mode. Pull mode uses historical queries to direct clustering in structured P2P overlay networks and push mode applies modified vector space model (VSM) to define document set on each peer in order to assist clustering. Experiments show that QBC can reduce the number of peers visited during complex search, hence both query response time and network traffic are decreased.",2331-9860,978-1-4244-5175-3,10.1109/CCNC.2010.5421738,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421738,,Clustering methods;Keyword search;Telecommunication traffic;Delay;Peer to peer computing;Space technology;Traffic control;Computer networks;Streaming media;Routing,data analysis;pattern clustering;peer-to-peer computing;query processing,query based clustering method;structured P2P overlay networks;structured peer-to-peer overlay networks;data identifiers;state-of-the-art complex query approaches;inverted list intersection;network traffic;pull mode;push mode;historical query;vector space model;query response time,,,,16,,25-Feb-10,,,IEEE,IEEE Conferences
Consensus clustering algorithms for Asset Management in power systems,電力系統資產管理共識集群算法,L. Yan; Y. Xin; W. Tang,"Department of Electrical Engineering& Electronics, University of Liverpool, Liverpool, United Kingdom; School of Electric Power Engineering, South China University of Technology, Guangzhou, China; School of Electric Power Engineering, South China University of Technology, Guangzhou, China",2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT),14-Mar-16,2015,,,1504,1510,"This paper aims to present an intelligent approach to the Asset Management (AM) in power systems. Three existing consensus functions (i.e., Non-negative matrix Factorisation based, Weighted Partition via Kernel and Information Theory-based) have been compared and applied to analyse a power system document depository (PSD). In addition, ontology is employed in this research to implement a document dataset modification, which shows a significant improvement for the single consensus clustering algorithm. Moreover, Genetic Algorithm (GA) is utilised to solve the weighted kernel-based consensus function. The influences of different settings of GA are analysed. Relevant improvements of GA are also presented.",,978-1-4673-7106-3,10.1109/DRPT.2015.7432484,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432484,Document Clustering;Consensus Clustering;Genetic Algorithm;Ontology,Clustering algorithms;Power systems;Partitioning algorithms;Genetic algorithms;Entropy;Indexes;Kernel,genetic algorithms;matrix decomposition;power systems,consensus clustering algorithms;asset management;power systems;nonnegative matrix factorisation;weighted partition via kernel;information theory;power system document depository;genetic algorithm,,1,,23,,14-Mar-16,,,IEEE,IEEE Conferences
A new hybrid binarization method based on Kmeans,基於Kmeans的混合二值化新方法,M. Soua; R. Kachouri; M. Akil,"ESIEE Paris, LIGM, A3SI, 2 Bd Blaise Pascal, BP 99, 93162 Noisy-Le-Grand, France; ESIEE Paris, LIGM, A3SI, 2 Bd Blaise Pascal, BP 99, 93162 Noisy-Le-Grand, France; ESIEE Paris, LIGM, A3SI, 2 Bd Blaise Pascal, BP 99, 93162 Noisy-Le-Grand, France","2014 6th International Symposium on Communications, Control and Signal Processing (ISCCSP)",14-Aug-14,2014,,,118,123,"The document binarization is a fundamental processing step toward Optical Character Recognition (OCR). It aims to separate the foreground text from the document background. In this article, we propose a novel binarization technique combining local and global approaches using the clustering algorithm Kmeans. The proposed Hybrid Binarization, based on Kmeans (HBK), performs a robust binarization on scanned documents. According to several experiments, we demonstrate that the HBK method improves the binarization quality while minimizing the amount of distortion. Moreover, it outperforms several well-known state of the art methods in the OCR evaluation.",,978-1-4799-2890-3,10.1109/ISCCSP.2014.6877830,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877830,Scanned documents;binarization;Kmeans;OCR,Optical character recognition software;Optical distortion;Histograms;Distortion measurement;Character recognition;Robustness;Clustering algorithms,document image processing;learning (artificial intelligence);optical character recognition;pattern clustering,hybrid binarization method;Kmeans clustering algorithm;document binarization;optical character recognition;OCR evaluation;binarization technique;HBK method;scanned documents;binarization quality;distortion amount,,7,,29,,14-Aug-14,,,IEEE,IEEE Conferences
Word Retrieval in Historical Document Using Character-Primitives,使用字符原語檢索歷史文檔中的單詞,P. P. Roy; J. Ramel; N. Ragot,"Lab. d'Inf., Univ. Francois Rabelais, Tours, France; Lab. d'Inf., Univ. Francois Rabelais, Tours, France; Lab. d'Inf., Univ. Francois Rabelais, Tours, France",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,678,682,"Word searching and indexing in historical document collections is a challenging problem because, characters in these documents are often touching or broken due to degradation/ ageing effects. For efficient searching in such historical documents, this paper presents a novel approach towards word spotting using string matching of character primitives. We describe the text string as a sequence of primitives which consists of a single character or a part of a character. Primitive segmentation is performed analyzing text background information that is obtained by water reservoir technique. Next, the primitives are clustered using template matching and a codebook of representative primitives is built. Using this primitive codebook, the text information in the document images are encoded and stored. For a query word, we segment it into primitives and encode the word by a string of representative primitives from codebook. Finally, an approximate string matching is applied to find similar words. The matching similarity is used to rank the retrieved words. The proposed method is tested on historical books of French alphabets and we have obtained encouraging results from the experiment.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065397,,Image segmentation;Reservoirs;Indexing;Layout;Text analysis;Shape,document handling;information retrieval;natural language processing,word retrieval;character primitives;word searching;historical document collections;text string;Primitive segmentation;water reservoir technique;template matching;primitive codebook;document images;French alphabets,,10,,9,,3-Nov-11,,,IEEE,IEEE Conferences
Keywords Extraction from Chinese Document Based on Complex Network Theory,基於復雜網絡理論的中文文檔關鍵詞提取,J. Nan; B. Xiao; Z. Lin; Q. Xu,"Inst. of Sensing Technol. & Bus., Beijing Univ. of Posts & Telecommun. Beijing, Beijing, China; Inst. of Sensing Technol. & Bus., Beijing Univ. of Posts & Telecommun. Beijing, Beijing, China; Inst. of Sensing Technol. & Bus., Beijing Univ. of Posts & Telecommun. Beijing, Beijing, China; Inst. of Sensing Technol. & Bus., Beijing Univ. of Posts & Telecommun. Beijing, Beijing, China",2014 Seventh International Symposium on Computational Intelligence and Design,9-Apr-15,2014,2,,383,386,"Keywords extraction is the process of choosing several words from a document to express its main idea. Keywords help people understand an article quickly and clearly. In recent years, more and more researchers pay attention to its research since its important role in text clustering, text classification, automatic abstracting, and text retrieval. This paper proposes an algorithm called EC-DC to extract keywords based on centrality measures of complex network. A document is mapped to a network with its words mapped to vertices and relations between words mapped to edges. Then, the importance of words is evaluated using eccentricity centrality and degree centrality. The most important K words are extracted as keywords. Experimental results show that the EC-DC algorithm has an improvement of about 9% in precision, recall and F-score compared to classical TFIDF algorithm.",,978-1-4799-7005-6,10.1109/ISCID.2014.183,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7082012,complex network;eccentricity centrality;degree centrality;document network;keywords extraction,Complex networks;Approximation algorithms;Semantics;Feature extraction;Business;Data mining;Internet,complex networks;feature extraction;text analysis,keywords extraction;Chinese document;complex network theory;text clustering;text classification;automatic abstracting;text retrieval;complex network centrality measures;eccentricity centrality;degree centrality;EC-DC algorithm,,2,,13,,9-Apr-15,,,IEEE,IEEE Conferences
Text extraction from degraded document images,從降級的文檔圖像中提取文本,R. Hedjam; R. Farrahi Moghaddam; M. Cheriet,"Synchromedia Laboratory for Multimedia Communication in Telepresence, ?cole de Technologie Sup矇erieure, Montr矇al (QC), H3C 1K3 Canada; Synchromedia Laboratory for Multimedia Communication in Telepresence, ?cole de Technologie Sup矇erieure, Montr矇al (QC), H3C 1K3 Canada; Synchromedia Laboratory for Multimedia Communication in Telepresence, ?cole de Technologie Sup矇erieure, Montr矇al (QC), H3C 1K3 Canada",2010 2nd European Workshop on Visual Information Processing (EUVIP),20-Jan-11,2010,,,247,252,"In this work, a robust segmentation method for text extraction from the historical document images is presented. The method is based on Markovian-Bayesian clustering on local graphs on both pixel and regional scales. It consists of three steps. In the first step, an over-segmented map of the input image is created. The resulting map provides a rich and accurate semi-mosaic fragments. The map is processed in the second step, similar and adjoining sub-regions are merged together to form accurate text shapes. The output of the second step, which contains accurate shapes, is processed in the final step in which, using clustering with fixed number of classes, the segmentation will be obtained. The method employs significantly the local and spatial correlation and coherence on both the image and between the stroke parts, and therefore is very robust with respect to the degradation. The resulting segmented text is smooth, and weak connections and loops are preserved thanks to robust nature of the method. The output can be used in succeeding skeletonization processes which require preservation of the text topology for achieving high performance. The method is tested on real degraded document images with promising results.",,978-1-4244-7289-5,10.1109/EUVIP.2010.5699135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5699135,Document image;Image segmentation;Image binarization;MRF;Graph-partitioning,Image segmentation;Shape;Merging;Degradation;Robustness;Pixel;Histograms,Bayes methods;document image processing;feature extraction;image segmentation;image thinning;Markov processes;text analysis,text extraction;degraded document image;robust segmentation method;historical document image;Markovian-Bayesian clustering;skeletonization process;text topology,,7,,14,,20-Jan-11,,,IEEE,IEEE Conferences
Scalable Constrained Spectral Clustering via the Randomized Projected Power Method,通過隨機投影功率法可擴展的約束譜聚類,W. Zhi; B. Qian; I. Davidson,"Dept. of Comput. Sci., Univ. of California - Davis, Davis, CA, USA; Dept. of Comput. Sci., Xi'an Jiaotong Univ., Xi'an, China; Dept. of Comput. Sci., Univ. of California - Davis, Davis, CA, USA",2017 IEEE International Conference on Data Mining (ICDM),18-Dec-17,2017,,,1201,1206,"Constrained spectral clustering is an important area with many applications. However, most previous work has only been applied to relatively small data sets: graphs with thousands of points. This prevents this work from being applied to the large data sets found in application domains such as medical imaging and document data. Recent work on constrained and unconstrained spectral clustering has explored scalability of these methods via data approximations such as the Nystrom method which requires the selection of landmarks. However, compressing a graph may lead to undesirable results and poses the additional problem of how to chose landmarks. Instead in this paper, we propose a fast and scalable numerical algorithmic solution for the constrained clustering problem. We show the convergence and stability of our approach by proving its rate of convergence and demonstrate the effectiveness of our algorithm with empirical results on several real data sets. Our approach achieved comparable accuracy as popular constrained spectral clustering algorithms but taking several hundred times less time.",2374-8486,978-1-5386-3835-4,10.1109/ICDM.2017.162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215625,Clustering;Spectral;Constraints;Constrained Clustering,Clustering algorithms;Convergence;Matrix decomposition;Eigenvalues and eigenfunctions;Algorithm design and analysis;Laplace equations;Computer science,approximation theory;convergence of numerical methods;graph theory;pattern clustering,convergence;constrained clustering problem;Nystrom method;data approximations;unconstrained spectral clustering;application domains;data sets;randomized projected power method,,,,25,,18-Dec-17,,,IEEE,IEEE Conferences
Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering,向隱藏的特徵學習：聯合因素分析和潛在聚類,B. Yang; X. Fu; N. D. Sidiropoulos,"Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA",IEEE Transactions on Signal Processing,4-Nov-16,2017,65,1,256,269,"Dimensionality reduction techniques play an essential role in data analytics, signal processing, and machine learning. Dimensionality reduction is usually performed in a preprocessing stage that is separate from subsequent data analysis, such as clustering or classification. Finding reduced-dimension representations that are well-suited for the intended task is more appealing. This paper proposes a joint factor analysis and latent clustering framework, which aims at learning cluster-aware low-dimensional representations of matrix and tensor data. The proposed approach leverages matrix and tensor factorization models that produce essentially unique latent representations of the data to unravel latent cluster structure-which is otherwise obscured because of the freedom to apply an oblique transformation in latent space. At the same time, latent cluster structure is used as prior information to enhance the performance of factorization. Specific contributions include several custom-built problem formulations, corresponding algorithms, and discussion of associated convergence properties. Besides extensive simulations, real-world datasets such as Reuters document data and MNIST image data are also employed to showcase the effectiveness of the proposed approaches.",1941-0476,,10.1109/TSP.2016.2614491,National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579562,Factor analysis;identifiability;matrix factorization;tensor factorization;dimensionality reduction;clustering;reduced $K$ -means;factorial $K$ -means;clustering prior;subspace learning,Tensile stress;Data models;Analytical models;Signal processing;Clustering algorithms;Algorithm design and analysis;Convergence,learning (artificial intelligence);matrix algebra;pattern clustering;signal processing,hidden traits;learning;joint factor analysis;dimensionality reduction techniques;data analytics;signal processing;machine learning;subsequent data analysis;reduced dimension representations;latent clustering framework;tensor data;matrix data;tensor factorization models;matrix factorization models;cluster structure,,9,,53,,29-Sep-16,,,IEEE,IEEE Journals
Item membership fuzzification in fuzzy co-clustering based on multinomial mixture concept,基於多項式混合概念的模糊共聚中的項目隸屬度模糊化,K. Honda; S. Oshio; A. Notsu,"Graduate School of Engineering, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531 Japan; Graduate School of Engineering, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531 Japan; Graduate School of Engineering, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531 Japan",2014 IEEE International Conference on Granular Computing (GrC),15-Dec-14,2014,,,94,99,"Co-clustering is a promising technique for summarizing cooccurrence information such as purchase history transactions and document-keyword frequencies. A close connection between fuzzy c-means (FCM) and Gaussian mixture models (GMMs) have been discussed and several extended FCM algorithms, which are induced by the GMMs concept, were proposed. Multinomial mixture models (MMMs) is a probabilistic model for co-clustering task and we have a possibility of inducing a fuzzy co-clustering model based on the MMMs concept, whose goal is to simultaneously estimate the cluster membership degrees of both objects and items. In this paper, a fuzzification mechanism for item memberships is proposed and its characteristic features are discussed.",,978-1-4799-5464-3,10.1109/GRC.2014.6982814,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982814,fazzy clustering;co-clustering;multinomial mixture models,Clustering algorithms;Linear programming;Probabilistic logic;Vectors;Partitioning algorithms;Educational institutions;History,fuzzy set theory;Gaussian processes;mixture models;pattern clustering;probability,item membership fuzzification;fuzzy co-clustering;multinomial mixture concept;cooccurrence information summarization;fuzzy c-means;FCM algorithms;Gaussian mixture models;GMM;multinomial mixture models;MMM;probabilistic model;cluster membership degree estimation,,9,,12,,15-Dec-14,,,IEEE,IEEE Conferences
Learning Term Spaces Based on Visual Feedback,基於視覺反饋學習術語空間,M. Granitzer; T. Neidhart; M. Lux,"Know-Center Graz, Austria; NA; NA",17th International Workshop on Database and Expert Systems Applications (DEXA'06),16-Oct-06,2006,,,176,180,"Extracting and visualizing concepts and relationship between text documents strongly depends on the used similarity measure. In order to provide meaningful visualizations and to extract useful knowledge from document collections, user needs must be captured by the internal representation of documents, and the used similarity measure. In most applications the vector space model and the cosine similarity are used therefore and serve as good approximations. Nevertheless, influencing similarities between documents is rather hard, since parameter tuning relies heavily on expert knowledge of the underlying algorithms, and the influence of different weighting schemes and similarity measures is not known before. In this paper we present an approach on how to adapt the vector space representation of documents by giving visual feedback to the system. Our approach starts by clustering a corpus of text documents and visualizing the results using multi dimensional scaling techniques. Afterwards, a 2D landscape visualization is shown which can be manipulated by the user. Based on these manipulations the high dimensional representation of the documents is adapted to fit the users need more precisely. Our experiments show that iterating these steps results in an adapted representation of documents and similarities, generating layouts as intended by the user and furthermore increases clustering accuracy. While this paper only investigates the influence on clustering and visualization, the method itself may also be used for increasing classification and retrieval performance since it adapts to the users need of similarity",2378-3915,0-7695-2641-1,10.1109/DEXA.2006.82,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1698330,,Feedback;Visualization;Vectors;Space technology;Knowledge management;Clustering algorithms;Performance analysis;Navigation;Current measurement;Information retrieval,information retrieval;learning (artificial intelligence);pattern clustering;text analysis,term space learning;visual feedback;vector space model;cosine similarity measure;vector space representation;document representation;text document clustering;multidimensional scaling;2D landscape visualization;classification;document collection,,1,,8,,16-Oct-06,,,IEEE,IEEE Conferences
KeyGraph and WordNet hypernyms for topic detection,KeyGraph和WordNet上義詞用於主題檢測,K. Perera; D. Karunarathne,"University of Colombo School of Computing, 07, Sri Lanka; University of Colombo School of Computing, 07, Sri Lanka",2015 12th International Joint Conference on Computer Science and Software Engineering (JCSSE),27-Aug-15,2015,,,303,308,"The Vast number of publicly available unstructured information on web and their rapid growth pose a great challenge in understanding, managing and structuring the information. Topic modeling algorithms have been developed with the purpose of analyzing these unstructured data and obtain abstract topics and clusters from these data collections. KeyGraph is a word co-occurrence based algorithm for topic modeling. We provide an extension for KeyGraph algorithm by incorporating WordNet hypernyms for Keywords in the data collection. Our results show that incorporating hypernyms for KeyGraph algorithm would result improved topic and document clusters.",,978-1-4799-1966-6,10.1109/JCSSE.2015.7219814,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219814,Topic Model;Document clustering;KeyGraph;Information Extraction;WordNet,Clustering algorithms;Standards;Security;Ontologies;Algorithm design and analysis;Sensitivity analysis;Gold,document handling;Internet;pattern clustering,KeyGraph hypernyms;WordNet hypernyms;topic detection;Web;topic modeling algorithms;unstructured data;abstract topics;data collections;cooccurrence based algorithm;document clusters;keywords,,4,,22,,27-Aug-15,,,IEEE,IEEE Conferences
Modeling a Hierarchical Abstraction Process on top of Co-Occurrence Graphs,在共現圖之上對分層抽象過程進行建模,S. Simcharoen; Y. Ruamsuk; A. Mingkhwan; H. Unger,"Faculty of Mathematics and Computer Science, Fern. Universit瓣t in Hagen,Hagen,Germany; Faculty of Industrial Technology and Management, King Mongkut's University of Technology North Bangkok,Bangkok,Thailand; Faculty of Industrial Technology and Management, King Mongkut's University of Technology North Bangkok,Bangkok,Thailand; Faculty of Mathematics and Computer Science, Fern. Universit瓣t in Hagen,Hagen,Germany","2019 Research, Invention, and Innovation Congress (RI2C)",17-Feb-20,2019,,,1,5,"A co-occurrence graph is incorporated from sets of documents that represent knowledge. However, determining number of groups or clusters of knowledge this may pertain to remains a challenge. This work will explore the hierarchical clustering algorithm for which a hierarchy is built from the cluster center (centroid) of each cluster that is read node by node. Each node finds an inter-cluster that will be assigned by referring to a distance from the node to the inter-cluster center which ensures that this node is a member of that inter-cluster. The inter-cluster center is an abstract identifier that represents all nodes of the respective cluster. When the next hierarchy level is built; the clustering will be applied again. All processes are repeated until the last remaining abstract identifier (root). The results of 10 datasets showed that the co-occurrence graph can be hierarchical clustering for which the hierarchical levels ended at level 4.",,978-1-7281-4100-8,10.1109/RI2C48728.2019.8999949,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999949,Abstraction Hierarchy;Clustering;Hierarchical Clustering,Technological innovation;Clustering algorithms;Search engines,graph theory;knowledge representation;pattern clustering,inter-cluster center;hierarchical abstraction process;co-occurrence graphs;hierarchical clustering algorithm;knowledge representation,,,,14,,17-Feb-20,,,IEEE,IEEE Conferences
Analyzing and Visualizing Web Opinion Development and Social Interactions With Density-Based Clustering,基於密度的聚類分析和可視化網絡意見的發展和社會互動,C. C. Yang; T. D. Ng,"College of Information Science and Technology, Drexel University, Philadelphia , PA, USA; Digital Library Laboratory, The Chinese University of Hong Kong, Shatin, NT , Hong Kong","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",17-Oct-11,2011,41,6,1144,1155,"Due to the advancement of Web 2.0 technologies, a large volume of Web opinions is available on social media sites such as Web forums and Weblogs. These technologies provide a platform for Internet users around the world to communicate with each other and express their opinions. Analysis of developing Web opinions is potentially valuable for discovering ongoing topics of interests of the public like terrorist and crime detection, understanding how topics evolve together with the underlying social interaction between participants, and identifying important participants who have great influence in various topics of discussions. Nonetheless, the work of analyzing and clustering Web opinions is extremely challenging. Unlike regular documents, Web opinions are short and sparse text messages with noisy content. Typical document clustering techniques with the goal of clustering all documents applied to Web opinions produce unsatisfactory performance. In this paper, we investigated the density-based clustering algorithm and proposed the scalable distance-based clustering technique for Web opinion clustering. We conducted experiments and benchmarked with the density-based algorithm to show that the new algorithm obtains higher microaccuracy and macroaccuracy. This Web opinion clustering technique enables the identification of themes within discussions in Web social networks and their development, as well as the interactions of active participants. We also developed interactive visualization tools, which make use of the identified topic clusters to display social network development, the network topology similarity between topics, and the similarity values between participants.",1558-2426,,10.1109/TSMCA.2011.2113334,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738691,Density-based clustering;information visualization;social media analytics;social network analysis;web opinion mining,Clustering algorithms;Algorithm design and analysis;Visualization;Web and internet services;Discussion forums;Social network services,data analysis;data visualisation;pattern clustering;social networking (online),Web opinion development;Web 2.0 technology;density-based clustering algorithm;social interaction;Web forum;Weblogs;Web opinion analysis;Web opinion visualization;distance-based clustering technique;Web opinion clustering;interactive visualization tool;social network development,,23,,23,,24-Mar-11,,,IEEE,IEEE Journals
A General Framework for Fast Co-clustering on Large Datasets Using Matrix Decomposition,使用矩陣分解在大型數據集上進行快速聯合聚類的通用框架,F. Pan; X. Zhang; W. Wang,"Department of Computer Science, University of North Carolina at Chapel Hill. panfeng@cs.unc.edu; Department of Computer Science, University of North Carolina at Chapel Hill. xiang@cs.unc.edu; Department of Computer Science, University of North Carolina at Chapel Hill. weiwang@cs.unc.edu",2008 IEEE 24th International Conference on Data Engineering,25-Apr-08,2008,,,1337,1339,"Simultaneously clustering columns and rows (co- clustering) of large data matrix is an important problem with wide applications, such as document mining, microarray analysis, and recommendation systems. Several co-clustering algorithms have been shown effective in discovering hidden clustering structures in the data matrix. For a data matrix of m rows and n columns, the time complexity of these methods is usually in the order of mtimesn (if not higher). This limits their applicability to data matrices involving a large number of columns and rows. Moreover, an implicit assumption made by existing co-clustering methods is that the whole data matrix needs to be held in the main memory. In this paper, we propose a general framework, CRD, for co-clustering large datasets utilizing recently developed sampling- based matrix decomposition methods. The time complexity of our approach is linear in m and n. And it does not require the whole data matrix be in the main memory. Experimental results show that CRD achieves competitive accuracy to existing co-clustering methods but with much less computational cost.",2375-026X,978-1-4244-1836-7,10.1109/ICDE.2008.4497548,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497548,,Matrix decomposition;Clustering algorithms;Data analysis;Gene expression;Partitioning algorithms;Computer science;Application software;Text analysis;Computational efficiency;Data mining,computational complexity;data handling;matrix decomposition;pattern clustering;sampling methods;very large databases,fast co-clustering method;large datasets;large data matrix;time complexity;sampling-based matrix decomposition methods,,3,,16,,25-Apr-08,,,IEEE,IEEE Conferences
The document spectrum for page layout analysis,用於頁面佈局分析的文檔範圍,L. O'Gorman,"AT&T Bell Labs., Murray Hill, NJ, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,6-Aug-02,1993,15,11,1162,1173,"Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independence from skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods.<>",1939-3539,,10.1109/34.244677,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=244677,,Optical character recognition software;Text analysis;Independent component analysis;Image analysis;Image segmentation;Optical sensors;Document image processing;Magnetic analysis;Character recognition;Performance analysis,document image processing;image segmentation,document spectrum;nearest-neighbor clustering;document image processing;docstrum;structural page layout analysis;bottom-up method;skew;within-line spacings;between-line spacings;text spacings,,399,,20,,6-Aug-02,,,IEEE,IEEE Journals
Analysing anchor links to enhance the web snippet clustering technique,分析錨鏈接以增強Web代碼段聚類技術,F. A. Omara; M. Amoon; N. A. El-Fishawy; S. El-kazaz,"Faculty of Computer and Information, Cairo University, Cairo, Egypt; Dept. of Computer Science and En., Faculty of Electronic Eng., Menoufia University Menouf. Egypt; Dept. of Computer Science and En., Faculty of Electronic Eng., Menoufia University, Menouf, Egypt; Dept. of Computer Science and En., Faculty of Electronic Eng., Menoufia University, Menouf, Egypt",2012 8th International Conference on Informatics and Systems (INFOS),12-Jul-12,2012,,,SE-7,SE-11,"New generations of Search Engines aim to focus on user's needs rather than user's queries. This means personalization of the returned results to suit the user needs and expectations. In this paper, we believe that in order to enhance the personalization process, we have to enhance the data that used as an input to the personalization algorithm. According to this work, we will analyze the web structure (anchor text) to group all anchors refers to each search results and use it to enhance the search results returned search engines and then enhance the total personalization technique. In this paper, we introduce a comparative study between clustering search results using snippets returned from search engines and clustering search results using enriched snippet. The enriched snippet is a collection of the returned snippet and a very precise description of the original page of this snippet called the anchor text ofthat page. The summation of the enriched snippet and the anchor text will be called enriched snippet. Anchor text is used in a web page to point to a related document/picture/media application. Many existing approaches are based on the use of anchor-text contained in the anchor tag and analyze them to get out the information about an associated web page. In the information retrieval field, many search engines analyze the anchor text and use it as a main factor in its ranking algorithm. It is known that anchor text is the most important factor in Google ranking algorithm. In this paper, we show that these enriched snippets are powerful than the normal snippet and theses enriched snippets give higher precision to the resulted clusters.",,978-977-403-506-7,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6236574,,Java;Search engines;Web pages;Sun;Clustering algorithms;Databases;Educational institutions,Internet;pattern clustering;search engines;text analysis,anchor links;Web snippet clustering technique;search engines;personalization algorithm;Web structure;total personalization technique;clustering search results;anchor text;Web page;anchor tag;Google ranking algorithm,,1,,17,,12-Jul-12,,,IEEE,IEEE Conferences
Identifying Distinct Components of a Multi-author Document,識別多作者文檔的不同組成部分,N. Akiva; M. Koppel,"Dept. of Comput. Sci., Bar Ilan Univ., Ramat Gan, Israel; Dept. of Comput. Sci., Bar Ilan Univ., Ramat Gan, Israel",2012 European Intelligence and Security Informatics Conference,13-Sep-12,2012,,,205,209,"Given a multi-author document, we use unsupervised methods to identify distinct authorial threads. Although this problem is of great practical interest for security and forensic reasons, as well as for commercial purposes, this paper is, to the best of our knowledge, the first presentation of a general-purpose method for solving it.",,978-1-4673-2358-1,10.1109/EISIC.2012.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298832,Authorship Attribution;Document Clustering;Text Mining,Writing;Vectors;Blogs;Supervised learning;Plagiarism;Security;Educational institutions,data mining;security of data;text analysis,distinct components;multiauthor document;distinct authorial threads;security;forensic reasons;general-purpose method;text mining,,4,,12,,13-Sep-12,,,IEEE,IEEE Conferences
Keyword clustering for automatic categorization,關鍵字聚類，實現自動分類,Q. Zhao; M. Rezaei; H. Chen; P. Fr瓣nti,"School of Computing, University of Eastern Finland; School of Computing, University of Eastern Finland; School of Computing, University of Eastern Finland; School of Computing, University of Eastern Finland",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),14-Feb-13,2012,,,2845,2848,"Processing short texts is becoming a trend in information retrieval. Since the text has rarely external information, it is more challenging than document. In this paper, keyword clustering is studied for automatic categorization. To obtain semantic similarity of the keywords, a broad-coverage lexical resource WordNet is employed. We introduce a semantic hierarchical clustering. For automatic keyword categorization, a validity index for determining the number of clusters is proposed. The minimum value of the index indicates the potentially appropriate categorization. We show the result in experiments, which indicates the index is effective.",1051-4651,978-4-9906441-0-9,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460758,,Indexes;Semantics;Search engines;Clustering algorithms;Google;Internet;Humans,information retrieval;pattern clustering,keyword clustering;automatic categorization;information retrieval;semantic similarity;broad-coverage lexical resource WordNet;semantic hierarchical clustering,,,,12,,14-Feb-13,,,IEEE,IEEE Conferences
Separation of overlapped color planes for document images,分離文檔圖像的重疊色平面,D. Zheng; J. Sun; S. Naoi; M. Suwa; H. Takebe; Y. Hotta,"Fujitsu R&D Co. Ltd, Beijing, China; Fujitsu R&D Co. Ltd, Beijing, China; Fujitsu R&D Co. Ltd, Beijing, China; Fujitsu Labs Ltd, Kawasaki, Japan; Fujitsu Labs Ltd, Kawasaki, Japan; Fujitsu Labs Ltd, Kawasaki, Japan",2010 IEEE International Conference on Image Processing,3-Dec-10,2010,,,1949,1952,"Color plane separation is very useful in processing color document images. Many reported methods take it as a multi-class classification problem and work not well in overlapped color regions. This paper proposed a simple but effective linear projection based method for separating overlapped color planes. The separation task is taken as a probability problem, i.e., in the output plane, target color should have high response and the other colors should have low response, or vice versa. Furthermore, it assumes that the number of foreground colors is low, typically one to four, and overlapped areas contain mixed colors instead of opaque covering. Experimental results demonstrate the effectiveness and flexibility of our method.",2381-8549,978-1-4244-7994-8,10.1109/ICIP.2010.5653139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653139,Color planes;color mixing;color clusters;projection axis;document images,Image color analysis;Pixel;Color;Principal component analysis;Conferences;Text analysis;Feature extraction,image colour analysis,overlapped color plane;document image;color plane separation;multiclass classification problem;linear projection method,,2,,10,,3-Dec-10,,,IEEE,IEEE Conferences
Improvised fuzzy clustering using name entity recognition and natural language processing,使用名稱實體識別和自然語言處理的改進的模糊聚類,K. R. Pole; V. R. Mote,"Computer Science, ME (Appear) (Dr. Babasaheb Amedkar University), PES College of Engineering, Aurangabad, Maharashtra; Computer Science, ME, PHD (Appear) (Dr. Babasaheb Amedkar University), PES College of Engineering, Aurangabad, Maharashtra",2017 1st International Conference on Intelligent Systems and Information Management (ICISIM),30-Nov-17,2017,,,123,126,"Word wide web is considered as the most important information store in recent years. Web development expands to a great extent with new technologies. Search engines are ineffective when the number of docs in the web is multiplied. In the same way, the retrieval of queries, most of which are not related to what the user was looking for. The documents are of varied and flexible web, there are tough relationships with a web docs and a connection with others. Basically more precise clustering methods are required to detect and denominate latency with consistency to monitor significance in context. This article presents a diffused language area of topology with a diffuse cluster algorithm to discover the contextual concept of Web docs. The chief objective and mission of this research is to focus on the clustering algorithm and to discover latent semantics within a diffused linguistic text body. In addition, the scope of applications can be stretched to accompany areas such as data mining, bioinformatics, content control or information gathering, and so on. Secondly, when it is observed that recovery docs usually belongs to one of the research topic that can be distinctly different as compared to other issues, the major difference between is usually with other issues. Web content can be grouped into hierarchy issues based on diffused language measures. Web data and files that constitutes in the definition of docs are complicated and complex in nature. There are complex links within single Web docs, and there may be complex relationships with other docs. The high interactions between the terms of the docs show only vague and little ambiguous concepts. However in our case study the algorithm mentioned for development extracts the functionality of Web docs using so called random hypothetical field methods and creates a diffused linguistic topology according to the attribute associations.",,978-1-5090-4264-7,10.1109/ICISIM.2017.8122161,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122161,collaborative filtering and information filtering;Web content;linguistic topological space;Name Entity Recognition,Clustering algorithms;Algorithm design and analysis;Forensics;Pragmatics;Guidelines;Classification algorithms;Silicon,fuzzy set theory;Internet;learning (artificial intelligence);linguistics;natural language processing;pattern clustering;query processing;search engines;text analysis,diffuse cluster algorithm;diffused linguistic text body;Web content;diffused language measures;diffused linguistic topology;improvised fuzzy clustering;name entity recognition;natural language processing;word wide web;Web development;Web docs,,,,18,,30-Nov-17,,,IEEE,IEEE Conferences
Visual content based clustering of near duplicate web search images,基於視覺內容的幾乎重複的Web搜索圖像的聚類,G. Kalaiarasi; K. K. Thyagharajan,"Department of CSE, Dhanalakshmi Srinivasan College of Engineering & Technology, Chennai, India; India","2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)",2-Jun-14,2013,,,767,771,"Near-duplicate detection has received substantial attention over the past few years due to applications in copyright enforcement, organizing large image databases, increasing focus in image search, duplication elimination of logos, saving storage space by removing redundancy, etc. In case of document images, near-duplicate detection can be used to increase the efficiency of tagging the documents by reducing the need for manual inspection of the documents. In this paper, an approach is presented to detect near-duplicate images using feature extraction and clustering process. Initially as a preprocessing step, noise removal and image enhancement is done. Image features are used for feature extraction and also for clustering the images. Appropriate similarity measure is used in accordance to the clustering algorithm. Clustering of images is performed which is followed by its evaluation. From the result of evaluation, the clustering process is refined to get better clusters. Each of these clusters will have one image as a representative of that cluster and other images in the cluster is called its near-duplicates. Finally performance measure is calculated for evaluating the algorithm accuracy.",,978-1-4673-6126-2,10.1109/ICGCE.2013.6823537,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823537,Near-duplicates;Feature Extraction;Clustering;Image Enhancement,Handheld computers;Decision support systems,content-based retrieval;feature extraction;image denoising;image enhancement;image retrieval;Internet;pattern clustering;visual databases,visual content based clustering;near duplicate Web search images;near-duplicate detection;copyright enforcement;large image databases;image search;logo duplication elimination;manual document inspection;feature extraction process;feature clustering process;noise removal;image enhancement;image clustering algorithm,,3,,18,,2-Jun-14,,,IEEE,IEEE Conferences
Graph-based Growing self-organizing map for Single Document Summarization (GGSDS),用於單文檔摘要（GGSDS）的基於圖的增長自組織映射,M. Alfarra; A. M. Alfarra; A. Salahedden,"University College of Science and Technology, Khan Younis, Palestine; University College of Science and Technology, Khan Younis, Palestine; Open University Sudan, Khartoum, Sudan",2019 IEEE 7th Palestinian International Conference on Electrical and Computer Engineering (PICECE),27-Jun-19,2019,,,1,6,"The huge collection of text available represents a remarkable challenge to process and exploit it in many fields. Therefore, there is a multitude of articles that are being proposed to summarize text automatically. More accurate and higher performing models are still required for text summarization. It is one of the most common tasks of text mining. In this paper, a novel Graph-based Growing self-organizing map for Single Document Summarization (GGSDS). GGSDS is an unsupervised extractive summarization approach composed mainly of five tasks: text pre-processing, document representation, sub-topics identification, sentence ranking and finally summary generation. The entire text of a document is represented in GGSDS by one accumulative graph. The choice of this representation model supports the extraction of all required features as to achieve the most suitable summary of text, especially the shared phrases between sentences. The impact of the sub-topics on the accuracy and comprehensiveness of the generated summary is taken into account in the design of GGSDS model. For this purpose, G-GSOM is employed to cluster sentences into clusters to represent the sub-topics of text. Next, sentences are scored using TextRank algorithm under the assumption that when a sentence has more relation with others, it is considered as more important and more representative to a sub-topic. Finally, the sentences with the highest score in each cluster are selected for generating the summary. Experimental results showed that GGSDS generated summaries of single documents with more than 80% accuracy of two datasets. Furthermore, these summaries covered most of the sub-topics of the documents.",,978-1-5386-6291-5,10.1109/PICECE.2019.8747236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747236,Growing Self-Organizing Map;Graph-based;Clustering;Text Mining;Automatic Text Summarization,Self-organizing feature maps;Text mining,data mining;feature extraction;graph theory;pattern clustering;self-organising feature maps;text analysis,text summarization;text mining;unsupervised extractive summarization approach;text pre-processing;document representation;sub-topics identification;sentence ranking;summary generation;accumulative graph;GGSDS model;graph-based growing self-organizing map for single document summarization;feature extraction;G-GSOM;sentence clustering;TextRank algorithm,,1,,26,,27-Jun-19,,,IEEE,IEEE Conferences
An Improved Density-Based Method for Reducing Training Data in KNN,一種改進的基於密度的KNN訓練數據約簡方法,Y. Jing; H. Gou; Y. Zhu,"Dept. of Inf. Technol., Qiongtai Teachers Coll., Haikou, China; Dept. of Inf. Technol., Qiongtai Teachers Coll., Haikou, China; Dept. of Software Eng., Lanzhou Inst. of Technol., Lanzhou, China",2013 International Conference on Computational and Information Sciences,24-Oct-13,2013,,,972,975,"k-Nearest Neighbor (KNN) algorithm was an efficient text categorization algorithm in recall and accuracy, but the computational overhead of KNN was directly proportional to the sample size, so its classification speed was low in large-scale sample data. Aiming at this problem, the paper presented a density-based method for reducing training data, the method clustered each class of sample data into several clusters and reduced the noise sample data, and then combined some higher similar sample documents in each cluster into one document. Results of the experiment indicated that the method can reduce the computational overhead of KNN text classification, and the performance is approximately equal to those of the traditional KNN.",,978-0-7695-5004-6,10.1109/ICCIS.2013.261,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643177,KNN text classification;samples reducing;text clustering;similarity,Classification algorithms;Text categorization;Clustering algorithms;Algorithm design and analysis;Training;Noise;Support vector machine classification,pattern classification;pattern clustering;text analysis,density-based method;training data reduction;KNN algorithm;k-nearest neighbor algorithm;text categorization algorithm;sample size;classification speed;large-scale sample data;sample data clustering;noise sample data reduction;documents;KNN text classification,,4,,7,,24-Oct-13,,,IEEE,IEEE Conferences
ICFHR 2012 Competition on Handwritten Document Image Binarization (H-DIBCO 2012),ICFHR 2012手寫文檔圖像二值化競賽（H-DIBCO 2012）,I. Pratikakis; B. Gatos; K. Ntirogiannis,"Dept. of Electr. & Comput. Eng., Democritus Univ. of Thrace, Xanthi, Greece; Comput. Intell. Lab., NCSR ?Demokritos?? Athens, Greece; Comput. Intell. Lab., NCSR ?Demokritos?? Athens, Greece",2012 International Conference on Frontiers in Handwriting Recognition,31-Jan-13,2012,,,817,822,H-DIBCO 2012 is the International Document Image Binarization Competition which is dedicated to handwritten document images organized in conjunction with ICFHR 2012 conference. The objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures. This paper reports on the contest details including the evaluation measures used as well as the performance of the 24 submitted methods along with a short description of each method.,,978-1-4673-2262-1,10.1109/ICFHR.2012.216,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424498,handwritten document image;binarization;performance evaluation,Educational institutions;Image edge detection;Clustering algorithms;Distortion measurement;Filtering algorithms;PSNR;Informatics,document image processing;handwriting recognition;image recognition;performance evaluation,ICFHR 2012 competition;handwritten document image binarization;H-DIBCO 2012;International Document Image Binarization Competition;ICFHR 2012 conference;performance evaluation measures,,60,,19,,31-Jan-13,,,IEEE,IEEE Conferences
Active fuzzy clustering for collaborative filtering,主動模糊聚類用於協同過濾,N. Srinivasa; S. Medasani,"LLC, HRL Laboratories, Malibu, CA, USA; LLC, HRL Laboratories, Malibu, CA, USA",2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542),10-Jan-05,2004,3,,1697,1702 vol.3,"We present a fuzzy clustering approach to collaborative filtering. Our approach allows for users to be clustered into multiple user groups. Furthermore, our approach is active in that it can rapidly adapt to both short and long term user interest changes. Our approach is capable of on-line collaborative filtering with simultaneous clustering at the document content level, user group level, as well as document clustering based on similarity of user interests. We demonstrate the various features of the approach using a synthetic example.",1098-7584,0-7803-8353-2,10.1109/FUZZY.2004.1375436,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1375436,,Collaboration;Filtering;Clustering algorithms;Collaborative work;Neural networks;Fuzzy neural networks;Laboratories;Fuzzy systems;Prototypes;Content based retrieval,fuzzy set theory;pattern clustering;filtering theory,active fuzzy clustering approach;multiple user groups;online collaborative filtering;document content level;user group level;document clustering,,2,,12,,10-Jan-05,,,IEEE,IEEE Conferences
Comparing Different Term Weighting Schemas for Topic Modeling,比較不同的術語加權方案進行主題建模,C. Truica; F. Radulescu; A. Boicea,"Comput. Sci. & Eng. Dept., Univ. Politeh. of Bucharest, Bucharest, Romania; Comput. Sci. & Eng. Dept., Univ. Politeh. of Bucharest, Bucharest, Romania; Comput. Sci. & Eng. Dept., Univ. Politeh. of Bucharest, Bucharest, Romania",2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),26-Jan-17,2016,,,307,310,"Topic Modeling is a type of statistical model that tries to determine the topics present in a corpus of documents. The accuracy measures applied to clustering algorithm can also be used to assess the accuracy of topic modeling algorithms because determining topics for documents is similar with clustering them. This paper presents an experimental validation regarding the accuracy of Latent Dirichlet Allocation in comparison with Non-Negative Matrix Factorization and K-Means. The experiments use different weighting schemas when constructing the document-term matrix to determine if the accuracy of the algorithm improves. Two well known, already labeled text corpora are used for testing. The Purity and Adjusted Rand Index are used to evaluate the accuracy. Also, a time performance comparison regarding the run-time of these algorithms is presented.",2470-881X,978-1-5090-5707-8,10.1109/SYNASC.2016.055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829627,Topic Modeling Evaluation;Clustering Evaluation;LDA;NMF;K-Means;Purity;Adjusted Rand Index,Computational modeling;Indexes;Clustering algorithms;Manuals;Numerical models;Semantics;Mathematical model,document handling;matrix algebra;pattern clustering;statistical analysis,term weighting schemas;topic modeling;statistical model;document corpus;clustering algorithm;latent Dirichlet allocation;document-term matrix;labeled text corpora;adjusted rand index,,5,,12,,26-Jan-17,,,IEEE,IEEE Conferences
Topic Detection in Conversational Telephone Speech Using CNN with Multi-stream Inputs,使用具有多流輸入的CNN，在通話電話語音中進行主題檢測,J. Sun; W. Guo; Z. Chen; Y. Song,"National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",16-Apr-19,2019,,,7285,7289,"Topic detection for conversational telephone speech (CTS) is addressed in this paper. The low accuracy of automatic speech recognition (ASR) will cause severe performance deterioration for topic detection. To make up for this, we adopt two ASR systems, HMM-BiLSTM and CTC systems, to provide complementary information for topic detection. After obtaining two sets of different recognized transcriptions, a CNN with multi-stream inputs is trained, and the pooling layer serves as document representations. Finally, element-wise summation of document representations from two streams is used as distributed representations of the documents, which are fed into agglomerative hierarchical clustering (AHC) algorithms to obtain clustering results. The experiments on a Japanese speech corpus demonstrate that the proposed approach can significantly improve the performance of topic detection.",2379-190X,978-1-4799-8131-1,10.1109/ICASSP.2019.8682201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682201,topic detection;consensus analysis;agglomerative hierarchical clustering,Hidden Markov models;Training;Acoustics;Task analysis;Decoding;Telephone sets;Clustering algorithms,convolutional neural nets;document handling;hidden Markov models;natural language processing;pattern clustering;speech recognition,topic detection;conversational telephone speech;multistream inputs;automatic speech recognition;document representations;CNN;ASR systems;HMM-BiLSTM systems;CTC systems;document representation;distributed representation;Japanese speech corpus;agglomerative hierarchical clustering algorithms;AHC algorithms,,2,,22,,16-Apr-19,,,IEEE,IEEE Conferences
Topic classification and clustering on Indonesian complaint tweets for bandung government using supervised and unsupervised learning,使用監督學習和無監督學習對萬隆政府印尼投訴推文進行主題分類和聚類,T. Pratama; A. Purwarianti,"Institut Teknologi Bandung, Indonesia; Institut Teknologi Bandung, Indonesia","2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)",2-Nov-17,2017,,,1,6,"Seeing the public of Bandung city as an active social media user, Bandung government provides channel in Twitter for citizen to report their complaints. In order to make the citizen complaint monitoring easier, there is a need to automatically detect the topics of complaint tweets (written in Indonesian language) in order to assist the government in managing the complaints reported. In this paper, a system to detect the topics of Indonesian complaint tweets automatically using supervised learning and unsupervised learning approaches is proposed. The supervised learning approach is implemented to classify complaint tweets topic, whereas the unsupervised learning approach is used to cluster complaint tweets based on the similarity of detail information contained in the complaints. Both the supervised learning and the unsupervised learning approaches are required to classify the topics of a tweet and to capture the detail information from each detected topic. The topics are classified using single label and multi label classification. The supervised learning approach is evaluated using accuracy, precision, recall, and F1 score. Three supervised machine learning algorithms are evaluated: Sequential Minimal Optimization, Na簿ve Bayes Multinomial, and Random Forests. The best algorithm for single label topic classification is SMO, with the accuracy average of 95%, whereas the best algorithm for multi-label topic classification is Random Forests, with 97.92% accuracy, 98.74% precision, 98.36% recall, and 98.44% F1 score. In the unsupervised learning approach, Clustering Index Value is used to evaluate the topic clusters detected. Two unsupervised learning algorithms are evaluated; Exemplar Based Topic Detection and Document Pivot Technique using TF-IDF. Exemplar Based Topic Detection has the best performance for detecting detail topic clusters with Clustering Index Value of 0.9653.",,978-1-5386-3001-3,10.1109/ICAICTA.2017.8090981,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090981,Indonesian complaint text;topic classification;topic clustering;supervised learning;unsupervised learning,Unsupervised learning;Training;Supervised learning;Twitter;Government;Classification algorithms;Clustering algorithms,Bayes methods;government data processing;optimisation;pattern classification;pattern clustering;random processes;social networking (online);unsupervised learning,clustering;Indonesian complaint tweets;citizen complaint monitoring;unsupervised learning approach;supervised learning approach;supervised machine learning algorithms;single label topic classification;multilabel topic classification;Bandung government;Twitter;sequential minimal optimization;Na簿ve Bayes Multinomial;Random Forests;Clustering Index Value;Exemplar Based Topic Detection;Document Pivot Technique,,,,20,,2-Nov-17,,,IEEE,IEEE Conferences
Comparison of two schemes for automatic keyword extraction from MEDLINE for functional gene clustering,兩種從MEDLINE自動提取功能基因聚類方案的比較,Ying Liu; B. J. Ciliax; K. Borges; V. Dasigi; A. Ram; S. B. Navathe; R. Dingledine,"Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA; NA; NA; NA","Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.",8-Oct-04,2004,,,394,404,"One of the key challenges of microarray studies is to derive biological insights from the unprecedented quantities of data on gene-expression patterns. Clustering genes by functional keyword association can provide direct information about the nature of the functional links among genes within the derived clusters. However, the quality of the keyword lists extracted from biomedical literature for each gene significantly affects the clustering results. We extracted keywords from MEDLINE that describe the most prominent functions of the genes, and used the resulting weights of the keywords as feature vectors for gene clustering. By analyzing the resulting cluster quality, we compared two keyword weighting schemes: normalized z-score and term frequency-inverse document frequency (TFIDF). The best combination of background comparison set, stop list and stemming algorithm was selected based on precision and recall metrics. In a test set of four known gene groups, a hierarchical algorithm correctly assigned 25 of 26 genes to the appropriate clusters based on keywords extracted by the TDFIDF weighting scheme, but only 23 of 26 with the z-score method. To evaluate the effectiveness of the weighting schemes for keyword extraction for gene clusters from microarray profiles, 44 yeast genes that are differentially expressed during the cell cycle were used as a second test set. Using established measures of cluster quality, the results produced from TFIDF-weighted keywords had higher purity, lower entropy, and higher mutual information than those produced from normalized z-score weighted keywords. The optimized algorithms should be useful for sorting genes from microarray lists into functionally discrete clusters.",,0-7695-2194-0,10.1109/CSB.2004.1332452,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1332452,,Clustering algorithms;Abstracts;Data mining;Educational institutions;Frequency;Testing;Biomedical measurements;Nervous system;Venus;Fungi,genetics;cellular biophysics;biology computing;entropy;optimisation;pattern clustering;information analysis,automatic keyword extraction;MEDLINE;functional gene clustering;microarray;gene-expression patterns;functional keyword association;biomedical literature;feature vectors;normalized z-score;term frequency-inverse document frequency;background comparison set;stop list;stemming algorithm;weighting schemes;yeast genes;cell cycle;optimized algorithms;high purity;low entropy;high mutual information,"Artificial Intelligence;Cluster Analysis;Information Storage and Retrieval;MEDLINE;Multigene Family;Natural Language Processing;Oligonucleotide Array Sequence Analysis;Structure-Activity Relationship;Vocabulary, Controlled",7,,25,,8-Oct-04,,,IEEE,IEEE Conferences
Words Clustering Based on Keywords Indexing from Large-scale Categorization Corpora,大規模分類語料庫中基於關鍵詞索引的詞聚類,L. Hua,"Coll. of Chinese Language & Culture, Jinan Univ., Guangzhou, China",2009 Fifth International Conference on Information Assurance and Security,9-Oct-09,2009,1,,407,410,"Keywords are indexed automatically for large-scale categorization corpora. Indexed keywords of more than 20 documents are selected as seed words, thus overcoming subjectivity of selecting seed words in clustering; at the same time, clustering is limited to particular category corpora and keywords indexed feature extraction method is adopted to obtain domanial words automatically, thus reducing noise of similarity calculation.",,978-0-7695-3744-3,10.1109/IAS.2009.271,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284129,Categorization corpora;Keywords indexing;Domanial words;Clustering,Indexing;Large-scale systems;Vocabulary;Statistics;Feature extraction;Noise reduction;Web pages;Materials science and technology;Societies;Information security,document handling;feature extraction;indexing,words clustering;keywords indexing;large-scale categorization;feature extraction,,,,4,,9-Oct-09,,,IEEE,IEEE Conferences
Used Hierarchical Topic to Generate Multi-document Automatic Summarization,使用分層主題生成多文檔自動匯總,X. Yong-Dong; Q. Guang-Ri; Z. Ting-Bin; W. Ya-Dong,"Sch. of Comput. Sci. of Technol., Harbin Inst. of Technol. at WeiHai, Wei Hai, China; Sch. of Comput. Sci. of Technol., Harbin Inst. of Technol. at WeiHai, Wei Hai, China; Sch. of Comput. Sci. of Technol., Harbin Inst. of Technol. at WeiHai, Wei Hai, China; Sch. of Comput. Sci. of Technol., Harbin Inst. of Technol. at WeiHai, Wei Hai, China",2011 Fourth International Conference on Intelligent Computation Technology and Automation,15-Apr-11,2011,1,,295,298,"A concept of hierarchical topic is proposed for multi-document automatic summarization task, which used multi-layer topic tree structure to represent the text set. Each node in the topic tree represent specific topic and contains multiple similar sentences in the text set. The hierarchical topic structure may describe accurately the similarity between sentences at different levels of granularity. Therefore it can reflect the real content of the text set than single layer topic set. And can be used to find the important sentences in the important topic which can compose the summary of the text set. Concretely, a series of algorithms including building hierarchical topic tree, key sentences extraction based on hierarchical topic tree and summarization generation are proposed. The capability of summarization system is testified by sets of experiments and shows good result.",,978-1-61284-289-9,10.1109/ICICTA.2011.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5750614,Natural lauguage processing;Multiple document automatic summarization;Hierarchical topic,Clustering algorithms;Redundancy;Feature extraction;Training;Entropy;Partitioning algorithms;Periodic structures,abstracting;set theory;text analysis,multiple document automatic summarization;multilayer topic tree structure;key sentence extraction;hierarchical topic,,1,,8,,15-Apr-11,,,IEEE,IEEE Conferences
Research in concept lattice based automatic document ranking,基於概念格的文檔自動排名研究,Tang Jun; Ya-Jin Du; Jie-Feng Shen,"Coll. of Comput. & Math.-Phys. Sci., Xihua Univ., Chengdu, China; Coll. of Comput. & Math.-Phys. Sci., Xihua Univ., Chengdu, China; Coll. of Comput. & Math.-Phys. Sci., Xihua Univ., Chengdu, China",2005 International Conference on Machine Learning and Cybernetics,7-Nov-05,2005,9,,5560,5565 Vol. 9,"The research on similarity for measuring document relevance is an important field in information retrieval. Many researchers are using concept lattice defined in formal concept analysis (FC A) as a basis for measuring query-document relevance in text retrieval, i.e. concept lattice-based ranking (CLR). However, formal concept analysis's notion of similarity for measuring documents relevance in text retrieval is only based on the shortest path linking the query to the document. It is not well defined. To resolve the problems of this approach, first, we evaluate reasonable different weights of edges in the Hasse diagram based on the conceptual generality or specificity. Second, we present a user profile based on concept lattice, and the algorithm for constructing concept lattice based user profile is provided. Third, we present a combination CLR approach by measuring the similarity among query, user profile and document according to the relation between query and user interest based on concept lattice. Our experiment shows that documents retrieved by our combination CLR approach achieve a higher measure of precision than the traditional CLR approach.",2160-1348,0-7803-9091-1,10.1109/ICMLC.2005.1527927,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1527927,Information retrieval;information filtering;Formal Concept Analysis;concept lattice;user profile best-match ranking;clustering-based ranking;Concept lattice-based ranking,Lattices;Information retrieval;Joining processes;Information analysis;Indexing;Educational institutions;Information filtering;Navigation;Knowledge management;Vocabulary,information filtering;text analysis;pattern clustering;knowledge representation;relevance feedback,automatic document ranking;document relevance;information retrieval;formal concept analysis;query-document relevance;text retrieval;Hasse diagram;information filtering;clustering-based ranking;concept lattice-based ranking,,,,16,,7-Nov-05,,,IEEE,IEEE Conferences
Applying Semantic Suffix Net to suffix tree clustering,將語義後綴網應用於後綴樹聚類,J. Janruang; S. Guha,"Computer Science & Information Management Program, Asian Institute of Technology, Pathumthani, Thailand; Computer Science & Information Management Program, Asian Institute of Technology, Pathumthani, Thailand",2011 3rd Conference on Data Mining and Optimization (DMO),4-Aug-11,2011,,,146,152,"In this paper we consider the problem of clustering snippets returned from search engines. We propose a technique to invoke semantic similarity in the clustering process. Our technique improves on the well-known STC method, which is a highly efficient heuristic for clustering web search results. However, a weakness of STC is that it cannot cluster semantic similar documents. To solve this problem, we propose a new data structure to represent suffixes of a single string, called a Semantic Suffix Net (SSN). A generalized semantic suffix net is created to represent suffixes of a set of strings by using a new operator to partially combine nets. A key feature of this new operator is to find a joint point by using semantic similarity and string matching; net pairs combination then begins at that joint point. This logic causes the number of nodes and branches of a generalized semantic suffix net to decrease. The operator then uses the line of suffix links as a boundary to separate the net. A generalized semantic suffix net is then incorporated into the STC algorithm so that it can cluster semantically similar snippets. Experimental results show that the proposed algorithm improves upon conventional STC.",2155-6946,978-1-61284-212-7,10.1109/DMO.2011.5976519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976519,semantic suffix net;semantic web search results clustering;data mining;text mining,Semantics;Clustering algorithms;Joints;Algorithm design and analysis;Pediatrics;Data structures,data structures;document handling;information retrieval;pattern clustering;search engines;string matching;trees (mathematics),suffix tree clustering;semantic suffix net;data structure;semantic similarity;string matching;STC algorithm;search engines,,,,16,,4-Aug-11,,,IEEE,IEEE Conferences
Affinity-based probabilistic reasoning and document clustering on the WWW,在WWW上基於親和力的概率推理和文檔聚類,Mei-Ling Shyu; Shu-Ching Chen; Chi-Min Shu,"Dept. of Electr. & Comput. Eng., Miami Univ., Coral Gables, FL, USA; NA; NA",Proceedings 24th Annual International Computer Software and Applications Conference. COMPSAC2000,6-Aug-02,2000,,,149,154,"The World Wide Web (WWW) has become one of the fastest growing applications on the Internet today. More and more information sources have linked online through WWW, but finding information on the WWW is also a great challenge. For most users, the information retrieved is not well organized and the access time is considered high on the WWW currently. Therefore, there is a need to develop a good mechanism to organize and manage the tremendous size and various kinds of information to facilitate the functionality of a search engine for information retrieval on the WWW. In response to such a demand we propose a Markov Model Mediator (MMM) mechanism which employs affinity based data mining techniques to organize and manage the information sources so that the most relevant documents are clustered together to achieve higher recall and precision values for information retrieval on the WWW.",0730-3157,0-7695-0792-1,10.1109/CMPSAC.2000.884705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=884705,,World Wide Web;Information retrieval;Data mining;Web sites;Internet;Search engines;Web server;Association rules;Computer science;Electrical safety,information resources;information retrieval;search engines;Markov processes;data mining;document handling;inference mechanisms,affinity based probabilistic reasoning;document clustering;WWW;World Wide Web;Internet;information sources;access time;search engine;information retrieval;Markov Model Mediator;MMM mechanism;affinity based data mining techniques;most relevant documents;precision values,,3,,16,,6-Aug-02,,,IEEE,IEEE Conferences
Locality-sensitive hashing scheme for Bangla news article clustering using bloom filter,使用Bloom過濾器的Bangla新聞文章聚類的局部敏感哈希方案,,,"2017 International Conference on Electrical, Computer and Communication Engineering (ECCE)",27-Apr-17,2017,,,17,21,"CXustering mechanism helps to organise a large amount of data items by grouping the similar items into meaningful clusters. A successful clustering approach depends on an effective similarity search algorithm. Similarity search problem for text documents can be turned into a problem domain of sets by using the method called ?shingling?? Characteristic matrix of sets is created by searching the shingles in each document. That's why the complexity to build the matrix is significantly high when the dataset is very large in size. Search time can be radically lessened if the characteristic matrix is made by utilizing Bloom Filter which reduces the search time to a constant time. Finding the similarity among all pairs of the set is a major issue since it takes O(n2) time to compare n sets. Locality-sensitive Hashing drastically diminishes the time complexity of searching by generating candidate pairs. Locality-sensitive Hashing focuses the similarity search on candidate pairs that are most likely to be similar. In this paper, the scheme for Bengali news article clustering based on the similarity search by Locality-sensitive Hashing(LSH) is presented.",,978-1-5090-5627-9,10.1109/ECACE.2017.7912871,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912871,Characteristic Matrix;Signature Matrix;Min-Hashing;Locality Sensitive Hashing;Candidate pair;Bloom Filter,Decision support systems;Handheld computers,computational complexity;data mining;data structures;information resources;matrix algebra;natural languages;pattern clustering;search problems;set theory;text analysis,Bangla news article clustering;Bloom filter;text document similarity search problem;shingling;search time;characteristic matrix;locality-sensitive hashing;time complexity;Bengali news article clustering;LSH,,,,16,,27-Apr-17,,,IEEE,IEEE Conferences
A novel method for document summarization using Word2Vec,一種使用Word2Vec進行文檔匯總的新方法,Z. Wang; L. Ma; Y. Zhang,"Department of Computer Science, Georgia State University, Atlanta, GA 30302-5060, USA; Department of Computer Science, Georgia State University, Atlanta, GA 30302-5060, USA; Department of Computer Science, Georgia State University, Atlanta, GA 30302-5060, USA",2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-Feb-17,2016,,,523,529,"Texting mining is a process to extract useful patterns and information from large volume of unstructured text data. Unlike other quantitative data, unstructured text data cannot be directly utilized in machine learning models. Hence, data pre-processing is an essential step to remove vague or redundant data such as punctuations, stop-words, low-frequency words in the corpus, and re-organize the data in a format that computers can understand. Though existing approaches are able to eliminate some symbols and stop-words during the pre-processing step, a portion of words are not used to describe the documents' topics. These irrelevant words not only waste the storage that lessen the efficiency of computing, but also lead to confounding results. In this paper, we propose an optimization method to further remove these irrelevant words which are not highly correlated to the documents' topics. Experimental results indicate that our proposed method significantly compresses the documents, while the resulting documents remain a high discrimination in classification tasks; additionally, storage is greatly reduced according to various criteria.",,978-1-5090-3846-6,10.1109/ICCI-CC.2016.7862087,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862087,Data pre-processing;Word2Vec;clustering;document similarity;document representation,Semantics;Mathematical model;Context;Automobiles;Dogs;Cats;Vocabulary,data mining;learning (artificial intelligence);optimisation;pattern classification;text analysis,classification tasks;documents compression;optimization;data reorganization;low-frequency words;stop-words;punctuations;data preprocessing;machine learning;unstructured text data;text mining;Word2Vec;document summarization,,2,,10,,23-Feb-17,,,IEEE,IEEE Conferences
Reference metadata extraction from scientific papers,從科學論文中提取參考元數據,Z. Guo; H. Jin,"Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Cluster & Grid Comput. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China","2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies",2-Jan-12,2011,,,45,49,"Bibliographical information of scientific papers is of great value since the Science Citation Index is introduced to measure research impact. Most scientific documents available on the web are unstructured or semi-structured, and the automatic reference metadata extraction process becomes an important task. This paper describes a framework for automatic reference metadata extraction from scientific papers. Our system can extract title, author, journal, volume, year, and page from scientific papers in PDF. We utilize a document metadata knowledge base to guide the reference metadata extraction process. The experiment results show that our system achieves a high accuracy.",2379-5352,978-1-4577-1807-6,10.1109/PDCAT.2011.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118965,metadata extraction;rule-based approach;reference,Data mining;Portable document format;Knowledge based systems;Accuracy;Libraries;Semantics;Hidden Markov models,citation analysis;document handling;information retrieval;Internet;knowledge based systems;meta data;natural sciences computing,automatic reference metadata extraction process;scientific papers;Bibliographical Information;science citation index;scientific documents;unstructured metadata extraction process;semistructured metadata extraction process;document metadata knowledge base;Web,,8,,13,,2-Jan-12,,,IEEE,IEEE Conferences
Semantic Web Content Analysis: A Study in Proximity-Based Collaborative Clustering,語義Web內容分析：基於鄰近的協作聚類研究,V. Loia; W. Pedrycz; S. Senatore,"Salerno Univ., Fisciano; NA; NA",IEEE Transactions on Fuzzy Systems,6-Dec-07,2007,15,6,1294,1312,"The semantic vision of the Web involves the processing of data by automated tools as well as by people, where the association of meaning with content, facilitates the search, the interoperability and the composition of several services. The Semantic Web forms a new scenario, where advanced methods and techniques are developed for the description, the retrieval and filtering of Web-based content. In the light of existing challenges and open issues concerning the actual cyberspace, this study proposes an approach for binding the ""semantic"" facet with the usual textual one, that together constitutes a typical web page, or specifically, a semantic web document. Through the use of unsupervised learning, we offer a new alternative of organizing web documents which emphasizes a direct separation between the syntactic and semantic facets of the web information. In this study, we discuss a collaborative proximity-based fuzzy clustering and show how this type of clustering is used to discover a structure of web information by a prudent reliance on the structures in the spaces of semantics and data. The method focuses on the reconciliation between the two separated facets of web information and a combination of results leading to a comprehensive data organization. The information arranged in this manner can provide an integral description of web resources, becoming in this manner an essential technique for the next generation of Web search engines.",1941-0034,,10.1109/TFUZZ.2006.889970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358788,Information retrieval;metadata;proximity-based collaborative clustering;Semantic Web;spaces of semantics and data;XML-based technologies;Information retrieval;metadata;proximity-based collaborative clustering;Semantic Web;spaces of semantics and data;XML-based technologies,Semantic Web;Collaboration;Content based retrieval;Information filtering;Information filters;Web pages;Unsupervised learning;Organizing;Web search;Search engines,content management;fuzzy set theory;groupware;information filtering;open systems;pattern clustering;search engines;semantic Web;Web sites,semantic Web content analysis;proximity-based collaborative clustering;semantic vision;automated tools;interoperability;content retrieval;content filtering;Web page;semantic Web document;collaborative proximity-based fuzzy clustering;Web information;comprehensive data organization;Web search engines,,52,,38,,6-Dec-07,,,IEEE,IEEE Journals
Data Extraction and Integration for Scholar Recommendation System,學者推薦系統的數據提取與集成,J. Chakraborty; G. Thopugunta; S. Bansal,"Sch. of Comput., Inf., Decision Syst. Eng., Arizona State Univ. Tempe, Tempe, AZ, USA; Sch. of Comput., Inf., Decision Syst. Eng., Arizona State Univ. Tempe, Tempe, AZ, USA; Sch. of Comput., Inf., Decision Syst. Eng., Arizona State Univ. Tempe, Tempe, AZ, USA",2018 IEEE 12th International Conference on Semantic Computing (ICSC),12-Apr-18,2018,,,397,402,"Recommendation systems have been an integral part of massive open online courses (MOOCs). With a large amount of availability of data and resources, recommending scholars and professors through general reviews and academic advisor applications has become a tiresome job. Finding professors and scholars relevant to a student's area of interest involves a combination of multiple factors like field of study, depth of research area, research background of professors, ongoing research opportunities, etc. As recommending scholars and professors deals with so many different factors, it is very complex and unreliable when done manually. In this paper, we present a content-based mining approach to go through all relevant resources, extract required information, and use it to recommend a list of scholars based on student's area of interest. For our experimental model, we gathered information about a number of professors at our institution from various web resources such as IEEE, Springer, ACM, Sciencedirect, arxiv and department website. We use topic modeling and clustering algorithms in our content-based mining approach. We present a comparative analysis of the following topic model algorithms: latent dirichlet allocation (LDA), hierarchical dirichlet process(HDP), latent semantic analysis (LSA) and clustering techniques: k-means and hierarchical clustering in determining the most accurate recommendation list of professors or scholars.",,978-1-5386-4408-9,10.1109/ICSC.2018.00079,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334503,Recommendation System;Topic modeling;Latent Dirichlet Allocation;Hierarchical Dirichlet Process;Latent Semantic Analysis;Clustering;k-means clustering;Hierarchical clustering,Clustering algorithms;Data mining;Elbow;Semantics;Data collection;Probability distribution;Data models,computer aided instruction;data integration;data mining;document handling;educational courses;pattern clustering;recommender systems,data extraction;scholar recommendation system;massive open online courses;general reviews;academic advisor applications;tiresome job;student;research area;ongoing research opportunities;relevant resources;web resources;topic modeling;clustering algorithms;topic model algorithms;latent semantic analysis;accurate recommendation list;data integration;latent dirichlet allocation;LDA;hierarchical dirichlet process;HDP;LSA;clustering techniques;content-based mining approach;IEEE;Springer;ACM;arxiv;Sciencedirect,,,,26,,12-Apr-18,,,IEEE,IEEE Conferences
Extracting most significant data about the user queries from the search engine by K-means++ algorithm,通過K-means ++算法從搜索引擎中提取有關用戶查詢的最重要數據,A. Gomathi; K. Raja,"Department of Computer Science and Engineering, Narasu's Sarathy Institute of Technology, Poosaripatty, Salem, Tamil Nadu, India; Department of Computer Science and Engineering, Alpha College of Engineering, Thirumazhisai, Chennai, Tamil Nadu, India","2016 International Conference on Emerging Trends in Engineering, Technology and Science (ICETETS)",24-Oct-16,2016,,,1,5,"Today every process requires very important data as well as updated data about that process or work. In order to acquire those data, employees from various fields are searching through different search engines. Only very few times search engines are helping to get the users expected data but many times they are providing only the approximate data about the user expectation. To avoid this state, we used the algorithm named K-means++ over the search engine documents to list the most relevant information, because this algorithm uses the special mathematical method to find the out the successive cluster center of each cluster documents and only the first center is random selection from the data unlike K-means algorithm, which randomly selects all the cluster centers. Segmentation fusion is applied to provide the most resultant list, which is accepting as input of each cluster's documents gradually increasing manner. Performance of the K-means++ algorithm is compared and evaluated with the measures like Mean Average Precision and F-measure, which shows the novel algorithm has providing the better relevant result than other traditional algorithms. Finally, this algorithm also works well for large data sets. So, the researchers can utilize this algorithm to their innovative ideas comes to reality with the performance matrices of high relevancies such as precision and recall.",,978-1-4673-6725-7,10.1109/ICETETS.2016.7603007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603007,K-means++ algorithm;Search engines;Segmentation fusion;Precision;Recall,Clustering algorithms;Search engines;Algorithm design and analysis;Semantics;Probabilistic logic;Classification algorithms,document handling;pattern clustering;query processing;search engines;sensor fusion,significant data extraction;user queries;k-means++ algorithm;search engine documents;successive cluster center;cluster documents;segmentation fusion,,,,21,,24-Oct-16,,,IEEE,IEEE Conferences
Text Extraction and Document Image Segmentation Using Matched Wavelets and MRF Model,使用匹配小波和MRF模型的文本提取和文檔圖像分割,S. Kumar; R. Gupta; N. Khanna; S. Chaudhury; S. D. Joshi,"IBM India Res. Lab., Delhi; NA; NA; NA; NA",IEEE Transactions on Image Processing,16-Jul-07,2007,16,8,2117,2128,"In this paper, we have proposed a novel scheme for the extraction of textual areas of an image using globally matched wavelet filters. A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images. We have extended our text extraction scheme for the segmentation of document images into text, background, and picture components (which include graphics and continuous tone images). Multiple, two-class Fisher classifiers have been used for this purpose. We also exploit contextual information by using a Markov random field formulation-based pixel labeling scheme for refinement of the segmentation results. Experimental results have established effectiveness of our approach.",1941-0042,,10.1109/TIP.2007.900098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271529,$\alpha $-expansion;document image;globally matched wavelets (GMWs);matched wavelets;Markov random field (MRF);scene image,Image segmentation;Data mining;Discrete wavelet transforms;Matched filters;Graphics;Markov random fields;Layout;Image color analysis;Asia;Labeling,document image processing;feature extraction;filtering theory;image classification;image segmentation;Markov processes;pattern clustering;random processes;text analysis;wavelet transforms,text extraction;document image segmentation;globally matched wavelet filters;clustering-based technique;groundtruth images;background components;picture components;two-class Fisher classifiers;contextual information;Markov random field formulation;MRF-based pixel labeling scheme,"Algorithms;Artificial Intelligence;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Natural Language Processing;Pattern Recognition, Automated;Printing;Reproducibility of Results;Sensitivity and Specificity",72,,25,,16-Jul-07,,,IEEE,IEEE Journals
Line Segmentation Approach for Ancient Palm Leaf Manuscripts Using Competitive Learning Algorithm,基於競爭學習算法的古棕櫚葉手稿線分割方法,D. Valy; M. Verleysen; K. Sok,"ICTEAM Inst., Univ. Catholique de Louvain, Louvain, Belgium; ICTEAM Inst., Univ. Catholique de Louvain, Louvain, Belgium; Dept. of Inf. & Commun. Eng., Inst. of Technol. of Cambodia, Phnom Penh, Cambodia",2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR),16-Jan-17,2016,,,108,113,"Line segmentation is very crucial in handwritten text recognition/analysis task. A new text line extraction scheme based on a data clustering algorithm is proposed. Our approach starts by determining the number of lines and setting up text line mid points' initial positions using a modified piece-wise projection profile technique. We apply afterwards competitive learning algorithm to adaptively move those mid points according to the geometrical information of connected components in the document page to form lines. Borders between text lines are defined so that they can be used to separate touching components that spread over multiple lines. The proposed method is robust in handling documents with skewed, fluctuated, or discontinued text lines. Experimental evaluations were made on a data set of Khmer ancient palm leaf manuscripts.",2167-6445,978-1-5090-0981-7,10.1109/ICFHR.2016.0032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814048,handwritten document analysis;text line segmentation;competitive learning algorithm,Clustering algorithms;Writing;Image segmentation;Histograms;Handwriting recognition;Data mining;Algorithm design and analysis,document image processing;handwritten character recognition;history;image segmentation;learning (artificial intelligence);pattern clustering;text analysis;text detection,line segmentation approach;competitive learning algorithm;handwritten text recognition task;text line extraction scheme;data clustering algorithm;text line mid points;modified piece-wise projection profile technique;geometrical information;connected components;document page;touching components;document handling;skewed text lines;fluctuated text lines;discontinued text lines;Khmer ancient palm leaf manuscripts;handwritten text analysis task,,5,,18,,16-Jan-17,,,IEEE,IEEE Conferences
Content-Based Clustered P2P Search Model Depending on Set Distance,基於內容的基於距離的集群式P2P搜索模型,J. Wang; S. Yang,"University of Science and Technology of China, China; University of Science and Technology of China, China",2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops,8-Jan-07,2006,,,471,476,"The main issues that affect query efficiency and search cost in content-based unstructured P2P search system are the complexity of computing the similarity of the documents brought by high dimensions and the great deal of redundant messages coming with flooding. This paper defines the documents similarity by the way of set distance. This method restrains the complexity of computing the document similarity in linear time. Also, this paper clusters the peers based on content by their set distance to reduce the query time and redundant messages. Simulations show that the content-based search model constructed by set distance not only has higher recall, but also reduce the search cost and query time to the rate of 40% and 30% of Gnutella",,0-7695-2749-3,10.1109/WI-IATW.2006.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053295,Peer-to-Peer;Gnutella;Distributed Hash Tables;Set Distance;Vector Space Model,Costs;Floods;Computational modeling;Contracts;Computer science;Peer to peer computing;Aerospace industry;Scalability;Real time systems;Network topology,document handling;peer-to-peer computing;query processing,content-based clustered P2P search model;query efficiency;redundant messages;set distance;documents similarity,,1,,3,,8-Jan-07,,,IEEE,IEEE Conferences
Document warehousing based on a multimedia database system,基於多媒體數據庫系統的文檔倉儲,H. Ishikawa; K. Kubota; Y. Noguchi; K. Kato; M. Ono; N. Yoshizawa; Y. Kanemasa,"Software Lab., Fujitsu Labs. Ltd., Kawasaki, Japan; NA; NA; NA; NA; NA; NA",Proceedings 15th International Conference on Data Engineering (Cat. No.99CB36337),6-Aug-02,1999,,,168,173,"Nowadays, structured data such as sales and business forms are stored in data warehouses for decision makers to use. Further, unstructured data such as emails, HTML texts, images, videos, and office documents are increasingly accumulated in personal computer storage due to spread of mailing, WWW, and word processing. Such unstructured data, or what we call multimedia documents, are larger in volume than structured data and precious as corporate assets as well. So we need a document warehouse as a software framework where multimedia documents are analyzed and managed for corporate-wide information sharing and reuse like a data warehouse for structured data. We describe a prototype document warehouse system, which supports management of simple and compound documents, keyword-based and content-based retrieval, rule-based classification, SOM-based clustering, and XML data query and view rules.",1063-6382,0-7695-0071-4,10.1109/ICDE.1999.754921,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=754921,,Warehousing;Multimedia databases;Data warehouses;Image storage;Marketing and sales;HTML;Videos;Microcomputers;World Wide Web;Text processing,data warehouses;multimedia databases;document handling;business forms;marketing data processing;office automation;electronic mail;business data processing,document warehousing;multimedia database system;structured data;sales forms;business forms;data warehouses;decision makers;unstructured data;email;HTML texts;images;videos;office documents;personal computer storage;WWW;mailing;word processing;corporate assets;software framework;corporate-wide information sharing;corporate-wide information reuse;simple documents;compound documents;keyword-based retrieval;content-based retrieval;rule-based classification;SOM-based clustering;XML data query rules;XML data view rules,,6,,6,,6-Aug-02,,,IEEE,IEEE Conferences
Comparing similarity of concepts identified by temporal patterns of terms in biomedical research documents,比較由生物醫學研究文檔中術語的時間模式確定的概念的相似性,S. Tsumoto; H. Abe,"Department of Medical Informatics, Faculty of Medicine, Shimane University, 89-1 Enya-cho, Izumo 693-8501 JAPAN; Department of Medical Informatics, Faculty of Medicine, Shimane University, 89-1 Enya-cho, Izumo 693-8501 JAPAN",2012 IEEE 11th International Conference on Cognitive Informatics and Cognitive Computing,24-Sep-12,2012,,,86,93,"In this paper, we present an analysis of a relationship between temporal trends of automatically extracted terms in medical research document and their similarities on a structured vocabulary. In order to obtain the temporal trends, we used our temporal pattern extraction method that combines an automatic term extraction, an importance index of the terms, and clustering for the values in each period. By using a set of medical research documents that were published every year, we extracted temporal patterns of the automatically extracted terms. Then, we calculated their similarities on the medical taxonomy by defining a distance on the tree structure. For analyzing the relationship between the terms included in the patterns and the similarity of the terms on the taxonomy, the differences of the averaged similarities of the terms in each pattern are compared between the two trends of the temporal patterns.",,978-1-4673-2795-4,10.1109/ICCI-CC.2012.6311131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311131,Temporal Text Mining;Knowledge Base;Temporal Clustering,Drugs,document handling;medical computing;pattern clustering;tree data structures,concept similarity;biomedical research document;relationship analysis;temporal trend;structured vocabulary;temporal pattern extraction method;automatic term extraction;value clustering;medical taxonomy;tree structure,,,,16,,24-Sep-12,,,IEEE,IEEE Conferences
Incremental learning of aspect model on streaming documents,流文檔上方面模型的增量學習,Te-Min Chang; Wen-Feng Hsiao; Cheng-Wei Wu,"Department of Information Management, National Sun Yat-sen University, Taiwan; Department of Information Management, National PingTung Institute of Commerce, Taiwan; Department of Information Management, National Sun Yat-sen University, Taiwan",5th International Conference on Computer Sciences and Convergence Information Technology,10-Feb-11,2010,,,360,365,"This research is to propose an IR related technique, the incremental aspect model (ISM), which not only uncovers latent aspects from the collected documents but also adapts the aspect model on streaming documents chronologically. ISM includes two stages: in Stage I, probabilistic latent semantic indexing (PLSI) technique is used to build a primary aspect model; and in Stage II, with out-of-date data removing and new data folding-in, the aspect model can be expanded using the derived spectral method if new aspects significantly exist. Two experiments on text clustering tasks are conducted accordingly. Results show the ISM has robust performance in terms of its incremental learning ability.",,978-89-88678-30-5,10.1109/ICCIT.2010.5711084,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711084,Probabilistic Latent Semantic Indexing;Aspect Model;Incremental Learning;Text Clustering,Data models;Semantics;Large scale integration;Indexing;Estimation;Buildings;Convergence,document handling;indexing;learning (artificial intelligence);pattern clustering;task analysis,document streaming;IR related technique;incremental aspect model;ISM;probabilistic latent semantic indexing technique;data folding;spectral method;text clustering task;incremental learning ability,,,,11,,10-Feb-11,,,IEEE,IEEE Conferences
Design to post-processing of IR based on fuzzy clustering analysis,基於模糊聚類分析的紅外後處理設計,Hai-Yan Kang; Xiao-Zhong Fan; Yan-Fang Li; Zhi-Yong Zhang; Pei-Guang Lin,"Dept. of Comput., Beijing Inst. of Technol., China; Dept. of Comput., Beijing Inst. of Technol., China; NA; NA; NA",Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),24-Jan-05,2004,3,,1859,1862 vol.3,"Man has been exploring continuously to improve the precision and intelligent degree of IR in order to deal with large quantities of information. In traditional methods of IR, retrieval results are disposed so little that the quality of IR is not high. To overcome this issue, a clustering algorithm based on cut-set is designed in this paper. This algorithm carries through clustering based on no leading text choice to many retrieval results so that users only consider the relevant sub-classes and abandons the non-relevant data. Hence, users can browse fewer documents. Experiments on live data of text snapshot show this algorithm can improve the query efficiency of IR system considerably at the cost of much less time.",,0-7803-8403-2,10.1109/ICMLC.2004.1382080,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1382080,,Clustering algorithms;Fuzzy sets;Algorithm design and analysis;Information retrieval;Natural languages;Testing;Information analysis;Information science;Costs;Explosives,pattern clustering;information retrieval;fuzzy set theory;text analysis;feature extraction,information retrieval system;fuzzy clustering analysis;intelligentized degree system;clustering algorithm;cut set theory;text snapshot data;feature extraction,,,,8,,24-Jan-05,,,IEEE,IEEE Conferences
Concurrent Optimization of Context Clustering and GMM for Offline Handwritten Word Recognition Using HMM,基於HMM的離線手寫單詞識別的上下文聚類和GMM並發優化,T. Hamamura; B. Irie; T. Nishimoto; N. Ono; S. Sagayama,"TOSHIBA Corp., Tokyo, Japan; TOSHIBA Corp., Tokyo, Japan; Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan; Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,523,527,"Context-dependent HMMs are commonly used in speech recognition. Parameter sharing needed for this model can be realized by two methods: context clustering or tied-mixture. In speech recognition, the former is reported to be more precise. However, there is some difficulty in applying context clustering to handwritten word recognition, since the distribution of each character is typically a mixture of different distributions, such as block-printed, cursive, etc. For this reason, successful results reported so far are limited to the tied-mixture approach. To deal with this problem, we propose a novel parameter tying method ``Partial Tied-Mixture"", where the Gaussian Mixture Model (GMM) consists of a portion of all Gaussians. Furthermore, we derive a method to concurrently optimize context clustering and GMM. Experiments on the CEDAR database show that the proposed method outperforms tied-mixture both in terms of precision and computational cost.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065366,Handwritten word recognition;Context clustering;GMM;Context-dependent HMM;Partial Tied-Mixture;EM algorithm,Context;Hidden Markov models;Handwriting recognition;Context modeling;Computer integrated manufacturing;Error analysis;Clustering algorithms,Gaussian processes;handwritten character recognition;hidden Markov models;optimisation;pattern clustering,context clustering;concurrent optimization;offline handwritten word recognition;GMM;HMM;parameter tying method;partial tied-mixture;Gaussian mixture model;CEDAR database,,1,,11,,3-Nov-11,,,IEEE,IEEE Conferences
Recursive partitional-hierarchical clustering for navigation in large media databases,遞歸分區分層聚類，用於大型媒體數據庫中的導航,A. Lozano; P. Villegas,"Telefonica I+D, Valladolid, Spain; Telefonica I+D, Valladolid, Spain",Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07),30-Jul-07,2007,,,84,84,"We present in this document a tool for automatic clustering of repositories of tagged media items (images, videos) and a 3D visualization interface to enable interactive browsing of repository contents based on the generated clusters.",,0-7695-2818-X,10.1109/WIAMIS.2007.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4279192,,Navigation;Multimedia databases;Visual databases;Image databases;Videos;Visualization;Clustering algorithms;Manuals;Organizing;Tree data structures,content-based retrieval;data visualisation;graphical user interfaces;interactive systems;multimedia databases;pattern clustering;very large databases,recursive partitional-hierarchical clustering;3D visualization interface;large media databases;tagged media items;automatic repository clustering;interactive browsing,,2,,12,,30-Jul-07,,,IEEE,IEEE Conferences
Fast latent semantic indexing of spoken documents by using self-organizing maps,使用自組織映射快速對語音文檔進行潛在語義索引,M. Kurimo,"IDIAP, Martigny, Switzerland","2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.00CH37100)",6-Aug-02,2000,4,,2425,2428 vol.4,"This paper describes a new latent semantic indexing (LSI) method for spoken audio documents. The framework is indexing broadcast news from radio and TV as a combination of large vocabulary continuous speech recognition (LVCSR), natural language processing (NLP) and information retrieval (IR). For indexing, the documents are presented as vectors of word counts, whose dimensionality is rapidly reduced by random mapping (RM). The obtained vectors are projected into the latent semantic subspace determined by SVD, where the vectors are then smoothed by a self-organizing map (SOM). The smoothing by the closest document clusters is important here, because the documents are often short and have a high word error rate (WER). As the clusters in the semantic subspace reflect the news topics, the SOMs provide an easy way to visualize the index and query results and to explore the database. Test results are reported for TREC's spoken document retrieval databases (www.idiap.ch/kurimo/thisl.html).",1520-6149,0-7803-6293-4,10.1109/ICASSP.2000.859331,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=859331,,Indexing;Information retrieval;Visual databases;Large scale integration;Radio broadcasting;TV broadcasting;Vocabulary;Speech recognition;Natural language processing;Smoothing methods,self-organising feature maps;speech recognition;natural languages;information retrieval;database indexing,latent semantic indexing;spoken document indexing;self-organizing maps;audio documents;radio broadcast news;TV broadcast news;large vocabulary continuous speech recognition;natural language processing;information retrieval;random mapping;vectors;closest document clusters;word error rate;query results;spoken document retrieval database;TREC,,5,,11,,6-Aug-02,,,IEEE,IEEE Conferences
GLSA based online essay grading system,基於GLSA的在線論文評分系統,A. A. P. Ratna; H. Artajaya; B. A. Adhi,"Electrical Engineering Department, University of Indonesia, Depok, Indonesia; Electrical Engineering Department, University of Indonesia, Depok, Indonesia; Electrical Engineering Department, University of Indonesia, Depok, Indonesia","Proceedings of 2013 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)",7-Nov-13,2013,,,358,361,"Document representation as Generalized Latent Semantic Analysis (GLSA) vectors were claimed to give performance improvement on several task such as synonymy test, document classification, and clustering compared to traditional Latent Semantic Analysis (LSA) based systems, however GLSA performance has never been tested on automated essay grading system. This experiment propose an GLSA based automatic essay grading system design that will be used to examine the effect of GLSA implementation on automated essay grading system and to evaluate its performance compared to LSA based system.",,978-1-4673-6355-6,10.1109/TALE.2013.6654461,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654461,GLSA;essay grading system,Semantics;Matrix decomposition;Vectors;Educational institutions;Conferences;Vocabulary,document handling;pattern classification;pattern clustering;vectors,GLSA based online essay grading system;generalized latent semantic analysis vectors;GLSA vectors;performance improvement;synonymy test;document classification;document representation;document clustering;GLSA performance testing,,1,,17,,7-Nov-13,,,IEEE,IEEE Conferences
Single Document Summarization Based on Local Topic Identification and Word Frequency,基於局部主題識別和詞頻的單文檔摘要,Z. Teng; Y. Liu; F. Ren; S. Tsuchiya; F. Ren,"Fac. of Eng., Univ. of Tokushima, Tokushima; Fac. of Eng., Univ. of Tokushima, Tokushima; Fac. of Eng., Univ. of Tokushima, Tokushima; Fac. of Eng., Univ. of Tokushima, Tokushima; Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., Beijing",2008 Seventh Mexican International Conference on Artificial Intelligence,18-Nov-08,2008,,,37,41,"In this task, an approach for single document summaries based on local topic identification and word frequency is proposed. In recent years, there has been increased interest in automatic summarization. The physical features are often used and have been successfully applied to this field; it also has some disadvantages of non-redundancy, structure and coherence. Therefore, we introduced logical structure feature which has been successfully applied in multi-document summarization (MDS), and we designed a system to accomplish this task. Documents can be clustered into local topic after sentences similarity is calculated, which can be sorted by the scoring. Then sentences from all local topics are selected by computing the word frequency. Using this proposed method, the information redundancy of each local topic and among local topic is reduced. The information coverage ratio and structure of the summarization is improved.",,978-0-7695-3441-1,10.1109/MICAI.2008.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682440,,Frequency;Artificial intelligence;Telecommunication computing;Computer architecture;Explosions;Data mining;Arithmetic;Art,document handling,single document summarization;local topic identification;word frequency;automatic summarization;logical structure feature;multi-document summarization,,8,,10,,18-Nov-08,,,IEEE,IEEE Conferences
A Study on the Document Similarity Judgment Method Using Similar Block Algorithm,基於相似塊算法的文檔相似度判斷方法研究,J. -. Jeong; J. Lee; J. Joung,"Office of Information & Knowledge Outreach , Korea Research Foundation; Office of Information & Knowledge Outreach , Korea Research Foundation; Office of Information & Knowledge Outreach , Korea Research Foundation",2007 International Conference on Convergence Information Technology (ICCIT 2007),7-Jan-08,2007,,,412,416,"It is very difficult and troublesome to judge piracy when evaluating students' homework or documents using Internet and computer. In particular when they write text regarding the same theme, it is not easy to judge if it is pirated or not. It is different issue from existing information search methods which look for the most appropriate clustering after abstracting key words in other words abstracting frequency of index words from the target document. Therefore we used string which classifies with space rather than words as an index, applied location vector with appearance frequency and then expanded it to block to judge the similarity of the blocks. It is the 'similar string block expansion' method. In this article, we studied the method to evaluate the piracy of the document by calculating the reference data according to piracy similarity in a short time.",,0-7695-3038-9,10.1109/ICCIT.2007.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420295,,Frequency;Fingerprint recognition;Testing;Protein sequence;Amino acids;Information technology;Internet;Home computing;Search methods;Abstracts,document handling,document similarity judgment method;similar block algorithm;information search methods;target document;string block expansion method;words abstracting;index words,,,,6,,7-Jan-08,,,IEEE,IEEE Conferences
Formal concept analysis and document clustering via granular computing,通過粒度計算進行正式概念分析和文檔聚類,T. Y. Lin; I. Chiang,"Department of Computer Science, San Jose State Univ, CA, USA; Graduate Institute of Medical Informatics, Taipei Medical University, Taiwan","2006 IEEE International Conference on Systems, Man and Cybernetics",16-Jul-07,2006,6,,4763,4767,A text/web document is a knowledge representation of a human idea (a structured set of thoughts). This paper refines TFIDF and extended TFIDF(ETFIDF)[16]; These values really measures the co-occurrences of tokens. The ETFID captures the semantic more accurately. Tokens with high TFIDF values are called keywords. The sets of (n+1) Co-occurring keywords with High ETFIDF are called n-granules. The collection of keywords and n-granules can be interpreted geometrically; they form a non-closed simplicial complex. The corresponding non-closed polyhedron is called latent semantic space(LSS). LSS is a geometric knowledge base that provides the semantic to search engine.,1062-922X,1-4244-0099-6,10.1109/ICSMC.2006.385058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4274667,Keyword;simplex;granules;Latent semantic space,Text analysis;Humans;Set theory;Cybernetics;Knowledge representation;Search engines;Topology;Extraterrestrial measurements;Uncertainty;Computer science,knowledge representation;pattern clustering;search engines;text analysis,formal concept analysis;document clustering;granular computing;text document;Web document;knowledge representation;extended TFIDF;token cooccurrences;keywords;latent semantic space;geometric knowledge base;search engine,,,,30,,16-Jul-07,,,IEEE,IEEE Conferences
A Web Mining Model for Real-time Webpage Personalization,用於實時網頁個性化的Web挖掘模型,S. Hui-zhang; Z. Ji-di; Y. Zhong-zhi,"Institute of Systen Engineering, Shanghai Jiaotong University, P.R.China, 200052; Aetna School of Management, Shanghai Jiaotong University, P.R.China, 200052; Aetna School of Management, Shanghai Jiaotong University, P.R.China, 200052",2006 International Conference on Management Science and Engineering,4-Sep-07,2006,,,8,12,"Determining the size of the World Wide Web is extremely difficult. The Web can be viewed as the largest data source available and presents a challenging task for effective design and access. One proposed Web mining approach to handling the problem of effective design and access is personalization. With personalization, Web access or the contents of a Web page are modified to better fit the desires of the user. This may involve dynamically creating Web pages that are unique per user or using the desires of a user to determine what Web documents to retrieve. This paper presents a Web mining model based on dynamic clustering and hidden Markov model. The output of the model is some information for dynamically creating a Web page which can best meet the user's desires. The assumption of the dynamic clustering is that if a group of users who have the same interest trend, those pages they have visited are probably related. We propose that human should be the authority to judge the correlation of two pages. First, the model statistic a user's Web browsing records in the log file; find a group of users who have the same interest trend with the user; collect all the pages in which this group of users are interested; calculate the correlation between pages; and cluster the pages into several categories according to a predetermined threshold. Each Web page category is considered as a stochastic state variable. In the second phase, our model based on hidden Markov model is further constructed to mine the latent desires of a user given an observed sequence of Web pages that the user have browsed. In order to get the optimal parameters (transition probability matrix, the conditional probability and the initial state) in the model, we applied the Baum-Welch parameter estimation method in EM algorithm to train the model on the data set. Experimental results show that the model is practicable and efficient",2155-1855,7-5603-2355-3,10.1109/ICMSE.2006.313915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4104858,Hidden Markov model;Dynamic clustering;Web mining,Web mining;Hidden Markov models;Web pages;Humans;Parameter estimation;Clustering algorithms;Engineering management;Web sites;Statistics;Stochastic processes,data mining;expectation-maximisation algorithm;hidden Markov models;Internet;pattern clustering,Web mining model;real-time Web page personalization;World Wide Web;Web document;dynamic clustering;hidden Markov model;expectation-maximisation algorithm,,2,,10,,4-Sep-07,,,IEEE,IEEE Conferences
An extension of the automatic cross-association method with a 3-dimensional matrix,使用3維矩陣擴展自動交叉關聯方法,W. Lee; C. Lim; U. Kang; H. Choi,"Department of Computer Science, Korea Advanced Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science, Korea Advanced Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science, Korea Advanced Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science, Korea Advanced Institute of Science and Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea",2015 International Conference on Big Data and Smart Computing (BIGCOMP),2-Apr-15,2015,,,43,46,"There are numerous 2-dimensional matrix data for clustering including a set of documents, citation networks, web graphs, etc. However, many real-world datasets have more than three modes which require at least 3-dimensional matrices or tensors. Focusing on the clustering algorithm known as cross-association, we extend the algorithm to deal with a 3-dimensional matrix. Our proposed method is fully automated, and simultaneously discovers clusters of both row, column, and tube groups. Experiments on real and synthetic datasets show that our method is effective. Through the proposed method, useful information can be obtained even from sparse datasets.",2375-9356,978-1-4799-7303-3,10.1109/35021BIGCOMP.2015.7072848,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072848,3-dimensional matrix;clustering;cross association;data analysis,Electron tubes;Tensile stress;Sparse matrices;Clustering algorithms;Indexes;Algorithm design and analysis;Complexity theory,pattern clustering;sparse matrices;tensors,automatic cross-association method;3-dimensional matrix;clustering algorithm;citation networks;Web graphs;document set;tensors;tube groups;column groups;row groups;sparse datasets,,1,,21,,2-Apr-15,,,IEEE,IEEE Conferences
Document image segmentation using fuzzy classifier and the dual-tree DWT,使用模糊分類器和雙樹DWT的文檔圖像分割,J. Saeedi; R. Safabakhsh; S. Mozaffari,"Amirkabir University of Technology, Electrical Engineering Department, Tehran, Iran; Amirkabir University of Technology, Computer Engineering Department, Tehran, Iran; Semnan University, Electrical and Computer Department, Semnan, Iran",2009 14th International CSI Computer Conference,8-Dec-09,2009,,,385,391,"In this paper, we propose a new method for textual areas extraction of an image using a fuzzy classifier and dual-tree discrete wavelet transform. We have extended our text extraction scheme for classification of document images into text, background, and picture components. Three class fuzzy classifiers and a morphological post-processing operation is used for this purpose. The proposed method shows better results compared to the previous wavelet-based document image segmentation algorithms.",,978-1-4244-4261-4,10.1109/CSICC.2009.5349611,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349611,document image segmentation;dual-tree discrete wavelet transform;fuzzy classifier;morphological post-processing,Image segmentation;Discrete wavelet transforms;Wavelet domain;Feature extraction;Image analysis;Image recognition;Text recognition;Clustering algorithms;Wavelet packets;Filter bank,discrete wavelet transforms;document image processing;feature extraction;image classification;image segmentation,document image segmentation;fuzzy classifier;dual-tree discrete wavelet transform;document image classification,,1,,18,,8-Dec-09,,,IEEE,IEEE Conferences
Large Scale Page-Based Book Similarity Clustering,基於頁面的大規模圖書相似度聚類,N. Spasojevic; G. Poncin,"Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,119,125,"The Google Books corpus now counts over 15M books spanning 7 centuries and countless languages. Traditional cataloguing at that scale is imprecise, and often fails to identify more complex book-to-book relationships, such as `same text, different pagination' or 'partial overlap'. Our contribution is a two-step technique for clustering books based on content similarity (at both book and page level) and classifying their relationships. We run this on our corpora consisting of more than 15M books (5B pages). We first detect similar books and similar pages within matching books, using hashing techniques and judicious thresholds. We then combine those features to identify the exact relationship between matching books. In this paper, we describe the basic approach to making the problem tractable, as well as the features and classifiers that we used. We enumerate a small number of relationships to qualify the link between scanned real-world books. Finally, we provide precision and recall measurements of the classifier.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065288,document similarity detection;relation classification;min-hash;book clustering,Feature extraction;Books;Error analysis;Optical character recognition software;Support vector machines;Correlation;Manuals,cataloguing;content management;file organisation;Internet;pattern classification;pattern clustering,page based book similarity clustering;Google Books corpus;books spanning;cataloguing;book-to-book relationships;content similarity;hashing techniques;judicious thresholds;relation classification,,1,,11,,3-Nov-11,,,IEEE,IEEE Conferences
Similar document detection using self-organizing maps,使用自組織圖進行類似文檔檢測,A. Lensu; P. Koikkalainen,"Dept. of Math. Inf. Technol., Jyvaskyla Univ., Finland; NA",1999 Third International Conference on Knowledge-Based Intelligent Information Engineering Systems. Proceedings (Cat. No.99TH8410),6-Aug-02,1999,,,174,177,"This paper describes how similar free-form textual documents can be matched using the self-organizing maps (SOMs). The analysis chain is made of three parts: first, similar words are located using an alphabet occurrence coding and SOM; second, three-word contexts are clustered using codes obtained from the word SOM to build a context map; and third, whole documents are clustered using codes from the context SOM. Although this work is inspired by the WEBSOM method, it is quite different since our goal was to build a fast system, which is tolerant to the special features of different languages.",,0-7803-5578-4,10.1109/KES.1999.820147,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=820147,,Self organizing feature maps;Data analysis;Algorithm design and analysis;Information technology;Neural networks;Humans;Computer vision;Web sites;Text analysis;Data mining,document handling;image matching;information retrieval;self-organising feature maps,similar document detection;self-organizing maps;free-form textual documents;alphabet occurrence coding;context map;WEBSOM method,,1,,8,,6-Aug-02,,,IEEE,IEEE Conferences
An Improved K-medoids Algorithm Based on Binary Sequences Similarity Measures,一種基於二元序列相似度的改進的K-medoids算法,F. Alalyan; N. Zamzami; M. Amayri; N. Bouguila,"Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; G-SCOP Lab, Grenoble Institute of Technology, Grenoble, France; Concordia Institute for Information Systems Engineering (CIISE), Concordia University, Montreal, QC, Canada","2019 6th International Conference on Control, Decision and Information Technologies (CoDIT)",2-Sep-19,2019,,,1723,1728,"Nowadays a massive amount of data is generated as the development of technology and services has accelerated. Therefore, the demand for data clustering in order to gain knowledge has increased in many sectors such as medical sciences, risk assessment and product sales. Moreover, binary data has been widely used in various applications including market basket data and text documents. While applying classic widely used k-means method is inappropriate to cluster binary data, this work proposes an improvement of K-medoids algorithm using binary similarity measures instead of Euclidean distance which is generally deployed in clustering algorithms. We have considered two challenging applications, namely; text clustering and binary images categorization to show the merits of the proposed framework.",2576-3555,978-1-7281-0521-5,10.1109/CoDIT.2019.8820298,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820298,Clustering;K-medoids;Binary data;Binary Sequences Similarity;Text Clustering;Image Categorization;Bag-of-Words,Clustering algorithms;Euclidean distance;Partitioning algorithms;Acceleration;Data mining;Machine learning;Vocabulary,binary sequences;pattern clustering;text analysis,data clustering;text documents;cluster binary data;binary similarity measures;clustering algorithms;text clustering;binary images categorization;binary sequences similarity measures;market basket data;k-means method;K-medoids algorithm,,2,,37,,2-Sep-19,,,IEEE,IEEE Conferences
Concept-based clustering of textual documents using SOM,使用SOM基於概念的文本文檔聚類,A. Amine; Z. Elberrichi; L. Bellatreche; M. Simonet; M. Malki,"EEDIS Laboratory, Department of computer science, Djillali Liabes University, Sidi Belabbes - Algeria; EEDIS Laboratory, Department of computer science, Djillali Liabes University, Sidi Belabbes - Algeria; LISI/ENSMA University of Poitiers, Futuroscope 86960 France; TIMC-IMAG Laboratory, IN3S, University Joseph Fourier, Grenoble - France; EEDIS Laboratory, Department of computer science, Djillali Liabes University, Sidi Belabbes - Algeria",2008 IEEE/ACS International Conference on Computer Systems and Applications,22-Apr-08,2008,,,156,163,"The classification of textual documents has been widely studied. The majority of classification approaches use supervised learning methods, which are acceptable for rather small corpora allowing experts to generate representative sets of data for the training, but are not feasible for significant flows of data. Unsupervised classification methods discover latent (hidden) classes automatically while minimizing human intervention. Many such methods exist, among which Kohonen self- organizing maps (SOM), which gather a certain number of similar objects without prior information. In this paper, we evaluate and compare the use of SOMs for the classification of textual documents in two situations: a conceptual representation of texts and a representation based on n-grams.",2161-5330,978-1-4244-1967-8,10.1109/AICCSA.2008.4493530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493530,,Laboratories;Computer science;Supervised learning;Unsupervised learning;Self organizing feature maps;Clustering algorithms;Humans;Internet;Software libraries;Learning systems,pattern classification;pattern clustering;self-organising feature maps;text analysis;unsupervised learning,concept-based clustering;textual documents;SOM;unsupervised classification methods;Kohonen self-organizing maps,,4,,20,,22-Apr-08,,,IEEE,IEEE Conferences
Segmentation of Handwritten Textlines in Presence of Touching Components,在觸摸組件的情況下手寫文本行的分割,J. Kumar; L. Kang; D. Doermann; W. Abd-Almageed,"Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA; Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA; Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA; Inst. of Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,109,113,"This paper presents an approach to text line extraction in handwritten document images which combines local and global techniques. We propose a graph-based technique to detect touching and proximity errors that are common with handwritten text lines. In a refinement step, we use Expectation-Maximization (EM) to iteratively split the error segments to obtain correct text-lines. We show improvement in accuracies using our correction method on datasets of Arabic document images. Results on a set of artificially generated proximity images show that the method is effective for handling touching errors in handwritten document images.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065286,Text-lines;Handwritten Documents;Arabic,Image segmentation;Text analysis;Accuracy;Least squares approximation;Clustering algorithms;Educational institutions;Estimation,document image processing;expectation-maximisation algorithm;feature extraction;graph theory;handwritten character recognition;image segmentation;natural language processing,handwritten textlines segmentation;touching components;textline extraction;handwritten document images;graph based technique;proximity errors;expectation maximization;error segments;Arabic document images,,11,,12,,3-Nov-11,,,IEEE,IEEE Conferences
Personalized search engines on mining user preferences using clickthrough data,個性化搜索引擎使用點擊數據挖掘用戶偏好,S. Preetha; K. N. V. Shankar,"Department of Computer Science & Engineering, V.S.B Engineering College, Karur; Department of Computer Science & Engineering, V.S.B Engineering College, Karur, India",International Conference on Information Communication and Embedded Systems (ICICES2014),9-Feb-15,2014,,,1,6,"In an web based application, different users may have different search goals when they submit it to a search engine. For a broad-topic and ambiguous query it is difficult. Here we Propose a novel approach to infer user search goals by analyzing search engine query logs. A major deficiency of generic search engines is that they follow the ?one size fits all??model and are not adaptable to individual users. This is typically shown in cases such as these: Different users have different backgrounds and interests. However, effective personalization cannot be achieved without accurate user profiles. We address the problem of learning the user profile within the user's ongoing behaviors by using the user search. We propose a framework that enables large-scale evaluation of personalized search. User interest is employed in the clustering process to achieve personalization effect. The goal of personalized IR (information retrieval) is to return search results that better match the user intent. First, we propose a framework to discover different user search goals for a query by clustering the proposed feedback sessions. Feedback sessions are get constructed from user click-through logs and can efficiently reflect the information needs of users. Second, we propose an approach to generate pseudo-documents to better represent the feedback sessions for clustering. Most document-based methods focus on analyzing users' clicking and browsing behaviors recorded at the users' clickthrough data. In the Web search engines, clickthrough data are important implicit feedback mechanism from users. An example of clickthrough data for the query ""apple,"" which contains a list of ranked search results presented to the user, which contains identification on the results that was previously clicked by the user. The bolded documents that have been clicked by the user have been ranked. Several personalized systems that employ clickthrough data to capture users' interest have been proposed.",,978-1-4799-3834-6,10.1109/ICICES.2014.7033953,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033953,Search Result Reorganization;Information Retrival;Restructuring;Implicit Feedback Mechanism;Ranking;Clickthrough Data,Search engines;Web search;Educational institutions;Uniform resource locators;Web mining;Databases,data mining;document handling;human computer interaction;information retrieval;Internet;pattern clustering;search engines,personalized search engines;user preference mining;clickthrough data;user search goal inference;search engine query logs analysis;clustering process;personalized IR;information retrieval;pseudo-documents generation;feedback session representation;document-based methods;Web search engines,,1,,6,,9-Feb-15,,,IEEE,IEEE Conferences
A Cost Efficient Approach to Correct OCR Errors in Large Document Collections,糾正大型文檔集中OCR錯誤的一種經濟高效的方法,D. Das; J. Philip; M. Mathew; C. V. Jawahar,"IIIT Hyderabad, India; IIIT Hyderabad, India; IIIT Hyderabad, India; IIIT Hyderabad, India",2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,655,662,"Word error rate of an OCR is often higher than its character error rate. This is especially true when OCRs are designed by recognizing characters. High word accuracies are critical for many practical applications like content creation and text-to-speech systems. In order to detect and correct the misrecognised words, it is common for an OCR to employ a post-processor module to improve the word accuracy. However, conventional approaches to post-processing like looking up a dictionary or using a statistical language model (SLM), are still limited. In many such scenarios, it is often required to remove the outstanding errors manually. We observe that the traditional post-processing schemes look at error words sequentially, since OCRs process documents one at a time. We propose a cost-efficient model to address the error words in batches rather than correcting them individually. We exploit the fact that a collection of documents (eg. a book), unlike a single document, has a structure leading to repetition of words. Such words, if efficiently grouped together and corrected together, can lead to a significant reduction in the effort. Error correction can be fully automatic or with a human in the loop. We compare the performance of our method with various baseline approaches including the case where all the errors are removed by a human. We demonstrate the efficacy of our solution empirically by reporting more than 70% reduction in the human effort with near perfect error correction. We validate our method on books in both English and Hind.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978071,batch ocr;clustering,Optical character recognition software;Dictionaries;Error correction;Image recognition;Pipelines;Manuals;Error analysis,document image processing;error correction;natural language processing;optical character recognition;speech synthesis;statistical analysis;text analysis,correct OCR errors;document collections;word error rate;character error rate;text-to-speech systems;post-processor module;statistical language model;cost-efficient model;error correction;SLM,,,,23,,3-Feb-20,,,IEEE,IEEE Conferences
Document clustering based on diffusion maps and a comparison of the k-means performances in various spaces,基於擴散圖的文檔聚類以及不同空間中k均值性能的比較,F. A. Allah; W. I. Grosky; D. Aboutajdine,"GSCM-LRIT Laboratory, Mohamed V-Agdal University, B.P. 1014, Rabat, Morocco; CIS Department, Michigan-Dearborn University, 48128, USA; GSCM-LRIT Laboratory, Mohamed V-Agdal University, B.P. 1014, Rabat, Morocco",2008 IEEE Symposium on Computers and Communications,16-Sep-08,2008,,,579,584,"A great challenge of text mining arises from the increasingly large text datasets and the high dimensionality associated with natural language. In this research, a systematic study is conducted in the context of the document clustering, using the recently introduced diffusion framework and some characteristics of the singular value decomposition. This study is three-fold. First, we propose to construct a diffusion kernel based on the cosine distance. Second, we compare the performances of the k-means algorithm in four different vector spaces: Salton space, latent semantic analysis space, diffusion space based on the cosine distance, and diffusion space based on the Euclidian distance. Third, we undertake a statistical study of the k-means algorithm in the LSA space and the diffusion space based on the cosine distance. In most of our experiments, k-means in diffusion space, based on the cosine distance performs better. In addition, the running time in this space is negligible compared to the time needed for k-means in Salton space.",1530-1346,978-1-4244-2702-4,10.1109/ISCC.2008.4625693,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625693,,Kernel;Distance measurement;Clustering algorithms;Symmetric matrices;Matrix decomposition;Geometry;Diffusion processes,natural language processing;pattern clustering;singular value decomposition;text analysis,document clustering;diffusion maps;k-means performances;text mining;text datasets;natural language;singular value decomposition;vector spaces;Euclidian distance;cosine distance;Salton space,,3,,14,,16-Sep-08,,,IEEE,IEEE Conferences
Distributed information retrieval for Multi-XML document based on keywords,基於關鍵字的Multi-XML文檔的分佈式信息檢索,S. M. Xi; Y. I. Cho,"School of Information, Shandong Polytechnic University, Jinan, China; College of Information Technology, The University of Suwon, Hwaseong-si, Korea","2012 3rd International Conference on System Science, Engineering Design and Manufacturing Informatization",10-Nov-12,2012,2,,96,99,"This article proposed a new approach for automatically correcting queries over Multi-XML, called MXDR(Multi-XML Distributed Retrieval). We first classed multi-XML documents by a clustering method, and elicited the common structure information. Then generated certifiable structured queries by analyzing the given keywords query and the common structure information of XML datasets. We can evaluate the generated structured queries over the XML data sources with any existing structure search engine.",,978-1-4673-0915-8,10.1109/ICSSEM.2012.6340817,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340817,multi-XML;keywords IR;structure IR;distributed,XML;Correlation;Feature extraction;Educational institutions;Data mining;Keyword search,distributed processing;pattern clustering;query processing;search engines;XML,distributed information retrieval;multiXML document;automatically correcting query;MXDR;multiXML distributed retrieval;clustering method;certifiable structured query;keywords query;common structure information;XML datasets;generated structured query;XML data sources;structure search engine,,,,12,,10-Nov-12,,,IEEE,IEEE Conferences
Chinese medical event detection based on feature extension and document consistency,基於特徵擴展和文檔一致性的中醫事件檢測,C. Wang; P. Zhai; Y. Fang,"Tongji University,Grid laboratory,Shanghai,China; Tongji University,Grid laboratory,Shanghai,China; Tongji University,Grid laboratory,Shanghai,China","2020 5th International Conference on Automation, Control and Robotics Engineering (CACRE)",22-Oct-20,2020,,,753,758,"Extracting valuable medical events from Chinese electronic medical records has important practical significance and application value for electronic medical record text mining, and event detection is a critical step in the event extraction task. Existing research methods for medical event detection in Chinese are mainly based on pattern matching and clustering, and they have two problems :(1) None of them consider the named entities distribution characteristics of medical events. (2) Ignore the distribution of document consistency between medical events in each document. Therefore, this paper proposes an event detection method based on feature extension and document consistency. Firstly, design the medical event representation template and construct the event trigger dictionary according to the ACE standard. Secondly, use semiautomatic corpus labeling method to label entities and events. Then, based on the basic features, according to the distribution characteristics of the entities in the event, different entity information features are selected as extension features. Finally, use the consistency of the distribution of medical documents to improve the final result of event detection. The experimental results show that our method is significantly superior to the baseline.",,978-1-7281-9888-0,10.1109/CACRE50138.2020.9230246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230246,event detection;entity;document consistency;feature extension,Event detection;Feature extraction;Dictionaries;Medical diagnostic imaging;Task analysis;Syntactics;Surgery,data mining;electronic health records;feature selection;information retrieval;natural language processing;pattern clustering;pattern matching;text analysis,electronic medical record text mining;named entities distribution characteristics;document consistency;feature extension;medical event representation template;event trigger dictionary;semiautomatic corpus labeling method;medical documents;Chinese medical event detection;Chinese electronic medical records;extension feature selection;entity information feature selection;valuable medical event extraction;ACE standard;pattern matching;pattern clustering,,,,18,,22-Oct-20,,,IEEE,IEEE Conferences
Hierarchical Co-Clustering: A New Way to Organize the Music Data,分層聯合群集：一種組織音樂數據的新方法,J. Li; B. Shao; T. Li; M. Ogihara,"School of Computing and Information Science, Florida International University, Miami, FL, USA; School of Computing and Information Science, Florida International University, Miami, FL, USA; School of Computing and Information Science, Florida International University, Miami, FL, USA; Department of Computer Science, University of Miami, Coral Gables, Florida, USA",IEEE Transactions on Multimedia,19-Mar-12,2012,14,2,471,481,"In music information retrieval (MIR) an important research topic, which has attracted much attention recently, is the utilization of user-assigned tags, artist-related style, and mood labels, which can be extracted from music listening web sites, such as Last.fm (http://www.last.fm/) and All Music Guide (http://www.allmusic.com/). A fundamental research problem in the area is how to understand the relationships among artists/songs and these different pieces of information. Co-clustering is the problem of simultaneously clustering two types of data (e.g., documents and words, and webpages and urls). We can naturally bring this idea to the situation at hand and consider clustering artists and tags together, artists and styles together, or artists and mood labels together. Once such co-clustering has been successfully completed, one can identify co-existing clusters of artists and tags, styles, or mood labels (T/S/M). For simplicity, we use the acronym T/S/M to refer to tag(s), style(s), or mood(s) for the rest of the paper. When dealing with tags it is worth noticing that some tags are more specific versions of others. This naturally suggests that the tags could be organized in hierarchical clusters. Such hierarchical organizations exist for styles and mood labels, so we will consider hierarchical co-clustering of artists and T/S/M. In this paper, we systematically study the application of hierarchical co-clustering (HCC) methods for organizing the music data. There are two standard strategies for hierarchical clustering. One is the divisive strategy, in which we attempt to divide the input data set into smaller groups recursively, and the other is the agglomerative strategy, in which we attempt to combine initially individually separated data points into larger groups by finding the most closely related pair at each iteration. We will compare these two strategies against each other. We apply a previously known divisive hierarchical co-clustering method and a novel agglomerative hierarchical co-clustering. In addition, we demonstrate that these two methods have the capability of incorporating instance-level constraints to achieve better performance. We perform experiments to show that these two hierarchical co-clustering methods can be effectively deployed for organizing the music data and they present reasonable clustering performance comparing with the other clustering methods. A case study is also conducted to show that HCC provides us a new method to quantify the artist similarity.",1941-0077,,10.1109/TMM.2011.2181151,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111486,Co-clustering;hierarchical clustering;user tags,Mood;Organizing;Clustering algorithms;Organizations;Multiple signal classification;Standards organizations;Music information retrieval,information retrieval;music;pattern clustering;Web sites,music data organization;music information retrieval;user-assigned tags;artist-related style;mood labels;music listening Web sites;Last.fm;All Music Guide;data clustering;hierarchical organization;agglomerative hierarchical coclustering;instance-level constraints;artist similarity,,8,,49,,22-Dec-11,,,IEEE,IEEE Journals
Opinion Extraction & Classification of Reviews from Web Documents,Web文檔的意見提取和評論分類,S. K. Shandilya; S. Jain,"Department of Computer Engineering, Institute of Engineering & Technology, Devi Ahilya University, Indore - MP - INDIA; Department of Computer Engineering, Institute of Engineering & Technology, Devi Ahilya University, Indore - MP - INDIA",2009 IEEE International Advance Computing Conference,31-Mar-09,2009,,,924,927,"Automatic extraction of opinions on products from Web has been receiving interest increasingly. Such extracted knowledge helps to find out what other people think about the particular product or service. With the growing availability of resources like online review sites and personal blogs, new opportunities and challenges arise as people can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden growth in the area of opinion mining, which deals with the computational techniques for opinion extraction and understanding created an utmost need to understand and view the Web in a different prospect. In this paper, we demonstrate an opinion-mining framework that extracts the opinions and views of the consumers/customers, and analyze them to provide concrete market flow along with proven statistical data. The software uses classification, clustering and lingual knowledge-based opinion mining for providing these features.",,978-1-4244-2927-1,10.1109/IADCC.2009.4809138,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4809138,Web Mining;Information Retrieval;Opinion Mining,Data mining;Information retrieval;Learning systems;Blogs;Information analysis;Availability;Information technology;Concrete;Web mining;Spatial databases,classification;data mining;document handling;information retrieval;Web sites,opinion extraction;reviews classification;Web document;knowledge extraction;online review site;personal blog;market flow;clustering;lingual knowledge-based opinion mining;information retrieval,,2,,18,,31-Mar-09,,,IEEE,IEEE Conferences
Semantic Feature Discovery of Trojan Malware using Vector Space Kernels,使用向量空間核的特洛伊木馬惡意軟件語義特徵發現,J. Musgrave; C. Purdy; A. L. Ralescu; D. Kapp; T. Kebede,"University of Cincinnati,EECS Department,Cincinnati,OH; University of Cincinnati,EECS Department,Cincinnati,OH; University of Cincinnati,EECS Department,Cincinnati,OH; Wright-Patt Air Force Base,Air Force Research Lab,Dayton,OH; Wright-Patt Air Force Base,Air Force Research Lab,Dayton,OH",2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS),2-Sep-20,2020,,,494,499,"Malware analysis techniques without context or abstraction can be easily evaded by small changes in syntax. Semantic features represent an underlying pattern of abstraction in a program. Vector space models of malicious program instructions enable malware analysis via supervised document analysis and unsupervised cluster analysis. Document level features can be used for supervised learning, and malicious program semantics can be captured by both the eigenvalue decomposition of term frequencies and community discovery in the feature vector space. To obtain a fine-grained view of malicious program semantics intra-document opcode structures can be viewed without class labels for unsupervised clustering, as well as used in aggregate with class labels for malicious topic discovery via Singular Value Decomposition and to create a fine-grained semantic kernel. These patterns can be used to train a suitable classifier based on semantic features. The selection of these features can be evaluated by several criteria, including their embedded classifier performance. This paper presents preliminary results of classifiers trained to recognize static features of malicious documents and subgroup discovery via clustering algorithms to identify relevant features based on a semantic interpretation.",1558-3899,978-1-7281-8058-8,10.1109/MWSCAS48704.2020.9184642,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184642,,Semantics;Kernel;Malware;Feature extraction;Support vector machines;Principal component analysis;Dictionaries,document handling;eigenvalues and eigenfunctions;feature selection;invasive software;pattern classification;pattern clustering;singular value decomposition;supervised learning;unsupervised learning,semantic feature discovery;vector space kernels;semantic features;vector space models;malicious program instructions;supervised document analysis;unsupervised cluster analysis;document level features;supervised learning;eigenvalue decomposition;community discovery;feature vector space;class labels;unsupervised clustering;malicious topic discovery;singular value decomposition;fine-grained semantic kernel;static features;malicious documents;subgroup discovery;semantic interpretation;malware analysis;Trojan malware;term frequencies;malicious program semantics;intra-document opcode structures;feature selection;embedded classifier performance,,,,24,,2-Sep-20,,,IEEE,IEEE Conferences
Cooperative based software clustering on dependency graphs,基於依賴關係圖的基於協作的軟件聚類,A. Ibrahim; D. Rayside; R. Kashef,"Electrical & Computer Engineering Dept., Faculty of Engineering, University of Waterloo, Canada; Electrical & Computer Engineering Dept., Faculty of Engineering, University of Waterloo, Canada; Dept. of Management Sciences, Faculty of Engineering, University of Waterloo, Canada",2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE),18-Sep-14,2014,,,1,6,"Software clustering involves the partitioning of software system components into clusters with the goal of obtaining optimum exterior and interior connectivity between the components. Research in this area has produced numerous algorithms with different methodologies and parameters. In this paper, we propose a novel ensemble approach that synthesizes a new solution from the outcomes of multiple constituent clustering algorithms. The main idea behind our cooperative approach was inherited from machine learning, as applied to document clustering, but has been modified for use in software clustering. The conceptual modifications include working with differing numbers of clusters produced by the input algorithms and using graph structures rather than feature vectors. The empirical modifications include experiments for selecting the optimal cluster merging criteria. Case studies using open source software systems show that forging cooperation between leading state-of-the-art algorithms produces better results than any one state-of-the-art algorithm considered.",0840-7789,978-1-4799-3101-9,10.1109/CCECE.2014.6900911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900911,,Clustering algorithms;Software algorithms;Partitioning algorithms;Merging;Benchmark testing;Software systems,graph theory;learning (artificial intelligence);pattern clustering;public domain software;software engineering,cooperative based software clustering;dependency graph;software system component partitioning;ensemble approach;clustering algorithm;cooperative approach;machine learning;document clustering;input algorithms;graph structures;optimal cluster merging criteria;open source software systems,,6,,27,,18-Sep-14,,,IEEE,IEEE Conferences
Feedback-driven clustering for automated linking of web pages,反饋驅動的群集，用於自動鏈接網頁,A. Oest; M. Rege,"Department of Computer Scienece, Rochester Institute of Technology, USA; Graduate Programs in Software, University of St. Thomas, USA",8th International Conference for Internet Technology and Secured Transactions (ICITST-2013),3-Mar-14,2013,,,344,349,"In this paper we propose and test a system that indexes a large collection of HTML documents (i.e. an entire web site) and automatically generates context-relevant inline text links between pairs of related documents (i.e. web pages). The goal of the system is threefold: to increase user interaction with the site being browsed, to discover relevant keywords for each document, and to effectively cluster the documents into semantically-significant groupings. The quality of the links is improved over time through passive user feedback collection. Our system can be deployed as a web service and has been tested on offline datasets as well as a live web site. A distinctive feature of our system is that it supports datasets that grow or change over time.",,978-1-908320-20-9,10.1109/ICITST.2013.6750219,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750219,,Servers;Web pages;HTML;Joining processes;Clustering algorithms;Arrays,data mining;hypermedia markup languages;indexing;information retrieval;pattern clustering;text analysis;Web services;Web sites,feedback driven clustering;automated Web pages linking;HTML document indexing;automatic context relevant inline text link generation;user interaction;Web browser;keywords discovery;Web document clustering;passive user feedback collection;Web service;live Web site;automated keyword extraction,,,,15,,3-Mar-14,,,IEEE,IEEE Conferences
Industrial Requirements Classification for Redundancy and Inconsistency Detection in SEMIOS,SEMIOS中冗餘和不一致檢測的工業需求分類,M. Mezghani; J. Kang; F. S癡des,"Semios, Prometil, Toulouse, France; SEMIOS for Requirements, Toulouse, France; IRIT, Univ. of Toulouse, Toulouse, France",2018 IEEE 26th International Requirements Engineering Conference (RE),14-Oct-18,2018,,,297,303,"Requirements are usually ""hand-written"" and suffers from several problems like redundancy and inconsistency. The problems of redundancy and inconsistency between requirements or sets of requirements impact negatively the success of final products. Manually processing these issues requires too much time and it is very costly. The main contribution of this paper is the use of k-means algorithm for a redundancy and inconsistency detection in a new context, which is Requirements Engineering context. Also, we introduce a filtering approach to eliminate ""noisy"" requirements and a preprocessing step based on the Natural Language Processing (NLP) technique to see the impact of this latter on the k-means results. We use Part-Of-Speech (POS) tagging and noun chunking to detect technical business terms associated to the requirements documents that we analyze. We experiment this approach on real industrial datasets. The results show the efficiency of the k-means clustering algorithm, especially with the filtering and preprocessing steps. Our approach is using the software SEMIOS and will be integrated as a new functionality.",2332-6441,978-1-5386-7418-5,10.1109/RE.2018.00037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491144,Requirements engineering;redundancy;inconsistency;clustering;NLP;technical documents,Redundancy;Clustering algorithms;Tagging;Natural language processing;Requirements engineering;Business;Software,document handling;formal specification;natural language processing;pattern clustering;redundancy;systems analysis,inconsistency detection;filtering approach;noisy requirements;noun chunking;requirements documents;preprocessing steps;software SEMIOS;redundancy;natural language processing technique;requirements engineering context;industrial requirements classification;part-of-speech tagging;k-means clustering algorithm;hand-written,,2,,25,,14-Oct-18,,,IEEE,IEEE Conferences
Topic mover's distance based document classification,基於主題移動者距離的文檔分類,X. Wu; H. Li,"Key Laboratory of Wireless-Optical Communications of Chinese Academy of Science, Department of Electronics Engineering and Information Science, University of Science and Technology of China, Hefei, 230027, P. R. China; Key Laboratory of Wireless-Optical Communications of Chinese Academy of Science, Department of Electronics Engineering and Information Science, University of Science and Technology of China, Hefei, 230027, P. R. China",2017 IEEE 17th International Conference on Communication Technology (ICCT),17-May-18,2017,,,1998,2002,"We propose the Topic Mover's Distance (TMD), a new topic-based distance metric for documents, which is inspired from recently proposed Word Mover's Distance (WMD). Similar to WMD, TMD metric measures the similarity between two documents as the minimum amount of distance that the topics in one document need to travel to the topics in the other document. In our scheme, topics are the basic units to modeling documents, which are clustered from a general word-word co-occurrence matrix by Poisson Infinite Relational Model (PIRM) and vectorized by Glove embedding algorithm. Experiments for document classification on six real world datasets show that compared with word-based WMD, the proposed TMD can achieve much lower time complexity with the same accuracy.",2576-7828,978-1-5090-3944-9,10.1109/ICCT.2017.8359979,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359979,WMD;document classification;IRM;topic;low time complexity,Clustering algorithms;Semantics;Time complexity;Measurement;Classification algorithms;Vocabulary,computational complexity;document handling;pattern classification,document classification;WMD;topic movers distance;TMD metric;topic-based distance metric;word-word co-occurrence matrix;Poisson infinite relational model;Glove embedding algorithm;time complexity,,1,,19,,17-May-18,,,IEEE,IEEE Conferences
Feature Weighting Information-Theoretic Co-Clustering for Document Clustering,基於特徵加權的信息理論聯合聚類,Y. Ye; X. Li; B. Wu; Y. Li,NA; NA; NA; NA,2009 2nd International Conference on Computer Science and its Applications,1-Jul-10,2009,,,1,6,,2159-7049,978-1-4244-4945-3,10.1109/CSA.2009.5404286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5404286,,Clustering algorithms;Mutual information;Computer science;Paper technology;Internet;Information retrieval;Search engines;Ontologies;Information theory;Frequency,,,,4,,11,,1-Jul-10,,,IEEE,IEEE Conferences
Off-line recognition of Korean scripts using distance matching and neural network classifiers,使用距離匹配和神經網絡分類器對韓語腳本進行離線識別,Soo Hyung Kim; Jeong-In Doh,"Multimedia Center, Samsung Electron. Co. Ltd., Kyungki, South Korea; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,34,37 vol.1,"An off-line recognition engine is proposed for handwritten Korean characters based on a distance matching and the neural network technique. The distance matching selects a set of several candidates from the large set of character classes, and the neural network performs a detailed classification on the candidates. As an approach for combining the two methodologies, a clustering method based on sample distributions has been devised. Recognition accuracy of the engine on a public database, PE92, is 84.1%. About four character patterns can be processed in a second on PC.",,0-8186-7128-9,10.1109/ICDAR.1995.598938,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=598938,,Neural networks;Image segmentation;Engines;Character recognition;Pattern recognition;Handwriting recognition;Pixel;Electronic mail;Clustering methods;Databases,character recognition;neural nets;character sets;microcomputer applications;image matching;document image processing,off-line recognition engine;Korean scripts;distance matching;neural network classifiers;handwritten Korean characters;character classes;clustering method;sample distributions;recognition accuracy;PE92 public database;character patterns;PC,,,,9,,6-Aug-02,,,IEEE,IEEE Conferences
Separation of Chinese characters from graphics,圖形中的漢字分離,Jiing-Yuh Wang; Liang-Hua Chen; Kuo-Chin Fan; Hong-Yuan Mark Liao,"Dept. of Inf. & Electron. Eng., Nat. Central Univ., Chung-Li, Taiwan; NA; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,948,951 vol.2,"In this paper, we propose a robust algorithm to separate Chinese characters from line drawings. This approach is based on the clustering of all feature points in the images. Using our algorithm, all Chinese characters can be completely separated from graphics without regard to the size, orientation and location of Chinese characters even if the characters touching or overlapping line problem occurs. Experiments show that our algorithm can be applied to both geographical information systems and forms processing.",,0-8186-7128-9,10.1109/ICDAR.1995.602058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=602058,,Clustering algorithms;Registers;Geographic Information Systems;Computer graphics;Computer science;Information science;Robustness;Engineering drawings;Management information systems;Data analysis,optical character recognition;geographic information systems;business forms;document image processing,Chinese characters;robust algorithm;line drawings;feature point clustering;overlapping line problem;geographical information systems;forms processing,,,,12,,6-Aug-02,,,IEEE,IEEE Conferences
Recognition of essential folding operations: a step for interpreting illustrated books of origami,識別必要的折疊操作：解釋插圖的摺紙書的步驟,J. Kato; T. Watanabe; T. Nakayama,"Dept. of Intellectual Inf. Syst., Toyama Univ., Japan; NA; NA",Proceedings of the Fourth International Conference on Document Analysis and Recognition,6-Aug-02,1997,1,,81,85 vol.1,"To interpret motions by analyzing a series of illustrations is an interesting topic. This paper describes an approach for interpreting essential folding operations in illustrated books of origami. Since a folding operation such as ""folding up"" can be defined by a rotative surface, axis and direction, we proposed the algorithms to identify arrows and dashed lines which represent rotative surfaces, axes and directions in a typical book of origami. The arrow detection process consists of the arrow-head detection and arc detection. The former is accomplished by applying the distance transformation and border following to an input image, while the latter is composed of two phases: all paths which would correspond to arcs are found out in the thinned and approximated image first, an evaluative function is then applied to them to determine arcs. Dashed lines with different patterns are detected using a nearest-neighbor clustering method. A folding operation is finally interpreted based on the identified graphical primitives and the spatial relationships among them. Experimental results on several test images are presented. The effectiveness of our approach in generating meaningful interpretations of origami diagrams is clear from these results.",,0-8186-7898-4,10.1109/ICDAR.1997.619818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619818,,Books;Graphics;Image motion analysis;Text analysis;Information systems;Motion analysis;Information analysis;Clustering methods;Marine vehicles;Testing,diagrams;image recognition;document image processing;rotation;paper,folding operations recognition;illustrated books;origami diagrams;motion interpretation;illustration series;rotative surface;axis;folding direction;arrow detection process;dashed lines;arrow-head detection;arc detection;distance transformation;border following;thinned approximated image;evaluative function;nearest-neighbor clustering method;graphical primitives;spatial relationships,,2,,5,,6-Aug-02,,,IEEE,IEEE Conferences
Application of Improved Fuzzy c-Means Clustering in Cell Image Segmentation,改進的模糊c均值聚類在細胞圖像分割中的應用,P. Ren; S. Hu; H. Zhu; Y. Cao,"Coll. of Life Sci. & Eng., Southwest Univ. of Sci. & Technol., Mianyang, China; NA; NA; Coll. of Life Sci. & Eng., Southwest Univ. of Sci. & Technol., Mianyang, China",2011 5th International Conference on Bioinformatics and Biomedical Engineering,31-May-11,2011,,,1,4,"Cell image show entirely different characteristic due to the biodiversity, complexity, culture conditions and acquisition methods. Segment image is the key step of cell image processing. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document. The fuzzy c-means (FCM) clustering algorithm is one of most widespread methods which has applied in image analyzing, pattern recognition and medical diagnosis. To overcome the limitation of FCM algorithm, several improved FCM algorithm have been compared by applicated in cell image segmentation. The simulation results and the comparison between FCM and improved algorithm indicate that AFCM as shown by experiment indicated the better effect.",2151-7622,978-1-4244-5089-3,10.1109/icbbe.2011.5779980,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779980,,Image segmentation;Clustering algorithms;Pixel;Presses;Partitioning algorithms,cellular biophysics;fuzzy set theory;image segmentation;medical image processing;patient diagnosis;pattern clustering,fuzzy c-means clustering;cell image segmentation;cell image processing;style sheet;FCM clustering algorithm;image analysis;pattern recognition;medical diagnosis;AFCM,,2,,11,,31-May-11,,,IEEE,IEEE Conferences
Mutually Reinforced Manifold-Ranking Based Relevance Propagation Model for Query-Focused Multi-Document Summarization,基於互增強歧管排序的相關性傳播模型，用於查詢集中的多文檔摘要,X. Cai; W. Li,"College of Information Engineering, Northwest A&F University, Shaanxi, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hung Hom, Kowloon, Hong Kong","IEEE Transactions on Audio, Speech, and Language Processing",19-Mar-12,2012,20,5,1597,1607,"Manifold-ranking has been recently exploited for query-focused summarization. It propagates query relevance from the given query to the document sentences by making use of both the relationships among the sentences and the relationships between the given query and the sentences. The sentences in a document set can be grouped into several topic themes with each theme represented by a cluster of highly related sentences. However, it is a well-recognized fact that a document set often covers a number of such topic themes. In this paper, we present a novel model to enhance manifold-ranking based relevance propagation via mutual reinforcement between sentences and theme clusters. Based on the proposed model, we develop two new sentence ranking algorithms, namely the reinforcement after relevance propagation (RARP) algorithm and the reinforcement during relevance propagation (RDRP) algorithm. The convergence issues of the two algorithms are examined. When evaluated on the DUC2005-2007 datasets and TAC2008 dataset, the performance of the two proposed algorithms is comparable with that of the top three systems. The results also demonstrate that the RDRP algorithm is more effective than the RARP algorithm.",1558-7924,,10.1109/TASL.2012.2186291,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143994,Manifold-ranking;mutual reinforcement;query- focused multi-document summarization;relevance propagation;theme clusters,Clustering algorithms;Educational institutions;Mathematical model;Data mining;Convergence;Feature extraction;Semantics,document handling;query processing,mutually reinforced manifold-ranking;relevance propagation model;query-focused multidocument summarization;query relevance;reinforcement after relevance propagation algorithm;reinforcement during relevance propagation algorithm;DUC2005-2007 datasets;TAC2008 dataset,,16,,36,,31-Jan-12,,,IEEE,IEEE Journals
Evaluation of clustering and summarizing in distributed latent semantic indexing,分佈式潛在語義索引中聚類和匯總的評估,M. Behshameh; H. Bashiri; S. Hooshmand,"Department of Computer Engineering, Islamic Azad University - Toyserkan Branch Toyserkan, Iran; Department of Computer Engineering, Hamedan University of Technology Hamedan Iran; Department of Computer Engineering, Hamedan University of Technology Hamedan Iran",2010 2nd IEEE International Conference on Information Management and Engineering,3-Jun-10,2010,,,49,53,"Latent Semantic Indexing is a conceptual method in information retrieval systems. In this method, a term-document matrix is built through term weighting techniques. This matrix is mapped to a conceptual space by mathematical decomposition techniques like Singular Value Decomposition. The more documents and key terms collection are, the more element of term-document matrix is created, causes difficulty to manage. Such a huge size of matrix needs more memory space to save and more calculation to find out the solutions. With the assumption of using distribution in order to decrease the required memory space and to reduce the run-time problem, we did a research and implemented distributed LSI. To meet a better improvement, clustering is concerned for document too. In this combination, term-document matrix is recreated for each cluster and retrieval is accomplished on these set of term-document matrices. We evaluate our combinational method on Hamshahri Collection which is the largest collection in Persian language. Evaluation shows remarkable improvement in contrast with non-combinational LSI method.",,978-1-4244-5263-7,10.1109/ICIME.2010.5477470,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477470,Information retrieval;Latent semantic indexing;Clustering;Precision;Recall;Summarization,Indexing;Large scale integration;Matrix decomposition;Distributed computing;Space technology;Information retrieval;Singular value decomposition;Runtime;Data mining;Clustering algorithms,indexing;information retrieval systems;natural language processing;pattern clustering;singular value decomposition,clustering evaluation;distributed latent semantic indexing summarization;information retrieval systems;term-document matrix;term weighting techniques;mathematical decomposition techniques;singular value decomposition;distributed LSI method;Hamshahri collection;Persian language;noncombinational LSI method,,,,16,,3-Jun-10,,,IEEE,IEEE Conferences
Automated Functional Dependency Detection Between Test Cases Using Doc2Vec and Clustering,使用Doc2Vec和群集的測試用例之間的自動功能依賴性檢測,S. Tahvili; L. Hatvani; M. Felderer; W. Afzal; M. Bohlin,RISE SICS V瓣ster疇s; M瓣lardalen University; University of Innsbruck; M瓣lardalen University; RISE SICS V瓣ster疇s,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),20-May-19,2019,,,19,26,"Knowing about dependencies and similarities between test cases is beneficial for prioritizing them for cost-effective test execution. This holds especially true for the time consuming, manual execution of integration test cases written in natural language. Test case dependencies are typically derived from requirements and design artifacts. However, such artifacts are not always available, and the derivation process can be very time-consuming. In this paper, we propose, apply and evaluate a novel approach that derives test cases' similarities and functional dependencies directly from the test specification documents written in natural language, without requiring any other data source. Our approach uses an implementation of Doc2Vec algorithm to detect text-semantic similarities between test cases and then groups them using two clustering algorithms HDBSCAN and FCM. The correlation between test case text-semantic similarities and their functional dependencies is evaluated in the context of an on-board train control system from Bombardier Transportation AB in Sweden. For this system, the dependencies between the test cases were previously derived and are compared to the results our approach. The results show that of the two evaluated clustering algorithms, HDBSCAN has better performance than FCM or a dummy classifier. The classification methods' results are of reasonable quality and especially useful from an industrial point of view. Finally, performing a random undersampling approach to correct the imbalanced data distribution results in an F1 Score of up to 75% when applying the HDBSCAN clustering algorithm.",,978-1-7281-0492-8,10.1109/AITest.2019.00-13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718215,Software Testing;Paragraph Vectors;Test Case Dependency;Clustering Doc2Vec;HDBSCAN;FCM,Manuals;Software;Natural languages;Clustering algorithms;Semantics;Software testing,formal specification;learning (artificial intelligence);natural language processing;pattern classification;pattern clustering;program testing;railways;text analysis,automated functional dependency detection;HDBSCAN clustering algorithm;FCM clustering algorithm;on-board train control system;Bombardier Transportation AB;Sweden;F1 score;random undersampling approach;imbalanced data distribution;test case text-semantic similarities;Doc2Vec algorithm;test specification documents;test case dependencies;natural language;integration test cases;cost-effective test execution,,2,,42,,20-May-19,,,IEEE,IEEE Conferences
A time-based self-organising model for document clustering,基於時間的自組織文檔聚類模型,Chihli Hung; S. Wermter,"De Lin Inst. of Technol., Taiwan; NA",2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),17-Jan-05,2004,1,,17,22,"Most current approaches for document clustering do not consider the non-stationary feature of real world document collection. In this paper, in a non-stationary environment, we propose a new self-organising model, namely the dynamic adaptive self-organising hybrid (DASH) model. The DASH model runs continuously since the new document set is formed consecutively for training while the old document set is still at the training stage. Knowledge learned from the old data set is adjusted to reflect the new data set and therefore document clusters are up-to-date. We test the performance of our model using the Reuters-RCV1 news corpus and obtain promising results based on the criteria of classification accuracy and average quantization error.",1098-7576,0-7803-8359-1,10.1109/IJCNN.2004.1379861,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1379861,,Knowledge transfer;Testing;Quantization;Technological innovation;Space technology;Artificial neural networks;Hybrid intelligent systems;Stress;Prototypes;Learning systems,self-organising feature maps;pattern clustering,document clustering;time-based self-organising model;dynamic adaptive self-organising hybrid model;average quantization error;classification accuracy criteria,,1,,17,,17-Jan-05,,,IEEE,IEEE Conferences
Postal address block location by contour clustering,通過輪廓聚類的郵政地址塊位置,V. Govindaraju; S. Tulyakov,"Dept. of Comput. Sci. & Eng., UB Commons, Amherst, NY, USA; Dept. of Comput. Sci. & Eng., UB Commons, Amherst, NY, USA","Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,429,432 vol.1,"We have developed a well performing algorithm for locating address blocks in postal parcel images. Both machine printed and handwritten addresses are processed by the algorithm. The algorithm is invariant to the image orientation and scale, and it works with high noise images. It could also serve as an additional step after other address block location algorithms.",,0-7695-1960-1,10.1109/ICDAR.2003.1227703,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227703,,Clustering algorithms;Postal services;Text analysis;Algorithm design and analysis;Character recognition;Graphics;Image segmentation;Feature extraction;Venus;Image analysis,mailing systems;postal services;handwritten character recognition;feature extraction;algorithm theory;optical character recognition;image denoising;grammars,postal address block location;contour clustering;postal parcel images;machine printed address;handwritten address;noise images;mail sorting;mail delivery;connected components;feature extraction,,2,,15,,8-Sep-03,,,IEEE,IEEE Conferences
Modeling of New Technical Systems Using Cause-Effect Relationships,基於因果關係的新技術系統建模,S. V. Davydova; D. M. Korobkin; S. A. Fomenkov; S. G. Kolesnikov,"CAD Department, Volgograd State Technical University, Volgograd, Russia; CAD Department, Volgograd State Technical University, Volgograd, Russia; CAD Department, Volgograd State Technical University, Volgograd, Russia; CAD Department, Volgograd State Technical University, Volgograd, Russia","2018 9th International Conference on Information, Intelligence, Systems and Applications (IISA)",3-Feb-19,2018,,,1,4,"A unit of spatiotemporal associative memory is studied in the paper. The heart of the unit is a very simple and obvious dynamical system which may be considered as a grid with moving objects. As a result of objects' movement, preferable trajectories are formed, which are a basis of temporal associations (cause-effect relations). Developing connections between grid nodes and interface neurons determine spatial associations (similarity associations). It is shown that the unit is easily trained and retrained with its facilities approaching those for a more complex dynamic-neurons-based system. Also describes the process of documents filtering based on the SOM algorithm. It is known that self-organizing map - SOM is the neural network with unsupervised learning that performs the task of visualization and clustering. The idea of the network proposed by the Finnish scientist T. Kohonen. Is a method of projection of the multidimensional space into space with lower dimension (usually two-dimensional), it is also used for the decision of tasks of modeling, forecasting, etc., is a version of Kohonen neural networks. Self-organizing maps are used to solve tasks such as modeling, forecasting, identifying sets of independent attributes, data compression, and for finding patterns in large data sets. The most commonly described algorithm is applied for clustering data.",,978-1-5386-8161-9,10.1109/IISA.2018.8633683,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633683,spatio-temporal memory;a dynamical system;filtering;clustering;SOM;physics,Neurons;Excitons;Lattices;Filtering;Self-organizing feature maps;Clustering algorithms;Physics,content-addressable storage;pattern clustering;self-organising feature maps;unsupervised learning,spatiotemporal associative memory;heart;grid nodes;interface neurons;SOM algorithm;unsupervised learning;visualization;multidimensional space;Kohonen neural networks;self-organizing maps;clustering data;documents filtering,,,,12,,3-Feb-19,,,IEEE,IEEE Conferences
Analysis of initialization method on fuzzy c-means algorithm based on singular value decomposition for topic detection,基於奇異值分解的模糊c-均值算法的初始化方法用於主題檢測,I. Mursidah; H. Murfi,"Department of Mathematics, Universitas Indonesia, Depok 16424, Indonesia; Department of Mathematics, Universitas Indonesia, Depok 16424, Indonesia",2017 1st International Conference on Informatics and Computational Sciences (ICICoS),1-Feb-18,2017,,,213,218,"Topic detection is the process of finding the topics in a document collection. For a large amount of dataset, manual topic detection is difficult or even impossible. Thus, we need an automatic method known as Topic Detection and Tracking (TDT). One of the TDT methods used for topic detection problem is a clustering-based method such as fuzzy C-means (FCM). FCM works reasonably well on low-dimensional data but fails on high-dimensional data. In the high-dimensional data, a random-based initialization of FCM converges to one cluster center called center of gravity, so that all topics generated are similar. In this paper, we examine a non-random initialization by using singular value decomposition (SVD). Our simulations show that the SVD-based initialization method solves the center of gravity problem in a certain degree of fuzziness and gives a better accuracy than the random-based initialization.",,978-1-5386-0903-3,10.1109/ICICOS.2017.8276364,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276364,topic detection;clustering;fuzzy c-means;initialization;Singular Value Decomposition,Clustering algorithms;Voting;Gravity;Matrix decomposition;Singular value decomposition;Matrix converters;Twitter,fuzzy set theory;pattern clustering;singular value decomposition;text analysis,low-dimensional data;high-dimensional data;random-based initialization;FCM;nonrandom initialization;singular value decomposition;initialization method;manual topic detection;TDT methods;topic detection problem;document collection;Topic Detection and Tracking;cluster center,,1,,14,,1-Feb-18,,,IEEE,IEEE Conferences
SeaLab Advanced Information Retrieval,SeaLab高級信息檢索,F. Sangiacomo; A. Leoncini; S. Decherchi; P. Gastaldo; R. Zunino,"SeaLab, Univ. of Genoa, Genoa, Italy; SeaLab, Univ. of Genoa, Genoa, Italy; SeaLab, Univ. of Genoa, Genoa, Italy; SeaLab, Univ. of Genoa, Genoa, Italy; SeaLab, Univ. of Genoa, Genoa, Italy",2010 IEEE Fourth International Conference on Semantic Computing,11-Nov-10,2010,,,444,445,"Information Retrieval is a well established interdisciplinary topic in which machine learning, computational linguistic, computer programming and data mining merge together. SLAIR stands for Sea Lab Advanced Information Retrieval and is an efficient software architecture that embeds these issues in a unique framework. SLAIR is expandable both from the data format and algorithm point of view. A pluggable notion of distance between documents drives the subsequent clustering/classification machinery, moreover SLAIR is explicitly designed to manage large scale text mining problems. The demo will be focused on the versatility of the framework, the main goal is to show how the different metrics provided by SLAIR can enhance clustering/classification ability and eventually lead to different views of the underlying textual data.",,978-1-4244-7912-2,10.1109/ICSC.2010.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629072,Text Clustering;Hybrid Metric;Semantic Representation;WordNet;Kernel K-Means;Support Vector Machines,Semantics;Calibration;Clustering algorithms;Kernel;Training;Databases,data mining;information retrieval;learning (artificial intelligence);pattern clustering;software architecture,SeaLab advanced information retrieval;machine learning;computational linguistic;computer programming;data mining;software architecture;data format;documents drives;clustering-classification machinery;text mining;textual data,,7,,3,,11-Nov-10,,,IEEE,IEEE Conferences
Iterative clustering of high dimensional text data augmented by local search,通過局部搜索增強的高維文本數據的迭代聚類,I. S. Dhillon; Yuqiang Guan; J. Kogan,"Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","2002 IEEE International Conference on Data Mining, 2002. Proceedings.",10-Mar-03,2002,,,131,138,"The k-means algorithm with cosine similarity, also known as the spherical k-means algorithm, is a popular method for clustering document collections. However spherical k-means can often yield qualitatively poor results, especially when cluster sizes are small, say 25-30 documents per cluster, where it tends to get stuck at a local maximum far away from the optimal solution. In this paper, we present a local search procedure, which we call 'first-variation"" that refines a given clustering by incrementally moving data points between clusters, thus achieving a higher objective function value. An enhancement of first variation allows a chain of such moves in a Kernighan-Lin fashion and leads to a better local maximum. Combining the enhanced first-variation with spherical k-means yields a powerful ""ping-pong"" strategy that often qualitatively improves k-means clustering and is computationally efficient. We present several experimental results to highlight the improvement achieved by our proposed algorithm in clustering high-dimensional and sparse text data.",,0-7695-1754-4,10.1109/ICDM.2002.1183895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183895,,Clustering algorithms;Iterative algorithms;Data mining;Frequency;Mathematics;Statistics;Euclidean distance;Information retrieval;Refining,data mining;pattern clustering;search problems;text analysis,cosine similarity;document collection clustering;high dimensional text data;iterative clustering;local search;first variation;incremental data point movement;objective function value;local maximum;ping-pong strategy;sparse text data clustering;spherical k-means algorithm,,33,,18,,10-Mar-03,,,IEEE,IEEE Conferences
Document classification using nonnegative matrix factorization and underapproximation,使用非負矩陣分解和欠逼近的文檔分類,M. W. Berry; N. Gillis; F. Glineur,"Dept. of Electrical Engineering and Computer Science, University of Tennessee, 203 Claxton Complex, Knoxville, 37996-3450, USA; Center for Operations Research and Econometrics, Universit矇 catholique de Louvain, Voie du Roman Pays, 34, B-1348, Louvain-La-Neuve, Belgium; Center for Operations Research and Econometrics, Universit矇 catholique de Louvain, Voie du Roman Pays, 34, B-1348, Louvain-La-Neuve, Belgium",2009 IEEE International Symposium on Circuits and Systems,26-Jun-09,2009,,,2782,2785,"In this study, we use nonnegative matrix factorization (NMF) and nonnegative matrix underapproximation (NMU) approaches to generate feature vectors that can be used to cluster aviation safety reporting system (ASRS) documents obtained from the distributed national ASAP archive (DNAA). By preserving nonnegativity, both the NMF and NMU facilitate a sum-of-parts representation of the underlying term usage patterns in the ASRS document collection. Both the training and test sets of ASRS documents are parsed and then factored by both algorithms to produce a reduced-rank representations of the entire document space. The resulting feature and coefficient matrix factors are used to cluster ASRS documents so that the (known) associated anomalies of training documents are directly mapped to the feature vectors. Dominant features of test documents are then used to generate anomaly relevance scores for those documents.We demonstrate that the approximate solution obtained by NMU using Lagrangrian duality can lead to a better sum-of-parts representation and document classification accuracy.",2158-1525,978-1-4244-3827-3,10.1109/ISCAS.2009.5118379,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5118379,,Automatic speech recognition;Data mining;Testing;Cost function;Convergence;Computer science;Operations research;Econometrics;Electrical safety;Clustering algorithms,approximation theory;document handling;matrix decomposition,document classification;nonnegative matrix factorization;nonnegative matrix underapproximation;aviation safety reporting system document;distributed national ASAP archive;sum-of-part representation;coefficient matrix factor;Lagrangrian duality,,12,,23,,26-Jun-09,,,IEEE,IEEE Conferences
A study on partition quality of Fuzzy Co-clustering with exclusive item memberships,排他項目隸屬度模糊共聚的劃分質量研究,K. Honda; T. Nakano; S. Ubukata; A. Notsu,"Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531 Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531 Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531 Japan; Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531 Japan","2015 International Conference on Informatics, Electronics & Vision (ICIEV)",23-Nov-15,2015,,,1,4,"Bag-of-Words data analysis is a fundamental issue in web data mining for Big Data utilization, and Co-clustering is often applied to cooccurrence information analysis in such problems of document-keyword association research. In probabilistic partition models such as Multinomial Mixtures and Fuzzy c-Means-type ones, different partition constraints are forced to rows (objects) and columns (items), and then item memberships may not be useful in revealing item partitions. A possible approach in clarifying the interpretability of item partitions is additional penalization for exclusive item memberships, which was shown to emphasize cluster-wise representative items in document analysis. In this paper, the utility of the penalization approach is further studied through comparisons of partition qualities with several benchmark data sets. Several experimental results show that the additional penalty may sometime contribute to slightly improving the partition quality in addition to improvement of interpretability of co-cluster partitions.",,978-1-4673-6902-2,10.1109/ICIEV.2015.7334058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334058,,Terrorism;Text analysis;Probabilistic logic;Benchmark testing;Big data;Mixture models,Big Data;data mining;document handling;fuzzy set theory;information analysis;Internet;pattern clustering;probability,benchmark data sets;penalization approach;document analysis;cluster-wise representative items;item partition interpretability clarification;partition constraints;fuzzy c-means-type ones;multinomial mixtures;probabilistic partition models;document-keyword association research;cooccurrence information analysis;big data utilization;Web data mining;bag-of-words data analysis;exclusive item memberships;fuzzy coclustering;partition quality improvement,,,,11,,23-Nov-15,,,IEEE,IEEE Conferences
A customer intention aware system for document analysis,客戶意圖感知系統，用於文檔分析,J. Ji; D. Kunita; Q. Zhao,"System Intelligence Lab, Department of Computer Sciences, University of Aizu, 965-8580 Aizu-wakamatsu, Fukushima, Japan; System Intelligence Lab, Department of Computer Sciences, University of Aizu, 965-8580 Aizu-wakamatsu, Fukushima, Japan; System Intelligence Lab, Department of Computer Sciences, University of Aizu, 965-8580 Aizu-wakamatsu, Fukushima, Japan",The 2010 International Joint Conference on Neural Networks (IJCNN),14-Oct-10,2010,,,1,6,"Document classification tasks can be divided into two sorts: supervised document classification and unsupervised document classification. Supervised learning algorithm always has a better performance than unsupervised learning. However, it is very difficult to assign enough teacher signal. In this study, we developed a customer intention aware system for document analysis. The system starts from an unlabeled document set, give out several cluster results. The user could fine tune the classifier by modifying some key documents' labels. After several circles of learning and feedback, the system will finally understand the users intention and generates a suitable expert system. This is a kind of semi-supervised learning. For clustering, we use weighted comparative advantage (WCA) algorithm for clustering and supervised WCA for classification algorithm, respectively.",2161-4407,978-1-4244-6918-5,10.1109/IJCNN.2010.5596289,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5596289,,Clustering algorithms;Algorithm design and analysis;Prototypes;Supervised learning;Abstracts;Text analysis;Humans,classification;document handling;expert systems;learning (artificial intelligence),customer intention aware system;document analysis;document classification;unsupervised learning;expert system;semi-supervised learning;weighted comparative advantage,,2,,13,,14-Oct-10,,,IEEE,IEEE Conferences
"A Comment on ?A Similarity Measure for Text Classification and Clustering??,N. Kumar Nagwani""",註釋“用於文本分類和聚類的A相似性度量”，N。 Kumar Nagwani”,"National Institute of Technology Raipur, Raipur, G E Road, India",IEEE Transactions on Knowledge and Data Engineering,4-Aug-15,2015,27,9,2589,2590,"A similarity measure namely, similarity measure for text processing (SMTP) is proposed by Lin et al. [1] for knowledge discovery on text collection. The proposed measure considered the three cases for similarity measurements between the pairs of documents. These cases are based on absence and presence of features in the pair of text documents. The first case covers the features appearing in both of the documents, second case covers the features appears in only one document and the third case covers the features appears in none of the documents. The proposed similarity measure considered to be ideal for finding similarity between the pair of text documents on the basis of presence or absence of features available in text documents, however, while exploring the SMTP similarity measurement it is found that the case of measuring similarity between the pair of similar documents is not covered. The objective of this work is to highlight this gap and propose a minor change to make the SMTP a complete similarity measurement technique for knowledge discovery in line with the other standard similarity techniques.",1558-2191,,10.1109/TKDE.2015.2451616,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177179,,Measurement techniques;Text processing;Knowledge discovery;Classification,data mining;pattern classification;pattern clustering;text analysis,text classification;text clustering;similarity measure for text processing;knowledge discovery;text collection;text documents;SMTP similarity measurement;similarity techniques,,14,,3,,4-Aug-15,,,IEEE,IEEE Journals,
Information Retrieval System in Bangla Document Ranking using Latent Semantic Indexing,使用潛在語義索引的孟加拉語文檔排名中的信息檢索系統,M. N. Hoque; R. Islam; M. S. Karim,"Science and Technology University,Dept. of Computer Science & Engineering,Gopalganj,Bangladesh,8100; Science and Technology University,Dept. of Computer Science & Engineering,Gopalganj,Bangladesh,8100; Science and Technology University,Dept. of Computer Science & Engineering,Gopalganj,Bangladesh,8100","2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)",19-Dec-19,2019,,,1,5,"Nowadays, like the English and other languages, Bangla also plays a significant role to strengthen the web repository. The storing rate of Bangla information is augmented day-by-day. Because of the numerous documents in the World Wide Web, it is very difficult for a user to retrieve the desired information. Furthermore, finding the useful documents tends to be more time spending as well as an annoying job. These demands emerge to develop an Information Retrieval (IR) system to document ranking for Bangla language. In this paper, we have built such a retrieval system where users can find their needed documents which correspond to their own query strings throughout the ranking index. Although a lot of works have been done for English and other languages to rank the documents, unfortunately, we have found a very negligible amount of contributions in Bangla Language. Many methods such as - Boolean model, Maximal Marginal Relevance (MMR), Portfolio Theory (PR), Quantum Probability Ranking Principle (QPRP), Query Directed Clustering (QDC), Vector-based TFIDF and so on, have been proposed to implement the document ranking system. Here, we have applied a new approach, called Latent Semantic Indexing (LSI) to do the same task for Bangla documents. LSI uses the mathematical method called Singular Value Decomposition (SVD). After that, we have applied the cosine similarity to rank all the documents. We believe that the performance result of our proposed system has reached the trustworthy level.",,978-1-7281-3445-1,10.1109/ICASERT.2019.8934837,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8934837,IR;Bangla document ranking;LSI;SVD;Cosine similarity,Large scale integration;Matrix decomposition;Eigenvalues and eigenfunctions;Semantics;Indexing;Singular value decomposition;Information retrieval,indexing;Internet;natural language processing;query processing;singular value decomposition;string matching;text analysis,English language;Web repository;World Wide Web;information retrieval system;Bangla language;latent semantic indexing;Bangla document ranking system;query strings;singular value decomposition,,,,15,,19-Dec-19,,,IEEE,IEEE Conferences
Clustering of automobile information using self organizing maps,使用自組織地圖對汽車信息進行聚類,P. R. Dalal; V. Joshi-lnamdar,"Gov. Eng. Coll., Pune, India; Gov. Eng. Coll., Pune, India",TENCON 2003. Conference on Convergent Technologies for Asia-Pacific Region,15-Mar-04,2003,4,,1267,1271 Vol.4,"This work develops methodology for clustering automobile related collection of documents. The work implements the system that organizes vast document collection according to textual similarity. It uses neural network approach to cluster collection of data, based on Kohonen's self-organizing maps (SOM ) which is unsupervised competitive method of learning.",,0-7803-8162-9,10.1109/TENCON.2003.1273119,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1273119,,Automobiles;Self organizing feature maps;Neurons;Educational institutions;Data mining;Vehicle safety;Web sites;Databases;Clustering algorithms;Data analysis,self-organising feature maps;pattern clustering;text analysis;unsupervised learning;automobile industry,self organizing maps;automobile information;clustering;textual similarity;neural network approach;learning;text semantics,,,,7,,15-Mar-04,,,IEEE,IEEE Conferences
Stochastic gradient descent based fuzzy clustering for large data,大數據的基於隨機梯度下降的模糊聚類,Y. Wang; L. Chen; J. Mei,"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China",2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),8-Sep-14,2014,,,2511,2518,"Data is growing at an unprecedented rate in commercial and scientific areas. Clustering algorithms for large data which require small memory consumption and scalability become increasingly important under this circumstance. In this paper, we propose a new clustering approach called stochastic gradient based fuzzy clustering(SGFC) which achieves the optimization based on stochastic approximation to handle such kind of large data. We derive an adaptive learning rate which can be updated incrementally and maintained automatically in gradient descent approach employed in SGFC. Moreover, SGFC is extended to a mini-batch SGFC to reduce the stochastic noise. Additionally, multi-pass SGFC is also proposed to improve the clustering performance. Experiments have been conducted on synthetic data to show the effectiveness of our derived adaptive learning rate. Experimental studies have been also conducted on several large benchmark datasets including real world image and document datasets. Compared with existing fuzzy clustering approaches for large data, the mini-batch SGFC shows comparable or better accuracy with significant less time consumption. These results demonstrate the great potential of SGFC for large data analysis.",1098-7584,978-1-4799-2072-3,10.1109/FUZZ-IEEE.2014.6891755,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891755,,Clustering algorithms;Equations;Mathematical model;Memory management;Algorithm design and analysis;Noise;Educational institutions,data analysis;fuzzy set theory;gradient methods;learning (artificial intelligence);pattern clustering,stochastic gradient descent;fuzzy clustering;large data analysis;clustering algorithm;SGFC approach;stochastic gradient based fuzzy clustering;stochastic approximation;adaptive learning rate;minibatch SGFC;multipass SGFC;clustering performance;synthetic data;image dataset;document dataset,,6,,26,,8-Sep-14,,,IEEE,IEEE Conferences
A system for document binarization,文件二值化系統,E. Badekas; N. Papamarkos,"Dept. of Electr. & Comput. Eng., Democritus Univ. of Thrace, Xanthi, Greece; Dept. of Electr. & Comput. Eng., Democritus Univ. of Thrace, Xanthi, Greece","3rd International Symposium on Image and Signal Processing and Analysis, 2003. ISPA 2003. Proceedings of the",10-May-04,2003,2,,909,914 Vol.2,"This paper presents a system for binarization of digital documents. The system comprises the benefits of a set of other binarization techniques by combining their results. This is necessary for bad illuminated and degraded document where there are many pixels that cannot be easily classified as foreground or background. For this reason, it is necessary to perform the final binarization by exploiting the results of a set of binarization algorithms, especially for the document pixels that have high vagueness. Also, in this paper significant improvements are proposed for two of the methods used, i.e. for the Adaptive Logical Level Technique (ALLT) and the Improvement of Integrated Function Algorithm (IIFA). The entire system is extensively tested with a variety of degraded and bad-illuminated documents.",,953-184-061-X,10.1109/ISPA.2003.1296408,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296408,,Pixel;Gray-scale;Fuzzy logic;Testing;Degradation;Clustering algorithms;Geometry,document image processing;fuzzy logic,digital document binarization;binarization algorithm;document pixel;adaptive logical level technique;improvement integrated function algorithm;degraded document;bad-illuminated document,,5,,22,,10-May-04,,,IEEE,IEEE Conferences
Comparison of Sentiment Analysis of Review Comments by Unsupervised Clustering of Features Using LSA and LDA,使用LSA和LDA的無監督特徵聚類對評論評論的情感分析的比較,S. Tseng; Y. Lu; G. Chakraborty; L. Chen,"Iwate Prefectural University,Faculty of Software and Information Science,Iwate,Japan; Iwate Prefectural University,Faculty of Software and Information Science,Iwate,Japan; Iwate Prefectural University,Faculty of Software and Information Science,Iwate,Japan; Chaoyang University of Technology,Department of Information Management,Taichung,Taiwan",2019 IEEE 10th International Conference on Awareness Science and Technology (iCAST),5-Dec-19,2019,,,1,6,"Text documents could be classified using words as features. As the number of words in the vocabulary is large, the dimension of the document space will be very high. In that case, the feature vector for a document is too long, and very sparse, and it makes clustering and classification algorithms fail. There are various ways to reduce this dimension. In this work, we used Latent Semantic Analysis (LSA), which is actuated by Singular Value Decomposition (SVD). After SVD, we have a compact representation of the documents, which are clustered. In a separate experiment, we did topic modeling using Latent Dirichlet Allocation (LDA). In this initial work, our premise is that comments are of two categories, positive and negative. We cluster the document, in the reduced dimensional space, into two, using K-means clustering. After dimension reduction by LSA and LDA, the ground truth for documents in two clusters was verified manually, and the results compared.In this work, we used tourists' comments as documents. Tourists visit to a place is influenced by comments from previous visitors. Our final goal is to extract factors that lead to positive comments and those leading to negative comments. That would help promoting tourist business by focusing on the factors that really matters for the customers.",2325-5994,978-1-7281-3821-3,10.1109/ICAwST.2019.8923267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923267,Text miming;Customer comments;Latent Semantic Analysis (LSA);Latent Dirichlet Allocation (LDA),Matrix decomposition;Semantics;Time division multiplexing;Resource management;Dimensionality reduction;Text mining;Software,data mining;pattern classification;pattern clustering;sentiment analysis;singular value decomposition;travel industry,positive comments;negative comments;sentiment Analysis;review comments;unsupervised clustering;text documents;document space;feature vector;classification algorithms;Latent Semantic Analysis;LSA;SVD;Latent Dirichlet Allocation;LDA;reduced dimensional space;dimension reduction;tourists;k-means clustering,,,,25,,5-Dec-19,,,IEEE,IEEE Conferences
Ranking Weak-Linked Documents on the Web,在網絡上對弱鏈接文檔進行排名,C. Zhou; C. Duda; Y. Lu,"Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China; Inst. fur Informationssyst., ETH Zurich, Zurich, Switzerland; Coll. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,1,,510,514,"The current commercial Web search engines do a good job at ranking web pages with hyperlink information. However, there are also many common documents such as PowerPoint files or Flash files which do not have enough hyperlink information. We call such documents weak-linked documents. Current search engines return therefore either completely irrelevant results or poorly ranked documents when searching for these files. This paper addresses this problem and proposes a solution: RoC (Ranking weak linked documents based on Clustering). For a given query q, RoC (1) first clusters traditional Web page search results in order to find what topics existing on the WWW are interesting to the users, (2) then assigns a weight to each topic cluster based on the ranks of the web pages in it, and finally (3) ranks all relevant weak-linked documents based on their similarity to the weighted clusters obtained from the Web. The experiments show that our approach considerably improves the result quality of current search engines and that of latent semantic indexing.",,978-0-7695-3735-1,10.1109/FSKD.2009.749,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358529,,Web pages;TV;Search engines;Web search;World Wide Web;Uniform resource locators;HTML;Fuzzy systems;Educational institutions;Computer science,Internet;pattern clustering;search engines,search engines;weak-linked documents ranking;pattern clustering;latent semantic indexing;Web pages;PowerPoint document files;Flash document files;hyperlink information,,,,12,,28-Dec-09,,,IEEE,IEEE Conferences
Lethe: Cluster-Based Indexing for Secure Multi-user Search,Lethe：用於安全多用戶搜索的基於群集的索引,E. C. Micheli; G. Margaritis; S. V. Anastasiadis,"Dept. of Comput. Sci. & Eng., Univ. of Ioannina, Ioannina, Greece; Dept. of Comput. Sci. & Eng., Univ. of Ioannina, Ioannina, Greece; Dept. of Comput. Sci. & Eng., Univ. of Ioannina, Ioannina, Greece",2014 IEEE International Congress on Big Data,25-Sep-14,2014,,,323,330,"Secure keyword search in shared infrastructures prevents stored documents from leaking sensitive information to unauthorized users. A shared index provides confidentiality if it is exclusively used by users authorized to search all the indexed documents. We introduce the Lethe indexing workflow to improve query and update efficiency in secure keyword search. The Lethe workflow clusters together documents with similar sets of authorized users, and creates shared indices for configurable document subsets accessible by the same users. We examine different datasets based on the empirical statistics of a document sharing system and alternative theoretical distributions. We apply Lethe to generate indexing organizations of different tradeoffs between the search and update cost. With measurements over an open-source distributed search engine, we experimentally confirm the improved search and update performance of particular configurations that we introduce.",2379-7703,978-1-4799-5057-7,10.1109/BigData.Congress.2014.54,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906797,confidentiality;privacy;access control;data storage;outsourced services;document sharing;search engines,Indexing;Time factors;Search engines;Silicon;Keyword search;Organizations,document handling;indexing;query processing;search engines,cluster-based indexing;secure multiuser keyword search;shared infrastructures;sensitive information;Lethe indexing;Lethe workflow clusters;document sharing system;open-source distributed search engine,,,,28,,25-Sep-14,,,IEEE,IEEE Conferences
A Modified Fuzzy C Means Clustering Using Neutrosophic Logic,使用中智邏輯的改進的模糊C均值聚類,N. Akhtar; M. V. Ahmad,"Dept. of Comput. Eng., Aligarh Muslim Univ., Aligarh, India; Dept. of Comput. Eng., Aligarh Muslim Univ., Aligarh, India",2015 Fifth International Conference on Communication Systems and Network Technologies,1-Oct-15,2015,,,1124,1128,"A cluster can be defined as the collection of data objects grouped into the same group which are similar to each other whereas data objects which are different are grouped into different groups. The process of grouping a set objects into classes of similar objects is called clustering. In fuzzy c means clustering, every data point belongs to every cluster by some membership value. Hence, every cluster is a fuzzy set of all data points. Neutrosophic logic adds a new component ""indeterminacy"" to the fuzzy logic. In Neutrosophic Logic, the rule of thumb is that every idea has a certain degree of truthiness, falsity and indeterminacy which are to be considered independently from others. In our proposed algorithm, we have used Neutrosophic logic to add the indeterminacy factor in the Fuzzy C-Means Algorithm. We have modified the formula of calculating the membership value as well as the cluster center calculation and generated the clusters of documents as output.",,978-1-4799-1797-6,10.1109/CSNT.2015.164,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280095,Clustering;Fuzzy Logic;Fuzzy C Means;Neutrosophic Logic,Clustering algorithms;Algorithm design and analysis;Electronic countermeasures;Data mining;Partitioning algorithms;Fuzzy logic;Computers,data mining;fuzzy logic;fuzzy set theory,modified fuzzy c mean clustering;neutrosophic logic;data collection;fuzzy logic,,3,,17,,1-Oct-15,,,IEEE,IEEE Conferences
Unsupervised Decomposition of Color Document Images by Projecting Colors to a Spherical Surface,通過將顏色投影到球面來對彩色文檔圖像進行無監督分解,Y. He; J. Sun; S. Naoi; Y. Fujii; K. Fujimoto,"Fujitsu R&D Co. Ltd., Beijing; Fujitsu R&D Co. Ltd., Beijing; Fujitsu R&D Co. Ltd., Beijing; Fujitsu Labs. Ltd., Kawasaki; Fujitsu Labs. Ltd., Kawasaki",2008 The Eighth IAPR International Workshop on Document Analysis Systems,11-Nov-08,2008,,,394,401,"A decomposition method for color document images is proposed in this paper. A two dimensional feature surface is constructed from the input color image, and then a novel and unsupervised method based on contour lines is proposed to segment the surface. In detail, colors of the image pixels are firstly projected to a spherical surface whose center is the background color. The projection is used to transform the observed colors to the corresponding 'ideal' foreground colors. Then the spherical surface is segmented into several non-overlapped regions, and each region corresponds to an individual layer of the input color document image. Finally, the image pixels are projected to the spherical surface and classified to the corresponding layers.",,978-0-7695-3337-7,10.1109/DAS.2008.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669986,,Surface morphology;Image color analysis;Image segmentation;Shape;Pixel;Clustering methods;Image decomposition;Text analysis;Image analysis;Helium,document image processing;image colour analysis;image segmentation,unsupervised decomposition;color document images;spherical surface;contour lines;images pixels,,2,,8,,11-Nov-08,,,IEEE,IEEE Conferences
On foreground-background separation in low quality color document images,在低質量彩色文檔圖像中進行前景背景分離,Utpal Garain; T. Paquet; L. Heutte,"Comput. Vision & Pattern Recognition Unit, Indian Stat. Inst., Kolkata, India; NA; NA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,585,589 Vol. 2,"This paper proposes an adaptive method for separation of foreground and background in low quality color document images. A connected component labelling is initially implemented to capture the spatially connected similar color pixels. Next, dominant background components are determined to divide the entire image into number of grids each representing local uniformity in illumination, background, etc. Finally foreground parts are located using local information around them. Several color images of old historical documents including manuscripts of high importance are used in the experiment. Apart from a qualitative evaluation, results are quantitatively compared with one popular foreground/background separation technique.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.174,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575612,,Clustering algorithms;Lighting;Labeling;Smoothing methods;Image color analysis;Computer vision;Pattern recognition;Image segmentation;Internet;Layout,document image processing;image colour analysis,adaptive foreground-background separation;low quality color document image;component labelling,,1,,10,,16-Jan-06,,,IEEE,IEEE Conferences
Semi-Supervised Clustering of XML Documents: Getting the Most from Structural Information,XML文檔的半監督群集：從結構信息中獲得最大收益,E. Bezerra; M. Mattoso; G. Xexeo,"CEFET/RJ & COPPE/UFRJ, Brazil; NA; NA",22nd International Conference on Data Engineering Workshops (ICDEW'06),24-Apr-06,2006,,,88,88,"As document providers can express more contextualized and complex information, semi-structured documents are becoming a major source of information in many areas, e.g., in digital libraries, e-commerce or Web applications. A particular characteristic of such document collections is the existence of some structure or metadata along with the data. In this scenario, clustering methods that can take advantage of such structural information to better organize such collections are highly relevant. Semi-structured documents pose new challenges to document clustering methods, however, since it is not clear how this structural information can be used to improve the quality of the generated clustering models. On the other hand, recently there has a growing interest in the semi-supervised clustering task, in which a little amount of prior knowledge is provided to guide the algorithm to a better clustering model. A particular type of semi-supervision is in the form of user-provided constraints defined over pairs of objects, where each pair informs if its objects must be in the same or in different clusters. In this paper, we consider the problem of constrained clustering in documents that present some form of structural information. We consider the existence of a particular form of information to be clustered: textual documents that present a logical structure represented in XML format. We define and extend methods to improve the quality of clustering results by using such structural information to guide the execution of the constrained clustering algorithm. Experimental results on the OHSUMED document collection show the effectiveness of our approach.",,0-7695-2571-7,10.1109/ICDEW.2006.136,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1623883,,XML;Clustering algorithms;Clustering methods;Information resources;Software libraries;Partitioning algorithms;Bellows;Data engineering;Conferences,,,,1,,17,,24-Apr-06,,,IEEE,IEEE Conferences
Clustering of Count Data Using Generalized Dirichlet Multinomial Distributions,使用廣義Dirichlet多項式分佈對計數數據進行聚類,N. Bouguila,"Concordia Univ., Montreal",IEEE Transactions on Knowledge and Data Engineering,26-Feb-08,2008,20,4,462,474,"In this paper, we examine the problem of count data clustering. We analyze this problem using finite mixtures of distributions. The multinomial distribution and the multinomial Dirichlet distribution (MDD) are widely accepted to model count data. We show that these two distributions cannot be the best choice in all the applications, and we propose another model called the multinomial generalized Dirichlet distribution (MGDD) that is the composition of the generalized Dirichlet distribution and the multinomial, in the same way that the MDD is the composition of the Dirichlet and the multinomial. The estimation of the parameters and the determination of the number of components in our model are based on the deterministic annealing expectation-maximization (DAEM) approach and the minimum description length (MDL) criterion, respectively. We compare our method to standard approaches such as multinomial and multinomial Dirichlet mixtures to show its merits. The comparison involves different applications such as spatial color image databases indexing, handwritten digit recognition, and text document clustering.",1558-2191,,10.1109/TKDE.2007.190726,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407701,clustering;Feature extraction;Image databases;clustering;Feature extraction;Image databases,Application software;Image databases;Image recognition;Text recognition;Parameter estimation;Annealing;Color;Spatial databases;Indexing;Handwriting recognition,expectation-maximisation algorithm;pattern clustering;statistical distributions,count data clustering;multinomial generalized Dirichlet distribution;parameter estimation;deterministic annealing expectation-maximization approach;minimum description length criterion;spatial color image databases indexing;handwritten digit recognition;text document clustering,,53,,69,,26-Feb-08,,,IEEE,IEEE Journals
NoCS2: Topic-Based Clustering of Big Data Text Corpus in the Cloud,NoCS2：基於主題的雲大數據文本語料庫集群,S. M. Zobaed; E. Haque; S. Kaiser; R. F. Hussain,"Sch. of Comput. & Inf., Univ. of Louisiana, Lafayette, LA, USA; University of Louisiana, School of Computing and Informatics, Lafayette, LA, 70504, USA; Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh; University of Louisiana, School of Computing and Informatics, Lafayette, LA, 70504, USA",2018 21st International Conference of Computer and Information Technology (ICCIT),3-Feb-19,2018,,,1,6,"Cloud services are widely deployed to store and process big data. Organizations who deal with big data, especially large document set, prefer utilizing cloud services for storage and computational efficiency. However, for processing large text corpus, an inefficient data processing is computationally expensive for real-time systems. In addition, efficient memory utilization is important to cluster big data including large text corpus. Clustering of the large text corpus is an important component of various document retrieval systems such as PubMed1. To address these challenges, in this paper, we present NoCS2 (Number of Cluster and Seed Selection) for efficient topic-based clustering from unstructured big data in the cloud. NoCS2 relies on computing and storage services in the cloud server. Traditional clustering solutions for text dataset consider a fixed number of clusters irrespective of the dataset size and characteristics such as science and technology. Alternatively, our solution dynamically determines the appropriate k number of clusters based on the characteristics of the dataset. Particularly, we use precomputed matrix trace as the number of clusters for a dataset that represents the total number of keywords using vector representation. Then, we build k clusters using topic-based similarity among keywords. Finally, we compare our proposed method with two state-of-the-art clustering methods. Empirical results demonstrate that the average closeness score of NoCS2 is better than other methods for large and sparse datasets.",,978-1-5386-9242-4,10.1109/ICCITECHN.2018.8631951,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631951,clustering;big data;vector representation;precomputed matrix;closeness,Indexes;Big Data;Mathematical model;Clustering algorithms;Organizations;Servers;Time complexity,Big Data;cloud computing;matrix algebra;pattern clustering;text analysis;vectors,NoCS2;unstructured big data;cloud server;topic-based similarity;big data text corpus;cloud services;topic-based clustering;Number of Cluster and Seed Selection;precomputed matrix trace;vector representation,,2,,29,,3-Feb-19,,,IEEE,IEEE Conferences
A Query Specific Graph Based Approach to Multi-document Text Summarization: Simultaneous Cluster and Sentence Ranking,基於查詢特定圖的多文檔文本摘要方法：同時聚類和句子排序,S. R. Pandit; M. A. Potey,"Dept. of Comput. Eng., D.Y. Patil Coll. of Eng., Pune, India; Dept. of Comput. Eng., D.Y. Patil Coll. of Eng., Pune, India",2013 International Conference on Machine Intelligence and Research Advancement,9-Oct-14,2013,,,213,217,"Recently the focus of query independent summary is shifted to query specific document summarization. This paper presents a graph based method to find query specific multi-document summarization. Our system is divided into two stages, off-line and on-line. We construct document as graph by considering paragraph as nodes in off-line stage. Edge scores are represented node similarities. In online stage, query specific weight are calculated and assigned to node. We then perform keyword search on the document graph and search a minimum top spanning tree for finding relevant nodes that satisfy the keyword search. Resultant summary looks coherent due to simultaneous cluster and sentence ranking. Experimental results for multi-document scenarios are encouraging.",,978-0-7695-5013-8,10.1109/ICMIRA.2013.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918824,Query-specific Summarization;ranking;Minimum spanning tree;Pre-processing;Stop Word,Clustering algorithms;Semantics;Keyword search;Machine intelligence;Redundancy;Computers;Educational institutions,graph theory;pattern clustering;query processing;text analysis,query specific graph based approach;multidocument text summarization;cluster ranking;sentence ranking;query independent summary;paragraph;edge score;node similarity;query specific weight;keyword search,,2,,9,,9-Oct-14,,,IEEE,IEEE Conferences
Using Burstiness to Improve Clustering of Topics in News Streams,利用突發性來改善新聞流中的主題聚類,Q. He; K. Chang; E. Lim,"Nanyang Technol. Univ., Nanyang Avenue; Nanyang Technol. Univ., Nanyang Avenue; Nanyang Technol. Univ., Nanyang Avenue",Seventh IEEE International Conference on Data Mining (ICDM 2007),12-Mar-08,2007,,,493,498,"Specialists who analyze online news have a hard time separating the wheat from the chaff. Moreover, automatic data-mining techniques like clustering of news streams into topical groups can fully recover the underlying true class labels of data if and only if all classes are well separated. In reality, especially for news streams, this is clearly not the case. The question to ask is thus this: if we cannot recover the full C classes by clustering, what is the largest K < C clusters we can find that best resemble the K underlying classes? Using the intuition that bursty topics are more likely to correspond to important events that are of interest to analysts, we propose several new bursty vector space models (B-VSM)for representing a news document. B-VSM takes into account the burstiness (across the full corpus and whole duration) of each constituent word in a document at the time of publication. We benchmarked our B-VSM against the classical TFIDF-VSM on the task of clustering a collection of news stream articles with known topic labels. Experimental results show that B-VSM was able to find the burstiest clusters/topics. Further, it also significantly improved the recall and precision for the top K clusters/topics.",2374-8486,978-0-7695-3018-5,10.1109/ICDM.2007.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4470279,,Nominations and elections;Data mining;Helium;Data engineering;Functional analysis;Organizing;Clustering methods;Telecommunication traffic,data mining;document handling;information resources;media streaming,burstiness;topics clustering;online news analysis;automatic data mining;news stream clustering;bursty topics;bursty vector space model;news document representation;news stream article;topic label,,25,,25,,12-Mar-08,,,IEEE,IEEE Conferences
Multi-Oriented English Text Line Extraction Using Background and Foreground Information,使用背景和前景信息的多方位英語文本行提取,P. P. Roy; U. Pal; J. Llad籀s; F. Kimura,"Comput. Vision Center, Univ. Autonoma De Barcelona, Barcelona; Comput. Vision & Pattern Recognition Unit, Indian Stat. Inst., Kolkata; Comput. Vision Center, Univ. Autonoma De Barcelona, Barcelona; Grad. Sch. of Eng., Mie Univ., Mie",2008 The Eighth IAPR International Workshop on Document Analysis Systems,11-Nov-08,2008,,,315,322,"In graphical documents (map, engineering drawing), artistic documents etc. there exist many printed materials where text lines are not parallel to each other and they are multi-oriented and curve in nature. For the OCR of such documents we need to extract individual text lines from the documents. Extraction of individual text lines from multi-oriented and/or curved text document is a difficult problem. In this paper, we propose a novel method to extract individual text lines from such document pages and the method is based on the foreground and background information of the characters of the text. To take care of background information, water reservoir concept is used here. In the proposed scheme at first, individual components are detected and grouped into 3-character clusters using their inter-component distance, size and positional information. Applying concept of graph, initial 3-character clusters are merged to have larger cluster group. Using inter-character background information, orientations of the extreme characters of a larger cluster are decided and based on these orientation, two candidate regions are formed from the cluster. Finally, with the help of these candidate regions, individual lines are extracted. From the experiment, we obtained encouraging result.",,978-0-7695-3337-7,10.1109/DAS.2008.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4669976,,Data mining;Water resources;Reservoirs;Optical character recognition software;Image segmentation;Character recognition;Text analysis;Information analysis;Pattern analysis;Pattern recognition,document image processing;feature extraction;optical character recognition;text analysis,multioriented english text line extraction;foreground information;background information;graphical documents;artistic documents;curved text document;document pages,,10,,11,,11-Nov-08,,,IEEE,IEEE Conferences
Organizing Web Documents Resulting from an Information Retrieval System Using Formal Concept Analysis,使用形式概念分析組織信息檢索系統產生的Web文檔,N. N. Myat; Khin Haymar Saw Hla,"University of Computer Studies, Yangon niimucsm@yahoo.com; NA",6th Asia-Pacific Symposium on Information and Telecommunication Technologies,13-Feb-06,2005,,,198,203,"To discover information needs among information resulting from an information retrieval (IR) system by a user, it is needed to be managed them in some effective ways. Document clustering is a common and useful technique for Web information retrieval. In this paper, we use formal concept analysis (FCA) method for reorganizing documents resulting from an IR system according to their formal concepts. We use tf.idf (term frequency x inverse document frequency) term weighting scheme in selecting terms from the documents to construct the formal context of documents and terms which will then be used to extract formal concepts among resulting documents. We use text mining technique, association rule mining, on the frequent termsets of the given domain to analyze relationships of terms existing in the resulted documents. Finally, the concept lattice of the documents is built on these associated terms to obtain formal concept based clustered resulted documents by discovering ordering relations among frequent termsets using lattice theory",,4-88552-216-1,10.1109/APSITT.2005.203656,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1593463,Information Retrieval;Concept-based;Document Clustring;Formal Concept Analysis;Frequent termsets;Text mining;Association Rule,Organizing;Information retrieval;Information analysis;Frequency;Lattices;Text mining;Association rules;Data mining,information retrieval;Internet,Web documents;information retrieval system;formal concept analysis;document clustering;inverse document frequency;term frequency;weighting scheme;text mining technique;rule mining;lattice theory,,4,,18,,13-Feb-06,,,IEEE,IEEE Conferences
Clustering of Web Documents with Structure of Webpages based on the HTML Document Object Model,基於HTML文檔對像模型的Web文檔與網頁結構的聚類,M. K. Sarma; A. K. Mahanta,"Gauhati University,Department of Computer Science,Assam,Guwahati,India,781014; Gauhati University,Department of Computer Science,Assam,Guwahati,India,781014","2019 IEEE International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",9-Jan-20,2019,,,1,6,The following topics are dealt with: power engineering computing; Matlab; photovoltaic power systems; feature extraction; power grids; power supply quality; power generation control; maximum power point trackers; learning (artificial intelligence); solar cell arrays.,,978-1-5386-9543-2,10.1109/INCOS45849.2019.8951405,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951405,Web Mining;Clustering;Webpage Structure;K-means;HTML-DOM,Clustering algorithms;Partitioning algorithms;Web mining;Flowcharts;Hardware;Software,feature extraction;Matlab;photovoltaic power systems;power engineering computing;power grids,power engineering computing;Matlab;photovoltaic power systems;feature extraction;power grids;power supply quality;power generation control;maximum power point trackers;learning (artificial intelligence);solar cell arrays,,,,23,,9-Jan-20,,,IEEE,IEEE Conferences
Discovering Event Evolution Patterns From Document Sequences,從文檔序列中發現事件演化模式,C. Wei; Y. Chang,"Inst. of Technol. Manage., Nat. Tsing Hua Univ., Hsinchu; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20-Feb-07,2007,37,2,273,283,"Recent advances in information and networking technologies have contributed significantly to global connectivity and greatly facilitated and fostered information creation, distribution, and access. The resultant ever-increasing volume of online textual documents creates an urgent need for new text mining techniques that can intelligently and automatically extract implicit and potentially useful knowledge from these documents for decision support. This research focuses on identifying and discovering event episodes together with their temporal relationships that occur frequently (referred to as evolution patterns (EPs) in this paper) in sequences of documents. The discovery of such EPs can be applied in domains such as knowledge management and used to facilitate existing document management and retrieval techniques [e.g., event tracking (ET)]. Specifically, we propose and design an EP discovery technique for mining EPs from sequences of documents. We experimentally evaluate our proposed EP technique in the context of facilitating ET. Measured by miss and false alarm rates, the EP-supported ET (EPET) technique exhibits better tracking effectiveness than a traditional ET technique. The encouraging performance of the EPET technique demonstrates the potential usefulness of EPs in supporting ET and suggests that the proposed EP technique could effectively discover event episodes and EPs in sequences of documents",1558-2426,,10.1109/TSMCA.2006.886377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100782,Document clustering;event evolution;event tracking (ET);evolution patterns (EPs);knowledge management;temporal patterns;text mining,Knowledge management;Text mining;Customer service;Technology management;Corporate acquisitions;Data mining;Web and internet services;Surveillance;Technological innovation;Councils,data mining;information retrieval,event evolution patterns;document sequences;global connectivity;online textual documents;text mining techniques;evolution patterns;knowledge management;document management;document retrieval,,26,,41,,20-Feb-07,,,IEEE,IEEE Journals
A fuzzy relative of the k-medoids algorithm with application to web document and snippet clustering,k-medoids算法的模糊關聯及其在Web文檔和摘要聚類中的應用,R. Krishnapuram; A. Joshi; Liyu Yi,"Dept. of Math. & Comput. Sci., Colorado Sch. of Mines, Golden, CO, USA; NA; NA",FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315),6-Aug-02,1999,3,,1281,1286 vol.3,This paper presents new algorithms (fuzzy e-methods (FCMdd) and fuzzy c trimmed medoids (FCTMdd)) for fuzzy clustering of relational data. The objective functions are based on selecting c representative objects (medoids) from the data set in such a way that the total dissimilarity within each cluster is minimized. A comparison of FCMdd with the relational fuzzy c-means algorithm shows that FCMdd is much faster. We present examples of applications of these algorithms to web document and snippet clustering.,1098-7584,0-7803-5406-0,10.1109/FUZZY.1999.790086,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=790086,,Clustering algorithms;Partitioning algorithms;Uniform resource locators;Application software;Computer science;Fuzzy sets;Merging;Couplings;Pattern recognition;Prototypes,relational databases;pattern recognition;fuzzy set theory;computational complexity;information retrieval,fuzzy c trimmed medoids;fuzzy clustering;relational data;objective functions;dissimilarity;fuzzy c-means algorithm;snippet clustering;web document;medoids algorithm,,89,,28,,6-Aug-02,,,IEEE,IEEE Conferences
MEU Analysis of DECICLUS Approach for Multilingual Text Categorization,多語言文本分類的DECICLUS方法的MEU分析,G. Kumar; M. K. Rai,"Dept. of Comput. Sci. & Eng., Lovely Prof. Univ., Jalandhar, India; Dept. of Comput. Sci. & Eng., Lovely Prof. Univ., Jalandhar, India",2012 International Conference on Computing Sciences,24-Dec-12,2012,,,100,104,"In this paper, we have showed a hybrid approach to categorize the multilingual text documents using supervised and unsupervised learning techniques. To serve the purpose, we have selected decision tree technique from supervised category and clustering technique from unsupervised category to form our hybrid system DECICLUS. In our following paper, we have also done analysis of our novel approach based on maintenance, efficiency, usability (MEU) which shows that our approach have potentials for multilingual text categorization.",,978-1-4673-2647-6,10.1109/ICCS.2012.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6391654,DECICLUS;decision tree;clusters;supervised;unsupervised;maintenance;efficiency;usability;stability;text categorization;corpora,Decision trees;Text categorization;Clustering algorithms;Stability analysis;Supervised learning;Unsupervised learning;Maintenance engineering,decision trees;maintenance engineering;natural language processing;pattern clustering;text analysis;unsupervised learning,MEU analysis;DECICLUS approach;multilingual text categorization;hybrid approach;multilingual text document categorization;supervised learning techniques;unsupervised learning techniques;decision tree technique;supervised category;clustering technique;unsupervised category;maintenance-efficiency-usability analysis,,,,6,,24-Dec-12,,,IEEE,IEEE Conferences
On the Use of Side Information for Mining Text Data,關於使用輔助信息挖掘文本數據,C. C. Aggarwal; Y. Zhao; P. S. Yu,"Department of Computer Science, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Sumo Logic Inc, Redwood City, CA, USA; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA",IEEE Transactions on Knowledge and Data Engineering,2-Jun-14,2014,26,6,1415,1429,"In many text mining applications, side-information is available along with the text documents. Such side-information may be of different kinds, such as document provenance information, the links in the document, user-access behavior from web logs, or other non-textual attributes which are embedded into the text document. Such attributes may contain a tremendous amount of information for clustering purposes. However, the relative importance of this side-information may be difficult to estimate, especially when some of the information is noisy. In such cases, it can be risky to incorporate side-information into the mining process, because it can either improve the quality of the representation for the mining process, or can add noise to the process. Therefore, we need a principled way to perform the mining process, so as to maximize the advantages from using this side information. In this paper, we design an algorithm which combines classical partitioning algorithms with probabilistic models in order to create an effective clustering approach. We then show how to extend the approach to the classification problem. We present experimental results on a number of real data sets in order to illustrate the advantages of using such an approach.",1558-2191,,10.1109/TKDE.2012.148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6247433,clustering;text mining;Data mining;clustering,Clustering algorithms;Database systems;Partitioning algorithms;Noise measurement;Probabilistic logic;Coherence;Approximation methods,data mining;pattern clustering;probability;text analysis,clustering approach;nontextual attributes;real data sets;probabilistic models;classical partitioning algorithms;Web logs;user-access behavior;document provenance information;text documents;text data mining;side information,,22,,37,,23-Jul-12,,,IEEE,IEEE Journals
Terms Mining in Document-Based NoSQL: Response to Unstructured Data,基於文檔的NoSQL中的術語挖掘：對非結構化數據的響應,R. K. Lomotey; R. Deters,"Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada; Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada",2014 IEEE International Congress on Big Data,25-Sep-14,2014,,,661,668,"Unstructured data mining has become topical recently due to the availability of high-dimensional and voluminous digital content (known as ""Big Data"") across the enterprise spectrum. The Relational Database Management Systems (RDBMS) have been employed over the past decades for content storage and management, but, the ever-growing heterogeneity in today's data calls for a new storage approach. Thus, the NoSQL database has emerged as the preferred storage facility nowadays since the facility supports unstructured data storage. This creates the need to explore efficient data mining techniques from such NoSQL systems since the available tools and frameworks which are designed for RDBMS are often not directly applicable. In this paper, we focused on topics and terms mining, based on clustering, in document-based NoSQL. This is achieved by adapting the architectural design of an analytics-as-a-service framework and the proposal of the Viterbi algorithm to enhance the accuracy of the terms classification in the system. The results from the pilot testing of our work show higher accuracy in comparison to some previously proposed techniques such as the parallel search.",2379-7703,978-1-4799-5057-7,10.1109/BigData.Congress.2014.99,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906842,Unstructured Data Mining;Big Bata;Viterbi algorithm;Terms;NoSQL;Association Rules;classification;clustering,Data mining;Databases;Viterbi algorithm;Dictionaries;Semantics;Big data;Classification algorithms,Big Data;data mining;database management systems;document handling;pattern classification;pattern clustering;text analysis,terms mining;document-based NoSQL;NoSQL database;unstructured data storage;data mining techniques;topics mining;clustering;analytics-as-a-service framework;Viterbi algorithm;term classification;Big Data,,2,,41,,25-Sep-14,,,IEEE,IEEE Conferences
An Improved LDA Multi-document Summarization Model Based on TensorFlow,基於TensorFlow的改進的LDA多文檔摘要模型,Y. Zhong; Z. Tang; X. Ding; L. Zhu; Y. Le; K. Li; K. Li,"Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Coll. of Inf. Sci. & Eng., Hunan Univ., Changsha, China; Dept. of Comput. Sci., State Univ. of New York, New Paltz, NY, USA",2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),7-Jun-18,2017,,,255,259,"Latent Dirichlet Allocation (LDA), has been recently used to automatically generate text corpora topics, and applied to sentences extraction based multi-document summarization algorithms. In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. Our approach is to combine the traditional summary generation algorithm and the the abstract generation algorithm based on deep learning.We employ the improved traditional summary generation algorithm to convert multiple documents into a single document, and then using the resulting single document with the deep learning method to extract the final summary. At first, we apply improved LDA model to cluster sentences in all documents. Second, We employ the extended LexRank algorithm to sort the sentences in each cluster. Third, we use extended Hedge Trimmer algorithm for sentence compression. Fourth, We apply Integer Linear Programming for sentence selection, and in this step ,we get the single document. Finally, We employ the textum on TensorFlow to get the final abstract. The experiments showed that the proposed algorithm achieved better performance compared the other state-of-the-art algorithms on DUC2005 and TAC2010 corpus.",2375-0197,978-1-5386-3876-7,10.1109/ICTAI.2017.00048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8371951,Natural Language Processing;multi document summarization;LDA;TensorFlow,Clustering algorithms;Task analysis;Machine learning;Mathematical model;Electronic mail;Integer linear programming;Information science,learning (artificial intelligence);pattern clustering;text analysis,abstract generation algorithm;deep learning method;LDA model;extended LexRank algorithm;Hedge Trimmer algorithm;sentence compression;sentence selection;TensorFlow;Latent Dirichlet Allocation;text corpora topics;sentences extraction;multidocument summarization algorithms;summary generation algorithm;sentences clustering;LDA multidocument summarization model;aspect-oriented summaries,,1,,22,,7-Jun-18,,,IEEE,IEEE Conferences
Feature extraction and clustering-based retrieval for mathematical formulas,基於特徵提取和基於聚類的數學公式檢索,K. Ma; S. C. Hui; K. Chang,"School of Computer Engineering Nanyang Technological University, Singapore 639798, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore 639798, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore 639798, Singapore",The 2nd International Conference on Software Engineering and Data Mining,9-Aug-10,2010,,,372,377,"Mathematical formulas or expressions are essential for presenting scientific knowledge in many research documents in academic areas such as physics and mathematics. Searching for related mathematical formulas is an important but challenging problem as formulas contain both structural and semantic information. Such information is hidden inside the mathematical expressions of the formulas. To support effective formula search, it is necessary to extract the structural and semantic features from the mathematical presentation of the formulas faithfully. In this paper, we propose an effective approach for formula feature extraction. To evaluate the proposed approach, the extracted features are tested with three popular clustering algorithms, namely K-means, Self Organizing Map (SOM), and Agglomerative Hierarchical Clustering (AHC), for formula retrieval. The performance of the clustering-based retrieval is measured based on a dataset of 881 formulas and promising results have been achieved.",,978-89-88678-22-0,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542894,feature extracction;formula search;clustering;information retrieval,Feature extraction;Data mining;Search engines;Clustering algorithms;Physics computing;Knowledge engineering;Organizing;Information retrieval;Mathematics;Automatic testing,feature extraction;information retrieval;mathematics computing;pattern clustering;self-organising feature maps,semantic feature extraction;clustering-based retrieval;mathematical formulas;semantic information;structural information;structural feature extraction;self organizing map clustering algorithms;K-mean clustering algorithms;agglomerative hierarchical clustering,,,,15,,9-Aug-10,,,IEEE,IEEE Conferences
Application of data mining for identifying topics at the document level,數據挖掘在文檔級別識別主題中的應用,M. F. Reza; R. Matin,"Computer Science and Engineering, BRAC University, Dhaka, Bangladesh; Computer Science and Engineering, BRAC University, Dhaka, Bangladesh","2013 International Conference on Informatics, Electronics and Vision (ICIEV)",1-Aug-13,2013,,,1,6,"Data mining techniques are very popular in modern days and are used in NLP (Natural Language Processing). It allows users to analyze data from many different perspectives, categorize it, and summarize the relationships identified. One of the techniques, clustering items to groups, has been very popular. We use this technique here to find different topics in a document. We aim to replicate previous results and empirically verify this measure to identify hypothetical topic boundaries.",,978-1-4799-0400-6,10.1109/ICIEV.2013.6572712,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572712,Data-mining;Natural language Processing (NLP);Unsupervised Learning;Artificial intelligence,Data mining;Natural language processing;Noise;Unsupervised learning;Clustering algorithms;Speech;Prediction algorithms,data mining;document handling;natural language processing;pattern clustering,data mining;document level topic identification;NLP;natural language processing;item clustering;hypothetical topic boundaries,,1,,15,,1-Aug-13,,,IEEE,IEEE Conferences
Using Correlation Based Subspace Clustering for Multi-label Text Data Classification,使用基於相關性的子空間聚類進行多標籤文本數據分類,M. S. Ahmed; L. Khan; M. Rajeswari,"Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Sch. of Comput. Sci., Univ. Sains Malaysia, Minden, Malaysia",2010 22nd IEEE International Conference on Tools with Artificial Intelligence,17-Dec-10,2010,2,,296,303,"With the boom of web and social networking, the amount of generated text data has increased enormously. Much of this data can be considered and modeled as a stream and the volume of such data necessitates the application of automated text classification strategies. Although streaming data classification is not new, considering text data streams for classification purposes has been extensively researched only recently. Before applying any classification method in text data streams, it is imperative that we apply them for existing well-known non-stream text data sets and evaluate their performance. One of the many characteristics of text data that has been pursued for research is its multi-labelity. A single text document may cover multiple class-labels at the same time and hence gives rise to the concept of multi-labelity. From classification perspective, an immediate drawback of such a characteristic is that traditional binary or multi-class classification techniques perform poorly on multi-label text data. In this paper, we extend our previously formulated SISC (Semi-supervised Impurity based Subspace Clustering) [1] approach and its multi-label variation SISC-ML [2]. We call this new algorithm H-SISC (Hierarchical SISC). H-SISC captures the underlying correlation that exists between each pair of class labels in a multi-label environment. Developing a robust multi-label classifier will allow us to apply such a model in classifying streaming text data more effectively. We have experimented with well known text data sets and empirical evaluation on these real world multi-label NASA ASRS (Aviation Safety Reporting System), Reuters and 20 Newsgroups data sets reveals that our proposed approach outperforms other state-of-the-art text classification as well as subspace clustering algorithms.",2375-0197,978-1-4244-8817-9,10.1109/ICTAI.2010.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670092,Subspace Clustering;Text Data Classification;Cluster Impurity;Fuzzy Clustering,Impurities;Speech recognition;Clustering algorithms;Support vector machines;Indexing;Aircraft;Correlation,correlation methods;pattern classification;pattern clustering,correlation;multilabel text data classification;streaming data classification;semisupervised impurity based subspace clustering;hierarchical SISC;aviation safety reporting system;state of the art text classification,,4,,16,,17-Dec-10,,,IEEE,IEEE Conferences
Improving recto document side restoration with an estimation of the verso side from a single scanned page,通過從單個掃描頁面估計背面來改善直腸文檔側恢復,C. Wolf,"Laboratoire d'informatique en images et syst癡mes d'information - UMR 5205, INSA-Lyon, B璽t. J.Verne; 20, Av. Albert Einstein, 69621 Villeurbanne cedex, France",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"We present a new method for blind document bleed through removal based on separately restoring the recto and the verso side. The segmentation algorithm is based on separate Markov random fields (MRF) which results in a better adaptation of the prior to the content creation process (e.g. superimposing two pages), and the improvement of the estimation of the verso pixels through an estimation of the verso pixels covered by recto pixels. The labels of the initial recto and verso clusters are recognized without using any color or gray value information. The proposed method is evaluated empirically as well as through OCR improvement.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761653,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761653,,Image segmentation;Probability distribution;Image restoration;Markov random fields;Pixel;Virtual colonoscopy;Optical character recognition software;Parameter estimation;Graphical models;Writing,document image processing;image resolution;image restoration;image segmentation,recto document side restoration;blind document;segmentation algorithm;Markov random fields;content creation process;recto pixels,,2,,19,,23-Jan-09,,,IEEE,IEEE Conferences
Document classification using Symbolic classifiers,使用符號分類器進行文檔分類,M. B. Revanasiddappa; B. S. Harish; S. Manjunath,"Department of Information Science & Engineering, SJCE, Mysore, Karnataka, India; Department of Information Science & Engineering, SJCE, Mysore, Karnataka, India; Department of Computer Science, Central University Kerala, Kasargod, India",2014 International Conference on Contemporary Computing and Informatics (IC3I),26-Jan-15,2014,,,299,303,"In this paper, we present symbolic classifiers to classify text documents. We propose to use cluster based symbolic representation followed by symbolic feature selection methods to classify text documents. In particular, we propose Symbolic clustering approaches; Symbolic cluster based without feature selection; Symbolic cluster based with feature selection (using similarity measure); Symbolic cluster based with feature selection (using dissimilarity measure) and Symbolic feature clustering approaches. The above mentioned representation methods are very powerful in reducing the dimensionality of feature vectors for text classification. To corroborate the efficacy of the proposed model, we conducted extensive experimentation on various standard text datasets. The experimental results reveal that the symbolic feature clustering approach achieves better classification accuracy over the existing cluster based symbolic approaches.",,978-1-4799-6629-5,10.1109/IC3I.2014.7019827,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019827,,Text categorization;Support vector machine classification;Accuracy;Training;Correlation;Standards;Internet,data mining;feature selection;pattern classification;pattern clustering;symbol manipulation;text analysis,symbolic classifiers;text document classification;cluster-based symbolic representation;similarity measure;symbolic cluster-based-with-feature selection;symbolic cluster-based-without-feature selection;dissimilarity measure;dimensionality reduction;standard text datasets;symbolic feature clustering approach;classification accuracy,,4,,17,,26-Jan-15,,,IEEE,IEEE Conferences
A New Method of Cluster-Based Topic Language Model for Genomic IR,基於聚類的基因組紅外主題語言模型的新方法,J. Wen; Z. Li; L. Zhang; X. Hu; H. Chen,"National University of Defence Technology, China; Beihang University, China; National University of Defence Technology, China; Drexel University, USA; National University of Defence Technology, China",21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07),27-Aug-07,2007,1,,301,306,"Accurately estimating language model is important to improve the performance of information retrieval. The key problems include solving synonymy and polysemy problem, and smoothing the seen term or not seen term in a document. In this paper, we propose a new method for topic language model. First, concept-based clustering is performed using improved fuzzy c-means. The clustering result is considered as the topics of document collections. The probability of a document generating the topics is estimated by the similarity between the document and each cluster. Then, the probability of the topics generating words is estimated using Expectation Maximization algorithm. At last, we integrate the above algorithms into aspect model to form our topic language model. This new language model accurately describes the distribution probability of the words in different topics and the probability of a document generating a topic. Moreover, it can solve synonymy and polysemy problems. The new method is evaluated on TREC 2004/05 Genomics Track collections. Experiments show that the retrieval performance is greatly improved by the new method compared with the simple language model.",,978-0-7695-2847-2,10.1109/AINAW.2007.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221077,Topic Language Model;Information Retrieval;Cluster,Genomics;Bioinformatics;Clustering algorithms;Information retrieval;Smoothing methods;Optical computing;Computer science;Educational institutions;Information science;Solid modeling,expectation-maximisation algorithm;fuzzy set theory;information retrieval;natural language processing;pattern clustering;probability,cluster-based topic language model;information retrieval;synonymy problem;polysemy problem;concept-based clustering;fuzzy c-means method;expectation maximization algorithm;distribution probability,,2,,13,,27-Aug-07,,,IEEE,IEEE Conferences
Tree Cluster of Text Data by NMF Based Neural Network,基於NMF的神經網絡的文本數據樹聚類。,P. C. Barman; S. Lee,"Brain Science Research Center and Computational NeuroSystems Lab, Department of BioSystems, Korea Advanced Institute of Science and Technology Daejeon, 305-701, Republic of Korea, E-mail: pcbarman@nuron.kaist.ac.kr; Brain Science Research Center and Computational NeuroSystems Lab, Department of BioSystems, Korea Advanced Institute of Science and Technology Daejeon, 305-701, Republic of Korea, E-mail: sylee@kaist.ac.kr",2006 International Conference on Electrical and Computer Engineering,7-May-07,2006,,,312,315,"This paper proposes the tree clustering of text documents using non-negative matrix factorization (NMF) based neural network. The main problem of the tree clustering is to find the number of branches of a parent node of the tree and to find the significant attributes which can cluster the documents of each node. In advance if we don't know the exact number of branches then it is not applicable for tree clustering. The paper proposed the min-max correlation coefficient method for finding the number of branches and by reducing the NMF basis vectors dimension according to their probability and removing the common terms from the NMF basis vectors, the authors find the significant attributes to cluster the documents. The approach is also helpful for prepruning the tree. For the justification of the approaches the authors use the CLASSIC3 text database",,98432-3814-1,10.1109/ICECE.2006.355634,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4178470,,Neural networks;Clustering algorithms;Biochemical analysis;Image analysis;Computer networks;Data analysis;Data engineering;Electronic mail;Data mining;Tree data structures,matrix decomposition;minimax techniques;neural nets;probability;trees (mathematics),text data;neural network;tree clustering;text documents;nonnegative matrix factorization;significant attributes;min-max correlation coefficient method;probability;CLASSIC3 text database,,,,5,,7-May-07,,,IEEE,IEEE Conferences
Hard-fuzzy clustering: A cooperative approach,硬模糊聚類：一種合作方法,R. Kashef; M. S. Kamel,"Department of Electrical and Computer Engineering Pattern Analysis and Machine Intelligence (PAMI) Research Group University of Waterloo, Canada; Department of Electrical and Computer Engineering Pattern Analysis and Machine Intelligence (PAMI) Research Group University of Waterloo, Canada","2007 IEEE International Conference on Systems, Man and Cybernetics",2-Jan-08,2007,,,425,430,"Data clustering plays an important role in many disciplines, where there is a need to learn the inherent grouping structure of the data in an unsupervised manner. It is well known that no clustering method can adequately handle all sorts of cluster structures and properties (e.g. shape, size, overlapping, and density). Combining multiple clustering methods is an approach to overcome the deficiency of single algorithms and further enhance their performances. Current approaches to multiple clusterings use ensemble clustering to generate aggregated solution from multiple clusterings or using a hybrid cascaded refinement to enhance the end-result clusters produced by a former clustering algorithm(s). A disadvantage of the cluster ensemble is the highly computational load of combing the clustering results especially for large and high dimensional datasets. A drawback of the hybrid approaches is that, one (or more) of the clustering algorithms stays idle until the previous algorithm(s) finishes its clustering. In this paper we propose a Cooperative Hard-Fuzzy Clustering (CHFC) model based on intermediate cooperation between the hard c-means (KM) and fuzzy c-means (FCM) to produce better clustering solutions. Our experimental results over artificial, real, and text documents datasets show that the quality of the clustering solutions obtained from the CHFC model is better than those obtained from both the KM and the FCM and also better than those obtained from hybrid cascaded models.",1062-922X,978-1-4244-0990-7,10.1109/ICSMC.2007.4413889,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4413889,,Clustering algorithms;Clustering methods;Partitioning algorithms;Shape;Refining;Pattern analysis;Machine intelligence;Hybrid power systems;Stability;Computational efficiency,data structures;fuzzy set theory;pattern clustering,cooperative hard-fuzzy data clustering;data structure;multiple clustering method;hybrid cascaded refinement;hard fuzzy c-means method,,4,,22,,2-Jan-08,,,IEEE,IEEE Conferences
Models for extensible multimedia document retrieval,可擴展多媒體文檔檢索模型,B. Milosavljevic,"Fac. of Eng., Novi Sad Univ., Serbia",IEEE Sixth International Symposium on Multimedia Software Engineering,17-Jan-05,2004,,,218,221,"A multimedia document retrieval system should be able to handle documents containing elements of multiple media types, both in terms of document representation and query formulation. This paper deals with the problems of extensibility in retrieval of multimedia documents by presenting features of a system capable of handling any media type and formal definitions of two retrieval models supporting this kind of extensibility. The modified extended Boolean model improves query expressiveness from the previous model while retaining the same query computation concepts. The model of similar clusters facilitates the use of information gathered in previous retrieval sessions to improve retrieval results.",,0-7695-2217-3,10.1109/MMSE.2004.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1376664,,Information retrieval;Multimedia systems;Content based retrieval;Image retrieval;XML;Document handling;Genomics;Bioinformatics;X-ray imaging;Prototypes,query formulation;information retrieval systems;multimedia databases;XML,extensible multimedia document retrieval system;query formulation;formal definitions;Boolean model;query expressiveness,,1,,11,,17-Jan-05,,,IEEE,IEEE Conferences
An approach to intelligent information filtering in Chinese document images based on garbage model,基於垃圾模型的中文文檔圖像智能信息過濾方法,Chen Jiewei; Xu Weiran; Guo Jun,"Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China; Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China; Sch. of Inf. Eng., Beijing Univ. of Posts & Telecommun., China","Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.",6-Jun-05,2004,,,198,201,"A fast approach to Chinese document image filtering is presented. Garbage models are built by keyword clustering prior to keyword searching. The retrieval process is accelerated by the Boyer-Moore algorithm. A character is classified as accepted or rejected by the distance from the garbage models. A confidence measure ensures precision. Document vectors are built, based on keyword spotting from the document image. We obtain the score of the document image by means of a vector space model. Experimental results confirmed the robustness of the proposed approach over a wide range of degradations.",,0-7803-8687-6,10.1109/ISIMP.2004.1434034,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1434034,,Information filtering;Optical character recognition software;Tiles;Information retrieval;Character recognition;Image recognition;Image segmentation;Information filters;Keyword search;Acceleration,optical character recognition;image segmentation;feature extraction;information filtering;document image processing;image retrieval,OCR;image segmentation;feature extraction;intelligent information filtering;Chinese document image filtering;image retrieval process;Boyer-Moore algorithm;character garbage model distance;confidence measure;keyword spotting;vector space model,,,,5,,6-Jun-05,,,IEEE,IEEE Conferences
A novel approach for skew estimation of document images in OCR system,OCR系統中文檔圖像偏斜估計的新方法,M. Sarfraz; A. Zidouri; S. A. Shahab,"Dept. of Inf. & Comput. Sci., KFUPM, Dhahran, Saudi Arabia; Dept. of Inf. & Comput. Sci., KFUPM, Dhahran, Saudi Arabia; Dept. of Inf. & Comput. Sci., KFUPM, Dhahran, Saudi Arabia","International Conference on Computer Graphics, Imaging and Visualization (CGIV'05)",24-Oct-05,2005,,,175,180,"Optical character recognition (OCR) is an area which has always received special attention. OCR systems are typically built on the strategy of divide and conquer, rather than recognizing documents at one go. They utilize several stages during the course of recognition. There have been many stages in a typical OCR system, preprocessing stage in considered to be indispensable. An input image or information need to be normalized and converted into format acceptable by OCR system. OCR systems typically assume that documents were printed with a single direction of the text and that the acquisition process did not introduce a relevant skew. Practically this assumption is not very strong and printed document could be skewed at some angle with horizontal axis. In this paper, we have proposed a new technique for skew estimation of image document. In the proposed scheme, multiscale properties of an image are utilized together with principal component analysis to estimate the orientation of principal axis of clustered data.",,0-7695-2392-7,10.1109/CGIV.2005.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521059,,Optical character recognition software;Fourier transforms;Discrete wavelet transforms;Principal component analysis;Wavelet transforms;Character recognition;Pattern recognition;Joining processes;Computer science;Image converters,document image processing;optical character recognition;principal component analysis,skew estimation;document image;OCR system;optical character recognition;divide and conquer strategy;principal component analysis,,7,,20,,24-Oct-05,,,IEEE,IEEE Conferences
Multi-document Summarization Based on Locally Relevant Sentences,基於局部相關句的多文檔摘要,E. Villatoro-Tello; L. Villase簽or-Pineda; M. Montes-y-G籀mez; D. Pinto-Avenda簽o,"Dept. of Comput. Sci., Nat. Inst. of Astrophys., Opt. & Electron. (INAOE), Mexico; NA; Dept. of Comput. Sci., Nat. Inst. of Astrophys., Opt. & Electron. (INAOE), Mexico; NA",2009 Eighth Mexican International Conference on Artificial Intelligence,17-Feb-10,2009,,,87,91,"Multi-document summarization systems must be able to draw the ""best"" information from a set of documents.In this paper we propose a novel extractive approach for multidocument summarization based on the detection of locally relevant sentences. Our main hypothesis is that by extracting relevant sentences from each document within a collection, instead of considering all documents at once, the final multi-document summary will be of higher quality. Performed experiments showed that the proposed method is able to outperform conventional baselines as well as traditional approaches by constructing summaries of high quality according to the ROUGE evaluation metrics.",,978-0-7695-3933-1,10.1109/MICAI.2009.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372713,Multi-Document Summarization;Clustering;Machine Learning;Relevant Sentences;Themes Identification,Data mining;Clustering algorithms;Artificial intelligence;Laboratories;Optical computing;Astrophysics;Computer science;Performance evaluation;Information resources,document handling,multidocument summarization;locally relevant sentences;extractive approach;ROUGE evaluation metrics;high quality summaries,,,,13,,17-Feb-10,,,IEEE,IEEE Conferences
A structured ontology construction by using data clustering and pattern tree mining,利用數據聚類和模式樹挖掘的結構化本體構建,Y. Yu; C. Hsu,"Department of Computer Science and Information Engineering, Fu Jen Catholic University, Taiwan; Department of Computer Science and Information Engineering, Fu Jen Catholic University, Taiwan",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,1,,45,50,"Ontology is used to express the concepts of domain knowledge. It can provide a common representation for different agents to share and communicate knowledge for conducting unified opinions. Nowadays ontology construction method is divided into man-made and machine-made mechanisms. The former constructs the ontology topology by domain expert. Generally the constructed ontology can fit human expectation but it needs more development time to construct the whole structure. The latter uses semi-automatic or automatic methods, such as statistic or machine learning, to build the ontology. The efficient ontology construction is the main advantage for machine-made method. However, the advantage is that it is easily influenced by the category and type of domain concepts to generate unbalanced or skewed ontology topology. This will increase the time complexity to search and retrieve the concept from the constructed ontology structure. The situation worsens from being unable to use the ontology properly. An important problem is constructing a reasonable and balanced ontology topology systematically and automatically. This paper proposes a structured ontology construction based on data clustering and pattern tree mining. The construction method uses data clustering and formal concept analysis to group similar documents for constructing ontology trees of each group individually. Then the method uses pattern tree mining to build an integrated ontology topology from partial ontology trees.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016746,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016746,Ontology;Data clustering;Formal concept analysis;Sequence pattern mining,Ontologies;Semantics;Skeleton;Matrix decomposition;Data mining;Machine learning,data mining;ontologies (artificial intelligence);pattern clustering;tree searching,structured ontology construction;data clustering;pattern tree mining;domain knowledge;ontology construction method;man-made mechanism;machine-made mechanism;domain expert;machine learning;time complexity;ontology structure;formal concept analysis;integrated ontology topology;partial ontology trees,,3,,17,,12-Sep-11,,,IEEE,IEEE Conferences
A new search method for ranking short text messages using semantic features and cluster coherence,一種利用語義特徵和聚類一致性對短消息進行排名的新搜索方法,M. Trifan; D. Ionescu,"University of Ottawa/SITE, Ottawa, Canada; University of Ottawa/SITE, Ottawa, Canada",2010 International Joint Conference on Computational Cybernetics and Technical Informatics,21-Jun-10,2010,,,643,648,"A search results ranking method that uses semantic features and a cluster coherence measure is introduced in this paper. The quality of the returned search results is improved by grouping semantically related texts into clusters displayed in descending cluster size order. First the term-document matrix is constructed where the documents correspond to individual texts. Then, nonnegative matrix factorization (NMF) is used to group the texts into semantically related clusters. Only those clusters whose coherence is greater than a threshold value are displayed. In this way trending conceptually similar texts that re-occur in the input of multiple users are identified. The advantage of this approach compared to other methods consists in the fact that the clusters in the approach introduced in this paper are computed by semantic similarity and not only by texts counters.",,978-1-4244-7433-2,10.1109/ICCCYB.2010.5491333,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5491333,,Search methods;Twitter;Search engines;Data mining;Counting circuits;Clustering algorithms;Navigation;Fabrics;Social network services;Noise figure,matrix decomposition;query formulation;text analysis,short text messages ranking;semantic features;search results ranking method;cluster coherence measure;term document matrix;nonnegative matrix factorization,,1,,9,,21-Jun-10,,,IEEE,IEEE Conferences
Revealing the Visually Unknown in Ancient Manuscripts with a Similarity Measure for IR-Imaged Inks,用紅外成像油墨的相似度量揭示古代手稿中的視覺未知,A. Licata; A. Psarrou; V. Kokla,"CVIR Res. Lab., Univ. of Westminster, Harrow, UK; CVIR Res. Lab., Univ. of Westminster, Harrow, UK; CVIR Res. Lab., Univ. of Westminster, Harrow, UK",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,818,822,One of the tasks facing historians and conservationists is the authentication or dating of medieval manuscripts. To this end it is important to them to verify whether writings on the same or different manuscripts are concurrent. In this work we explore this task by capturing images of manuscript pages in infrared (IR) and modelling and then comparing the ink appearance of segmented text. The modelling of the text appearance relies on the unsupervised multimodal clustering of ink descriptors and the derived probability density functions. The similarity measure is built around the distribution of cluster labels and their proportions. We demonstrate our method by using both model inks of known composition and authentic Byzantine manuscripts.,2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277519,Image Analysis;feature extraction;Document image processing;Ink Type Modelling,Ink;Writing;Art;Authentication;Computer vision;Text analysis;Infrared imaging;Image segmentation;Probability density function;Materials testing,art;document image processing;feature extraction;history;image classification;image colour analysis;image segmentation;image texture;infrared imaging;ink;pattern clustering;statistical distributions;text analysis,ancient Byzantine manuscript image capture;similarity measure;IR-imaged ink;infrared imaging;history;art conservation;medieval manuscript authentication;medieval manuscript dating;segmented text;text ink appearance modelling;unsupervised multimodal clustering;ink descriptor;probability density function;cluster label distribution;visually unknown composition;texture recognition classifier;feature extraction;image colour analysis,,2,,9,,2-Oct-09,,,IEEE,IEEE Conferences
A genetic algorithm approach for topic clustering: A centroid-based encoding scheme,主題聚類的遺傳算法方法：基於質心的編碼方案,D. N. Sotiropoulos; D. E. Pournarakis; G. M. Giaglis,"Dept. of Management Science & Techonology, Athens University of Economics and Business, Evelpidon 47a & Lefkados St.; Dept. of Management Science & Techonology, Athens University of Economics and Business, Evelpidon, 47a & Lefkados St.; Dept. of Management Science & Techonology, Athens University of Economics and Business, Evelpidon 47a & Lefkados St.","2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)",19-Dec-16,2016,,,1,8,"This paper addresses the problem of topic clustering, through the utilization of a novel genetic algorithm approach which is highly scalable on large volumes of textual data, by introducing a centroid-based encoding scheme. The proposed topic clustering method is anchored on the Latent Dirichlet Allocation (LDA) probabilistic topic modeling framework, aiming at identifying cluster formations that are optimal in terms of semantic coherence. Our work focuses on reformulating the clustering problem as a discrete optimization problem within the n-dimensional standard simplex since all the LDA-based data patterns correspond to n-valued probability distribution vectors. The novelty of our proposed genetic algorithm approach lies primarily upon the adaptation of the centroid-based encoding scheme, in the sense that cluster assignments are implicitly extracted by assigning each data point to the nearest cluster center. Experimentation was conducted on a large corpus of twitter posts, particularly relating to the UBER transportation network. The obtained topic clustering results indicate significant improvement in extracting semantically focused groups of documents when compared against traditional clustering algorithms, such as the k-means. The clustering superiority of our proposed genetic algorithm is also justified by measuring the intra- and inter-cluster semantic distances of the obtained cluster formations.",,978-1-5090-3429-1,10.1109/IISA.2016.7785378,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785378,,Genetic algorithms;Semantics;Probabilistic logic;Standards;Clustering algorithms;Encoding;Probability distribution,genetic algorithms;pattern clustering;probability;social networking (online);statistical distributions;text analysis,intracluster semantic distances;intercluster semantic distances;cluster formations;semantically focused document group extraction;UBER transportation network;Twitter posts;nearest cluster center;n-valued probability distribution vectors;LDA-based data patterns;n-dimensional standard simplex;discrete optimization problem;semantic coherence;LDA probabilistic topic modeling;latent Dirichlet allocation probabilistic topic modeling;textual data;centroid-based encoding;topic clustering;genetic algorithm,,,,47,,19-Dec-16,,,IEEE,IEEE Conferences
A Novel Self-Organizing Map for Text Document Organization,一種新穎的文本文檔組織自組織圖,H. Yang; C. Lee,"Dept. Inf. Manage., Nat. Univ. of Kaohsiung, Kaohsiung, Taiwan; Dept. Electr. Eng., Nat. Kaohsiung Univ. of Appl. Sci., Kaohsiung, Taiwan",2012 Third International Conference on Innovations in Bio-Inspired Computing and Applications,25-Oct-12,2012,,,39,44,"The self-organizing map (SOM) model is a well-known neural network model with wide spread of applications. The main characteristics of SOM are two-fold, namely dimension reduction and topology preservation. Using SOM, a high-dimensional data space will be mapped to some low-dimensional space. Meanwhile, the topological relations among data will be preserved. With such characteristics, the SOM was usually applied on data clustering and visualization tasks. One major shortage of classical SOM learning algorithm is the necessity of predefined map topology. Furthermore, hierarchical relationships among data are also difficult to be revealed. In this work, we propose a novel SOM learning algorithm which incorporates several text mining techniques in expanding the map both laterally and hierarchically that could discover the relationships among documents in both perspectives. The proposed algorithm will first cluster a set of training documents using classical SOM algorithm. We then identify the topics of each cluster and use them to evaluate the criteria on expanding the map. We applied the algorithm on medium-size datasets and obtained promising result.",,978-1-4673-2838-8,10.1109/IBICA.2012.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337634,Text Mining;Self-organizing Map;Topic Identification;Hierarchy Generation,Neurons;Training;Vectors;Neural networks;Clustering algorithms;Topology;Text categorization,data mining;data visualisation;learning (artificial intelligence);pattern clustering;self-organising feature maps;text analysis;topology,novel self-organizing map;text document organization;SOM model;well-known neural network model;dimension reduction;topology preservation;high-dimensional data space;topological relations;classical SOM learning algorithm;data clustering;data visualization;novel SOM learning algorithm;text mining techniques;training documents;medium-size datasets,,2,,26,,25-Oct-12,,,IEEE,IEEE Conferences
A CRF Based Scheme for Overlapping Multi-colored Text Graphics Separation,基於CRF的多色文本圖形重疊處理方案,R. Garg; E. Hassan; S. Chaudhury; M. Gopal,"Dept. of Electr. Eng., Indian Inst. of Technol. Delhi, New Delhi, India; Dept. of Electr. Eng., Indian Inst. of Technol. Delhi, New Delhi, India; Dept. of Electr. Eng., Indian Inst. of Technol. Delhi, New Delhi, India; Dept. of Electr. Eng., Indian Inst. of Technol. Delhi, New Delhi, India",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1215,1219,"In this paper, we propose a novel framework for segmentation of documents with complex layouts. The document segmentation is performed by combination of clustering and conditional random fields (CRF) based modeling. The bottom-up approach for segmentation assigns each pixel to a cluster plane based on color intensity. A CRF based discriminative model is learned to extract the local neighborhood information in different cluster/color planes. The final category assignment is done by a top-level CRF based on the semantic correlation learned across clusters. The proposed framework has been extensively tested on multi-colored document images with text overlapping graphics/image.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065503,Document image analysis;Conditional random fields;Complex layout analysis,Image color analysis;Image segmentation;Layout;Support vector machines;Text analysis;Feature extraction,computer graphics;document image processing;image segmentation;text analysis,CRF based scheme;overlapping multicolored text graphics separation;document segmentation;conditional random fields;color intensity;discriminative model;semantic correlation;multicolored document image,,10,,19,,3-Nov-11,,,IEEE,IEEE Conferences
Image Clustering Algorithms to Identify Complicated Cerebral Diseases. Description and Comparison,圖像聚類算法來識別複雜的腦部疾病。說明和比較,M. Nichita; M. Paun; V. Paun; V. Paun,"Doctoral School, Faculty of Applied Sciences, University Politehnica of Bucharest, Bucharest, Romania; School of Engineering, Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland; Five Rescue Research Laboratory, Paris, France; Department of Physics, Faculty of Applied Sciences, University Politehnica of Bucharest, Bucharest, Romania",IEEE Access,20-May-20,2020,8,,88434,88442,"This article presents two algorithms developed based on two different techniques, from clusterization theory, namely k-means clustering technique and Fuzzy C-means technique, respectively. In this context, the study offers a sustained comparison of the two algorithms in order to properly choose one of them, depending on the image to be analyzed and the solution that is desired. Algorithms are used in image processing, respectively as application of image processing techniques in brain computed tomography image analysis. There were also compared the results obtained by running the algorithms with a different number of centroids, as well as the execution times of each algorithm in part. Image processing and obtaining the results presented in this document was made possible by using the MATLAB R2018b environment. This fact is possible because some components of the brain, such as the blood vessel network or the neural network, have a fractal arrangement, which makes it easy to analyze their structure, in order to provide predictions or treatments to patients in discussion afflicted with a serious brain disease, as accurately as possible.",2169-3536,,10.1109/ACCESS.2020.2992937,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9088243,Clustering theory;fractal analysis;Fuzzy C-means technique;k-means clustering technique;tomographic image processing,Clustering algorithms;Tomography;Diseases;Image processing;Matlab;Biomedical imaging;Neural networks,blood vessels;brain;computerised tomography;diseases;fractals;fuzzy set theory;medical image processing;neural nets;pattern clustering,image processing;brain computed tomography image analysis;brain disease;image clustering algorithms;cerebral diseases;clusterization theory;fuzzy c-means technique;k-means clustering;MATLAB R2018b environment;blood vessel network;neural network;fractal arrangement,,,,23,CCBY,6-May-20,,,IEEE,IEEE Journals
Conceptual graph generation from text documents based on perceptual balance,根據感知平衡從文本文檔生成概念圖,A. Notsu; K. Honda; H. Ichihashi,"Department of Computer Science and Intelligent Systems, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531, Japan; Department of Computer Science and Intelligent Systems, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531, Japan; Department of Computer Science and Intelligent Systems, Osaka Prefecture University, 1-1 Gakuen-cho, Nakaku, Sakai, Osaka 599-8531, Japan",2009 IEEE International Conference on Fuzzy Systems,2-Oct-09,2009,,,1551,1556,"A conceptual graph generation method is proposed in this paper. A conceptual graph is useful for studying human verbal caring interactions such as counseling, based on an interpersonal psychological approach referred to as 'naive psychology'. We apply the visual assessment of clustering tendency (VAT) to naive psychology, with particular reference to the visual understanding of people. A conceptual graph is constructed from words and sentences selected by morphological analysis. Furthermore, the VAT algorithm produces a visual display that can be used to assess clustering tendencies in a set of persons (notions) by reconstructing a digital image representation of a square relational dissimilarity matrix. This algorithm clearly represents two types of imbalanced situations in naive psychology: namely the crisp and fuzzy situations. In addition, social simulations that utilize several graphs are introduced.",1098-7584,978-1-4244-3596-8,10.1109/FUZZY.2009.5277334,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277334,,Psychology;Clustering algorithms;Humans;Employee welfare;Displays;Image reconstruction;Digital images;Computer science;Intelligent systems;Research and development,data visualisation;document image processing;fuzzy set theory;graph theory;human factors;image reconstruction;image representation;matrix algebra;pattern clustering;psychology;text analysis,conceptual graph generation method;text document;human verbal caring interaction;interpersonal psychological approach;VAT algorithm;visual assessment-of-clustering tendency algorithm;naive psychology;people visual understanding;morphological analysis;visual display;digital image representation;image reconstruction;square relational dissimilarity matrix;fuzzy set theory;social simulation,,,,14,,2-Oct-09,,,IEEE,IEEE Conferences
BookLeaf: An Unstructured Hydrodynamics Mini-Application,BookLeaf：非結構化流體力學微型應用程序,D. Truby; S. Wright; R. Kevis; S. Maheswaran; A. Herdman; S. Jarvis,"Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK; Dept. of Comput. Sci., Univ. of York, York, UK; Atomic Weapons Establ., Aldermaston, UK; Atomic Weapons Establ., Aldermaston, UK; Atomic Weapons Establ., Aldermaston, UK; Dept. of Comput. Sci., Univ. of Warwick, Coventry, UK",2018 IEEE International Conference on Cluster Computing (CLUSTER),1-Nov-18,2018,,,615,622,"With the age of Exascale computing causing a diversification away from traditional CPU-based homogeneous clusters, it is becoming increasingly difficult to ensure that computationally complex codes are able to run on these emerging architectures. This is especially important for large physics simulations that are themselves becoming increasingly complex and computationally expensive. One proposed solution to the problem of ensuring these applications can run on the desired architectures is to develop representative mini-applications that are simpler and so can be ported to new frameworks more easily, but which are also representative of the algorithmic and performance characteristics of the original applications. In this paper we present BookLeaf, an unstructured Arbitrary Lagrangian-Eulerian mini-application to add to the suite of representative applications developed and maintained by the UK Mini-App Consortium (UK-MAC). First, we outline the reference implementation of our application in Fortran. We then discuss a number of alternative implementations using a variety of parallel programming models and discuss the issues that arise when porting such an application to new architectures. To demonstrate our implementation, we present a study of the performance of BookLeaf on number of platforms using alternative designs, and we document a scaling study showing the behaviour of the application at scale.",2168-9253,978-1-5386-8319-4,10.1109/CLUSTER.2018.00078,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8514922,HPC mini apps openmp fortran,Hydrodynamics;Electric shock;Computer architecture;Mathematical model;Complexity theory;Electron tubes;Kernel,FORTRAN;hardware-software codesign;hydrodynamics;mainframes;multiprocessing systems;parallel architectures;parallel machines;parallel processing;parallel programming,Exascale computing;traditional CPU-based homogeneous clusters;computationally complex codes;emerging architectures;physics simulations;representative mini-applications;algorithmic performance characteristics;BookLeaf;Lagrangian-Eulerian mini-application;representative applications;UK Mini-App Consortium;UK-MAC;reference implementation;alternative implementations;unstructured hydrodynamics mini-application,,,,36,,1-Nov-18,,,IEEE,IEEE Conferences
Transformed Subspace Clustering,變換子空間聚類,J. Maggu; A. Majumdar; E. Chouzenoux,"CS, Indraprastha Institute of Information Technology, 243095 New Delhi, Delhi India (e-mail: jyotim@iiitd.ac.in); ECE, Indraprastha Institute of Information Technology, 243095 New Delhi, Delhi India 110078 (e-mail: angshul@iiitd.ac.in); Centre pour la Vision Num矇rique, Inria Centre de Recherche Saclay Ile-de-France, 202552 Palaiseau, ?le-de-France France (e-mail: emilie.chouzenoux@u-pem.fr)",IEEE Transactions on Knowledge and Data Engineering,,2020,PP,99,1,1,"Subspace clustering assumes that the data is separable into separate subspaces. Such a simple assumption, does not always hold. We assume that, even if the raw data is not separable into subspaces, one can learn a representation (transform coefficients) such that the learnt representation is separable into subspaces. To achieve the intended goal, we embed subspace clustering techniques (locally linear manifold clustering, sparse subspace clustering and low rank representation) into transform learning. The entire formulation is jointly learnt; giving rise to a new class of methods called transformed subspace clustering (TSC). In order to account for non-linearity, kernelized extensions of TSC are also proposed. To test the performance of the proposed techniques, benchmarking is performed on image clustering and document clustering datasets. Comparison with state-of-the-art clustering techniques shows that our formulation improves upon them.",1558-2191,,10.1109/TKDE.2020.2969354,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968629,Transform Learning;Subspace Clustering;Image Clustering;Document Clustering,Transforms;Kernel;Integrated circuits;Manifolds;Clustering algorithms;Sparse matrices;Symmetric matrices,,,,,,,,24-Jan-20,,,IEEE,IEEE Early Access Articles
A Fuzzy Self-Constructing Feature Clustering Algorithm for Text Classification,文本分類的模糊自構造特徵聚類算法,J. Jiang; R. Liou; S. Lee,"National Sun Yat-Sen University, Taiwan; National Sun Yat-Sen University, Taiwan; National Sun Yat-Sen University, Taiwan",IEEE Transactions on Knowledge and Data Engineering,20-Jan-11,2011,23,3,335,349,"Feature clustering is a powerful method to reduce the dimensionality of feature vectors for text classification. In this paper, we propose a fuzzy similarity-based self-constructing algorithm for feature clustering. The words in the feature vector of a document set are grouped into clusters, based on similarity test. Words that are similar to each other are grouped into the same cluster. Each cluster is characterized by a membership function with statistical mean and deviation. When all the words have been fed in, a desired number of clusters are formed automatically. We then have one extracted feature for each cluster. The extracted feature, corresponding to a cluster, is a weighted combination of the words contained in the cluster. By this algorithm, the derived membership functions match closely with and describe properly the real distribution of the training data. Besides, the user need not specify the number of extracted features in advance, and trial-and-error for determining the appropriate number of extracted features can then be avoided. Experimental results show that our method can run faster and obtain better extracted features than other methods.",1558-2191,,10.1109/TKDE.2010.122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5530315,Fuzzy similarity;feature clustering;feature extraction;feature reduction;text classification.,Classification algorithms;Clustering algorithms;Text categorization;Feature extraction;Data mining;Clustering methods;Testing;Training data;Principal component analysis;Linear discriminant analysis,fuzzy set theory;pattern clustering;statistical analysis;text analysis,fuzzy self-constructing feature clustering algorithm;text classification;statistical mean;deviation;derived membership functions,,84,,45,,29-Jul-10,,,IEEE,IEEE Journals
High Efficient Compression Strategy for Scanned Receipts and Handwritten Documents,掃描收據和手寫文檔的高效壓縮策略,D. Xu; X. Bao,"Sch. of Comput. & Software, Nanjing Univ. of Inf. Sci. & Technol., Nanjing, China; Lab. of Image Sci. & Technol., Southeast Univ., Nanjing, China",2009 First International Conference on Information Science and Engineering,26-Apr-10,2009,,,1270,1273,"Image compression is one of the traditional topics in image processing and has been widely discussed and applied. Some standards, such as, JPEG and JPEG 2000, have also been published for the applications dealing with gray or color photos and medical images. However, for some specific applications, such as, electronic financial management systems (eFMS), much higher efficient algorithms have to be designed for the compression of receipts or handwritten documents. A new strategy is discussed for the compression based on the separation of foreground and background according to the assumption that less degradation of foreground is allowed because of the most important information represented, while more degradation of background is acceptable because it only provides the sense of reality of the document. The image is firstly transformed to YCbCr color space to separate intensities from tones. Then, foreground and background are extracted from the intensity subimage with median filter. Both foreground and background are down-sampled and respectively clustered based on the gray histograms. The chromatic aberration subimages are also down-sampled and transformed to palette-index model by the clustering based on the 2D histogram. All clustered subimages are encoded with JPEG introduced RLE algorithm and synthesized finally. The results demonstrated much higher compression rates of presented strategy than that of JPEG standard.",2160-1291,978-1-4244-5728-1,10.1109/ICISE.2009.632,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5455170,,Image coding;Transform coding;Degradation;Histograms;Image processing;Standards publication;Biomedical imaging;Financial management;Algorithm design and analysis;Data mining,data compression;document image processing;handwriting recognition;handwritten character recognition;image coding,scanned receipts;handwritten documents;image compression;image processing;JPEG 2000;medical images;electronic financial management systems;median filter;gray histograms;chromatic aberration subimages;palette-index model;2D histogram;JPEG standard,,1,,7,,26-Apr-10,,,IEEE,IEEE Conferences
Classified information: the data clustering problem,分類信息：數據聚類問題,N. Memarsadeghi; D. P. O'Leary,"Dept. of Comput. Sci., Maryland Univ., MD, USA; NA",Computing in Science & Engineering,4-Sep-03,2003,5,5,54,60,"Many projects in engineering and science require data classification based on different heuristics. designers, for example, classify automobile engine performance as acceptable or unacceptable based on a combination of efficiency, emissions, noise levels, and other criteria. Researchers routinely classify documents as ""relevant to the current project"" or ""irrelevant"". Genome decoding divides chromosomes into genes, regulatory regions, signals, and so on. Pathologists identify cells as cancerous or benign. We can classify data into different groups by clustering data that are close with respect to some distance measure. In this project, we investigate the design, use, and pitfalls of a popular clustering algorithm, the k-means algorithm.",1558-366X,,10.1109/MCISE.2003.1225861,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1225861,,Clustering algorithms;Automotive engineering;Data engineering;Design engineering;Automobiles;Engines;Noise level;Genomics;Bioinformatics;Decoding,pattern clustering;pattern classification;cellular biophysics;genetics;cancer;biology computing,data clustering;data classification;genome decoding;chromosomes;genes;regulatory regions;signals;cancerous cells;benign cells;distance measure;pathology;k-means algorithm,,3,,6,,4-Sep-03,,,IEEE,IEEE Magazines
MultiSpectral Image Binarization using GMMs,使用GMM的多光譜圖像二值化,F. Hollaus; M. Diem; R. Sablatnig,"Comput. Vision Lab., Tech. Univ. Wien, Vienna, Austria; Comput. Vision Lab., Tech. Univ. Wien, Vienna, Austria; Comput. Vision Lab., Tech. Univ. Wien, Vienna, Austria",2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR),20-Dec-18,2018,,,570,575,MultiSpectral Imaging enhances the study of degraded historical documents. It allows for visualizing washed out or even invisible ink but also improves the automated analysis because of a denser spectral sampling. We present a new methodology for binarization of multispectral document images that groups spectral signatures of different sources by fitting two Gaussian Mixture Models (GMMs) with Expectation Maximization. Both GMMs assign cluster labels to the multispectral samples and the clustering results are combined for the identification of the handwriting regions. The method is evaluated on the ICDAR 2015 MS-TEx dataset. Results on this publicly available benchmarking set are encouraging.,,978-1-5386-5875-8,10.1109/ICFHR-2018.2018.00105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583823,MSI;Binarization;GMM,Clustering algorithms;Gray-scale;Writing;Covariance matrices;Clutter;Ink;Hyperspectral imaging,document image processing;expectation-maximisation algorithm;Gaussian processes;image colour analysis;mixture models;pattern clustering;text analysis,degraded historical documents;invisible ink;automated analysis;denser spectral sampling;multispectral document images;GMMs assign cluster labels;multispectral samples;clustering results;Gaussian mixture models;expectation maximization;multispectral image binarization,,3,,17,,20-Dec-18,,,IEEE,IEEE Conferences
A General Approach for Handwritten Digits Segmentation Using Spectral Clustering,基於譜聚類的手寫數字分割的通用方法,C. Chen; J. Guo,"Comput. Center, East China Normal Univ., Shanghai, China; Comput. Center, East China Normal Univ., Shanghai, China",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,547,552,"In this paper, an approach is proposed to solve a classic segmentation problem on handwritten touching digit pairs using spectral clustering (SC). SC has appeared in many of the state-of-the-art algorithms on image grouping problems recently, while it is a challenging work to build similarities between each pixel. In this paper, support vector machine (SVM) is used to predict the affinity matrix in SC instead of designing a complex function, hence making it a general approach. Different from traditional methods which focus on finding the cutting points or lines, we treat the handwritten string segmentation as a graph partitioning problem, which enables us to separate those digits connected in a very complicated way. We also introduce a 'second-segmentation' to optimize the segmentation result, and find out that the whole algorithm is similar to a multi-layer perception (MLP). Experiment results show that the proposed approach performs satisfactorily with high correct rate while keeping its own advantages as a general approach.",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.95,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270026,segmentation;digit;Spectral Clustering;SVM,Clustering algorithms;Support vector machines;Training;Handwriting recognition;Image segmentation;Partitioning algorithms;Prediction algorithms,graph theory;handwritten character recognition;image segmentation;multilayer perceptrons;pattern clustering;support vector machines,handwritten digits segmentation;spectral clustering;classic segmentation problem;handwritten touching digit pairs;state-of-the-art algorithms;image grouping problems;support vector machine;handwritten string segmentation;graph partitioning problem;affinity matrix;multilayer perception,,2,,21,,29-Jan-18,,,IEEE,IEEE Conferences
Implication intensity: Randomized F-measure for cluster evaluation,蘊含強度：用於聚類評估的隨機F量度,Limin Li; Junjie Wu; Shiwei Zhu,"School of Economics and Management, Beihang University, Beijing 100083, China; School of Economics and Management, Beihang University, Beijing 100083, China; School of Economics and Management, Beihang University, Beijing 100083, China",2009 6th International Conference on Service Systems and Service Management,28-Jul-09,2009,,,510,515,"The ever-growing resources of information and services on World Wide Web provide a welcome boost for the researches in the information retrieval space. Text clustering groups a set of documents into subsets or clusters so that the vast retrieved documents can be browsed selectively and efficiently. Many cluster validation measures, such as the F-measure, are then introduced to evaluate the clustering qualities. In this paper, however, we demonstrate that this widely adopted F-measure suffers from the so-call increment effect which may mislead the comparison of clustering results with different cluster numbers. To meet this challenge, we propose a novel ldquoimplication intensityrdquo (IMI) measure based on the F-measure and a random clustering perspective. Experimental results on real-world data sets demonstrate that IMI shows merits on alleviating the increment effect introduced by the F-measure.",2161-1904,978-1-4244-3661-3,10.1109/ICSSSM.2009.5174937,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5174937,cluster evaluation;increment effect;F-measure;implication intensity,Information retrieval;Search engines;Navigation;Web sites;Web and internet services;Clustering algorithms;Organizing,information resources;information retrieval;Internet;pattern clustering;text analysis,implication intensity;F-measure;cluster evaluation;information resources;World Wide Web;information retrieval;text clustering;increment effect,,,,22,,28-Jul-09,,,IEEE,IEEE Conferences
Hierarchical FCA-based conceptual model of text documents used in information retrieval system,信息檢索系統中基於層次FCA的文本文檔概念模型,P. Butka; J. P籀csov獺,"Technical University of Ko禳ice, Faculty of Economics, Slovakia; Technical University of Ko禳ice, BERG Faculty, Institute of Control and Informatization of Production Processes, Slovakia",2011 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI),9-Jun-11,2011,,,199,204,"Searching for relevant documents in large sets of documents is one of the key tasks in the areas of semantic web and knowledge technologies. This paper deals with analysis and design of improvement for information retrieval (IR) using specific conceptual model automatically created from semantically non-annotated set of text documents. This conceptual model combines locally applied Formal Concept Analysis (FCA) and agglomerative clustering of particular models into one structure, which is suitable to support information retrieval process and can be combined with standard full-text search. Formal Concept Analysis (FCA) is one of the approaches which can be applied in process of conceptual modeling in domain of text documents. Extension of classic FCA (binary table data) is one-sided fuzzy version that works with real values in the object-attribute table (document-term matrix in case of vector representation of text documents). In our approach, starting set of documents is decomposed to smaller sets of similar documents with the use of some partitional clustering algorithm. Then one concept lattice is built for every cluster using FCA method and these FCA-based models are combined to hierarchy of concept lattices using agglomerative clustering algorithm. Finally, we define basic details and methods of IR system that combines standard full-text search and conceptual search (using extracted concept hierarchy).",,978-1-4244-9109-4,10.1109/SACI.2011.5872999,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872999,,Lattices;Clustering algorithms;Indexes;Information retrieval;Analytical models;Computational modeling;Merging,formal concept analysis;fuzzy set theory;information retrieval;pattern clustering;semantic Web;text analysis,FCA;hierarchical conceptual model;text documents;information retrieval system;semantic Web;knowledge technologies;formal concept analysis;object attribute table;agglomerative clustering algorithm;one sided fuzzy version,,4,,9,,9-Jun-11,,,IEEE,IEEE Conferences
Similarity detection between Turkish text documents with distance metrics,具有距離度量的土耳其文本文檔之間的相似性檢測,M. K. Kele?; S. A. ?zel,"Bilgisayar M羹hendisli?i B繹l羹m羹, Adana Bilim ve Teknoloji ?niversitesi, Adana, T羹rkiye; Bilgisayar M羹hendisli?i B繹l羹m羹, ?ukurova ?niversitesi, Adana, T羹rkiye",2017 International Conference on Computer Science and Engineering (UBMK),2-Nov-17,2017,,,316,321,"The aim of this study is to compare the successes of various distance metrics and to determine the most appropriate methods in order to detect similarities among textual documents written in Turkish. Computing similarities between text documents is the basic step of plagiarism detection, and text mining methods like author detection, text classification and clustering. Therefore, plagiarism detection and text mining applications will be more successful by using the distance metrics that are determined according to the results obtained in this study. For this purpose, chunks of texts in different lengths are selected as the experimental dataset in this study. After that, preprocessing methods are applied to the dataset that is used; therefore new and different experimental scenarios are created by removing stopwords and Turkish characters, and stemming words with Zemberek. According to the experimental results, it is observed that the preprocessing phase increases the accuracy of similarity detection. Especially, stemming using Zemberek increases the success rate. In all cases, the Cosine Similarity method has been observed as more successful than other distance metrics, because of producing more realistic results.",,978-1-5386-0930-9,10.1109/UBMK.2017.8093399,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093399,Document similarity;Turkish texts;Distance metrics;Zemberek;Cosine similarity,Measurement;Java;Plagiarism;Text mining;Text categorization;Force;DNA,data mining;natural language processing;pattern clustering;text analysis,similarity detection;Turkish text documents;distance metrics;textual documents;plagiarism detection;text mining methods;author detection;text classification;text mining applications;preprocessing methods;Turkish characters;Cosine Similarity method;text clustering,,,,,,2-Nov-17,,,IEEE,IEEE Conferences
Topic Modeling for Short Texts via Word Embedding and Document Correlation,通過詞嵌入和文檔相關性對短文本進行主題建模,F. Yi; B. Jiang; J. Wu,"School of Computer Science, Zhongshan Institute, University of Electronic Science and Technology of China, Zhongshan, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Beijing College of Politics and Law, Beijing, China",IEEE Access,19-Feb-20,2020,8,,30692,30705,"Topic modeling is a widely studied foundational and interesting problem in the text mining domains. Conventional topic models based on word co-occurrences infer the hidden semantic structure from a corpus of documents. However, due to the limited length of short text, data sparsity impedes the inference process of conventional topic models and causes unsatisfactory results on short texts. In fact, each short text usually contains a limited number of topics, and understanding semantic content of short text needs to the relevant background knowledge. Inspired by the observed information, we propose a regularized non-negative matrix factorization topic model for short texts, named TRNMF. The proposed model leverages pre-trained distributional vector representation of words to overcome the data sparsity problem of short texts. Meanwhile, the method employs the clustering mechanism under document-to-topic distributions during the topic inference by using Gibbs Sampling Dirichlet Multinomial Mixture model. TRNMF integrates successfully both word co-occurrence regularization and sentence similarity regularization into topic modeling for short texts. Through extensive experiments on constructed real-world short text corpus, experimental results show that TRNMF can achieve better results than the state-of-the-art methods in term of topic coherence measure and text classification task.",2169-3536,,10.1109/ACCESS.2020.2973207,National Natural Science Foundation of China; National Social Science Foundation of China; National Basic Research Program of China (973 Program); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993771,Topic model;short texts;word embedding;document correlation;non-negative matrix factorization;regularization,Semantics;Data models;Solid modeling;Knowledge based systems;Computational modeling;Correlation;Task analysis,data mining;learning (artificial intelligence);Markov processes;matrix decomposition;mixture models;Monte Carlo methods;pattern classification;pattern clustering;text analysis,short text;conventional topic models;nonnegative matrix factorization topic model;topic coherence measure;text classification task;word embedding;document correlation;text mining domains;semantic content;data sparsity problem;regularized nonnegative matrix factorization topic model;pre-trained distributional vector representation;TRNMF;clustering mechanism;document-to-topic distributions;Gibbs sampling Dirichlet multinomial mixture model;word co-occurrence regularization;sentence similarity regularization;real-world short text corpus,,3,,45,CCBY,11-Feb-20,,,IEEE,IEEE Journals
Distributed Evaluation of XPath Axes Queries over Large XML Documents Stored in MapReduce Clusters,MapReduce群集中存儲的大型XML文檔對XPath軸查詢的分佈式評估,A. enk; M. Valenta; W. Benn,"Czech Tech. Univ. FIT, Prague, Czech Republic; Czech Tech. Univ. FIT, Prague, Czech Republic; Dept. of Comput. Sci., Chemnitz Univ. of Technol., Chemnitz, Germany",2014 25th International Workshop on Database and Expert Systems Applications,4-Dec-14,2014,,,253,257,"The MR (MapReduce) framework, a programming model for parallel computation over data stored in a cluster of commodity computers, established itself as one of the leading solutions for Big Data processing. This framework is also being used like a query language in many database systems, because it can process data stored in various unstructured, semi-structured, and structured formats. Nevertheless, the MR framework can be used for XML data processing too, it does not allow to write queries in a declarative manner, like XPath or XQuery. To overcome this problem, we propose a system that enables to query XML data with XPath, but it evaluates the queries in parallel using the MR framework. First, we introduce a persistent storage that maps XML data into a wide-column store. The proposed mapping enables efficient and distributed data processing. Secondly, we describe a query processor translating an XPath language subset to MR jobs. Finally, we present tests and their results showing the scalability of our system.",2378-3915,978-1-4799-5722-4,10.1109/DEXA.2014.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974858,,XML;Indexes;Computers;Query processing;Data models;Scalability,Big Data;distributed processing;document handling;query processing;XML,query processor;wide-column store;declarative manner;XQuery;XPath;database systems;big data processing;commodity computer cluster;parallel computation;MR framework;MapReduce clusters;XML documents;distributed XPath axes queries evaluation,,1,,15,,4-Dec-14,,,IEEE,IEEE Conferences
Parallel Community Detection for Cross-Document Coreference,跨社區共引用的並行社區檢測,F. Rahimian; S. Girdzijauskas; S. Haridi,"Swedish Inst. of Comput. Sci., KTH - R. Inst. of Technol., Stockholm, Sweden; KTH - R. Inst. of Technol., Stockholm, Sweden; Swedish Inst. of Comput. Sci., KTH - R. Inst. of Technol., Stockholm, Sweden",2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),20-Oct-14,2014,2,,46,53,"This paper presents a highly parallel solution for cross-document co reference resolution, which can deal with billions of documents that exist in the current web. At the core of our solution lies a novel algorithm for community detection in large scale graphs. We operate on graphs which we construct by representing documents' keywords as nodes and the colocation of those keywords in a document as edges. We then exploit the particular nature of such graphs where co referent words are topologically clustered and can be efficiently discovered by our community detection algorithm. The accuracy of our technique is considerably higher than that of the state of the art, while the convergence time is by far shorter. In particular, we increase the accuracy for a baseline dataset by more than 15% compared to the best reported result so far. Moreover, we outperform the best reported result for a dataset provided for the Word Sense Induction task in SemEval 2010.",,978-1-4799-4143-8,10.1109/WI-IAT.2014.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927606,cross-document coreference;coreference resolution;community detection;distributed algorithm,Color;Communities;Context;Measurement;Clustering algorithms;Force;Accuracy,document handling;graph theory;natural language processing,parallel community detection;cross-document coreference resolution;large scale graph;word sense induction task;SemEval 2010,,10,,37,,20-Oct-14,,,IEEE,IEEE Conferences
Geometric Centroids and their Relative Distances for Off-line Signature Verification,離線簽名驗證的幾何質心及其相對距離,H. N. Prakash; D. S. Guru,"Dept. of Studies in Comput. Sci., Univ. of Mysore, Mysore, India; Dept. of Studies in Comput. Sci., Univ. of Mysore, Mysore, India",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,121,125,"In this paper, we propose a new approach for symbolic representation of off-line signatures based on relative distances between centroids useful for verification. Distances between centroids of off-line signatures are used to form an interval valued symbolic feature vector for representing signatures. A method of off-line signature verification based on the symbolic representation is presented. We investigate the feasibility of the proposed representation scheme for signature verification on a MCYT_ signature database. We cluster similar signatures in each class and also investigate the cluster based symbolic representation for signature verification. Unlike other signature verification methods, the proposed method is simple and efficient. Several experiments are conducted to demonstrate the efficacy of the proposed scheme.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277765,Off-line signature verification;Relative distance between Geometric centroid;Symbolic feature vector;Fuzzy c means clustering,Handwriting recognition;Hidden Markov models;Support vector machines;Shape;Text analysis;Computer science;Spatial databases;Testing;Pattern matching;Pattern recognition,computational geometry;feature extraction;fuzzy set theory;handwriting recognition;image recognition;pattern clustering,geometric centroid;relative distance;off-line signature verification;interval valued symbolic feature vector representation;MCYT signature database;fuzzy c means clustering,,9,,14,,2-Oct-09,,,IEEE,IEEE Conferences
Research on XML Element Search Results Clustering,XML元素搜索結果聚類研究,M. Zhong; C. Wan; D. Liu; X. Jiao,NA; NA; NA; NA,2012 International Conference on Management of e-Commerce and e-Government,6-Dec-12,2012,,,285,290,"Clustering XML search results is an effective way to improve performance. However, the key problem is how to measure similarity between XML documents. This paper studies XML search results clustering based on element granularity and proposes one similarity measurement method. The method firstly uses latent semantic indexing technology(LSI) to obtain term semantics and then combines the XML element node content and semantic structure properties(CASS). To evaluate clustering performance, two new performance evaluation methodologies, namely R_ClusterRatio and R_DocuRatio are introduced. It is motivated by the observations of relevant documents distribution and the fact that the experiment data collection, IEEE CS corpus, do not provide classification information. Experiment results show that proposed similarity method combining term semantics with content and structure semantics integration(LSI-CASS) is feasible, and it produces better clustering quality than LSI-CAS.",,978-0-7695-4853-1,10.1109/ICMeCG.2012.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374926,XML element clustering;term semantics;content and structure semantic,Semantics;XML;Indexing;Matrix decomposition;Information retrieval;Singular value decomposition;Large scale integration,pattern clustering;XML,XML element search results clustering;XML documents;element granularity;similarity measurement method;latent semantic indexing technology;XML element node content;semantic structure properties;clustering performance;performance evaluation methodology;classification information;structure semantics integration;clustering quality,,,,16,,6-Dec-12,,,IEEE,IEEE Conferences
Document clustering using Multi-Objective Genetic Algorithms with parallel programming based on CUDA,基於CUDA的多目標遺傳算法與並行編程的文檔聚類,J. S. Lee; S. C. Park; J. J. Lee; H. H. Ham,"Division of Electronics and Information Engineering, Chonbuk National University, Jeonju-si, Republic of Korea; Division of Electronics and Information Engineering, Chonbuk National University, Jeonju-si, Republic of Korea; Department of Korean Language and Literature, Chonbuk National University, Jeonju-si, Republic of Korea; Department of Archeology and Cultural Anthropology, Chonbuk National University, Jeonju-si, Republic of Korea","2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)",26-Feb-15,2014,1,,280,287,"In this paper, we propose a method of enhancing Multi-Objective Genetic Algorithms (MOGAs) for document clustering with parallel programming. The document clustering using MOGAs shows better performance than other clustering algorithms. However, the overall computation time of the MOGAs is considerably long as the number of documents increases. To effectively avoid this problem, we implement the MOGAs with General-Purpose computing on Graphics Processing Units (GPGPU) to compute the document similarities for the clustering. Furthermore, we introduce two thread architectures (Term-Threads and Document-Threads) in the CUDA (Compute Unified Device Architecture) language. The experimental results show that the parallel MOGAs with CUDA are tremendously faster than the general MOGAs.",,978-9-8975-8062-8,10.5220/0005057502800287,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049783,Document Clustering;Genetic Algorithms;Multi-Objective Genetic Algorithms;GPGPU;CUDA,Graphics processing units;Genetic algorithms;Instruction sets;Vectors;Linear programming;Computer architecture;Sociology,,,,,,18,,26-Feb-15,,,IEEE,IEEE Conferences
Clustering User Preferences Using W-kmeans,使用W-kmeans聚類用戶首選項,C. Bouras; V. Tsogkas,"Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece; Comput. Eng. & Inf. Dept., Univ. of Patras, Patras, Greece",2011 Seventh International Conference on Signal Image Technology & Internet-Based Systems,2-Jan-12,2011,,,75,82,"Although commonly only document clustering is suggested by Web mining techniques for recommendation systems, one of the various tasks of personalized recommendation is categorization of Web users. In this paper, a method for clustering navigation patterns of Web users is proposed. We adapt the WordNet-enabled W-kmeans algorithm, an enhancement of standard k-means algorithm which uses the external knowledge from WordNet hypernyms and that has been previously used for document clustering, to user profile clustering by analyzing the users' historical data. We also investigate the effects this approach has on the recommendation engine by evaluating the overall performance it has in terms of precision -- recall on our online recommendation system.",,978-1-4673-0431-3,10.1109/SITIS.2011.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120632,User clustering;session identification;recommendation system;personalization;k-means;W-kmeans,,pattern clustering,user preferences clustering;W-kmeans algorithm;document clustering;Web mining techniques;recommendation systems;clustering navigation patterns;online recommendation system,,7,,15,,2-Jan-12,,,IEEE,IEEE Conferences
Knowledge-Supervised Learning by Co-clustering Based Approach,基於共聚方法的知識監督學習,C. Zhang; D. Xing,"Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China",2008 Seventh International Conference on Machine Learning and Applications,22-Dec-08,2008,,,773,776,"Traditional text learning algorithms need labeled documents to supervise the learning process, but labeling documents of a specific class is often expensive and time consuming. We observe it is convenient to use some keywords(i.e. class-descriptions) to describe class sometimes. However, short class-description usually does not contain enough information to guide classification. Fortunately, large amount of public data is easily acquired, i.e. ODP, Wikipedia and so on, which contains enormous knowledge. In this paper, we address the text classification problem with such knowledge rather than any labeled documents and propose a co-clustering based knowledge-supervised learning algorithm (CoCKSL) in information theoretic framework, which effectively applies the knowledge to classification tasks.",,978-0-7695-3495-4,10.1109/ICMLA.2008.116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4725064,,Testing;Machine learning;Supervised learning;Labeling;Wikipedia;Text categorization;Internet;Application software;Computer science;Knowledge engineering,learning (artificial intelligence);pattern classification;pattern clustering;text analysis,knowledge-supervised learning;co-clustering based approach;text learning algorithms;labeled documents;text classification problem;CoCKSL,,,,11,,22-Dec-08,,,IEEE,IEEE Conferences
Transfer Learning via Cluster Correspondence Inference,通過聚類對應推理進行學習轉移,M. Long; W. Cheng; X. Jin; J. Wang; D. Shen,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Sch. of Software, Tsinghua Univ., Beijing, China; Sch. of Software, Tsinghua Univ., Beijing, China; Sch. of Software, Tsinghua Univ., Beijing, China; Microsoft Adcenter Labs., Redmond, WA, USA",2010 IEEE International Conference on Data Mining,20-Jan-11,2010,,,917,922,"Transfer learning targets to leverage knowledge from one domain for tasks in a new domain. It finds abundant applications, such as text/sentiment classification. Many previous works are based on cluster analysis, which assume some common clusters shared by both domains. They mainly focus on the one-to-one cluster correspondence to bridge different domains. However, such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain. In this paper, we propose a Cluster Correspondence Inference (CCI) method to iteratively infer many-to-many correspondence among clusters from different domains. Specifically, word clusters and document clusters are exploited for each domain using nonnegative matrix factorization, then the word clusters from different domains are corresponded in a many-to-many scheme, with the help of shared word space as a bridge. These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence. Experiments on various real data sets demonstrate that our method outperforms several state-of-the-art approaches for cross-domain text classification.",2374-8486,978-1-4244-9131-5,10.1109/ICDM.2010.146,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694061,Transfer Learning;Text Classification;Cluster Correspondence Inference,Optimization;Bridges;Support vector machines;Accuracy;Joints;Clustering algorithms;Convergence,inference mechanisms;learning (artificial intelligence);matrix decomposition;pattern classification;pattern clustering;text analysis,transfer learning;cluster correspondence inference;sentiment classification;text classification;cluster analysis;nonnegative matrix factorization;one-to-one scheme;many-to-many scheme,,5,,13,,20-Jan-11,,,IEEE,IEEE Conferences
Topic grouping by spectral clustering,通過光譜聚類對主題進行分組,Y. Jeong; W. Lee; H. Choi,"Department of Computer Science, KAIST(Korea Advanced Institute of Science and Technology), 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science, KAIST(Korea Advanced Institute of Science and Technology), 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science, KAIST(Korea Advanced Institute of Science and Technology), 291 Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea",16th International Conference on Advanced Communication Technology,27-Mar-14,2014,,,657,661,"With the growing number of web documents, it becomes difficult to analyze and obtain information from such an array of documents. Furthermore, unsupervised methods are preferable, as most web documents are unlabeled. Probabilistic topic modeling is one such method. It discovers latent structures among unstructured documents. While many traditional topic models usually assume that the topics are independent of each other, some models have been proposed to obtain correlations between the topics or a hierarchy of the topics. These models are designed to obtain both the topics and the correlations without using any other method. Therefore, very few studies apply other methods to determine a correlation between topics. In this paper, we apply spectral clustering to group the topics obtained from a traditional topic model, in this case the Latent Dirichlet Allocation model. To the best of our knowledge, this is the first approach that uses spectral clustering for the grouping of topics. We demonstrate the experimental results with various settings.",1738-9445,978-89-968650-3-2,10.1109/ICACT.2014.6779044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779044,Topic model;Spectral clustering;Topic grouping,Correlation;Clustering algorithms;Computational modeling;Data models;Educational institutions;Hidden Markov models;Neural networks,pattern clustering;probability;text analysis,topic grouping;spectral clustering;Web document;unsupervised method;probabilistic topic modeling;latent structure;latent Dirichlet allocation model,,,,14,,27-Mar-14,,,IEEE,IEEE Conferences
Smoothness Regularized Multiview Subspace Clustering With Kernel Learning,具有核學習的平滑正則化多視圖子空間聚類,C. -D. Wang; M. -S. Chen; L. Huang; J. -H. Lai; P. S. Yu,"School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510006, China, also with the Guangdong Province Key Laboratory of Computational Science, Yat-sen University, Guangzhou 510006, China, and also with the Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Yat-sen University, Guangzhou 510006, China (e-mail: changdongwang@hotmail.com); School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510006, China, also with the Guangdong Province Key Laboratory of Computational Science, Yat-sen University, Guangzhou 510006, China, and also with the Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Yat-sen University, Guangzhou 510006, China.; College of Mathematics and Informatics, South China Agricultural University, Guangzhou 510642, China.; School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510006, China, also with the Guangdong Province Key Laboratory of Computational Science, Yat-sen University, Guangzhou 510006, China, and also with the Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Yat-sen University, Guangzhou 510006, China.; Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607 USA, and also with the Institute for Data Science, Tsinghua University, Beijing 100084, China.",IEEE Transactions on Neural Networks and Learning Systems,,2020,PP,99,1,14,"Multiview subspace clustering has attracted an increasing amount of attention in recent years. However, most of the existing multiview subspace clustering methods assume linear relations between multiview data points when learning the affinity representation by means of the self-expression or fail to preserve the locality property of the original feature space in the learned affinity representation. To address the above issues, in this article, we propose a new multiview subspace clustering method termed smoothness regularized multiview subspace clustering with kernel learning (SMSCK). To capture the nonlinear relations between multiview data points, the proposed model maps the concatenated multiview observations into a high-dimensional kernel space, in which the linear relations reflect the nonlinear relations between multiview data points in the original space. In addition, to explicitly preserve the locality property of the original feature space in the learned affinity representation, the smoothness regularization is deployed in the subspace learning in the kernel space. Theoretical analysis has been provided to ensure that the optimal solution of the proposed model meets the grouping effect. The unique optimal solution of the proposed model can be obtained by an optimization strategy and the theoretical convergence analysis is also conducted. Extensive experiments are conducted on both image and document data sets, and the comparison results with state-of-the-art methods demonstrate the effectiveness of our method.",2162-2388,,10.1109/TNNLS.2020.3026686,National Key Research and Development Program of China; National Natural Science Foundation of China; Guangdong Natural Science Funds for Distinguished Young Scholar; NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216513,Clustering;grouping effect;kernel learning;multiview;subspace.,Kernel;Clustering methods;Analytical models;Optimization;Data models;Clustering algorithms;Learning systems,,,,,,,,7-Oct-20,,,IEEE,IEEE Early Access Articles
Data categorization for a context return applied to logical document structure recognition,用於上下文返回的數據分類應用於邏輯文檔結構識別,Y. Rangoni; A. Belaid,"Loria Res. Center, Nancy, France; Loria Res. Center, Nancy, France",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,297,301 Vol. 1,"The purpose of this work is to develop a pattern recognition system simulating the human vision. A transparent neural network, with context returns is used. The context returns consist in using global vision to correct local vision (i.e. input data are corrected according to neural network outputs). In order not to compute all the input features during these context returns, a filter-based method was designed to organize the features in clusters. This allows finding a good subset of input features during each cycle, which reduce the computations. The method interest is shown in the case of logical document structure retrieval.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575557,,Pattern recognition;Humans;Text analysis;Neural networks;Computational modeling;Context modeling;Design methodology;Shape;Sections;Streaming media,document image processing;information retrieval;neural nets;pattern recognition,data categorization;logical document structure recognition;pattern recognition system;human vision;transparent neural network;context returns;filter-based method;logical document structure retrieval,,3,,17,,16-Jan-06,,,IEEE,IEEE Conferences
A diversifying hidden units method based on NMF for document representation,基於NMF的多樣化隱藏單元表示方法。,X. Jiang; H. Zhang; R. Liu; Y. Zuo,"School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China",2016 IEEE International Conference on Knowledge Engineering and Applications (ICKEA),2-Jan-17,2016,,,103,107,"Document modeling with hidden units as known as topics are very popular. Non-negative matrix factorization(NMF) is one of the most important techniques in document representation, which decomposes a document-term matrix into a document-topic matrix and a topic-term matrix. Since orthogonal constraint would limit terms occur only in one topic, we abandon this strong constraint. Furthermore, in order to represent documents in a certain number of topics with more semantic information, we add diversifying regularization and sparse constraint into NMF, which shows a great improvement in text classification and clustering. In the end, we draw the figure of topics similarities and display the top 20 weighted words in each topic to reveal that diversifying regularization can efficiently reduce the overlapping terms.",,978-1-5090-3471-0,10.1109/ICKEA.2016.7803001,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803001,hidden units;non-negative matrix factorization;diversifying regularization;sparse constraint,,classification;matrix decomposition;pattern clustering;text analysis,diversifying hidden units method;NMF;document representation;document modeling;nonnegative matrix factorization;document-term matrix;document-topic matrix;topic-term matrix;semantic information;text classification;text clustering,,2,,17,,2-Jan-17,,,IEEE,IEEE Conferences
High Dimensional Data Clustering by means of Distributed Dirichlet Process Mixture Models,通過分佈式Dirichlet過程混合模型進行高維數據聚類,K. Meguelati; B. Fontez; N. Hilgert; F. Masseglia,"LIRMM, Univ Montpellier, CNRS. Montpellier,Inria,France; INRA, Univ Montpellier,MISTEA, Montpellier SupAgro,Montpellier,France; INRA, Univ Montpellier,MISTEA, Montpellier SupAgro,Montpellier,France; LIRMM, Univ Montpellier, CNRS. Montpellier,Inria,France",2019 IEEE International Conference on Big Data (Big Data),24-Feb-20,2019,,,890,899,"Clustering is a data mining technique intensively used for data analytics, with applications to marketing, security, text/document analysis, or sciences like biology, astronomy, and many more. Dirichlet Process Mixture (DPM) is a model used for multivariate clustering with the advantage of discovering the number of clusters automatically and offering favorable characteristics. However, in the case of high dimensional data, it becomes an important challenge with numerical and theoretical pitfalls. The advantages of DPM come at the price of prohibitive running times, which impair its adoption and makes centralized DPM approaches inefficient, especially with high dimensional data. We propose HD4C (High Dimensional Data Distributed Dirichlet Clustering), a parallel clustering solution that addresses the curse of dimensionality by two means. First it gracefully scales to massive datasets by distributed computing, while remaining DPM-compliant. Second, it performs clustering of high dimensional data such as time series (as a function of time), hyperspectral data (as a function of wavelength) etc. Our experiments, on both synthetic and real world data, illustrate the high performance of our approach.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006065,Gaussian random process;Dirichlet Process Mixture Model;Clustering;Parallelism;Reproducing Kernel Hilbert Space,Distributed databases;Time series analysis;Clustering algorithms;Gaussian processes;Kernel;Hilbert space;Big Data,data analysis;data mining;mixture models;pattern clustering;statistical distributions;text analysis,data mining technique;data analytics;DPM;multivariate clustering;parallel clustering solution;hyperspectral data;synthetic world data;real world data;distributed Dirichlet process mixture models;high dimensional data distributed Dirichlet clustering,,1,,43,,24-Feb-20,,,IEEE,IEEE Conferences
"Categorization, clustering and association rule mining on WWW",WWW上的分類，聚類和關聯規則挖掘,S. S. Bedi; H. Yadav; P. Yadav,"M.J.P.Rohilkhand University, Bareilly, India; Khandelwal College of Management and Technology, India; M.J.P.Rohilkhand University, Bareilly, India","2009 International Multimedia, Signal Processing and Communication Technologies",14-Jul-09,2009,,,173,177,"Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related Web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, AutoClass.",,978-1-4244-3602-6,10.1109/MSPCT.2009.5164177,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5164177,,Association rules;Data mining;World Wide Web;Clustering algorithms;Bayesian methods;Intelligent agent;Software agents;Information filtering;Information filters;Web sites,data mining;feature extraction;graph theory;information filtering;Internet;pattern classification;pattern clustering;probability;query formulation;search engines;software agents;text analysis,text categorization;clustering technique;association rule mining;WWW;World Wide Web;intelligent software agent;information retrieval;information filtering;Web document structure;query formulation;feature extraction;search engine;probabilistic technique;graph partitioning,,,,18,,14-Jul-09,,,IEEE,IEEE Conferences
Context-based Hierarchical Clustering for the Ontology Learning,用於本體學習的基於上下文的層次聚類,L. Karoui; M. Aufaure; N. Bennacer,"Ecole Superieure d'Electricite, France; Ecole Superieure d'Electricite, France; Ecole Superieure d'Electricite, France",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,420,427,"Ontologies provide a common layer which plays a major role in supporting information exchange and sharing. In this paper, we focus on the ontological concept extraction process from HTML documents. In order to improve this process, we propose an unsupervised hierarchical clustering algorithm namely ""contextual ontological concept extraction"" (COCE) which is an incremental use of the partitioning algorithm Kmeans and is guided by a structural context. Our context exploits the HTML structure and the location of words to select the semantically closer cooccurrents for each word and to improve the words weighting. Guided by this context definition, we perform an incremental clustering that refines the context of each word clusters to obtain semantically extracted concepts. The COCE algorithm offers the choice between either an automatic execution or a user's interaction. We experiment our algorithm on HTML documents related to the tourism domain. Our results show how the execution of our context-based algorithm which implements an incremental process and a successive refinement of clusters improves their conceptual quality and the relevance of the extracted ontological concepts",,0-7695-2747-7,10.1109/WI.2006.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061406,,Ontologies;HTML;Clustering algorithms;Data mining;Partitioning algorithms;Logic programming;Web pages;Semantic Web;Buildings;Learning systems,hypermedia markup languages;ontologies (artificial intelligence);pattern clustering;semantic networks;unsupervised learning,context-based hierarchical clustering;ontology learning;information exchange;information sharing;HTML documents;unsupervised hierarchical clustering algorithm;contextual ontological concept extraction;incremental clustering;context-based algorithm,,11,,23,,15-Jan-07,,,IEEE,IEEE Conferences
Suffix Tree Clustering with Named Entity Recognition,具有命名實體識別的後綴樹聚類,J. Zhang; Q. Dang; Y. Lu; S. Sun,"Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China; Sch. of Inf. & Commun. Eng., Beijing Univ. of Posts & Telecommun., Beijing, China",2013 International Conference on Cloud Computing and Big Data,26-May-14,2013,,,549,556,The news searching is challengeable in providing web users with clear and readable lists of news reports. This paper proposes the Suffix Tree Clustering with Named Entity Recognition (STC-NER). STC-NER is supposed to cluster news searching results returned by the search engine. STC-NER uses the snippets returned from the searching results and then derives patterned information by means of named entity recognition. STC-NER makes a great contribute to the reduction of storage as well as the time complexity. Experiments show that STC-NER has a better performance in precision and efficiency than the traditional Suffix Tree Clustering (STC).,,978-1-4799-2830-9,10.1109/CLOUDCOM-ASIA.2013.102,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821048,suffix Tree;clustering;news reports;named entity recognition,Clustering algorithms;Educational institutions;Organizations;Search engines;Vectors;Tagging;Algorithm design and analysis,pattern clustering;storage management;text analysis,Web users;STC-NER;suffix tree clustering with named entity recognition;search engine;snippets;storage reduction;text document clustering algorithm;news searching,,,,20,,26-May-14,,,IEEE,IEEE Conferences
New approach for human detection in images using histograms of oriented gradients,使用定向梯度直方圖在圖像中進行人體檢測的新方法,N. G. Bardeh; M. Palhang,"Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran; Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran",2013 21st Iranian Conference on Electrical Engineering (ICEE),16-Sep-13,2013,,,1,4,"Histograms of Oriented Gradients (HoG) is one of the most used descriptors in human detection. Although it has good performance compared to other descriptors in the area, if the size and number of images increase, the dimension of the descriptor vectors would become extremely large and therefore makes the training process computationally complex. To overcome this, in this paper a human detection method based on bag-of-features model is represented. Visual words are patches of pictures described with HoG and then clustered using K-means algorithm. To highlight the most important visual words, a weighting method could be applied to the descriptor vectors. Here we used Term Frequency-Inverse Document Frequency (Tf_Idf) which has been used in document classification. In the proposed approach, Support Vector Machine (SVM) is used as the binary classifier. We applied our proposed method to the MIT and INRIA datasets and compared the performance of our algorithm with a similar method in the literature. The results of our experiments show that our method performs at least as well as other available methods.",2164-7054,978-1-4673-5634-3,10.1109/IranianCEE.2013.6599619,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599619,Human Detection;Histogram of Oriented Gradients;Bag-of-Features Model;Tf_Idf Weighting,Visualization;Histograms;Support vector machine classification;Feature extraction;Detectors;Training,document image processing;image classification;object detection;pattern clustering;support vector machines,human detection method;histograms of oriented gradients;HoG;descriptor vector dimension;bag-of-features model;visual words;k-means algorithm;clustering;weighting method;term frequency-inverse document frequency;Tf-Idf;document classification;support vector machine;SVM;binary classifier;MIT datasets;INRIA datasets,,6,,12,,16-Sep-13,,,IEEE,IEEE Conferences
Construction of similarity profile of dynamic web documents for Similarity-aware web content management,構造動態Web文檔的相似性概要文件，以進行相似性感知的Web內容管理,J. Xiao,"School of Computer and Security Science, Edith Cowan University, 2 Bradford Street, Mt Lawley, WA 6050, Australia","IET 3rd International Conference on Wireless, Mobile and Multimedia Networks (ICWMNN 2010)",28-Jan-11,2010,,,335,338,"Discovering and establishing similarities among web documents is one of the key research streams in web usage mining community in the recent years. The knowledge obtained from the exercise can be used for many applications such as optimizing web cache organization and improving the quality of web document pre-fetching. This paper presents a matrix-based method to establish similarities among web documents, which are then applied to a Similarity-aware web content management system, facilitating offline building of the similarity-ware web caches and online updating similarity profiles of the system.",,978-1-84919-240-8,10.1049/cp.2010.0684,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703022,similarity discovery;profile;cluster;web content management,,Internet;storage management,dynamic Web document;Web content management;Web cache organization;Web document prefetching,,1,,,,28-Jan-11,,,IET,IET Conferences
Detection of curved text path based on the fuzzy curve-tracing (FCT) algorithm,基於模糊曲線跟?（FCT）算法的彎曲文本路徑檢測,Hong Yan,"Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,266,270,"Artistic documents often contain text along curved paths. Commonly used computer algorithms for text extraction, which only deal with text along straight lines, cannot be employed to analyze these artistic documents. In this paper, we solve this problem using the fuzzy curve-tracing algorithm. In our method, the character pixels are grouped based on the fuzzy c-means algorithm to reduce the amount of data. Then the cluster centers are connected to form the initial curve representing the text path. Finally the character pixels are clustered again under the constraint that the path must be smooth. Results from several experiments are presented to show the effectiveness of our method.",,0-7695-1263-1,10.1109/ICDAR.2001.953796,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953796,,Clustering algorithms;Iterative algorithms;Image analysis;Character recognition;Text analysis;Optical character recognition software;Graphics;Histograms;Data mining;Algorithm design and analysis,fuzzy logic;feature extraction;character recognition,curved text path detection;fuzzy curve-tracing algorithm;artistic documents;computer algorithms;text extraction;fuzzy curve tracing algorithm;character pixels;fuzzy c-means algorithm;cluster centers,,3,,18,,7-Aug-02,,,IEEE,IEEE Conferences
A Hybrid Solution To Abstractive Multi-Document Summarization Using Supervised and Unsupervised Learning,使用有監督和無監督學習的抽像多文檔摘要混合解決方案,G. Bhagchandani; D. Bodra; A. Gangan; N. Mulla,"Sardar Patel Inst. Of Technology,Information Technology,Mumbai,India; Sardar Patel Inst. Of Technology,Information Technology,Mumbai,India; Sardar Patel Inst. Of Technology,Information Technology,Mumbai,India; Sardar Patel Inst. Of Technology,Information Technology,Mumbai,India",2019 International Conference on Intelligent Computing and Control Systems (ICCS),16-Apr-20,2019,,,566,570,"In this work, we aim to develop an abstractive summarization system in the multi-document setup. The main challenge in this kind of a system is the identification of redundant information. Our approach hybridizes three components, viz. Clustering, Word Graphs, Neural Networks. In clustering, all the information from multiple documents is divided amongst clusters based on context and importance analysis, such that each cluster possesses sentences of a similar context - Redundancy Identification. Further, Shortest Path Detection in Word Graphs reduces the text. Along with that, we use a sequence to sequence sentence compression and perform paraphrasing using Supervised Recurrent Neural Network to generate an almost completely abstractive summary. The dataset DUC 2004 that was used indicates that the proposed system outperforms other systems in terms of metrics like ROUGE[1] and BLEU[2].",,978-1-5386-8113-8,10.1109/ICCS45141.2019.9065724,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065724,Abstractive summarization;multi-document;clustering;sentence compression;paraphrasing;neural networks;natural language processing;long short-term memory cell (LSTM);redundancy detection;ROUGE;BLEU,Neural networks;Redundancy;Measurement;Training;Conferences;Control systems;Information technology,graph theory;pattern clustering;recurrent neural nets;text analysis;unsupervised learning,supervised recurrent neural network;multidocument setup;redundant information;word graphs;multiple documents;redundancy identification;sequence sentence compression;abstractive multidocument summarization;unsupervised learning;shortest path detection;neural networks,,,,19,,16-Apr-20,,,IEEE,IEEE Conferences
On-line evolving clustering for financial statements' anomalies detection,在線進化聚類，用於財務報表異常檢測,S. Omanovic; Z. Avdagic; S. Konjicija,"Department of Computing and Informatics, Faculty of Electrical Engineering, Sarajevo, Bosnia and Herzegovina; Department of Computing and Informatics, Faculty of Electrical Engineering, Sarajevo, Bosnia and Herzegovina; Department of Computing and Informatics, Faculty of Electrical Engineering, Sarajevo, Bosnia and Herzegovina","2009 XXII International Symposium on Information, Communication and Automation Technologies",4-Dec-09,2009,,,1,4,"This document proposes an approach for financial statements' anomalies detection by using on-line evolving clustering. Official records of the financial activities of a business are called financial statements and they are recorded in journals and general ledger in a supervised process. Anomalies in financial statements are caused by human mistakes during forming of financial statements, or as a result of changes in the software that produced un-expected errors, or as possible financial fraud.",,978-1-4244-4220-1,10.1109/ICAT.2009.5348416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5348416,anomalies detection;evolving clustering;fraud detection,Authentication;Radiofrequency identification;Information security;Product codes;Scalability;Privacy;Radio frequency;Performance analysis;Databases;Data security,accounts data processing;fraud;management accounting;pattern classification;pattern clustering;software maintenance,online evolving clustering algorithm;financial statement anomaly detection;official record;business financial activity;journal recording;ledger recording;supervised classification process;financial fraud detection;software error;accounting system;software maintenance lifecycle change,,3,,9,,4-Dec-09,,,IEEE,IEEE Conferences
An Aging Theory for Event Life-Cycle Modeling,事件生命週期建模的老化理論,C. C. Chen; Y. Chen; M. C. Chen,"Inst. of Inf. Sci., Acad. Sinica, Taipei; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",20-Feb-07,2007,37,2,237,248,"An event can be described by a sequence of chronological documents from several information sources that together describe a story or happening. The goal of event detection and tracking is to automatically identify events and their associated documents during their life cycles. Conventional document clustering and classification techniques cannot effectively detect and track sequential events, as they ignore the temporal relationships among documents related to an event. The life cycle of an event is analogous to living beings. With abundant nourishment (i.e., related documents for the event), the life cycle is prolonged; conversely, an event or living fades away when nourishment is exhausted. Improper tracking algorithms often unnecessarily prolong or shorten the life cycle of detected events. In this paper, we propose an aging theory to model the life cycle of sequential events, which incorporates a traditional single-pass clustering algorithm to detect and track events. Our experiment results show that the proposed method achieves a better overall performance for both long-running and short-term events than previous approaches. Moreover, we find that the aging parameters of the aging schemes are profile dependent and that using proper profile-specific aging parameters improves the detection and tracking performance further",1558-2426,,10.1109/TSMCA.2006.886370,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4100769,Clustering;knowledge life cycle;web mining,Aging;Event detection;Clustering algorithms;Internet;Search engines;Information science;Text categorization;Web mining;Publishing,data mining;information resources;Internet;pattern clustering,aging theory;event life-cycle modeling;chronological documents;event detection;document clustering;classification techniques;single-pass clustering algorithm;knowledge life cycle,,29,,31,,20-Feb-07,,,IEEE,IEEE Journals
A Simple and Fast Term Selection Procedure for Text Clustering,文本聚類的簡單快速術語選擇過程,L. Gonzaga; M. Grivet; A. T. Vasconcelos,"Laboratorio Nacional de Computacao Cientifica, Rio de Janeiro; Pontificia Univ. Catolica do Rio de Janeiro, Rio de Janeiro; NA",Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007),27-Nov-07,2007,,,777,781,"Text clustering is a theme that is receiving considerable attention nowadays in areas such as text mining and information retrieval. A starting point for clustering methods applied on unstructured document collection is the creation of a vector-space model usually known as bag-ofwords model [1J. Documents are then usually described by a matrix which happens to be huge and extremely sparse which is due to the exceeding number of terms describing the set of documents. Although several techniques can be employed to reduce this number, the final figure is still high thus leading to a feature space of high dimensionality. This paper presents a simple procedure that not only considerably reduces the dimensionality of the feature space and hence the processing time, but also produces clustering performances comparable or even better when confronted with the full set of terms.",2164-7151,978-0-7695-2976-9,10.1109/ISDA.2007.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4389702,,Sparse matrices;Neodymium;Text mining;Information retrieval;Clustering methods;Abstracts;Frequency;Clustering algorithms;Intelligent systems;Broadcasting,data mining;data reduction;information retrieval;pattern clustering;sparse matrices;text analysis,text clustering;text mining;information retrieval;unstructured document collection;vector-space model;sparse matrix;dimensionality reduction;feature space,,1,,18,,27-Nov-07,,,IEEE,IEEE Conferences
An enhanced fuzzy similarity based concept mining model for text classification using feature clustering,基於特徵聚類的基於模糊相似度的概念挖掘模型的增強模型,S. Puri; S. Kaushik,"Computer Science from Birla Institute of Technology, Mesra - 835215, Ranchi, Jharkhand, India; Computer Science from Birla Institute of Technology, Mesra - 835215, Ranchi, Jharkhand, India",2012 Students Conference on Engineering and Systems,14-May-12,2012,,,1,6,"In the current era, the feature reduction is an essential part of the classification algorithms, methodologies and algorithms. When a set of features is extracted from a text document, then these features collectively make a huge, large volume high dimensional data set and contain large space and long time to be processed each time. This challenge requires a new text classification forum which can find the solution to remedy it. In this paper, a Fuzzy Similarity based Concept Mining Model Using Feature Clustering (FSCMM-FC) is proposed which capably categorizes various seen and known text documents into different predefined and mutually exclusive categories groups by keeping the data (or feature set dimension) very low. The paper also discusses a case study of 4 text documents to analyze the proposed system. The analysis shows that the system provides 70-75% improved results; thereby increasing the system performance drastically.",,978-1-4673-0455-9,10.1109/SCES.2012.6199126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199126,Concept mining;feature clustering;fuzzy similarity measure;sentence level;document level;integrated corpora level processing,Feature extraction;Paints;Vectors;Computers;Text categorization;System performance;Accuracy,category theory;data mining;fuzzy set theory;pattern classification;pattern clustering;text analysis,fuzzy similarity based concept mining model;text classification;feature clustering;feature reduction;feature extraction;text document;mutually exclusive category groups;large volume high dimensional data set,,6,,16,,14-May-12,,,IEEE,IEEE Conferences
Personalized Hierarchical Clustering,個性化層次聚類,K. Bade; A. Nurnberger,"Otto-von-Guericke-University Magdeburg, Germany; Otto-von-Guericke-University Magdeburg, Germany",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,181,187,"A hierarchical structure can provide efficient access to information contained in a collection of documents. However, such a structure is not always available, e.g. for a set of documents a user has collected over time in a single folder or the results of a Web search. We therefore investigate in this paper how we can obtain a hierarchical structure automatically, taking into account some background knowledge about the way a specific user would structure the collection. More specifically, we adapt a hierarchical agglomerative clustering algorithm to take into account user specific constraints on the clustering process. Such an algorithm could be applied, e.g., for user specific clustering of Web search results, where the user's constraints on the clustering process are given by a hierarchical folder or bookmark structure. Besides the discussion of the algorithm itself, we motivate application scenarios and present an evaluation of the proposed algorithm on benchmark data",,0-7695-2747-7,10.1109/WI.2006.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061364,,Clustering algorithms;Web search;Computer science;Cultural differences;Search engines;Information retrieval;Clustering methods;Web pages;Libraries;Catalogs,Internet;learning (artificial intelligence);pattern clustering,document collection;personalized hierarchical agglomerative clustering algorithm;bookmark structure;search engines,,14,,14,,15-Jan-07,,,IEEE,IEEE Conferences
On the Network and Topological Analyses of Legal Documents using Text Mining Approach,文本挖掘法研究法律文件的網絡和拓撲分析,S. Somsakul; S. Prom-on,"King Mongkut?s University of Technology Thonburi,Department of Computer Engineering,Bangkok,Thailand; King Mongkut?s University of Technology Thonburi,Department of Computer Engineering,Bangkok,Thailand",2020 1st International Conference on Big Data Analytics and Practices (IBDAP),5-Nov-20,2020,,,1,6,"This paper presents a computational study of Thai legal documents using text mining and network analytic approach. Thai legal systems rely much on the existing judicial rulings. Thus, legal documents contain complex relationships and require careful examination. The objective of this study is to use text mining to model relationships between these legal documents and draw useful insights. A structure of document relationship was found as a result of the study in forms of a network that is related to the meaningful relations of legal documents. This can potentially be developed further into a document retrieval system based on how documents are related in the network.",,978-1-7281-8106-6,10.1109/IBDAP50342.2020.9245615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9245615,Document Mining;Graph Analysis;Network Science;Graph Clustering,,,,,,,8,,5-Nov-20,,,IEEE,IEEE Conferences
Dynamic filters selection for textual document image binarization,用於文本文檔圖像二值化的動態過濾器選擇,H. Cecotti; A. Belad,"Institute of Automation (IAT), University of Bremen, Germany; LORIA, Vandoeuvre-L矇s-Nancy, France",2008 19th International Conference on Pattern Recognition,23-Jan-09,2008,,,1,4,"For a document class, one challenge in document binarization is to automatically find a set of techniques, which are adapted to the different degradation level of the images. It is important to know the methods to use and where they can be applied advantageously. A multi-classifiers solution is presented for pixel classification. These classifiers act as filters and are used for binarization. The technique starts by clustering close pixels by K-means. A classifier, which corresponds to a supervised neural network, is dedicated to each cluster. They are trained according to a binarized image where its pixels are weighted function to erosion transformation effects. The presented method is compared to classical binarization techniques in the literature. Its effect on the commercial OCR performance reaches a gain from 0.16% for Finereader7 and 1.06% for Omnipage14 for the recognition rate.",1051-4651,978-1-4244-2174-9,10.1109/ICPR.2008.4761842,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761842,,Filters;Pixel;Optical character recognition software;Image recognition;Distributed control;Automation;Degradation;Neural networks;Performance gain;Image restoration,document image processing;image classification;learning (artificial intelligence);neural nets,dynamic filters selection;textual document image binarization;degradation level;multiclassifiers solution;pixel classification;K-means;supervised neural network;erosion transformation effects;commercial OCR performance;Finereader7;Omnipagel4,,3,,12,,23-Jan-09,,,IEEE,IEEE Conferences
Classification and clustering for neuroinformatics: Assessing the efficacy on reverse-mapped NeuroNLP data using standard ML techniques,神經信息學的分類和聚類：使用標準ML技術評估反向映射NeuroNLP數據的功效,N. Melethadathil; P. Chellaiah; B. Nair; S. Diwakar,"Amrita School of Biotechnology, Amrita Vishwa Vidyapeetham, (Amrita University), Kollam, Kerala, India; Amrita School of Biotechnology, Amrita Vishwa Vidyapeetham, (Amrita University), Kollam, Kerala, India; Amrita School of Biotechnology, Amrita Vishwa Vidyapeetham, (Amrita University), Kollam, Kerala, India; School of Biotechnology Amrita Vishwa Vidyapeetham (Amrita University), Kollam, Kerala, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",28-Sep-15,2015,,,1065,1070,"NeuroinformaticsNatural Language Processing (NeuroNLP) relies on clustering and classification for information categorization of biologically relevant extraction targets and for interconnections to knowledge-related patterns in event and text mined datasets. The accuracy of machine learning algorithms depended on quality of text-mined data while efficacy relied on the context of the choice of techniques. Although developments of automated keyword extraction methods have made differences in the quality of data selection, the efficacy of the Natural Language Processing (NLP) methods using verified keywords remain a challenge. In this paper, we studied the role of text classification and document clustering algorithms on datasets, where features were obtained by mapping to manually verified MESH terms published by National Library of Medicine (NLM). In this study, NLP data classification involved comparing 8techniques and unsupervised learning was performed with 6 clustering algorithms. Most classification techniques except meta-based algorithms namely stacking and vote, allowed 90% or higher training accuracy. Test accuracy was high (=>95%) probably due to limited test dataset. Logistic Model Trees had 30-fold higher runtime compared to other classification algorithms including Naive Bayes, AdaBoost, Hoeffding Tree. Grouped error rate in clustering was 0-4%. Runtime-wise, clustering was faster than classification algorithms on MESH-mapped NLP data suggesting clustering methods as adequate towards Medline-related datasets and text-mining big data analytic systems.",,978-1-4799-8792-4,10.1109/ICACCI.2015.7275751,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275751,NeuroNLP;classification;clustering;Neuroinformatics;accuracy,Classification algorithms;Filtering algorithms;Clustering algorithms;Biology;Text categorization;Accuracy;Error analysis,Big Data;bioinformatics;data mining;natural language processing;pattern classification;pattern clustering;text analysis;trees (mathematics);unsupervised learning,reverse-mapped NeuroNLP data;standard ML techniques;neuroinformatics natural language processing NeuroNLP;information categorization;knowledge-related patterns;text mined datasets;machine learning algorithms;automated keyword extraction methods;data selection;text classification;document clustering algorithms;MESH terms;NLP data classification;unsupervised learning;meta-based algorithms;logistic model trees;Medline-related datasets;text-mining big data analytic systems,,5,,28,,28-Sep-15,,,IEEE,IEEE Conferences
Multi-label text categorization based on feature optimization using ant colony optimization and relevance clustering technique,基於特徵優化的蟻群優化和相關性聚類的多標籤文本分類,P. Nema; V. Sharma,"Dept. of Information Technology, SATI Vidisha, Vidisha India; Dept. of Information Technology, SATI Vidisha, Vidisha India","2015 International Conference on Computers, Communications, and Systems (ICCCS)",8-Sep-16,2015,,,1,5,"Feature optimization and feature selection play an important role multi-label text categorization. In multi-label text categorization multiple features share a common class and the process of classification suffered a problem of selection of relevance feature for the classification. In this paper proposed feature optimization based multi-label text categorization. The process of feature optimization is done by ant colony optimization. The ant colony optimization accrued the relevant common feature of document to class. For the process of classification used cluster mapping classification technique. The feature optimization process reduces the loss of data during the transformation of feature mapping during the classification. For the validation of proposed algorithm used some standard dataset such as webpage data, medical search data and RCV1 dataset. Our empirical evaluation shows that proposed algorithm is better than fuzzy relevance technique and other classification technique.",,978-1-4673-9756-8,10.1109/CCOMS.2015.7562842,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562842,Multi-label classification;feature extraction;clustering ACO,Feature extraction;Text categorization;Classification algorithms;Optimization;Clustering algorithms;Ant colony optimization;Semantics,ant colony optimisation;feature selection;pattern classification;pattern clustering;text analysis,multilabel text categorization;feature optimization;ant colony optimization;relevance clustering technique;relevance feature selection;document feature;cluster mapping classification technique;data loss reduction;feature mapping;Web page data;medical search data;RCV1 dataset;empirical evaluation,,1,,19,,8-Sep-16,,,IEEE,IEEE Conferences
Low Cost Correction of OCR Errors Using Learning in a Multi-Engine Environment,在多引擎環境中使用學習對OCR錯誤進行低成本校正,A. Abdulkader; M. R. Casey,"Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,576,580,We propose a low cost method for the correction of the output of OCR engines through the use of human labor. The method employs an error estimator neural network that learns to assess the error probability of every word from ground truth data. The error estimator uses features computed from the outputs of multiple OCR engines. The output probability error estimate is used to decide which words are inspected by humans. The error estimator is trained to optimize the area under the word error ROC leading to an improved efficiency of the human correction process. A significant reduction in cost is achieved by clustering similar words together during the correction process. We also show how active learning techniques are used to further improve the efficiency of the error estimator.,2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.242,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277588,OCR Correction;Multiple Engines;Machine Learning;Clustering;Active Learning,Costs;Optical character recognition software;Error correction;Search engines;Humans;Books;Error analysis;Machine learning;Text analysis;Neural networks,error correction;estimation theory;learning (artificial intelligence);neural nets;optical character recognition;pattern clustering,OCR errors low cost correction;multi-engine environment;human labor;error estimator neural network;word error probability output;ground truth data;multiple OCR engines output;human correction process efficiency;words clustering;active learning technique,,13,,10,,2-Oct-09,,,IEEE,IEEE Conferences
Automatic story segmentation for spoken document retrieval,自動故事分割，用於語音文檔檢索,Pui Yu Hui; Xiaoou Tang; H. M. Meng; Wai Lam; Xinbo Gao,"Dept. of Syst. Eng. & Eng. Manage., Chinese Univ. of Hong Kong, Shatin, China; NA; NA; NA; NA",10th IEEE International Conference on Fuzzy Systems. (Cat. No.01CH37297),7-Aug-02,2001,3,,1319,1322 vol.2,"We have been working on speech retrieval based on Cantonese television news programs. Our video archive contains over 20 hours of news programs provided by a local television station. These programs have been hand-segmented into video clips, where each clip is a self-contained news story. The audio tracks in our archive are indexed by Cantonese speech recognition. This is integrated with a vector-space information retrieval model to achieve speech retrieval. This paper proposes an approach for automatic story segmentation from television news programs, intended to replace hand-segmentation as described above. Automatic story segmentation is critical for rapid expansion of our video archive. Our approach relies on the assumption that nearly all the news stories follow the temporal syntax of (begin-story /spl rarr/ anchor shots /spl rarr/ field shots /spl rarr/ end-story). Therefore, our algorithm aims to detect field-to-anchor shot boundaries that should also coincide with the story boundaries. The proposed approach utilizes the video frame information for story boundary detection, and involves such techniques as fuzzy c-means and graph-theoretical clustering. The approach achieved precision and recall values of over 70%, based on a 20-hour video corpus.",,0-7803-7293-X,10.1109/FUZZ.2001.1008901,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008901,,Information retrieval;Video on demand;TV broadcasting;Speech recognition;Content based retrieval;Digital video broadcasting;Digital audio broadcasting;Laboratories;Systems engineering and theory;Research and development management,television broadcasting;image retrieval;speech recognition;feature extraction;pattern clustering;fuzzy set theory,automatic story segmentation;spoken document retrieval;television news programs;speech retrieval;video clips;Cantonese speech recognition;story boundary detection;fuzzy c-means;graph-theoretical clustering;key frames extraction,,,,8,,7-Aug-02,,,IEEE,IEEE Conferences
A performance evaluation of NSHP-HMM based on conditional ZONE observation probabilities application to offline handwriting word recognition,基於條件區觀察概率的NSHP-HMM性能評估在離線手寫單詞識別中的應用,H. Boukerma; C. Choisy; A. Benouareth; N. Farah,"Ecole Normale Sup矇rieur de l'Enseignement Technologique (ENSET), Skikda, Algeria; ALTRAN, France; LABoratoire de Gestion Electronique du Documents (LABGED), Universit矇 Badji Mokhtar, Annaba, Algeria; LABoratoire de Gestion Electronique du Documents (LABGED), Universit矇 Badji Mokhtar, Annaba, Algeria",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,1091,1095,"The two-dimensional approach based on Non-Symmetric Half-Plane Hidden Markov Model (NSHP-HMM) has been successfully applied to the area of off-line handwriting recognition. A new version of NSHP-HMM model based on conditional ZONE observation probabilities was recently introduced. This new version, called NSHPZ-HMM, provides an optimal solution to combine the effectiveness of 2-D modeling by NSHP-HMM with a zoning-based appropriate pattern representation. The contribution of this paper is the use of NSHPZ-HMM based classifier for the recognition of handwritten words. In the experimental tests, we compare the performance of two feature extraction methods with and without K-means clustering algorithm. Three handwritten databases have been used to evaluate the proposed approach. Preliminary results are promising.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333929,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333929,Non-Symmetric Half-Plan;Hidden Markov Model;zoning;handwritten word recognition,Hidden Markov models;Handwriting recognition;XML;Vocabulary;Image recognition;Text analysis,feature extraction;handwriting recognition;hidden Markov models;image classification;pattern clustering,NSHP-HMM performance evaluation;conditional zone observation probabilities;two-dimensional approach;nonsymmetric half-plane Hidden markov model;offline handwriting recognition;2-D modeling;zoning-based appropriate pattern representation;NSHPZ-HMM based classifier;handwritten word recognition;feature extraction methods;k-means clustering algorithm;handwritten databases,,2,,11,,23-Nov-15,,,IEEE,IEEE Conferences
Entity-Based Language Model Smoothing Approach for Smart Search,智能搜索的基於實體的語言模型平滑方法,F. Zhao; Z. Tian; H. Jin,"Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, HuaZhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, HuaZhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, HuaZhong University of Science and Technology, Wuhan, China",IEEE Access,13-Mar-18,2018,6,,9991,10002,"Smart search plays an important role in all walks of life, for example, according to business needs, accurate search of required knowledge from massive resources is an important way to enhance industrial intelligence. Smoothing of the language model is essential for obtaining high-quality search results because it helps to reduce mismatching and overfitting problems caused by data sparseness. Traditional smoothing methods lexically focus on the global corpus and locally cluster documents information without semantic analysis, which leads to deficiency of the semantic correlations between query statements and documents. In this paper, we propose an entity-based language model smoothing approach for smart search that uses semantic correlation and takes entities as bridges to build the entity semantic language model using a knowledge base. In this approach, entities in the documents are linked to an external knowledge base, such as Wikipedia. Then, the entity semantic language model is generated by using soft-fused and hardfused methods. A two-level merging strategy is also presented to smooth the language model according to whether a given word is semantically relevant to the document or not, which integrates the Dir-smoothing and JM-smoothing methods. Experimental results show that the smoothed language model more closely approximates the word probability distribution under the document semantic theme and more accurately estimates the relevance between query and document.",2169-3536,,10.1109/ACCESS.2017.2788417,National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242357,Language model smoothing;entity;knowledge base;semantic relevance,Smoothing methods;Semantics;Computational modeling;Correlation;Information retrieval;Knowledge based systems;Probability distribution,information retrieval;natural language processing;pattern clustering;query processing;smoothing methods;text analysis,language model smoothing approach;smart search;high-quality search results;semantic correlation;entity semantic language model;external knowledge base;JM-smoothing methods;smoothed language model;cluster document information;Dir-smoothing methods,,,,37,,1-Jan-18,,,IEEE,IEEE Journals
Activity Detection from Email Meta-Data Clustering,通過電子郵件元數據聚類進行活動檢測,M. Patidar; S. Rohatgi; A. Chaudhary; M. P. Singh; P. Agarwal; G. Shroff,"TCS Res., Delhi, India; TCS Res., Delhi, India; TCS Res., Delhi, India; TCS Res., Delhi, India; TCS Res., Delhi, India; TCS Res., Delhi, India",2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),2-Feb-17,2016,,,568,575,"Information workers in a large enterprise often deal with large volumes of e-mail traffic every day. In such a scenario, automatic detection of activities that they are involved in has many potential uses, and even presenting users with a summary of their current set of activities was found to be of value in itself. In this paper, we describe the problem of automatically detecting user activities from e-mails, while using only meta-data of e-mails, i.e., we do not process email contents. We present a novel two stage algorithm for automatic activity detection from users' e-mails: We first represent the e-mail dataset as a rectangular matrix using features such as other e-mails, people involved, and names of the documents attached in the e-mails. We next represent the emails in latent feature space using SVD, followed by further dimensionality reduction using t-Distributed Stochastic Neighbor embedding(t-SNE). We then cluster e-mails using density based clustering algorithm in t-SNE space. In the second stage we merge these clusters based on group properties and a community detection algorithm on the graph of clusters, to yield our set of automatically detected activities. We analyse public e-mail datasets and present benchmarks of our approach on real-life datasets collected from our target users, and also compare our algorithm with alternative approaches as well as those published in recent literature.",2375-9259,978-1-5090-5910-2,10.1109/ICDMW.2016.0087,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836718,Activity Detection;Singular Value Decomposition;t-SNE;Email Clustering;Bayesian Optimization,Electronic mail;Clustering algorithms;Postal services;Feature extraction;Approximation algorithms;Proposals;Data models,electronic mail;group theory;meta data;pattern clustering;singular value decomposition,email meta-data clustering;information workers;e-mail traffic;automatic user activity detection;rectangular matrix;latent feature space;t-distributed stochastic neighbor embedding;density based clustering;t-SNE space;community detection;cluster graph,,,,30,,2-Feb-17,,,IEEE,IEEE Conferences
ClustHOSVD: Item Recommendation by Combining Semantically Enhanced Tag Clustering With Tensor HOSVD,ClustHOSVD：通過語義增強的標籤聚類與Tensor HOSVD結合使用的項目推薦,P. Symeonidis,"Department of Informatics, Aristotle University, Thessaloniki, Greece","IEEE Transactions on Systems, Man, and Cybernetics: Systems",16-Aug-16,2016,46,9,1240,1251,"Social tagging systems (STSs) allow users to annotate information items (songs, pictures, etc.) to provide them item/tag or even user recommendations. STSs consist of three main types of entities: 1) users; 2) items; and 3) tags. These data usually are represented by a three-order tensor, on which Tucker decomposition (TD) models are performed, such as higher order singular value decomposition. However, TD models require cubic computations for the tensor decomposition. Furthermore, TD models suffer from sparsity that incurs in social tagging data. Thus, TD models have limited applicability to large-scale datasets, due to their computational complexity and data sparsity. In this paper, we use two different ways to compute similarity/distance between tags (i.e., the term frequency - inverse document frequency vector space model and the semantic similarity of tags using the ontology of WordNet). Moreover, to reduce the size of the tensor's dimensions and its data sparsity, we use clustering methods (i.e., k-means, spectral clustering, etc.) for discovering tag clusters, which are the intermediaries between a user's profile and items. Thus, instead of inserting the tag dimension in the tensor, we insert the tag cluster dimension, which is smaller and has less noise, resulting to better item recommendation accuracy. We perform experimental comparison of the proposed method against a state-of-the-art item recommendation algorithm with two real datasets (Last.fm and BibSonomy). Our results show significant improvements in terms of effectiveness and efficiency.",2168-2232,,10.1109/TSMC.2015.2482458,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295613,Clustering algorithms;information filtering;recommender systems,Tensile stress;Clustering algorithms;Computational modeling;Kernel;Semantics;Tagging;Taxonomy,computational complexity;matrix decomposition;pattern clustering;recommender systems;social networking (online);tensors,ClustHOSVD;semantically enhanced tag clustering;tensor HOSVD;social tagging systems;STS;three-order tensor;Tucker decomposition models;TD models;tensor decomposition;cubic computations;social tagging data;data sparsity;tensor dimension size reduction;clustering methods;user profile;tag cluster dimension;item recommendation accuracy;item recommendation algorithm;Last.fm;BibSonomy,,23,,33,,9-Oct-15,,,IEEE,IEEE Journals
A Two Stage Recognition Scheme for Handwritten Tamil Characters,手寫泰米爾文字的兩階段識別方案,U. Bhattacharya; S. K. Ghosh; S. Parui,"Indian Statistical Institute, Kolkata, India; Indian Statistical Institute, Kolkata, India; Indian Statistical Institute, Kolkata, India",Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),12-Nov-07,2007,1,,511,515,"India is a multilingual multiscript country with more than 18 languages and 10 different major scripts. Not enough research work towards recognition of handwritten characters of these Indian scripts has been done. Tamil, an official as well as popular script of the southern part of India, Singapore, Malaysia, and Sri Lanka has a large character set which includes many compound characters. Only a few works towards handwriting recognition of this large character set has been reported in the literature. Recently, HP Labs India developed a database of handwritten Tamil characters. In the present paper, we describe an off-line recognition approach based on this database. The proposed method consists of two stages. In the first stage, we apply an unsupervised clustering method to create a smaller number of groups of handwritten Tamil character classes. In the second stage, we consider a supervised classification technique in each of these smaller groups for final recognition. The features considered in the two stages are different. The proposed two-stage recognition scheme provided acceptable classification accuracies on both the training and test sets of the present database.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4378762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378762,,Character recognition;Handwriting recognition;Spatial databases;Pattern recognition;Testing;Speech recognition;Computer vision;Natural languages;Clustering methods;Personal communication networks,handwriting recognition;handwritten character recognition;pattern classification;pattern clustering,handwritten Tamil characters;multilingual multiscript country;handwriting recognition;character set;unsupervised clustering method;supervised classification technique,,14,,17,,12-Nov-07,,,IEEE,IEEE Conferences
Character extraction and recognition in natural scene images,自然場景圖像中的字符提取與識別,Xuewen Wang; Xiaoqing Ding; Changsong Liu,"Image Process. Div., Tsinghua Univ., Beijing, China; NA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,1084,1088,"With the proposal of the concept of a ""smart camera"", character recognition in natural scene images has become an interesting but difficult task nowadays. In this paper, we propose an algorithm for extracting characters from text regions of natural scene images with complex backgrounds. Our method first clusters the color feature vectors of the text regions into a number of color classes by applying a modified coarse-fine fuzzy c-means algorithm. Then, different slices are constructed according to these color classes. Characters are eventually extracted from the images using the information of segmentation and recognition. Some experiments have shown that this method is a promising starting point for such applications.",,0-7695-1263-1,10.1109/ICDAR.2001.953953,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953953,,Character recognition;Layout;Clustering algorithms;Image recognition;Text recognition;Color;Data mining;Image segmentation;Image processing;Roads,character recognition;feature extraction;image recognition;image segmentation;natural scenes;cameras;pattern clustering;image colour analysis;fuzzy set theory;image processing equipment,character extraction;character recognition;smart camera;natural scene images;text regions;complex background;color feature vector clustering;color classes;coarse-fine fuzzy c-means algorithm;slices;image segmentation;image recognition,,5,,8,,7-Aug-02,,,IEEE,IEEE Conferences
Systematic Multi-Path HMM Topology Design for Online Handwriting Recognition of East Asian Characters,用於東亞文字在線手寫識別的系統多路徑HMM拓撲設計,S. Han; M. Chang; Y. Zou; X. Chen; D. Zhang,Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia,Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),5-Nov-07,2007,2,,604,608,"This paper presents a systematic multi-path HMM topology design algorithm to better model online handwriting of East Asian characters. This data-driven algorithm solves three key problems in HMM topology design. First, HMM path number determination is formalized as a clustering problem using subsequence direction histogram vector (SDHV) as feature of both writing order and style. Second, curvature scale space-based (CSS-based) substroke segmentation is used to calculate the optimal state number and initial state parameters. Third, self-rotation restricted corner state and imaginary stroke state are designed to determine state connectivity and Gaussian mixture number in order to achieve better state alignment. Experiments on large character sets demonstrate both a significant relative error reduction rate and high recognition accuracy using the proposed algorithm.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4376986,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376986,,Hidden Markov models;Topology;Handwriting recognition;Writing;Algorithm design and analysis;Clustering algorithms;Histograms;Image segmentation;Ink;Asia,Gaussian processes;handwritten character recognition;hidden Markov models;image segmentation;natural language processing;pattern clustering;text analysis,systematic multipath HMM topology design;online handwriting recognition;East Asian character;data-driven algorithm;HMM path number determination;clustering problem;subsequence direction histogram vector;curvature scale space-based substroke segmentation;self-rotation restricted corner state;imaginary stroke state;Gaussian mixture number,,3,,10,,5-Nov-07,,,IEEE,IEEE Conferences
A Probability Model for Projective Clustering on High Dimensional Data,高維數據投影聚類的概率模型,L. Chen; Q. Jiang; S. Wang,"Dept. of Comput. Sci., Fujian Normal Univ., Fuzhou; Software Sch., Xiamen Univ., Xiamen; Dept. of Comput. Sci., Univ. of Sherbooke, Sherbooke, QC",2008 Eighth IEEE International Conference on Data Mining,10-Feb-09,2008,,,755,760,"Clustering high dimensional data is a big challenge in data mining due to the curse of dimensionality. To solve this problem, projective clustering has been defined as an extension of traditional clustering that seeks to find projected clusters in subsets of dimensions of a data space. In this paper, the problem of modeling projected clusters is first discussed, and an extended Gaussian model is proposed. Second, a general objective criterion used with k-means type projective clustering is presented based on the model. Finally, the expressions to learn model parameters are derived and then used in a new algorithm named FPC to perform fuzzy clustering on high dimensional data. The experimental results on document clustering show the effectiveness of the proposed clustering model.",2374-8486,978-0-7695-3502-9,10.1109/ICDM.2008.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781174,,Clustering algorithms;Flexible printed circuits;Data mining;Computer science;Partitioning algorithms;Los Angeles Council;Clustering methods;Extraterrestrial phenomena;Random variables;Monte Carlo methods,data mining;fuzzy set theory;Gaussian processes;learning (artificial intelligence);pattern clustering;probability,projective clustering;high dimensional data;probability model;data mining;Gaussian model;learning algorithm;fuzzy clustering,,10,,14,,10-Feb-09,,,IEEE,IEEE Conferences
Real-time unsupervised classification of web documents,Web文檔的實時無監督分類,A. Sigogne; M. Constant,"Universit矇 Paris-Est, Laboratoire d'Informatique Gaspard-Monge, 5, bd Descartes, Champs-sur-Marne, 77454 Marne-la-Vall矇e, France; Universit矇 Paris-Est, Laboratoire d'Informatique Gaspard-Monge, 5, bd Descartes, Champs-sur-Marne, 77454 Marne-la-Vall矇e, France",2009 International Multiconference on Computer Science and Information Technology,11-Dec-09,2009,,,281,286,"This paper addresses the problem of clustering dynamic collections of web documents. We show an iterative algorithm based on a fine-grained keyword extraction (simple, compound words and proper nouns). Each new document inserted in the collection is either assigned to an existing class containing documents of the same topic, or assigned to a new class. After each step, when necessary, classes are refined using statistical techniques. The implementation of this algorithm was successfully integrated in an application used for Information Intelligence.",2157-5525,978-1-4244-5314-6,10.1109/IMCSIT.2009.5352714,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352714,,Iterative algorithms;Data mining;Classification algorithms;Computer science;Information technology;Clustering algorithms;Support vector machines;Frequency;Large-scale systems,algorithm theory;document handling;Internet;pattern classification;real-time systems,real time classification;Web documents;dynamic collections web documents;iterative algorithm based;fine grained keyword extraction;class containing documents;statistical techniques;algorithm implementation;information Intelligence,,1,,8,,11-Dec-09,,,IEEE,IEEE Conferences
Improving clustering efficiency by SimHash-based K-Means algorithm for big data analytics,通過基於SimHash的K-Means算法提高大數據分析的集群效率,J. Wang; J. Lin,"Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, Taiwan",2016 IEEE International Conference on Big Data (Big Data),6-Feb-17,2016,,,1881,1888,"K-Means algorithm is one of the most popular methods for flat clustering, but it's time-consuming in similarity calculation for big data, which causes lower performance in practice. Previous studies proposed improvements for finding better initial centroids to facilitate effective assignment of the data points to suitable clusters with reduced time complexity. However, in vector space representation, as the data volume increases, the dimension of vector space becomes higher which takes more time in similarity calculation. In this paper, we propose a SimHash-based K-Means clustering algorithm that used locality-sensitive hashing and dimensionality reduction to improve the efficiency in big data analytics. The experimental results showed that our proposed method greatly reduces the processing time of K-Means clustering without significantly affecting the effectiveness. Further investigation is needed to verify the performance for data in larger scale.",,978-1-4673-9005-7,10.1109/BigData.2016.7840807,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840807,Document Clustering;SimHash;K-Means;Dimension Reduction;Similarity Calculation,Clustering algorithms;Algorithm design and analysis;Hamming distance;Metadata;Big data;Fingerprint recognition;Feature extraction,Big Data;data analysis;data reduction;pattern clustering,clustering efficiency;SimHash-based k-means algorithm;Big Data analytics;locality-sensitive hashing;dimensionality reduction,,1,,11,,6-Feb-17,,,IEEE,IEEE Conferences
Chinese query expansion based on user log clustering,基於用戶日誌聚類的中文查詢擴展,Shufang Jia; Lei Li,"Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, China; Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, China",2009 IEEE International Conference on Network Infrastructure and Digital Content,31-Dec-09,2009,,,446,451,"Most previous query expansion researches are based on pseudo relevant documents. In this study, we present a novel expansion method by clustering the real user log. Because not all of the clicked pages are suitable for query expansion, we de-noised the clicked results by reliability to enhance the performance. After HTML labels removing, the page body contents are clustered and the cluster centers cover various aspects of the original query. The terms used in log queries can provide a better choice of features, from the user's point of view, for summarizing the Web pages that were clicked from these queries. Therefore, the associated queries, reverse queries, Webpage title and keyword phrases are combined with the cluster centers to attain high-quality expansion terms for new queries. We also propose a new terminology extraction method through Baidu Baike. It can identify and extract the terminology phrase based on the manual edited dictionary online.",2374-0272,978-1-4244-4898-2,10.1109/ICNIDC.2009.5360836,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360836,Query expansion;log mining;LSI clustering;Baike terminology extraction;webpage de-noising,Data mining;Search engines;Terminology;Information retrieval;Dictionaries;Computer science;HTML;Web pages;Large scale integration;Noise reduction,data mining;hypermedia markup languages;query processing;Web sites,Chinese query expansion;user log clustering;pseudo relevant documents;HTML labels removal;page body contents;Web page denoising;keyword phrases;Baidu Baike;terminology phrase identification;terminology phrase extraction;manual edited online dictionary,,,,7,,31-Dec-09,,,IEEE,IEEE Conferences
A Document Clustering Approach for Search Engines,搜索引擎的文檔聚類方法,C. Tsai; T. Liang; J. Ho; C. Yang; M. Chiang,"Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, 80424 Taiwan, R.O.C. cwtsai87@gmail.com; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, 80424 Taiwan, R.O.C. devin@mail.wtuc.edu.tw; Department of Electronic Engineering, Cheng Shiu University, Kaohsiung, 83347 Taiwan, R.O.C. jhho.csu@msa.hinet.net; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, 80424 Taiwan, R.O.C. csyang@mail.cse.nsysu.edu.tw; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, 80424 Taiwan, R.O.C. mcchiang@cse.nsysu.edu.tw","2006 IEEE International Conference on Systems, Man and Cybernetics",16-Jul-07,2006,2,,1050,1055,"This paper presents a new internet search engine system called document clustering for search engines (DCSE). This system focuses on overcoming the following challenges faced by search engines: (1) relevance of the search results in response to a user query and (2) information coverage. The DCSE system is based upon a meta-search engine that integrates information retrieval (IR), information extraction (IE), genetic algorithm (GA) and document clustering algorithm into a single system. DCSE utilizes information extraction techniques and vector space model (VSM) calculations to determine the relevance of various data, and then categorizes the data via information retrieval and document clustering algorithm in order to better refine the result. Users will receive information that has been calculated and sorted and web links that are ranked according to their relevance. The end result will reduce the amount of time that users spend filtering out irrelevant data.",1062-922X,1-4244-0099-6,10.1109/ICSMC.2006.384538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4273986,,Search engines;Internet;Information retrieval;Data mining;Clustering algorithms;Computer science;Information filtering;Information filters;Catalogs;IP networks,genetic algorithms;information retrieval;Internet;search engines,Internet search engine system;user query;information coverage;meta-search engine;information retrieval;information extraction;genetic algorithm;document clustering algorithm;information extraction techniques;vector space model calculations,,4,,33,,16-Jul-07,,,IEEE,IEEE Conferences
FLATM: A fuzzy logic approach topic model for medical documents,FLATM：醫療文件的模糊邏輯方法主題模型,A. Karami; A. Gangopadhyay; B. Zhou; H. Kharrazi,"Information Systems Department, University of Maryland Baltimore County, 21250, USA; Information Systems Department, University of Maryland Baltimore County, 21250, USA; Information Systems Department, University of Maryland Baltimore County, 21250, USA; Bloomberg School of Public Health, Johns Hopkins University, Baltimore, Maryland 21205, USA",2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC),1-Oct-15,2015,,,1,6,"One of the challenges for text analysis in medical domains is analyzing large-scale medical documents. As a consequence, finding relevant documents has become more difficult. One of the popular methods to retrieve information based on discovering the themes in the documents is topic modeling. The themes in the documents help to retrieve documents on the same topic with and without a query. In this paper, we present a novel approach to topic modeling using fuzzy clustering. To evaluate our model, we experiment with two text datasets of medical documents. The evaluation metrics carried out through document classification and document modeling show that our model produces better performance than LDA, indicating that fuzzy set theory can improve the performance of topic models in medical domains.",,978-1-4673-7248-0,10.1109/NAFIPS-WConSC.2015.7284190,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284190,,Biomedical imaging;Computational modeling;Fuzzy set theory;Data models;Analytical models;Medical services;Bioinformatics,data mining;fuzzy set theory;information retrieval;medical information systems;pattern classification;pattern clustering;text analysis,FLATM model;fuzzy logic approach topic model;medical documents analysis;text analysis;information retrieval;fuzzy clustering;document classification;document modeling;fuzzy set theory,,13,,41,,1-Oct-15,,,IEEE,IEEE Conferences
Toward Theme Development Analysis with Topic Clustering,通過主題聚類進行主題發展分析,X. Geng; J. Wang,"Sch. of Civil Eng., Qingdao Technol. Univ., Qingdao; Sch. of Comput. Eng., Qingdao Technol. Univ., Qingdao",2008 International Conference on Advanced Computer Theory and Engineering,6-Jan-09,2008,,,628,632,"Topic summarization and analysis is very important to understand an academic document collection and is very paramount for scientific research, which can help researchers find the hot field. Many scholars used the topic model to analyze the theme development, such as LDA. However, these methods need a pre-specified number of latent topics and manual topic labeling, which is usually difficult for people. Aiming to this problem, this paper proposes a method to analyze theme development with topic clustering. Different from the existing works, this paper uses the sliding window to cluster topics extracted in different time incrementally, the topic distance can be measured with KL-divergence. Some experiments on real data sets validate the effectiveness of our proposed method.",2154-7505,978-0-7695-3489-3,10.1109/ICACTE.2008.206,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4737033,,Linear discriminant analysis;Data mining;Polynomials;Information analysis;Civil engineering;Labeling;Time measurement;Frequency;Text mining,data mining;natural sciences computing;pattern clustering;text analysis,theme development analysis;topic clustering;topic summarization;topic analysis;academic document collection;scientific research;sliding window;topic distance;KL-divergence;topic model mining,,2,,16,,6-Jan-09,,,IEEE,IEEE Conferences
A Cluster-based Approach to Filtering Spam under Skewed Class Distributions,傾斜分類分佈下基於集群的垃圾郵件過濾方法,W. Hsiao; T. Chang; G. Hu,National Pingtung Institute of Commerce; National Sun Yat-sen University; National Pingtung Institute of Commerce,2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07),29-Jan-07,2007,,,53,53,"The purpose of this research is to propose an appropriate classification approach to improving the effectiveness of spam filtering on the issue of skewed class distributions. A clustering-based classifier is proposed to first cluster documents into several groups, and then an equal number of keywords are extracted from each group to alleviate the problem caused by skewed class distributions. Experiments are conducted to validate the effectiveness of the proposed classifier. The results show that our proposed classifier can effectively deal with the issue of skewed class distributions in the task of spam filtering",1530-1605,0-7695-2755-8,10.1109/HICSS.2007.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076478,,Unsolicited electronic mail;Support vector machines;Text mining;Information filtering;Information filters;Information management;Matched filters;Frequency;Boosting;Decision trees,data mining;pattern classification;pattern clustering;text analysis;unsolicited e-mail,classification;spam filtering;skewed class distribution;document clustering;keyword extraction;text mining,,4,,12,,29-Jan-07,,,IEEE,IEEE Conferences
What You See is What You Get? Automatic Image Verification for Online News Content,你所看到的就是你得到的？在線新聞內容的自動圖像驗證,S. Elkasrawi; A. Dengel; A. Abdelsamad; S. S. Bukhari,"German Res. Center for Artificial Intell., Kaiserslautern, Germany; German Res. Center for Artificial Intell., Kaiserslautern, Germany; German Univ. in Cairo, Cairo, Egypt; German Res. Center for Artificial Intell., Kaiserslautern, Germany",2016 12th IAPR Workshop on Document Analysis Systems (DAS),13-Jun-16,2016,,,114,119,"Consuming news over online media has witnessed rapid growth in recent years, especially with the increasing popularity of social media. However, the ease and speed with which users can access and share information online facilitated the dissemination of false or unverified information. One way of assessing the credibility of online news stories is by examining the attached images. These images could be fake, manipulated or not belonging to the context of the accompanying news story. Previous attempts to news verification provided the user with a set of related images for manual inspection. In this work, we present a semi-automatic approach to assist news-consumers in instantaneously assessing the credibility of information in hypertext news articles by means of meta-data and feature analysis of images in the articles. In the first phase, we use a hybrid approach including image and text clustering techniques for checking the authenticity of an image. In the second phase, we use a hierarchical feature analysis technique for checking the alteration in an image, where different sets of features, such as edges and SURF, are used. In contrast to recently reported manual news verification, our presented work shows a quantitative measurement on a custom dataset. Results revealed an accuracy of 72.7% for checking the authenticity of attached images with a dataset of 55 articles. Finding alterations in images resulted in an accuracy of 88% for a dataset of 50 images.",,978-1-5090-1792-8,10.1109/DAS.2016.75,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490103,Media Verification;News;Journalism;Online Document Analysis,Image edge detection;Media;Google;Feature extraction;Medical services;Image color analysis;Manuals,document image processing;feature extraction;hypermedia;information dissemination;meta data;pattern clustering;social networking (online);text analysis,automatic image verification;online news content;online social media;information dissemination;hypertext news article;metadata;feature analysis;image clustering;text clustering,,6,,15,,13-Jun-16,,,IEEE,IEEE Conferences
Design of a Metacrawler for web document retrieval,用於Web文檔檢索的Metacrawler的設計,K. R. R. Babu; A. P. Arya,"Department of Information Technology, Government Engineering College, Idukki, India; Department of Computer Science and Engineering, PSG College of Technology, Coimbatore, India",2012 12th International Conference on Intelligent Systems Design and Applications (ISDA),24-Jan-13,2012,,,478,484,"Web Crawlers `browse' the World Wide Web (WWW) on behalf of search engine, to collect web pages from numerous collections of billions of documents. Metacrawler is similar to that of a meta search engine that combines the top web search results from popular search engines. World Wide Web is growing rapidly. This possesses great challenges to general purpose crawlers. This paper introduces an architectural framework of a Metacrawler. This crawler enables the user to retrieve information that is relevant to the topic from more than one traditional web search engines. The crawler works in such a way that it fetches only the pages that are relevant to the topic. The PageRank algorithm is often used in ranking web pages. But, the ranking causes the problem of topic-drift. So, modified PageRank algorithm is used to rank the retrieved web pages in such a way that it reduces this problem. The clustering method is used to combine the search results so that the user can easily select web pages from the clustered results based upon the requirement. Experimental results show the effectiveness of the Metacrawler.",2164-7151,978-1-4673-5119-5,10.1109/ISDA.2012.6416585,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416585,Search Engine;Web Crawler;Metacrawler;Ranking Algorithms;Clustering,Crawlers;Search engines;Web pages;Algorithm design and analysis;Clustering algorithms;Engines;Metasearch,information retrieval;Internet;pattern clustering;search engines;Web sites,metacrawler design;Web document retrieval;World Wide Web;Web crawlers;metasearch engines;general purpose crawlers;architectural framework;information retrieval;Web search engines;topic-drift problem;modified PageRank algorithm;Web page retrieval;clustering method,,2,,18,,24-Jan-13,,,IEEE,IEEE Conferences
Ant clustering based text detection in natural scene images,自然場景圖像中基於螞蟻聚類的文本檢測,P. Tomer; A. Goyal,"Information Technology, Samrat Ashok Technological Institute, Vidisha, India; Information Technology, Samrat Ashok Technological Institute, Vidisha, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)",30-Jan-14,2013,,,1,7,"Text pictures in natural scene are necessary clues for several image-based applications like scene understanding, content-based image retrieval, helpful navigation, and automatic geocoding. However, locating text from a fancy atmosphere with multiple colors may be a difficult task. Although there are various techniques implemented for the detection of the text in natural scenes. The work implemented in paper(Text string detection from natural scenes by structural based partition and grouping) is based on Gradient based Partition and Color based Partition but the techniques implemented here provides less efficiency and the chances of error rate is more hence have less accuracy. But the technique implemented in this paper for the detection of texts in natural scenes is based on Ant based clustering. By using Ant clustering the performance of the technique for the detection of text has been increased.",,978-1-4799-3926-8,10.1109/ICCCNT.2013.6726747,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726747,Automatic geocoding;Connected Component;Gradient based partition;ant clustering,Image color analysis;Clustering algorithms;Partitioning algorithms;Optical character recognition software;Image edge detection;Sorting;Character recognition,ant colony optimisation;document image processing;error statistics;image colour analysis;natural scenes;object detection;pattern clustering;text analysis,ant clustering based text detection;natural scene images;text pictures;image-based applications;scene understanding;content-based image retrieval;navigation;automatic geocoding;text location;text string detection;structural based partition;grouping;gradient based partition;color based partition;error rate,,1,,27,,30-Jan-14,,,IEEE,IEEE Conferences
Task-based clustering on search queries,基於搜索查詢的基於任務的集群,A. S. Akg羹n; Y. Yaslan,"Bilgisayar M羹hendisli?i B繹l羹m羹, 襤stanbul Teknik ?niversitesi, 34469 Maslak, 襤stanbul, T羹rkiye; Bilgisayar M羹hendisli?i B繹l羹m羹, 襤stanbul Teknik ?niversitesi, 34469 Maslak, 襤stanbul, T羹rkiye",2018 26th Signal Processing and Communications Applications Conference (SIU),9-Jul-18,2018,,,1,4,"Task extraction on query logs is one of the important and interesting topics used on search engines and many search-based applications. Similar queries entered by users can be aggregated according to their various features to make meaningful tasks. Task extraction is important for providing suggestions in the direction of the user's intention, in the search text completion, in returning the correct results for the domain being searched. Session information, clicked document contents and query entities are used for feature extraction in existing approaches. In recent studies, task extraction is made by the Wikipedia category hierarchies that is used at clustering level. In this study, entity categories are used for feature extraction instead of clustering phase. Therefore, it is aimed to measure sentimental similarity between search queries on categories precisely. In addition, the queries are clustered by central based and density based clustering methods with the different combination of feature sets. The evaluation of the methods are obtained by considering the similarities within cluster members and between clusters' centers. As a result of this work, entity and category vectors generated by word2vec method are treated as query feature and task based clustering performance is increased.",,978-1-5386-1501-0,10.1109/SIU.2018.8404510,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404510,task-based clustering;search query analysis;entity and category extraction,Internet;Feature extraction;Task analysis;Encyclopedias;Electronic publishing;Uniform resource locators,data mining;feature extraction;pattern clustering;query processing;text analysis;Web sites,search queries;task extraction;query logs;search engines;search-based applications;search text completion;query entities;feature extraction;Wikipedia category hierarchies;clustering level;central based density;clustering methods;feature sets;cluster members;category vectors;query feature;clustering performance;task-based clustering,,,,,,9-Jul-18,,,IEEE,IEEE Conferences
Identifying Document Topics Using the Wikipedia Category Network,使用Wikipedia類別網絡識別文檔主題,P. Schonhofen,"Hungarian Academy of Sciences, Hungary",2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06),15-Jan-07,2006,,,456,462,"In the size and coverage of Wikipedia, a freely available online encyclopedia has reached the point where it can be utilized similar to an ontology or taxonomy to identify the topics discussed in a document. In this paper we show that even a simple algorithm that exploits only the titles and categories of Wikipedia articles can characterize documents by Wikipedia categories surprisingly well. We test the reliability of our method by predicting categories of Wikipedia articles themselves based on their bodies, and by performing classification and clustering on 20 newsgroups and RCV1, representing documents by their Wikipedia categories instead of their texts",,0-7695-2747-7,10.1109/WI.2006.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4061411,,Wikipedia;Ontologies;Taxonomy;Information retrieval;Content based retrieval;Computer networks;Automation;Encyclopedias;Clustering algorithms;Testing,document handling;encyclopaedias;ontologies (artificial intelligence);pattern classification;pattern clustering;Web sites,document topics;Wikipedia category network;online encyclopedia;ontology;newsgroups,,43,,22,,15-Jan-07,,,IEEE,IEEE Conferences
A dynamic window based method for binarization of document images,一種基於動態窗口的文檔圖像二值化方法,K. Vatwani; A. Dwivedi,"BITS PILANI, Pilani (India); IIT Rajasthan, Jodhpur (India)",2013 International Conference on Advanced Electronic Systems (ICAES),11-Nov-13,2013,,,142,146,Thresholding is an important step in the identification of text in document images. A no. of global and local thresholding techniques have been proposed for the binarization of document images. In this paper we present a a new dynamic window based thresholding approach where the thresholding is performed locally over a dynamic window by means of a hard clustering based approach. A performance comparison of the proposed method is also carried out with some of the widely used thresholding techniques.,,978-1-4799-1441-8,10.1109/ICAES.2013.6659379,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6659379,Thresholding;GrayLevel Co-occurrence matrix;Dissimalarity based classification;global and local thresholding,,document image processing;image segmentation;text analysis,dynamic window based method;document image binarization;text identification;thresholding techniques;thresholding approach,,,,14,,11-Nov-13,,,IEEE,IEEE Conferences
Clustering Workflow Requirements Using Compression Dissimilarity Measure,使用壓縮差異度量對工作流需求進行聚類,L. Wei; J. Handley; N. Martin; T. Sun; E. Keogh,"University of California, Riverside, CA; Xerox Corporation; Xerox Corporation; Xerox Corporation; University of California, Riverside",Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06),15-Jan-07,2006,,,50,54,"Xerox offers a bewildering array of printers and software configurations to satisfy the need of production print shops. A configuration tool in the hands of sales analysts elicits requirements from customers and recommends a list of product configurations. This tool generates special question and answer case logs that provide useful historical data. Given the unusual semi-structured question and answer format, this data is not amenable to any standard document clustering method. The authors discovered that a hierarchical agglomerative approach using a compression-based dissimilarity measure (CDM) provided readily interpretable clusters. The authors compared this method empirically to two reasonable alternatives, latent semantic analysis and probabilistic latent semantic analysis, and conclude that CDM offers an accurate and easily implemented approach to validate and augment our configuration tool",2375-9259,0-7695-2792-2,10.1109/ICDMW.2006.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4063597,,Printing;Production;Books;Marketing and sales;Presses;Finishing;Computer science;Printers;Clustering methods;Publishing,configuration management;digital printing;printers;workflow management software,clustering workflow requirements;bewildering array;printers configurations;software configurations;answer case logs;unusual semistructured question;document clustering;hierarchical agglomerative;compression dissimilarity measure;probabilistic latent semantic analysis,,5,,8,,15-Jan-07,,,IEEE,IEEE Conferences
Topic Calculation and Clustering: An Application to Wikipedia,主題計算和聚類：對維基百科的應用,S. E. Garza; R. F. Brena; E. Ram穩rez,"Centro de Sist. Inteligentes, Tecnol. de Monterrey, Monterrey; Centro de Sist. Inteligentes, Tecnol. de Monterrey, Monterrey; Centro de Sist. Inteligentes, Tecnol. de Monterrey, Monterrey",2008 Seventh Mexican International Conference on Artificial Intelligence,18-Nov-08,2008,,,88,93,"Wikipedia is nowadays one of the most valuable information resources; nevertheless, its current structure, which has no formal organization, does not allow to always have a useful browsing among topics. Moreover, even though most Wikipedia pages include a ""See Also"" section for navigating through those articles' related Wikipedia pages, the only references included here are those which authors are aware of, leading to incompleteness and other irregularities. In this work, a method for finding related Wikipedia articles is proposed; this method relies on a framework that clusters documents into semantically-calculated topics and selects the closest documents which could enrich the ""See Also"" section.",,978-0-7695-3441-1,10.1109/MICAI.2008.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682448,Wikipedia;Semantic Web;document clustering;semantic browsing,Wikipedia;Navigation;Web pages;Object oriented programming;Equations;Frequency;Artificial intelligence;Information resources;Information retrieval;World Wide Web,Internet;pattern clustering;Web sites,Wikipedia pages;information resources;semantically-calculated topics;documents clustering,,,,30,,18-Nov-08,,,IEEE,IEEE Conferences
A parallel approach to context-based term weighting,基於上下文的術語加權的並行方法,S. Arora; S. Chakravarty,"Dept. of Information Technology, Netaji Subhas Institute of Technology, New Delhi, India; Dept. of Computer Engineering, Netaji Subhas Institute of Technology, New Delhi, India",2011 World Congress on Information and Communication Technologies,30-Jan-12,2011,,,951,956,"Information retrieval and extraction essentially rely on estimating the relevance of words present in a large corpus of documents or text. One of the approaches to measuring relevance is analyzing the importance of words based on their statistical distribution within a document. Quite another approach ensues from their linguistic relevance within a logically perceived context. Literature presents a body of work done employing both statistical as well as contextual approaches. The challenge currently is on enhancing the performance of document analysis and clustering systems. Ever since we witnessed a massive explosion of information and raw data available on the web, their analysis demands more rigorous computations and processing. Given the widely distributed environment as a backbone platform for these systems to operate, there is an urgent need to develop techniques to scale up their performance on multiple processors. We propose a parallelized strategy to estimate the statistical as well as contextual relevance of words, employing master-slave configuration on a cluster of processors. Our parallel algorithm has been successfully tested on a self-made Beowulf cluster comprising ten nodes, showing significant performance improvement over single processor.",,978-1-4673-0126-8,10.1109/WICT.2011.6141376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141376,Retrieval;TF-IDF;Context based Text Classification;Amdahl's Law;Cluster computing,Context;Program processors;Clustering algorithms;Measurement;Algorithm design and analysis;Switches;Pragmatics,information retrieval;parallel algorithms;pattern clustering;statistical distributions;text analysis,context-based term weighting;information retrieval;information extraction;document corpus;text corpus;statistical distribution;linguistic relevance;parallel approach;document analysis;clustering system;parallel algorithm;Beowulf cluster,,,,15,,30-Jan-12,,,IEEE,IEEE Conferences
Novel hybrid feature selection models for unsupervised document categorization,用於無監督文檔分類的新型混合特徵選擇模型,A. P. Bhopale; S. S. Kamath,"Department of Information Technology, National Institute of Technology Karnataka, Surathkal, India; Department of Information Technology, National Institute of Technology Karnataka, Surathkal, India","2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",4-Dec-17,2017,,,1471,1477,"Dealing with high dimensional data is a challenging and computationally complex task in the data pre-processing phase of text clustering. Conventionally, union and intersection approaches have been used to combine results of different feature selection methods to optimize relevant feature space for document collection. Union method selects all features from considered sub-models, whereas, intersection method selects only common features identified by sub-models. However, in reality, any type of feature selection can cause a loss of some potentially important features. In this paper, a hybrid feature selection model called Modified Hybrid Union (MHU) is proposed, which selects features by considering the individual strengths and weaknesses of each constituent component of the model. A comparative evaluation of its performance for K-means clustering and Bio-inspired Flock-based clustering is also presented on standard data sets such as OWL-S TC and Reuters-21578.",,978-1-5090-6367-3,10.1109/ICACCI.2017.8126048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8126048,Text categorization;Feature selection;Dimensionality reduction;Unsupervised learning,Feature extraction;Biological system modeling;Clustering algorithms;Mathematical model;Noise measurement;Genetic algorithms,feature selection;pattern clustering;text analysis;unsupervised learning,hybrid feature selection model;standard data sets;unsupervised document categorization;high dimensional data;text clustering;document collection;intersection method;data preprocessing phase;modified hybrid union,,1,,23,,4-Dec-17,,,IEEE,IEEE Conferences
A new approach for segmentation of image and text in natural and commercial color documents,在自然和商業彩色文檔中分割圖像和文本的新方法,M. K. Kundu; S. Dhar; M. Banerjee,"Machine Intelligence Unit, Indian Statistical Institute, Kolkata, India; RCC Institute of Information Technology, Kolkata, India; RCC Institute of Information Technology, Kolkata, India","2012 International Conference on Communications, Devices and Intelligent Systems (CODIS)",31-Jan-13,2012,,,85,88,"This paper presents an efficient method for segmenting text and non text parts of natural real life images and colored document images using M-band wavelet packet frames. Various combinations of band pass channels of M-band wavelet packet frames represent the image at different scale and orientations in the frequency planes of YCbCr components of color images. The scale space feature vector comprises of the local energy around each pixel at different scales and segmentation is achieved using fuzzy C-means clustering. No information regarding font size, scaling representation, type of layout etc. of the images are considered in our algorithm.",,978-1-4673-4700-6,10.1109/CODIS.2012.6422142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6422142,color document segmentation;M-band packet wavelet;adaptive basis selection;texture segmentation,Decision support systems;Intelligent systems;Manganese,image colour analysis;image segmentation;text analysis;wavelet transforms,image segmentation;text segmentation;natural color documents;commercial color documents;natural real life images;M-band wavelet packet frames;band pass channels;frequency planes;color images;scale space feature vector;YNbCr,,2,,9,,31-Jan-13,,,IEEE,IEEE Conferences
Extractive Document Summarization Based on Dynamic Feature Space Mapping,基於動態特徵空間映射的抽取性文檔摘要,S. Ghodratnama; A. Beheshti; M. Zakershahrak; F. Sobhanmanesh,"Department of Computing, Macquarie University, Sydney, NSW, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia; ASU School of Computing, Informatics, and Decision Systems Engineering (CIDSE), Arizona State University, Tempe, AZ, USA; Department of Computing, Macquarie University, Sydney, NSW, Australia",IEEE Access,7-Aug-20,2020,8,,139084,139095,"The exponential growth of the Web documents has constituted the need for automatic document summarization. In this context, extractive document summarization, i.e., that task of extracting the most relevant information, removing redundancy and presenting the remained data in a coherent and cohesive structure, is a challenging task. In this article, we propose a novel intelligent approach, namely ExDoS, that harvests benefits of both supervised and unsupervised algorithms simultaneously. To the best of our knowledge, ExDoS is the first approach to combine both supervised and unsupervised algorithms in a single framework and an interpretable manner for document summarization purpose. ExDoS iteratively minimizes the error rate of the classifier in each cluster with the help of dynamic local feature weighting. Moreover, this approach specifies the contribution of features to discriminate each class, which is a challenging issue in the summarization task. Therefore, in addition to summarizing text, ExDoS is also able to measure the importance of each feature in the summarization process. We evaluate our model both automatically (in terms of ROUGE factor) and empirically (human analysis) on the benchmark datasets: the DUC2002 and CNN/DailyMail. Results show that our model obtains higher ROUGE scores comparing to most state-of-the-art models. The human evaluation also demonstrates that our model is capable of generating informative and readable summaries.",2169-3536,,10.1109/ACCESS.2020.3012539,AI-Enabled Processes (AIP) Research Centre; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151114,Automatic text summarization;extractive summarization;feature weighting;multi-document summarization,Feature extraction;Data mining;Task analysis;Clustering algorithms;Heuristic algorithms;Semantics;Social network services,information retrieval;Internet;pattern classification;supervised learning;text analysis;unsupervised learning,extractive document summarization;Web documents;automatic document summarization;ExDoS;supervised algorithms;unsupervised algorithms;dynamic local feature weighting;dynamic feature space mapping;classifier;ROUGE factor;text summarization,,,,48,CCBY,28-Jul-20,,,IEEE,IEEE Journals
Expert system for retrieval of documents using evolutionary approaches incorporating clustering,使用包含聚類的進化方法檢索文檔的專家系統,S. Deshpande; M. Doke; A. Deshpande; A. N. Chaudhari,"Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India","2017 International conference of Electronics, Communication and Aerospace Technology (ICECA)",18-Dec-17,2017,2,,414,418,"Classification is a central problem in the fields of data mining and machine learning. Using a training set of labeled instances, the task is to build a model (classifier) that can be used to predict the class of new unlabelled instances. Data preparation is crucial to the data mining process, and its focus is to improve the fitness of the training data for the learning algorithms to produce more effective classifiers. Searching for the frequent pattern within a specific sequence has become a much needed task in various sectors. Feature selection is selecting a subset of optimal features. Feature selection is being used in high dimensional data reduction and it is being used in several applications like medical, image processing, text mining, etc. In the existing work, unsupervised feature selection methods using Artificial Bee Colony Optimization Algorithm, Bat Algorithm and Ant Colony Optimization have been introduced. We have compared these three algorithms and concluded that Bat Algorithm proves to be better in performance than the rest. The proposed system will use a novel method to select subset of features from unlabelled data using Bat algorithm with one of the clustering algorithm and develop an expert information retrieval system.",,978-1-5090-5686-6,10.1109/ICECA.2017.8212847,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8212847,unsupervised feature selection;Binary Bat algorithm;K-means;Ant Colony Optimization (ACO);Artificial Bee Colony (ABC);Data Mining;Classification;Data Reduction;Instance Selection,Clustering algorithms;Feature extraction;Information retrieval;Evolutionary computation;Classification algorithms;Databases;Optimization,data mining;information retrieval;learning (artificial intelligence);optimisation,expert system;machine learning;data mining process;learning algorithms;high dimensional data reduction;image processing;text mining;unsupervised feature selection methods;Artificial Bee Colony Optimization Algorithm;Bat Algorithm;Ant Colony Optimization;Bat algorithm;expert information retrieval system,,,,12,,18-Dec-17,,,IEEE,IEEE Conferences
A Step toward Speeding Up Cross-Cut Shredded Document Reconstruction,加快跨切碎文檔重建的步驟,A. S. Atallah; E. Emary; M. S. El-Mahallawy,"Technol. & Maritime Transp., Cairo, Egypt; Dept. of Inf. Technol., Cairo Univ., Cairo, Egypt; Technol. & Maritime Transp., Cairo, Egypt",2015 Fifth International Conference on Communication Systems and Network Technologies,1-Oct-15,2015,,,345,349,"Recovery of shredded documents helps in security informatics, forensic and investigation science. Shredded document reconstruction requires much time and human effort. Hence, there is a great need to enhance its performance due to the high growth of critical cases requiring fast shredded document reconstruction. In this paper, we focus particularly on the most influential sub-problem which is enhancing and speeding up the matching process in addition to reducing the search space. Furthermore, fully automated pre-processing, feature extraction and matching are applied in order to minimize the user interaction and reduce the time needed for reconstructing enormous number of shredded documents.",,978-1-4799-1797-6,10.1109/CSNT.2015.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279937,Image Processing;Computational Geometry;Shredded;Document Reconstruction;Cross-Cut;De-shred,Image reconstruction;Feature extraction;Ink;Accuracy;Merging;Clustering algorithms;Classification algorithms,document handling;security of data,step toward speeding up cross cut shredded document reconstruction;security informatics;forensic;investigation science;matching process;search space;feature extraction;feature matching;user interaction,,3,,21,,1-Oct-15,,,IEEE,IEEE Conferences
Primary Content Block Detection from Web Page Clusters through Entropy and Semantic Distance,通過熵和語義距離從網頁群集中檢測主要內容塊,J. Meng; Q. Liu; K. Li,"Dept. of Comput. Sci. & Eng., Dalian Univ. of Technol., Dalian; Dept. of Comput. Sci. & Eng., Dalian Univ. of Technol., Dalian; Dept. of Comput. Sci. & Eng., Dalian Univ. of Technol., Dalian",2008 3rd International Conference on Innovative Computing Information and Control,22-Aug-08,2008,,,5,5,"A new method named ENP-DOM tree is proposed in this paper, which extends the document object module tree by adding two properties, i.e., entropy and relativity, to some nodes. Semantic distance is used to extract the primary content accurately from the same source based on three facts: noise blocks always have high entropy property within a given Web site; primary content blocks are often made up of few link words and many text words; useful links are contained in a useful content blocks and have a close semantic distance with page titles. The proposed method can identify the primary content blocks with higher precision and recall rate and reduce the storage requirement for search engines; thus, result in smaller indexes, faster search time, and better user satisfaction. Extensive experiments are also conducted to evaluate the proposed method by comparison with existing methods. The experimental results show that the method outperforms existing methods with better satisfying recall rate and higher precision.",,978-0-7695-3161-8,10.1109/ICICIC.2008.430,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4603194,,Web pages;Distance measurement;Data mining;Entropy;HTML;Information filters;Information filtering,document handling;entropy;search engines;trees (mathematics);Web sites,primary content block detection;Web page clusters;entropy;semantic distance;ENP-DOM tree;document object module tree;noise blocks;Web site;primary content blocks;storage requirement;search engines;user satisfaction,,,,4,,22-Aug-08,,,IEEE,IEEE Conferences
Visual clustering in web search: An effective approach,網絡搜索中的視覺集群：一種有效的方法,H. Badesh; J. Blustein,"Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science and School of Information Management, Dalhousie University, Halifax, Canada",International Conference on Information Society (i-Society 2011),8-Aug-11,2011,,,34,38,Presenting search results as a list of hits can be ineffective in assisting users to find relevant documents and discover varied topics among search results. Visualization can assist users to find relevant documents and make the search interface more effective. This paper presents a Data Mountain Search Results Presentation Interface (DMSRPI). The interface is intended to improve the effectiveness in how users search the Web and find relevant information. A user study is yet to be conducted for evaluating the DMSRPI.,,978-0-9564263-8-3,10.1109/i-Society18435.2011.5978504,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978504,Web information retrieval;visualization;techniques;visual clustering;query reformulation,Data visualization;World Wide Web;Navigation;Educational institutions;HTML,data visualisation;document handling;information retrieval;Internet;pattern clustering,visual clustering;Web search;relevant documents;data visualization;search interface;data mountain search results presentation interface;DMSRPI;World Wide Web;relevant information,,,,27,,8-Aug-11,,,IEEE,IEEE Conferences
RepoZip: A technique for lossless compression of document collections,RepoZip：一種無損壓縮文檔集合的技術,D. N. Sumanaweera; F. F. Doole; D. P. Pathiraja; G. G. K. Deshapriya; G. Dias,"Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka",2015 Moratuwa Engineering Research Conference (MERCon),1-Jun-15,2015,,,330,335,"Many computer systems; especially in corporations, contain large amount of documents such as letters, reports and presentations. Many such documents are present in several versions. Such data needs to be synchronized with branch offices and mobile devices, often over slow and expensive connections. However, as many documents are stored in an already compressed format, it is difficult to compress them further by exploiting the hidden redundancies. We present a novel approach named RepoZip which improves the compression of an existing compression algorithm over a document collection, by exploiting the inter-document meta-data and content-level redundancies. It concentrates on compressing OOXML documents that have been constructed through the archival of a hierarchy of meta-data files and PDF documents which include deflated content streams. Therefore, the RepoZip approach achieves larger compression gains over OOXML document collections or PDF document collections by exploiting usually undetected meta-data level similarities.",,978-1-4799-1740-2,10.1109/MERCon.2015.7112368,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112368,Iossless compression;meta-data similarity;OOXML;PDF;clusters;generalized suffix tree,Decision support systems,data compression;document handling;meta data;mobile computing,lossless compression technique;RepoZip;computer systems;branch offices;mobile devices;compressed format;compression algorithm;interdocument metadata;content level redundancies;compressing OOXML documents;PDF document collections;undetected metadata,,,,10,,1-Jun-15,,,IEEE,IEEE Conferences
Jeffreys Centroids: A Closed-Form Expression for Positive Histograms and a Guaranteed Tight Approximation for Frequency Histograms,Jeffreys重心：正直方圖的閉式表達式和頻率直方圖的保證緊逼近,F. Nielsen,"Sony Computer Science Laboratories, Inc., Shinagawa-ku, Tokyo, Japan",IEEE Signal Processing Letters,17-May-13,2013,20,7,657,660,"Due to the success of the bag-of-word modeling paradigm, clustering histograms has become an important ingredient of modern information processing. Clustering histograms can be performed using the celebrated k-means centroid-based algorithm. From the viewpoint of applications, it is usually required to deal with symmetric distances. In this letter, we consider the Jeffreys divergence that symmetrizes the Kullback-Leibler divergence, and investigate the computation of Jeffreys centroids. We first prove that the Jeffreys centroid can be expressed analytically using the Lambert W function for positive histograms. We then show how to obtain a fast guaranteed approximation when dealing with frequency histograms. Finally, we conclude with some remarks on the k-means histogram clustering.",1558-2361,,10.1109/LSP.2013.2260538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509930,Centroid;clustering;histogram;Jeffreys divergence;Kullback?Leibler divergence;Lambert $W$  function,Histograms;Approximation methods;Approximation algorithms;Clustering algorithms;Signal processing algorithms;Visualization;Databases,approximation theory;document handling;pattern classification;pattern clustering,Jeffreys centroids;closed-form expression;positive histograms;guaranteed tight approximation;frequency histograms;bag-of-word modeling paradigm;histograms clustering;k-means centroid-based algorithm;Kullback-Leibler divergence;Lambert W function;k-means histogram clustering;document classification,,8,,17,,29-Apr-13,,,IEEE,IEEE Journals
Modeling Document Summarization as Multi-objective Optimization,將文檔摘要建模為多目標優化,L. Huang; Y. He; F. Wei; W. Li,"Dept. of Comput. Sci. & Technol., Wuhan Univ., Wuhan, China; Dept. of Comput. Sci. & Technol., Wuhan Univ., Wuhan, China; Dept. of Comput. Sci. & Technol., Wuhan Univ., Wuhan, China; Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China",2010 Third International Symposium on Intelligent Information Technology and Security Informatics,22-Apr-10,2010,,,382,386,"In this paper, we consider document summarization as a multi-objective optimization problem involving four objective functions, namely information coverage, significance, redundancy and text coherence. These functions measure the possible summaries based on the identified core terms and main topics (i.e. a cluster of semantically or statistically related core terms). We choose the DUC 2005 and 2006 query-oriented summarization tasks to exam the proposed model. The encouraging results indicate that the multi-objective optimization based framework for document summarization is truly a promising research direction.",,978-1-4244-6743-3,10.1109/IITSI.2010.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453598,document summarization;multi-objective optimization;query-sensitive term weighting and clustering,Inference algorithms;Data mining;Information technology;Computer security;Information security;Informatics;Computer science;Optimization methods;Greedy algorithms;Dynamic programming,document handling;optimisation,document summarization modeling;multiobjective optimization;information coverage;redundancy;text coherence;query-oriented summarization tasks,,19,,12,,22-Apr-10,,,IEEE,IEEE Conferences
Automatic Content Analysis of Legislative Documents by Text Mining Techniques,利用文本挖掘技術自動分析立法文件的內容,F. Lin; S. Chou; D. Liao; D. Hao,"Inst. of Service Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Inst. of Service Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Inst. of Service Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Inst. of Service Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan",2015 48th Hawaii International Conference on System Sciences,30-Mar-15,2015,,,2199,2208,"The Parliamentary Library of Taiwan's Legislative Yuan website provides a fair and objective channel for the public to track daily activities of the Legislative Yuan and legislators' inquiries. However the quantity of generated documents is so large that the general public may not be able to keep track of the legislative performance of each legislator from these contents. To mitigate the gap of legislative document generation and the sense making by the general public, this study proposed a text mining mechanism to automatically classify legislative documents referring to each legislator, and then represent the proportion of their legislative performance on certain categories. This study first initiated a basic legislative categorical structure by domain experts. Then a two-stage clustering was applied to perform feature selection for legislative documents. The SVM method was applied to build a model to classify the new document to the appropriate category. In order to maintain the classification categories up to date, in this study, we also evaluate the difference between labeling contents by domain experts and the general public. Experimental results show the effectiveness of the proposed test mining mechanism, which automatically classifies legislative documents to reveal legislators' performance accordingly. With this result, people can monitor legislators and track their legislative activities using the information from the Parliamentary Library of Legislative Yuan to update their perception on legislative performance in various categories.",1530-1605,978-1-4799-7367-5,10.1109/HICSS.2015.263,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070077,Text mining;SVM;legislative performance;classification;two-stage clustering,Support vector machines;Labeling;Proposals;Libraries;Speech;Feature extraction;Clustering methods,data mining;legislation;text analysis;Web sites,automatic content analysis;legislative documents;text mining techniques;Parliamentary Library of Taiwan;Legislative Yuan Web site;legislative document generation;legislative document classification;SVM method,,2,,14,,30-Mar-15,,,IEEE,IEEE Conferences
Unsupervised feature selection technique based on harmony search algorithm for improving the text clustering,基於和聲搜索算法的無監督特徵選擇技術在文本聚類中的應用,L. M. Abualigah; A. T. Khader; M. A. Al-Betar,"School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; School of Computer Sciences, Universiti Sains Malaysia (USM), Pulau Pinang, Malaysia 11800; Department of information technology, Al-Huson University College, Irbid-Jordan",2016 7th International Conference on Computer Science and Information Technology (CSIT),25-Aug-16,2016,,,1,6,"The increasing amount of text information on the Internet web pages affects the clustering analysis. The text clustering is a favorable analysis technique used for partitioning a massive amount of information into clusters. Hence, the major problem that affects the text clustering technique is the presence uninformative and sparse features in text documents. The feature selection (FS) is an important unsupervised technique used to eliminate uninformative features to encourage the text clustering technique. Recently, the meta-heuristic algorithms are successfully applied to solve several optimization problems. In this paper, we proposed the harmony search (HS) algorithm to solve the feature selection problem (FSHSTC). The proposed method is used to enhance the text clustering (TC) technique by obtaining a new subset of informative or useful features. Experiments were applied using four benchmark text datasets. The results show that the proposed FSHSTC is improved the performance of the k-mean clustering algorithm measured by F-measure and Accuracy.",,978-1-4673-8914-3,10.1109/CSIT.2016.7549456,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549456,Unsupervised Feature Selection;Harmony Search Algorithm;K-mean Text Clustering;Informative features;Sparse features,Clustering algorithms;Optimization;Information technology;Text mining;Mathematical model;Computer science;Web pages,Internet;optimisation;pattern clustering;search problems;statistical analysis;text analysis;unsupervised learning,unsupervised feature selection;harmony search algorithm;text clustering;Internet Web page;metaheuristic algorithm;optimization problem;k-mean clustering algorithm;F-measure,,9,,19,,25-Aug-16,,,IEEE,IEEE Conferences
A new method for fuzzy information retrieval based on fuzzy hierarchical clustering and fuzzy inference techniques,基於模糊層次聚類和模糊推理技術的模糊信息檢索新方法,Yih-Jen Horng; Shyi-Ming Chen; Yu-Chuan Chang; Chia-Hoang Lee,"Dept. of Comput. & Inf. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; NA; NA; NA",IEEE Transactions on Fuzzy Systems,4-Apr-05,2005,13,2,216,228,"In this paper, we extend the work of Kraft et al. to present a new method for fuzzy information retrieval based on fuzzy hierarchical clustering and fuzzy inference techniques. First, we present a fuzzy agglomerative hierarchical clustering algorithm for clustering documents and to get the document cluster centers of document clusters. Then, we present a method to construct fuzzy logic rules based on the document clusters and their document cluster centers. Finally, we apply the constructed fuzzy logic rules to modify the user's query for query expansion and to guide the information retrieval system to retrieve documents relevant to the user's request. The fuzzy logic rules can represent three kinds of fuzzy relationships (i.e., fuzzy positive association relationship, fuzzy specialization relationship and fuzzy generalization relationship) between index terms. The proposed fuzzy information retrieval method is more flexible and more intelligent than the existing methods due to the fact that it can expand users' queries for fuzzy information retrieval in a more effective manner.",1941-0034,,10.1109/TFUZZ.2004.840134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1411824,Fuzzy agglomerative hierarchical clustering;fuzzy information retrieval systems;fuzzy logic rules;fuzzy relationships;query expansion,Information retrieval;Fuzzy logic;Fuzzy systems;Fuzzy set theory;Clustering algorithms;Inference algorithms;Boolean functions;Councils;Information science,fuzzy systems;query processing;query formulation;pattern clustering;fuzzy reasoning;fuzzy logic;vocabulary,fuzzy information retrieval;fuzzy hierarchical clustering;fuzzy inference technique;fuzzy agglomerative;fuzzy logic rules;query expansion;index term,,57,,30,,4-Apr-05,,,IEEE,IEEE Journals
A document classification approach by GA feature extraction based corner classification neural network,基於遺傳特徵提取的角點分類神經網絡的文檔分類方法,Weifeng Zhang; Baowen Xu; Zifeng Cui,"Dept. of Comput. Sci. & Eng., Nanjing Univ. of Posts & Telecommun., China; NA; NA",2005 International Conference on Cyberworlds (CW'05),6-Feb-06,2005,,,6 pp.,504,"The CC4 neural network is a new type of corner classification training algorithm for three-layered feed forward neural networks. CC4 is now successfully used in meta search engine Anvish. When the documents are almost of the same size, CC4 neural network is an effective document classification algorithm. However, there is great difference in document sizes in general, and CC4 use the whole dictionary as the space of vector which leads to a lot of documents represented by sparse vectors. This paper brings forward feature extraction based neural network GA-CC4. The method of GA feature extraction extracts the feature items really representing the documents in the document set, which are constructed as the set of feature items. Based on the set of feature items and combining the document frequency, the document can be represented. By this method, the dimensions representing the documents can be reduced, which can solve the precise problem caused by the different document sizes, and it can also map the scalar features to the Boolean input of the neural network by binary coding, by which the quality of input data of neural network is improved",,0-7695-2378-1,10.1109/CW.2005.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587586,,Feature extraction;Neural networks;Clustering algorithms;Machine learning algorithms;Artificial neural networks;Feedforward neural networks;Computer science;Classification algorithms;Metasearch;Search engines,classification;document handling;feature extraction;feedforward neural nets;genetic algorithms,document classification;genetic algorithm;corner classification neural network;corner classification training algorithm;feedforward neural network;meta search engine Anvish;sparse vector;feature extraction based neural network;document frequency;scalar feature;Boolean input;binary coding,,1,,22,,6-Feb-06,,,IEEE,IEEE Conferences
Trend Analysis for Large Document Streams,大型文檔流的趨勢分析,C. Zhang; S. Zhu; Y. Gong,"University of Rochester, USA; NEC Laboratories America, Inc., USA; NEC Laboratories America, Inc., USA",2006 5th International Conference on Machine Learning and Applications (ICMLA'06),26-Dec-06,2006,,,285,295,"More and more powerful computer technology inspires people to investigate information hidden under huge amounts of documents. In this report, we are especially interested in documents with relative time order, which we also call document streams. Examples include TV news, forums, emails of company projects, call center telephone logs, etc. To get an insight into these document streams, first we need to detect the events among the document streams. We use a time-sensitive Dirichlet process mixture model to find the events in the document streams. A time sensitive Dirichlet process mixture model is a generative model, which allows a potentially infinite number of mixture components and uses a Dirichlet compound multinomial model to model the distribution of words in documents. In this report, we consider three different time sensitive Dirichlet process mixture models: an exponential decay kernel model, a polynomial decay function kernel Dirichlet process model and a sliding window kernel model. Experiments on the TDT2 dataset have shown that the time sensitive models perform 18-20% better in terms of accuracy than the Dirichlet process mixture model. The sliding windows kernel and the polynomial kernel are more promising in detecting events. We use ThemeRiver to provide a visualization of the events along the time axis. With the help of ThemeRiver, people can easily get an overall picture of how different events evolve. Besides ThemeRiver, we investigate using top words as a high-level summarization of each event. Experiment results on TDT2 dataset suggests that the sliding window kernel is a better choice both in terms of capturing the trend of the events and expressibility",,0-7695-2735-3,10.1109/ICMLA.2006.51,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041505,,Text analysis;Kernel;Sampling methods;Polynomials;TV;Telephony;Event detection;Clustering methods;Clustering algorithms;Information retrieval,data encapsulation;document handling,document stream;information hidden;time-sensitive Dirichlet process mixture model;Dirichlet compound multinomial model;polynomial decay function;sliding window kernel model;ThemeRiver visualization,,2,,20,,26-Dec-06,,,IEEE,IEEE Conferences
Categorizing Overlapping Regions in Clustering Analysis Using Three-Way Decisions,使用三向決策的聚類分析中的重疊區域分類,H. Yu; P. Jiao; G. Wang; Y. Yaoy,"Chongqing Key Lab. of Comput. Intell., Chongqing Univ. of Posts & Telecommun., Chongqing, China; Chongqing Key Lab. of Comput. Intell., Chongqing Univ. of Posts & Telecommun., Chongqing, China; Chongqing Key Lab. of Comput. Intell., Chongqing Univ. of Posts & Telecommun., Chongqing, China; Dept. of Comput. Sci., Univ. of Regina, Regina, SK, Canada",2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),20-Oct-14,2014,2,,350,357,"Clustering is a common technique for data analysis, has been widely used in many practical area. In many real applications such as social network analysis, wireless sensor networks, document clustering, and so on, there exist overlaps between different clusters due to various reasons. In this paper, we propose to use the three-way decisions approach to address categorizing overlapping regions. In contrast to existing soft clustering methods that just point out the objects whether in overlapping regions, the three-way decisions method provides a greater refinement of the categorization to system operators for further analysis, which is believed to show clearly the objects have different impacts to construct clusters. Besides, we provide a new relation-graph based clustering algorithm to obtain different overlapping region types. The results of comparison experiments are better and more reasonable to overlapping clustering.",,978-1-4799-4143-8,10.1109/WI-IAT.2014.118,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927645,,Clustering algorithms;Bones;Communities;Fans;Corporate acquisitions;Clustering methods;Upper bound,data analysis;decision trees;graph theory;pattern clustering,overlapping region categorization;clustering analysis;three-way decisions;data analysis;relation-graph based clustering algorithm,,5,,27,,20-Oct-14,,,IEEE,IEEE Conferences
Visualizing the Performance of Computational Linguistics Algorithms,可視化計算語言學算法的性能,S. G. Eick; J. Mauger; A. Ratner,"SSS Research, Inc. eick@sss-research.com; SAIC Advanced Systems & Concepts, maugerj@saic.com; National Security Agency, asratne@nsa.gov",2006 IEEE Symposium On Visual Analytics Science And Technology,26-Dec-06,2006,,,151,157,"We have built a visualization system and analysis portal for evaluating the performance of computational linguistics algorithms. Our system focuses on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high dimensional reference concept vectors. The visualization and algorithm analysis techniques include confusion matrices, ROC curves, document visualizations showing word importance, and interactive reports. One of the unique aspects of our system is that the visualizations are thin-client Web-based components built using SVG visualization components",,1-4244-0591-2,10.1109/VAST.2006.261417,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4035760,AJAX;thin-client;SVG;ROC curves;confusion matrices;document categorization,Visualization;Computational linguistics;Portals;Algorithm design and analysis;Clustering algorithms;Information retrieval;Content based retrieval;Text analysis;Performance analysis;Storage automation,computational linguistics;document handling;portals;program visualisation;software performance evaluation,computational linguistics algorithm performance visualization;analysis portal;document classification;document clustering;confusion matrices;ROC curves;document visualizations;interactive reports;thin-client Web-based components;SVG visualization components;scalable vector graphics;AJAX,,2,,6,,26-Dec-06,,,IEEE,IEEE Conferences
Weakly supervised relevance feedback based on an improved language model,基於改進的語言模型的弱監督關聯反饋,Xin-Sheng Li; Si Li; Wei-Ran Xu; Guang Chen; Jun Guo,"School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China",Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010),30-Sep-10,2010,,,1,5,"Relevance feedback, which traditionally uses the terms in the relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. This approach has another problem is that Relevance feedback assumes that most frequent terms in the feedback documents are useful for the retrieval. In fact, the reports of some experiments show that it does not hold in reality many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. In this paper, we propose to select better and more relevant documents with a clustering algorithm. And then we present an improved Language Model to help us identify the good terms from those relevant documents. Ours experiments on the 2008 TREC collection show that retrieval effectiveness can be much improved when the improved Language Model is used.",,978-1-4244-6899-7,10.1109/NLPKE.2010.5587859,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587859,Information retrieval (IR);relevance feedback;cluster;relevant documents;query expansion,HTML,natural language processing;pattern clustering;relevance feedback,relevance feedback;language model;clustering algorithm,,,,6,,30-Sep-10,,,IEEE,IEEE Conferences
The Role of Clustering in Search Computing,集群在搜索計算中的作用,A. Campi; S. Ronchi,"Dipt. di Elettron. e Inf., Politec. di Milano, Milan, Italy; Dipt. di Elettron. e Inf., Politec. di Milano, Milan, Italy",2009 20th International Workshop on Database and Expert Systems Application,17-Nov-09,2009,,,432,436,"This paper proposes a novel language in order to explore the results retrieved by several internet search services and search engines that cluster retrieved documents. The goal of this work, rooted in the new context of Search Computing, is to offer users a tool to discover relevant hidden relationships between clustered documents. When the same query is submitted to distinct search services, they may produce partially overlapped clustered results, where clusters identified by distinct labels collect some common documents. Moreover, clusters with similar labels, but containing distinct documents, may be produced as well. In such a situation, it may be useful to compare, combine, and rank the cluster contents, to filter out relevant documents. The proposed language is composed of several operators that can be used in order to combine groups of clusters.",2378-3915,978-0-7695-3763-4,10.1109/DEXA.2009.89,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337277,,Search engines;Information retrieval;Web and internet services;Visualization;Databases;Expert systems;Filters;Proposals;Web pages;Hospitals,document handling;information retrieval;Internet;search engines,document clustering;search computing;Internet search service;search engines;document retrieval,,2,,14,,17-Nov-09,,,IEEE,IEEE Conferences
Tovel: Distributed Graph Clustering for Word Sense Disambiguation,Tovel：用於詞義消歧的分佈式圖聚類,A. Guerrieri; F. Rahimian; S. Girdzijauskas; A. Montresor,"DISI, Univ. of Trento, Trento, Italy; LCN, R. Inst. of Technol., Stockholm, Sweden; LCN, R. Inst. of Technol., Stockholm, Sweden; DISI, Univ. of Trento, Trento, Italy",2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),2-Feb-17,2016,,,623,630,"Word sense disambiguation is a fundamental problem in natural language processing (NLP). In this problem, a large corpus of documents contains mentions to well-known (non-ambiguous) words, together with mentions to ambiguous ones. The goal is to compute a clustering of the corpus, such that documents that refer to the same meaning appear in the same cluster, subsequentially, each cluster is assigned to a different semantic meaning. In this paper, we propose a mechanism for word sense disambiguation based on distributed graph clustering that is incremental in nature and can scale to big data. A novel, heuristic vertex-centric algorithm based on the metaphor of the water cycle is used to cluster the graph. Our approach is evaluated on real datasets in both centralized and decentralized environments.",2375-9259,978-1-5090-5910-2,10.1109/ICDMW.2016.0094,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836725,,Color;Image color analysis;Clouds;Clustering algorithms;Context;Companies;Rain,document handling;graph theory;natural language processing;pattern clustering,Tovel;distributed graph clustering;word sense disambiguation;natural language processing;NLP;Big Data;heuristic vertex-centric algorithm;water cycle metaphor,,1,,21,,2-Feb-17,,,IEEE,IEEE Conferences
Unsupervised text extraction from G-maps,從G地圖中無監督地提取文本,C. Adak,"Department of Computer Science and Engineering University of Kalyani West Bengal-741235, India",2013 International Conference on Human Computer Interactions (ICHCI),1-Sep-14,2013,,,1,4,"This paper represents an text extraction method from Google maps, GIS maps/images. Due to an unsupervised approach there is no requirement of any prior knowledge or training set about the textual and non-textual parts. Fuzzy C-Means clustering technique is used for image segmentation and Prewitt method is used to detect the edges. Connected component analysis and gridding technique enhance the correctness of the results. The proposed method reaches 98.5% accuracy level on the basis of experimental data sets.",,978-1-4673-5703-6,10.1109/ICHCI-IEEE.2013.6887782,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6887782,clustering;connected component;Fuzzy C-Means clustering;gridding;text extraction,Image segmentation;Accuracy;Image edge detection;Data mining;Google;Graphics,document image processing;edge detection;fuzzy set theory;image segmentation;pattern clustering;text detection;unsupervised learning,unsupervised text extraction;G-maps;Google maps;GIS maps;fuzzy C-means clustering;image segmentation;Prewitt method;edge detection;connected component analysis;gridding technique,,1,,33,,1-Sep-14,,,IEEE,IEEE Conferences
Clustering and identifying temporal trends in document databases,聚類和識別文檔數據庫中的時間趨勢,A. Popescul; G. W. Flake; S. Lawrence; L. H. Ungar; C. L. Giles,"Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA; NA; NA",Proceedings IEEE Advances in Digital Libraries 2000,6-Aug-02,2000,,,173,182,"We introduce a simple and efficient method for clustering and identifying temporal trends in hyper-linked document databases. Our method can scale to large datasets because it exploits the underlying regularity often found in hyper-linked document databases. Because of this scalability, we can use our method to study the temporal trends of individual clusters in a statistically meaningful manner. As an example of our approach, we give a summary of the temporal trends found in a scientific literature database with thousands of documents.",,0-7695-0659-3,10.1109/ADL.2000.848380,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=848380,,Databases;Clustering algorithms;National electric code;Scalability;Web sites;Publishing;Delay;Merging;Citation analysis,citation analysis;scientific information systems;information resources,temporal trend clustering;temporal trend identification;hyper-linked document databases;large datasets;scientific literature database,,36,,14,,6-Aug-02,,,IEEE,IEEE Conferences
Improving search engine query expansion using clustering and indexing based approach,使用基於聚類和索引的方法改善搜索引擎的查詢擴展,S. Deep; V. Chawra,"Central College of Engineering and Management Dept. of Computer Science and Engineering Raipur, Chhattisgarh, India; Central College of Engineering and Management Dept. of Computer Science and Engineering Raipur, Chhattisgarh, India","2017 IEEE International Conference on Power, Control, Signals and Instrumentation Engineering (ICPCSI)",21-Jun-18,2017,,,1836,1839,"In old days, people have become conscious about the consequences of archiving and finding information. With the arrival of computers, it became possible to store huge amount of information; and finding the useful information from images collections which is became a necessity. Out of this necessity in the 1950s, the field of Information Retrieval (IR) was born. The field of information retrieval has matured considerably over the last forty years. Query expansion main objective is to extend the user query with related terms to improvise the search results. It is done to improve the relevance of the documents which are retrieved by the IR systems. In this paper, we propose a Cluster and Indexing based approach for query expansion. We also presents that our proposed mechanism produces better recall and precision.",,978-1-5386-0814-2,10.1109/ICPCSI.2017.8392032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392032,Information retrieval;relevance feedback;query expansion,Conferences;Instruments;Erbium,image retrieval;indexing;pattern clustering;search engines,clustering;indexing based approach;images collections;information retrieval;user query;IR systems;information archiving;search engine query expansion;document retrieval,,1,,9,,21-Jun-18,,,IEEE,IEEE Conferences
A Parametric Architecture for Tags Clustering in Folksonomic Search Engines,民俗搜索引擎中標籤聚類的參數化體系結構,N. R. Di Matteo; S. Peroni; F. Tamburini; F. Vitali,"Dept. of Comput. Sci., Univ. of Bologna, Bologna, Italy; Dept. of Comput. Sci., Univ. of Bologna, Bologna, Italy; Dept. of Linguistics & Oriental Studies, Univ. of Bologna, Bologna, Italy; Dept. of Comput. Sci., Univ. of Bologna, Bologna, Italy",2009 Ninth International Conference on Intelligent Systems Design and Applications,28-Dec-09,2009,,,279,282,"Semantic search engines rely on the existence of a rich set of semantic connections between the concepts associated to documents and those used for the queries. With folksonomies, this is not always guaranteed. Creating clusters of folksonomic tags around terms of controlled ontological vocabularies is a potentially sophisticated approach, but algorithms abound for this clustering and no clear cut winner exists. In this paper we introduce FolksEngine, a parametric search engine for folksonomies allowing to specify any clustering algorithm as a three step process: the user's query is expanded according to semantic rules associated to the terms of the query, the new query is then executed on the plain folksonomy search engine, and the results are ranked according to semantic rules associated to the folksonomic tags actually used for the documents.",2164-7151,978-1-4244-4735-0,10.1109/ISDA.2009.125,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364819,FolksEngine;folksonomies;query expansion;semantic search engine,Search engines;Ontologies;Clustering algorithms;Testing;Computer science;Vocabulary;Performance analysis;Algorithm design and analysis;Thesauri;Intelligent systems,information retrieval;ontologies (artificial intelligence);search engines;vocabulary,parametric architecture;tags clustering;folksonomic search engines;semantic search engines;document queries;controlled ontological vocabularies;FolksEngine;semantic rules,,5,,8,,28-Dec-09,,,IEEE,IEEE Conferences
Scalable clustering using multiple GPUs,使用多個GPU的可擴展集群,M. K. Wasif; P. J. Narayanan,"Center for Visual Information Technology, International Institute of Information and Technology, Hyderabad, India; Center for Visual Information Technology, International Institute of Information and Technology, Hyderabad, India",2011 18th International Conference on High Performance Computing,16-Feb-12,2011,,,1,10,"K-Means is a popular clustering algorithm with wide applications in Computer Vision, Data mining, Data Visualization, etc. Clustering is an important step for indexing and searching of documents, images, video, etc. Clustering large numbers of high-dimensional vectors is very computation intensive. In this paper, we present the design and implementation of the K-Means clustering algorithm on the modern GPU. All steps are performed entirely on the GPU efficiently in our approach. We also present a load balanced multi-node, multi-GPU implementation which can handle up to 6 million, 128-dimensional vectors. We use efficient memory layout for all steps to get high performance. The GPU accelerators are now present on high-end workstations and low-end laptops. Scalability in the number and dimensionality of the vectors, the number of clusters, as well as in the number of cores available for processing are important for usability to different users. Our implementation scales linearly or near-linearly with different problem parameters. We achieve up to 2 times increase in speed compared to the best GPU implementation for K-Means on a single GPU. We obtain a speed up of over 170 on a single Nvidia Fermi GPU compared to a standard sequential implementation. We are able to execute one iteration of K-Means in 136 seconds on off-the-shelf GPUs to cluster 6 million vectors of 128 dimensions into 4K clusters and in 2.5 seconds to cluster 125K vectors of 128 dimensions into 2K clusters.",1094-7256,978-1-4577-1950-9,10.1109/HiPC.2011.6152713,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152713,,Vectors;Graphics processing unit;Clustering algorithms;Kernel;Computer architecture;Message systems;Instruction sets,graphics processing units;pattern clustering;resource allocation,scalable clustering;k-means clustering;computer vision;data mining;data visualization;indexing;document searching;image searching;video searching;high dimensional vectors;load balanced multinode multiGPU implementation;GPU accelerators;high end workstations;low end laptops;Nvidia Fermi GPU,,5,,22,,16-Feb-12,,,IEEE,IEEE Conferences
Low-complexity fuzzy relational clustering algorithms for Web mining,Web挖掘的低複雜度模糊關係聚類算法,R. Krishnapuram; A. Joshi; O. Nasraoui; L. Yi,"IBM India Res. Lab., Indian Inst. of Technol., New Delhi, India; NA; NA; NA",IEEE Transactions on Fuzzy Systems,7-Aug-02,2001,9,4,595,607,"This paper presents new algorithms-fuzzy c-medoids (FCMdd) and robust fuzzy c-medoids (RFCMdd)-for fuzzy clustering of relational data. The objective functions are based on selecting c representative objects (medoids) from the data set in such a way that the total fuzzy dissimilarity within each cluster is minimized. A comparison of FCMdd with the well-known relational fuzzy c-means algorithm (RFCM) shows that FCMdd is more efficient. We present several applications of these algorithms to Web mining, including Web document clustering, snippet clustering, and Web access log analysis.",1941-0034,,10.1109/91.940971,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=940971,,Clustering algorithms;Web mining;Robustness;Evolution (biology);Internet;Explosions;Telecommunication traffic;Matched filters;Fuzzy sets;Algorithm design and analysis,information resources;Internet;data mining;pattern clustering;fuzzy set theory;relational databases;minimisation,low-complexity fuzzy relational clustering algorithms;World Wide Web mining;FCMdd;robust fuzzy c-medoids;RFCMdd;relational data;total fuzzy dissimilarity minimization;Web document clustering;snippet clustering;Web access log analysis;data mining,,230,,54,,7-Aug-02,,,IEEE,IEEE Journals
A co-evolutionary framework for clustering in information retrieval systems,信息檢索系統中的集群共進化框架,A. Aizawa,"Nat. Inst. of Informatics, Tokyo, Japan",Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),7-Aug-02,2002,2,,1787,1792 vol.2,"This paper presents a co-evolutionary framework for clustering in text-based information retrieval systems. The prominent feature of the proposed method is that documents and terms are clustered simultaneously into overlapping multiple clusters. The mathematical formulation and implementation of the clustering method are briefly introduced, together with some experimental results.",,0-7803-7282-4,10.1109/CEC.2002.1004513,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004513,,Information retrieval;Sparse matrices;Clustering methods;Evolutionary computation;Symmetric matrices;Informatics;Navigation;Explosives;History;Frequency,information retrieval systems;pattern clustering;evolutionary computation;information retrieval,co-evolutionary framework;text-based information retrieval systems;document clustering;term clustering;overlapping multiple clusters;mathematical formulation,,3,,16,,7-Aug-02,,,IEEE,IEEE Conferences
Parallel Metagenomic Sequence Clustering Via Sketching and Maximal Quasi-clique Enumeration on Map-Reduce Clouds,通過Map-Reduce雲上的草圖繪製和最大擬氣候枚舉進行並行元基因組序列聚類,X. Yang; J. Zola; S. Aluru,"Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA, USA",2011 IEEE International Parallel & Distributed Processing Symposium,8-Sep-11,2011,,,1223,1233,"Taxonomic clustering of species is an important and frequently arising problem in metagenomics. High-throughput next generation sequencing is facilitating the creation of large metagenomic samples, while at the same time making the clustering problem harder due to the short sequence length supported and unknown species sampled. In this paper, we present a parallel algorithm for hierarchical taxonomic clustering of large metagenomic samples with support for overlapping clusters. We adapt the sketching techniques originally developed for web document clustering to deduce significant similarities between pairs of sequences without resorting to expensive all vs. all alignments. We formulate the metagenomics classification problem as that of maximal quasi-clique enumeration in the resulting similarity graph, at multiple levels of the hierarchy as prescribed by different similarity thresholds. We cast execution of the underlying algorithmic steps as applications of the map-reduce framework to achieve a cloud based implementation. Apart from solving an important problem in metagenomics, this work demonstrates the applicability of map-reduce framework in relatively complicated algorithmic settings.",1530-2075,978-1-61284-372-8,10.1109/IPDPS.2011.116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012859,,Clustering algorithms;Organisms;Silicon;DNA;Strontium;Couplings,biology computing;cloud computing;genomics;graph theory;parallel algorithms;pattern clustering,parallel metagenomic sequence clustering;maximal quasi-clique enumeration;map-reduce clouds;parallel algorithm;hierarchical taxonomic clustering;sketching technique;web document clustering;metagenomics classification problem;similarity graph;similarity threshold;map-reduce framework,,14,,48,,8-Sep-11,,,IEEE,IEEE Conferences
iWIN: A Summarizer System Based on a Semantic Analysis of Web Documents,iWIN：基於Web文檔語義分析的摘要係統,A. dAcierno; V. Moscato; F. Persia; A. Picariello; A. Penta,"ISA, Avellino, Italy; DIS, Univ. of Naples, Naples, Italy; DIS, Univ. of Naples, Naples, Italy; DIS, Univ. of Naples, Naples, Italy; ECS, Univ. of Southampton, Southampton, UK",2012 IEEE Sixth International Conference on Semantic Computing,25-Oct-12,2012,,,162,169,"Seeking bits of useful information from a large amount of data on the Web still remains a difficult and time consuming task for a wide range of people such as students, reporters, and many other types of professionals. This problem requires to investigate new ways to handle and process information, that has to be delivered in a rather small space, retrieved in a short time, and represented as accurately as possible. This is surely one of the most important reasons for searching suitable and efficient summarization techniques capable of ""distilling"" the most important information from a variety of logically related sources, as the one returned from classic search engines, in order to produce a short, concise and grammatically meaningful version of information spread out in pages and pages of texts. In this paper we present a summarizer system, named iWIN (information on the Web In a Nutshell), that is able to perform an automatic summarization of multiple documents through: a semantic analysis of the text, a ranking method used to evaluate the relevance of the information for the specific user, a clustering method based on the document representation in terms of set of triplets (subject, verb, object) and a sentences' selection/ordering process to make the final summary as much readable as possible. Some preliminary results about system performances obtained using the ROUGE evaluation software are presented and discussed.",,978-1-4673-4433-3,10.1109/ICSC.2012.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337100,Summarization;NLP;Semantic Analysis,Semantics;Resource description framework;Computer crashes;Clustering algorithms;Text analysis;Terrorism,Internet;pattern clustering;relational databases;relevance feedback;search engines;text analysis,iWIN;summarizer system;semantic analysis;Web documents;information handling;information processing;information distilling;classic search engines;text semantic analysis;information on the Web In a Nutshell;ranking method;information relevance;clustering method;document representation;subject;verb;object;sentence selection process;sentence ordering process;ROUGE evaluation software,,13,,16,,25-Oct-12,,,IEEE,IEEE Conferences
Name Disambiguation Using Semantic Association Clustering,使用語義關聯聚類消除名稱歧義,H. Jin; L. Huang; P. Yuan,"Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China",2009 IEEE International Conference on e-Business Engineering,1-Dec-09,2009,,,42,48,"Due to homonyms, abbreviations, etc., name ambiguity is widely available in Web and e-document. For example, when integrating heterogeneous literature databases, because there are different name specifications, different authors may be thought of as the same author, and vice versa. Therefore, name ambiguity makes data robust even dirty and lowers the precision of information retrieval. In this paper, we present an approach, named as semantic association based name disambiguation method (SAND), to solve person name ambiguity. The basic idea of SAND is to explore the semantic association of name entities and cluster name entities according to their associations. Finally, the name entities in the same group are considered as the same entities. We test SAND using data from CitesSeer, DBLP and Libra. The test results show that SAND is an effective approach to solve the problem of name ambiguity.",,978-0-7695-3842-6,10.1109/ICEBE.2009.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342132,semantic association;name disambiguation;clusting,Databases;Testing;Grid computing;Computer science;Robustness;Information retrieval;Couplings;Clustering algorithms;Frequency;Bibliographies,document handling;information retrieval;pattern clustering;Web sites,SAND;semantic association-based name disambiguation method;semantic association clustering;e-document;heterogeneous literature database;name specification;information retrieval;DBLP;Libra;CitesSeer;Website,,7,,19,,1-Dec-09,,,IEEE,IEEE Conferences
Clustering to determine predictive model for news reports analysis and econometric modeling,聚類以確定新聞報導分析和計量經濟模型的預測模型,S. K. Mukherjee; S. Bandyopadhyay,"Computer Science & Engineering Formerly at Narula Institute of Technology Calcutta, India; Computer Science & Engineering Jadavpur University Calcutta, India",2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS),3-Sep-15,2015,,,302,309,A tree model is constructed for the econometric problem domain and for topic modeling of news reports using a clustering approach. Here segments are represented as discretized intervals defined on econometric variables for speeding up the construction of regression tree. This discretization is achieved from variances defined on variables with predictability for that generated for calculating category utility values defined on correlated variables where the discretization method proposed has the aim to satisfy a constraint of minimum entropy distribution of values of the predictor variable among the categories. An algorithm is proposed for tree merging which is used for incrementally incorporating information for new time intervals with the existing model to generate updated tree model for maintaining logical consistency. The tree merging algorithm has been shown to be suitable for applying to news report documents or econometric information. This is accomplished with a proposed Pruning procedure for maintaining logical consistency in the merged tree which is applied together with existing approaches for limiting pruning and access costs for reducing misclassification error.,,978-1-4799-8349-0,10.1109/ReTIS.2015.7232895,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232895,discretization;tree merging;regression tree;news report;econometric model,Regression tree analysis;Econometrics;Merging;Predictive models;Clustering algorithms;Probabilistic logic;Advertising,document handling;econometrics;entropy;information resources;pattern clustering;trees (mathematics),clustering;news report analysis;econometric modeling;tree model;regression tree;discretization method;minimum entropy distribution;tree merging algorithm;news report documents,,1,,61,,3-Sep-15,,,IEEE,IEEE Conferences
Cross-Modal Subspace Learning via Pairwise Constraints,通過成對約束進行跨模態子空間學習,R. He; M. Zhang; L. Wang; Y. Ji; Q. Yin,"National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Control Science and Engineering, Shandong University, Jinan, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Image Processing,13-Oct-15,2015,24,12,5543,5556,"In multimedia applications, the text and image components in a web document form a pairwise constraint that potentially indicates the same semantic concept. This paper studies cross-modal learning via the pairwise constraint and aims to find the common structure hidden in different modalities. We first propose a compound regularization framework to address the pairwise constraint, which can be used as a general platform for developing cross-modal algorithms. For unsupervised learning, we propose a multi-modal subspace clustering method to learn a common structure for different modalities. For supervised learning, to reduce the semantic gap and the outliers in pairwise constraints, we propose a cross-modal matching method based on compound ??1 regularization. Extensive experiments demonstrate the benefits of joint text and image modeling with semantically induced pairwise constraints, and they show that the proposed cross-modal methods can further reduce the semantic gap between different modalities and improve the clustering/matching accuracy.",1941-0042,,10.1109/TIP.2015.2466106,National Natural Science Foundation of China; National Basic Research Program of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182358,multi modal;pairwise constraint;subspace clustering;Multi modal;pairwise constraint;subspace clustering,Semantics;Multimedia communication;Clustering algorithms;Clustering methods;Supervised learning;Dictionaries,image matching;Internet;multimedia computing;pattern clustering;unsupervised learning,cross-modal subspace learning;multimedia applications;text components;image components;Web document;compound regularization framework;cross-modal algorithms;unsupervised learning;multimodal subspace clustering method;supervised learning;cross-modal matching method;compound ??1 regularization;image modeling;text modeling;semantically induced pairwise constraints,,28,,70,,7-Aug-15,,,IEEE,IEEE Journals
Text mining in 'Request for Comments Document Series',“請求註釋文檔系列”中的文本挖掘,S. Gurusamy; D. Manjula; T. V. Geetha,"Sch. of Comput. Sci. & Eng., Anna Univ., India; Sch. of Comput. Sci. & Eng., Anna Univ., India; Sch. of Comput. Sci. & Eng., Anna Univ., India","Language Engineering Conference, 2002. Proceedings",28-Feb-03,2002,,,147,155,"This paper discusses the knowledge discovery in text (KDT) system for the 'Request for Comments (RFC) Document Series'. The paper proposes a versatile system architecture for text mining in RFC that maintains structured and unstructured data components of the document. The documents are represented by keywords and knowledge discovery is performed by analysing the co-occurrence frequencies of the various keywords representing the document. The clustering of documents is done by extracted knowledge, which can reduce the search space. The relevant documents retrieved during the search process for a query are ranked based on relevance of the topic in it. This paper describes RFC Viewer, our tool for viewing the RFC document in rich text format rather than text format, which also provides knowledge extracted from the RFC document and supports various KDD operations on the document.",,0-7695-1885-0,10.1109/LEC.2002.1182302,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1182302,,Text mining;Data mining;Frequency;Labeling;Databases;Knowledge engineering;Computer science;Maintenance engineering;Computer architecture;Performance analysis,data mining;text analysis,knowledge discovery in text system;Request for Comments Document Series;text mining;unstructured data components;structured data components;system architecture;keyword co-occurrence frequencies;document clustering;search space;query;topic relevance;RFC Viewer;rich text format,,,,10,,28-Feb-03,,,IEEE,IEEE Conferences
Annotating handwritten characters with minimal human involvement in a semi-supervised learning strategy,在半監督學習策略中用最少的人力來註釋手寫字符,J. Richarz; S. Vajda; G. A. Fink,"Fac. of Comput. Sci. XII, Tech. Univ. Dortmund, Dortmund, Germany; Fac. of Comput. Sci. XII, Tech. Univ. Dortmund, Dortmund, Germany; Fac. of Comput. Sci. XII, Tech. Univ. Dortmund, Dortmund, Germany",2012 International Conference on Frontiers in Handwriting Recognition,31-Jan-13,2012,,,23,28,"One obstacle in the automatic analysis of handwritten documents is the huge amount of labeled data typically needed for classifier training. This is especially true when the document scans are of bad quality and different writers and writing styles have to be covered. Consequently, the considerable human effort required in the process currently prohibits the automatic transcription of large document collections. In this paper, two semi-supervised multiview learning approaches are presented, reducing the manual burden by robustly deriving a large number of labels from relatively few manual annotations. The first is based on cluster-level annotation followed by a majority decision, whereas the second casts the labeling process as a retrieval task and derives labels by voting among ranked lists. Both methods are thoroughly evaluated in a handwritten character recognition scenario using realistic document data. It is demonstrated that competitive recognition performance can be maintained by labeling only a fraction of the data.",,978-1-4673-2262-1,10.1109/ICFHR.2012.181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424365,document analysis;semi-supervised annotation;multiview learning;handwritten character recognition,Labeling;Training;Manuals;Handwriting recognition;Reliability;Character recognition;Humans,document image processing;handwritten character recognition;image classification;image retrieval;learning (artificial intelligence);pattern clustering,handwritten character annotation;human involvement;semisupervised multiview learning strategy;handwritten document analysis;classifier training;document collection;cluster-level annotation;retrieval task;handwritten character recognition;competitive recognition performance,,6,,18,,31-Jan-13,,,IEEE,IEEE Conferences
"A document segmentation, classification and recognition system",文件分割，分類和識別系統,F. Y. Shih; S. -. Chen; D. C. D. Hung; P. A. Ng,"Dept. of Comput. & Inf. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; Dept. of Comput. & Inf. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; Dept. of Comput. & Inf. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; Dept. of Comput. & Inf. Sci., New Jersey Inst. of Technol., Newark, NJ, USA",Proceedings of the Second International Conference on Systems Integration,6-Aug-02,1992,,,258,267,"A discussion is given on a document segmentation, classification and recognition system for automatically reading daily-received office documents that have complex layout structures, such as multiple columns and mixed-mode contents of texts, graphics and half-tone pictures. First, the block segmentation employs a two-step run-length smoothing algorithm for decomposing any document into single-mode blocks. Next, based on clustering rules the block classification classifies each block into one of text, horizontal or vertical lines, graphics, and pictures. The text block is separated into isolated characters using projection profiles, and which are translated into ASCII codes through a font- and size-independent character recognition subsystem. Logo pictures discriminated from half-tone pictures are identified and converted into symbolic words. The experimental results show that the proposed system is capable of correctly reading different styles of mixed-mode printed documents.<>",,0-8186-2697-6,10.1109/ICSI.1992.217295,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=217295,,Image segmentation;Computer graphics;Character recognition;Smoothing methods;Facsimile;Image coding;Image processing;Computer vision;Information science;Text recognition,document handling;image recognition;office automation,Logo pictures;document segmentation;classification;recognition system;daily-received office documents;complex layout structures;multiple columns;mixed-mode contents;half-tone pictures;block segmentation;two-step run-length smoothing algorithm;single-mode blocks;clustering rules;block classification;vertical lines;projection profiles;ASCII codes;size-independent character recognition subsystem;symbolic words;mixed-mode printed documents,,13,,21,,6-Aug-02,,,IEEE,IEEE Conferences
Visualization for the document space,可視化文檔空間,X. Lin,"Center for Comput. Legal Res., Pace Univ., White Plains, NY, USA",Proceedings Visualization '92,6-Aug-02,1992,,,274,281,"An information retrieval frame work that promotes graphical displays, and that will make documents in the computer visualizable to the searcher, is described. As examples of such graphical displays, two simulation results of using a Kohonen feature map to generate map displays for information retrieval are presented and discussed. The map displays are a mapping from a high-dimensional document space to a two-dimensional space. They show document relationships by various visual cues, such as dots, links, clusters, and areas, as well as their measurement and spatial arrangement. Using the map displays as an interface for document retrieval systems, the user is provided with richer visual information to support browsing and searching.<>",,0-8186-2897-9,10.1109/VISUAL.1992.235198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=235198,,Visualization;Information retrieval;Computer displays;Books;Libraries;Area measurement;Law;Legal factors;Computational modeling;Humans,document handling;information retrieval;information retrieval systems;self-organising feature maps,document space;information retrieval;graphical displays;simulation results;Kohonen feature map;document relationships;dots;links;clusters;areas;spatial arrangement;document retrieval systems;visual information;browsing;searching,,17,,17,,6-Aug-02,,,IEEE,IEEE Conferences
Using the Web 1T 5-Gram Database for Attribute Selection in Formal Concept Analysis to Correct Overstemmed Clusters,在正式概念分析中使用Web 1T 5-Gram數據庫進行屬性選擇以糾正過度擁擠的集群,G. R. Hall; K. Taghva,"Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NE, USA; Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NE, USA",2015 12th International Conference on Information Technology - New Generations,1-Jun-15,2015,,,651,654,"As part of information retrieval processes, words are often stemmed to a common root. The Porter Stemming Algorithm operates as a rule-based suffix-removal process. Stemming can be viewed as a way to cluster related words together according to one common stem. Sometimes Porter includes words in a cluster that are un-related. This experiment attempts to correct this using Formal Concept Analysis (FCA). FCA is the process of formulating formal concepts from a given formal context. A formal context consists of objects and attributes, and a binary relation that indicates the attributes possessed by each object. A formal concept is formed by computing the closure of subsets of objects and attributes. Using the Cranfield document collection, this experiment crafted a comparison measure between each word in the stemmed cluster using the Google Web 1T 5-gram data set. Using FCA to correct the clusters, the results showed a varying level of success dependent upon the error threshold allowed.",,978-1-4799-8828-0,10.1109/ITNG.2015.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113548,information retrieval;stemming;formal concept analysis,Context;Standards;Formal concept analysis;Clustering algorithms;Algorithm design and analysis;Training;Testing,formal concept analysis;information retrieval;Internet;search engines,Web 1T 5-gram database;attribute selection;formal concept analysis;overstemmed cluster correction;information retrieval processes;Porter stemming algorithm;rule-based suffix-removal process;FCA;binary relation;cranfield document collection;Google Web 1T 5-gram data set;error threshold,,,,8,,1-Jun-15,,,IEEE,IEEE Conferences
A data selection framework for k-means algorithm to mine high precision clusters,k均值算法挖掘高精度集群的數據選擇框架,Z. Lou; C. Zhang,"School of Information Engineering, Zhengzhou University, Zhengzhou, China, 450001; School of Software and Applied Technology, Zhengzhou University, Zhengzhou, China, 450001","2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",25-Jun-18,2017,,,1651,1657,"Traditional clustering algorithms employ all the data items to learn the cluster patterns. However, in real-world applications, some data show clear coherent behaviour and can be summarized well, while some data present weak tendencies to be assigned to any particular pattern. For such situation, this paper presents a data selection framework for K-Means algorithm to get high precision clusters from the data collection. It differs from traditional k-means-type algorithms in three respects. First, in the cluster learning process, we take the changed value of cluster's Bregman Information, which is generated by merging one data item into the potential clusters, as the measure of data item's clustering tendency. Second, only data items with strong clustering tendencies, that is the changed value of cluster's Bregman Information is less than the predefined radius, are selected to learn the cluster patterns, while the remaining data points are ignored and belong to no cluster. The clustering is non-exhaustive. Third, the radius of the clusters can be changed in the learning process. It is a dynamic learning framework. Experiments on synthetic, document and image data show the effectiveness of the proposed algorithm.",,978-1-5386-2165-3,10.1109/FSKD.2017.8393013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8393013,,Clustering algorithms;Linear programming;Heuristic algorithms;Merging;Data collection;Data mining;Optimization,data mining;learning (artificial intelligence);pattern clustering,high precision clusters;data item;cluster patterns;data collection;k-means-type algorithms;cluster learning process;Bregman Information;data selection;clustering algorithms,,1,,21,,25-Jun-18,,,IEEE,IEEE Conferences
Software Architecture Module-View Recovery Using Cluster Ensembles,使用群集集成的軟件體系結構模塊視圖恢復,C. Cho; K. Lee; M. Lee; C. Lee,"Department of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; Da Vinci College of General Education, Chung-Ang University, Seoul, South Korea; Department of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; Department of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea",IEEE Access,13-Jun-19,2019,7,,72872,72884,"Software architecture documents are valuable assets supporting the maintenance process for software systems. Unfortunately, in many projects, software architecture documentation is not conducted properly or the documents become obsolete due to a discrepancy with the current architecture. To address this, various automated methods to recover software architecture have been proposed in the literature. We argue that most previous studies have not considered cluster ensembles but relied on a single clustering algorithm. In this paper, we propose to take advantage of cluster ensembles for software architecture recovery. Our experiments on five open-source projects are reported and the results are analyzed.",2169-3536,,10.1109/ACCESS.2019.2920427,Chung-Ang University Excellent Student Scholarship; National Research Foundation of Korea; Korea Atomic Energy Research Institute; Korea Atomic Energy Research Institute; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727904,Software architecture recovery;module-view;cluster ensembles,Clustering algorithms;Software architecture;Computer architecture;Software algorithms;Software systems;Feature extraction,pattern clustering;software architecture;software maintenance;system recovery,software architecture module-view recovery;cluster ensembles;software systems;software architecture documentation;software architecture recovery;clustering algorithm,,,,47,,3-Jun-19,,,IEEE,IEEE Journals
Entropy Based Fuzzy C Means Clustering and Key Frame Extraction for Sports Video Summarization,運動視頻摘要的基於熵的模糊C均值聚類和關鍵幀提取,S. Angadi; V. Naik,"Dept. of Comput. Sci. & Eng., Basaveshwar Eng. Coll., Bagalkot, India; Dept. of Comput. Sci. & Eng., Basaveshwar Eng. Coll., Bagalkot, India",2014 Fifth International Conference on Signal and Image Processing,31-Mar-14,2014,,,271,279,"Recent advances in technology have made tremendous amount of multimedia information available to the general population. To access the needed information in this scenario there is a need for automatic tools to filter and present information summary. Summarization techniques will give a choice to users to browse and select the multimedia documents of their choice for complete viewing later. In this work a new summarization technique to collect frames of importance in a video is presented. The method is based on selection of frames typically different from their immediate neighbors as key frames from group of similar frames. It uses the process of clustering, where visually similar frames are collected into one group using Fuzzy C means clustering algorithm. When clusters are formed, the frames that exhibit a change ratio which is a measure of the content variation, greater than the average value of the cluster are treated as Key frames. The summary is created by merging Key frames on the basis of their timeline. This method ensures that video summary represents the most unique frames of the input video and gives equal attention to preserving continuity of the summarized video. The robustness of the algorithm is validated by average values of performance parameters. The average compression ratio of 92% is indication of higher conciseness. The average fidelity of 95% is an indicative of comprehensive representation of video by the key frames selected using proposed algorithm.",,978-0-7695-5100-5,10.1109/ICSIP.2014.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754888,Fuzzy C means;Clustering;Keyframe extraction;Video summarization;fidelity;Informativeness,Feature extraction;Entropy;Clustering algorithms;Vectors;Image color analysis;Video sequences;Histograms,data compression;fuzzy set theory;multimedia computing;pattern clustering;sport;video coding,entropy based fuzzy c means clustering;key frame extraction;sports video summarization;multimedia information;general population;automatic tools;multimedia documents;average compression ratio;comprehensive video representation,,10,,26,,31-Mar-14,,,IEEE,IEEE Conferences
Automatic Opinion Extraction from Web Documents,從Web文檔自動提取意見,S. K. Shandilya; S. Jain,"Dept. of Comput. Eng., Devi Ahilya Univ., Indore; Dept. of Comput. Eng., Devi Ahilya Univ., Indore",2009 International Conference on Computer and Automation Engineering,15-Nov-10,2009,,,351,355,"Automatic extraction of human opinions from Web documents has been receiving increasing interest. An important part of the information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining, which deals with the computational treatment of opinion and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. In this paper, we demonstrate an opinion mining framework that extracts the opinions and views of the consumers/customers, and analyze them to provide concrete market flow along with proven statistical data. The software uses classification, clustering and lingual knowledge-based opinion mining for providing these features.",,978-0-7695-3569-2,10.1109/ICCAE.2009.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804547,Web Mining;Information Retrieval;Opinion Mining,Data mining;Blogs;Surges;Information retrieval;Internet;Information analysis;Learning systems;Automation;Humans;Availability,data mining;document handling;information retrieval;Internet,Web documents;automatic opinion extraction;information-gathering behavior;online review sites;personal blogs;concrete market flow;lingual knowledge-based opinion mining,,7,,18,,15-Nov-10,,,IEEE,IEEE Conferences
Research of Web Data Mining Based on Fuzzy Logic and Neural Networks,基於模糊邏輯和神經網絡的Web數據挖掘研究,L. Ren,"Electron. & Inf. Eng. Dept., Tianjin Inst. of Urban Constr., Tianjin, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,3,,485,489,"Web document classification and clustering are two crucial sections in Web data mining. The models, algorithms and simulation experiments for both Web document classification and clustering have been studied separately to support for the personalized services and to overcome the deficiencies and shortcomings of the same type's algorithms in the paper. The Web document classification based on fuzzy reasoning with comprehensive weights and Web search result clustering based on fuzzy logic and neural networks are presented for Web data mining to obtain easily understood, robust and low-priced solutions by exploring the greatest possible extents of imprecision, uncertainty, fuzzy reasoning and partial correctness. The experiments have demonstrated that the established intelligent Web information mining system here makes Web document classification and clustering more accurate, more credible and more rapid than the exciting ones.",,978-0-7695-3735-1,10.1109/FSKD.2009.344,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359023,web data mining;fuzzy logic;neural network;web document classification;web search result clustering,Data mining;Fuzzy logic;Neural networks;Internet;Information retrieval;Fuzzy reasoning;Web search;Uncertainty;Web mining;Machine learning,data mining;fuzzy logic;Internet;neural nets;pattern clustering,Web data mining;Web document classification;Web document clustering;fuzzy logic;neural networks;fuzzy reasoning,,2,,6,,28-Dec-09,,,IEEE,IEEE Conferences
A New Text Categorization Technique Using Distributional Clustering and Learning Logic,利用分佈聚類和學習邏輯的文本分類新技術,H. Al-Mubaid; S. A. Umair,"Houston Univ., TX; Houston Univ., TX",IEEE Transactions on Knowledge and Data Engineering,24-Jul-06,2006,18,9,1156,1165,"Text categorization is continuing to be one of the most researched NLP problems due to the ever-increasing amounts of electronic documents and digital libraries. In this paper, we present a new text categorization method that combines the distributional clustering of words and a learning logic technique, called Lsquare, for constructing text classifiers. The high dimensionality of text in a document has not been fruitful for the task of categorization, for which reason, feature clustering has been proven to be an ideal alternative to feature selection for reducing the dimensionality. We, therefore, use distributional clustering method (IB) to generate an efficient representation of documents and apply Lsquare for training text classifiers. The method was extensively tested and evaluated. The proposed method achieves higher or comparable classification accuracy and F1 results compared with SVM on exact experimental settings with a small number of training documents on three benchmark data sets WebKB, 20Newsgroup, and Reuters-21578. The results prove that the method is a good choice for applications with a limited amount of labeled training data. We also demonstrate the effect of changing training size on the classification performance of the learners",1558-2191,,10.1109/TKDE.2006.135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661508,Text categorization;feature selection;machine learning.,Text categorization;Support vector machines;Support vector machine classification;Training data;Machine learning;Data mining;Logic testing;Benchmark testing;Electronic mail;Machine learning algorithms,classification;digital libraries;learning (artificial intelligence);pattern clustering;text analysis;word processing,text categorization technique;distributional word clustering method;Lsquare learning logic technique;NLP problem;electronic document;digital library;text classifier;feature clustering;feature selection;dimensionality reduction;document representation;SVM;machine learning,,59,,30,,24-Jul-06,,,IEEE,IEEE Journals
OCR-independent and segmentation-free word-spotting in handwritten Arabic Archive documents,手寫阿拉伯檔案文件中與OCR無關且無分段的單詞發現,N. Aouadi; A. Kacem,"LaTICE, Research Laboratory of Technology of Information and Communication & Electrical Engineering 5, Avenue Taha Hussein, BP 56 Bab Mnara, Tunis, Tunisia; LaTICE, Research Laboratory of Technology of Information and Communication & Electrical Engineering. 5, Avenue Taha Hussein, BP 56 Bab Mnara, Tunis, Tunisia",2013 International Conference on Electrical Engineering and Software Applications,15-Aug-13,2013,,,1,6,"In this paper, a word-spotting approach is presented that can help in reading handwritten Arabic Archive Documents. Because of the low quality of these documents, the proposed approach is free segmentation, independent of OCR, using a global transformation of word images. It is a based learning approach which employs Generalized Hough Transform (GHT) technique. It detects words, described by their models, in documents images by finding the model's position in the image. With the GHT, the problem of finding the model's position is transformed to a problem of finding the transformation's parameter that maps the model into the image. Parameters such as Hough threshold and distance between voting points are considered for a better location and recognition of words. We tested our system on registers from the 19th century onwards, held in the National Archives of Tunisia. Our first experiments reach an average of 94% of well-spotted words.",,978-1-4673-6301-3,10.1109/ICEESA.2013.6578363,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578363,OCR;Word-spotting;Generalized Hough Transform;Clustering;Handwritten Recognition;Historical document,Shape;Image segmentation;Transforms;Optical character recognition software;Training;Dictionaries;Registers,document image processing;handwritten character recognition;Hough transforms;optical character recognition,OCR;segmentation-free word-spotting approach;handwritten Arabic archive documents;optical character recognition;word image transformation;GHT technique;generalized Hough transform;Hough threshold parameter;distance parameter;word location;word recognition,,2,,29,,15-Aug-13,,,IEEE,IEEE Conferences
Comparison of algorithms for patent documents clusterization,專利文獻聚類算法的比較,D. Kukolj; Z. Tekic; L. Nikolic; Z. Panjkov; M. Pokric; M. Drazic; M. Vitas; D. Nemet,"University of Novi Sad, Faculty of Technical Sciences, Serbia; University of Novi Sad, Faculty of Technical Sciences, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia; RT-RK, Institute for Computer Based Systems, Novi Sad, Serbia",2012 Proceedings of the 35th International Convention MIPRO,16-Jul-12,2012,,,995,997,"Ever increasing number of patents makes impossible to find and analyze relevant documents manually. Various software tools have been developed in the patent field. They could analyze individual patents as well as patent portfolios; retrieve patents and make basic statistics as well as visualize, map and landscape the same data. The essential function any tool should provide is patent clustering. There have been many different clustering approaches. In this paper we compare performances of k-means, the neural-gas, fuzzy c-means and ronn clustering technique when used on patent data set that was also clustered by the experts.",,978-953-233-068-7,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240789,,Patents;Clustering algorithms;Data visualization;Classification algorithms;Data mining;Accuracy;Neural networks,fuzzy systems;patents;software tools,patent documents clusterization;relevant documents;software tools;patent field;patent portfolios;patent retrieval;k-means;neural-gas;fuzzy c-means;patent data set,,2,,8,,16-Jul-12,,,IEEE,IEEE Conferences
Recurrent-Neural-Network-Based Boolean Factor Analysis and Its Application to Word Clustering,基於遞歸神經網絡的布爾因子分析及其在詞聚類中的應用,A. A. Frolov; D. Husek; P. Y. Polyakov,"Inst. of Higher Nervous Activity & Neurophysiol., Russian Acad. of Sci., Moscow, Russia; NA; NA",IEEE Transactions on Neural Networks,7-Jul-09,2009,20,7,1073,1086,"The objective of this paper is to introduce a neural-network-based algorithm for word clustering as an extension of the neural-network-based Boolean factor analysis algorithm (Frolov , 2007). It is shown that this extended algorithm supports even the more complex model of signals that are supposed to be related to textual documents. It is hypothesized that every topic in textual data is characterized by a set of words which coherently appear in documents dedicated to a given topic. The appearance of each word in a document is coded by the activity of a particular neuron. In accordance with the Hebbian learning rule implemented in the network, sets of coherently appearing words (treated as factors) create tightly connected groups of neurons, hence, revealing them as attractors of the network dynamics. The found factors are eliminated from the network memory by the Hebbian unlearning rule facilitating the search of other factors. Topics related to the found sets of words can be identified based on the words' semantics. To make the method complete, a special technique based on a Bayesian procedure has been developed for the following purposes: first, to provide a complete description of factors in terms of component probability, and second, to enhance the accuracy of classification of signals to determine whether it contains the factor. Since it is assumed that every word may possibly contribute to several topics, the proposed method might be related to the method of fuzzy clustering. In this paper, we show that the results of Boolean factor analysis and fuzzy clustering are not contradictory, but complementary. To demonstrate the capabilities of this attempt, the method is applied to two types of textual data on neural networks in two different languages. The obtained topics and corresponding words are at a good level of agreement despite the fact that identical topics in Russian and English conferences contain different sets of keywords.",1941-0093,,10.1109/TNN.2009.2016090,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4982625,Associative memory;Boolean factor analysis;concepts search;Hopfield-like neural network;information retrieval;neural network application;neural network architecture;recurrent neural network;statistics;unsupervised learning,Neurons;Neural networks;Recurrent neural networks;Hopfield neural networks;Clustering algorithms;Algorithm design and analysis;Hebbian theory;Bayesian methods;Natural languages;Information analysis,Bayes methods;Boolean functions;fuzzy set theory;Hebbian learning;pattern classification;pattern clustering;probability;recurrent neural nets;text analysis,recurrent neural network algorithm;Boolean factor analysis algorithm;word clustering;textual document;Hebbian learning rule;word semantics;Bayesian procedure;component probability;signal classification;fuzzy clustering,"Algorithms;Artificial Intelligence;Computer Simulation;Fuzzy Logic;Language;Mathematical Computing;Models, Statistical;Neural Networks (Computer);Semantics",19,,18,,27-May-09,,,IEEE,IEEE Journals
Supporting Web Search with Near Keywords,使用近關鍵字支持網頁搜索,H. Chen; K. Yamamoto; K. Furuse; N. Ohbo,University of Tsukuba; University of Tsukuba; University of Tsukuba; University of Tsukuba,Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007),18-Dec-07,2007,2,,411,415,"The goal of information retrieval (IR) is to identify documents which best satisfy users' information need. The task of formulating an effective query becomes much more difficult when the target is the Web. Proposals on query refinement in IR, such as relevant feedback which needs to analyse the whole document database, cannot be applied to Web search. We propose a new method to support Web query refinement. Our methods is based on local analysis which clustering the search result. Unlike other clustering-base approaches, we take into consideration the distance between keywords, and guarantee no information loss. A Web search system is implemented for investigation of the feasibility. Candidate keywords generated by our method are provided to user to refine his/her query. We also confirmed the effectiveness by experiments on well known test collections.",,978-0-7695-2874-8,10.1109/FSKD.2007.551,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4406111,,Web search;Proposals;Web pages;Java;Databases;Testing;Search engines;Blogs;Computer science;Cities and towns,database management systems;document handling;Internet;pattern clustering;query formulation,Web search;information retrieval;query formulation;query refinement;relevant feedback;document database;search clustering,,,,16,,18-Dec-07,,,IEEE,IEEE Conferences
Incorporated preprocessing and physical layout analysis of a binary document image using a two stage classification,使用兩階段分類對二進製文檔圖像進行預處理和物理佈局分析,H. Behin; A. Ebrahimi; S. Ebrahimi,"Faculty of Electrical Engineering, Sahand University of Technology, Tabriz, Iran; Faculty of Electrical Engineering, Sahand University of Technology, Tabriz, Iran; School of Electrical and Computer Engineering, University of Tehran",International Conference on Computer and Communication Engineering (ICCCE'10),23-Aug-10,2010,,,1,5,"Before the image of a document enter an OCR module, it should undergo Preprocessing and Document Layout Analysis steps. Document layout analysis usually comes after preprocessing. Noise removal and skew correction are two major preprocessing operations. Document layout analysis itself is divided into physical and logical layout analysis. Physical layout analysis decomposes the image of a document into homogenous regions such as ""text"", ""graphics"", and ""lines"". In physical layout analysis, first, the image is segmented to homogenous regions, and then each homogenous region is classified into one of the present classes. On the other hand, logical layout analysis tries to assign functional labels (such as ""title"", ""author"", and ""footnote"") to some of the classified regions to find relationship between some regions, and to discover reading order of different parts of a document. This article presents an innovative method for preprocessing and physical layout analysis of binary documents. Although, most of the present systems give the result of preprocessing to document layout analysis; state-of-the-art algorithms try to postpone the processing operations as much as possible in order to prevent irreparable mistakes. These two steps are incorporated in our approach. This is achieved through using segmentation results for noise removal. One reason of effectiveness of this approach is appropriate arrangement of procedures. Also, a neural classifier is so trained that the output is robust to the skew. A two stage classification is used for determining pixel classes. In the first step, the Haar wavelet transform is computed on resized and gray leveled image. The coefficients are normalized, and then 10% of them are picked up randomly. Selected coefficients are clustered into 4 groups using Kmeans. A novel algorithm is introduced for assigning these 4 clusters to a background, a vertical, and two horizontal classes. Other wavelet coefficients are also classified to one of these classes by KNN algorithm. The results of this stage as well as other features help a MLP network to perform the classification of the second stage. As well as regular classes, an ambiguous class is considered to take the regions that are the result of erroneous segmentation. Using the statistics of connected component sizes and horizontal projection profile, the regions are re-segmented and reclassified by another neural network. The presented approach is designed for textual documents with horizontal text extensions, and is applicable for vertical text extension manuscripts by a little change. As well as the proposed method has a fair computational complexity and robustness to skew, it has offered satisfactory results on different types of databases such as magazine, book, newspaper, and official letters.",,978-1-4244-6235-3,10.1109/ICCCE.2010.5556766,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5556766,Preprocessing;Physical Layout Analysis;OCR;Wavelet;KNN;Kmeans;Neural Network,Layout;Classification algorithms;Pixel;Noise;Algorithm design and analysis;Image segmentation;Feature extraction,document image processing;image classification;image segmentation;multilayer perceptrons;text analysis,physical layout analysis;binary document image;two-stage classification;OCR module;optical character recognition;document preprocessing step;noise removal;skew correction;text region;graphics region;lines region;logical layout analysis;neural classifier;pixel class determination;MLP network;multilayer perceptron network;k-nearest neighbor;region resegmentation;region reclassification;vertical text extension;computational complexity;textual documents;horizontal text extensions,,2,,14,,23-Aug-10,,,IEEE,IEEE Conferences
Double-Hypergraph Based Sentence Ranking for Query-Focused Multi-document Summarizaton,基於雙象形圖的查詢多文檔摘要句子排序,X. Cai; J. Han; L. Guo; L. Yang,"Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China",2016 IEEE/WIC/ACM International Conference on Web Intelligence Workshops (WIW),16-Jan-17,2016,,,112,118,"Traditional graph based sentence ranking approaches modeled the documents as a text graph where vertices represent sentences and edges represent pairwise similarity relationships between two sentences. Such modeling cannot capture complex group relationships shared among multiple sentences which can be useful for sentence ranking. In this paper, we propose two different group relationships (sentence-topic relationship and document-topic relationship) shared among sentences, and construct a double-hypergraph integrating these relationships into a unified framework. Then, a double-hypergraph based sentence ranking algorithm is developed for query-focused multi-document summarization, in which Markov random walk is defined on each hypergraph and the mixture Markov chains are formed so as to perform transductive learning in the double-hypergraph. When evaluated on DUC datasets, performance of the proposed approach is remarkable.",,978-1-5090-4771-0,10.1109/WIW.2016.041,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814491,hypergraph; sentence ranking; query-focused multi-document summarization,Markov processes;Optimization;Data mining;Clustering algorithms;Conferences;Automation;Internet,document handling;graph theory;learning (artificial intelligence);Markov processes;mixture models;query processing,double-hypergraph based sentence ranking;group relationships;query-focused multidocument summarization;Markov random walk;mixture Markov chains;transductive learning;sentence-topic relationship;document-topic relationship,,,,18,,16-Jan-17,,,IEEE,IEEE Conferences
Automatically generating multi-document summarizations,自動生成多文檔摘要,D. Van Britsom; A. Bronselaer; G. De Tr矇,"Department of Telecommunications and Information Processing, Ghent University, Ghent; Department of Telecommunications and Information Processing, Ghent University, Ghent; Department of Telecommunications and Information Processing, Ghent University, Ghent",2011 11th International Conference on Intelligent Systems Design and Applications,2-Jan-12,2011,,,142,147,"This paper describes the News Summarization or NEWSUM algorithm designed to automatically generate multi-document summarizations, hereby focusing on textual documents that concern news items. The NEWSUM algorithm has been implemented and tested in several ways. An overview of both the implementation and the test results are covered in this document.",2164-7151,978-1-4577-1676-8,10.1109/ISDA.2011.6121645,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121645,Multi-document;summarization;sentence extraction,Clustering algorithms;Algorithm design and analysis;Intelligent systems;Educational institutions;Visualization;Information processing;Telecommunications,document handling,automatically generating multidocument summarizations;news summarization;NEWSUM algorithm;textual documents,,4,,6,,2-Jan-12,,,IEEE,IEEE Conferences
Clustering of biomedical scientific papers,生物醫學科學論文的聚類,D. Bravo-Alcobendas; C. O. S. Sorzano,"Bioengineering Lab., Univ. San Pablo - CEU, Campus Urb. Montepr穩ncipe, s/n, Boadilla del Monte, Madrid, Spain; Bioengineering Lab., Univ. San Pablo - CEU, Campus Urb. Montepr穩ncipe, s/n, Boadilla del Monte, Madrid, Spain",2009 IEEE International Symposium on Intelligent Signal Processing,13-Oct-09,2009,,,205,209,"In this paper we present a methodology for document clustering based on non-negative matrix factorization (NMF) and ensemble clustering. Thanks to the ensemble clustering the algorithm is less prone to get into a local minimum caused by the initialization of the NMF. Despite the ensemble clustering, the algorithm keeps the semantic interpretability of the NMF and constructs a coocurrence matrix that allows the projection of the documents onto a two-dimensional space suitable for visualization. The algorithm is freely available for the information retrieval community from the bioengineering laboratory Web page.",,978-1-4244-5057-2,10.1109/WISP.2009.5286530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286530,,Clustering algorithms;Signal processing algorithms;Matrix decomposition;Biomedical engineering;Information retrieval;Iterative algorithms;Biomedical signal processing;Visualization;Web pages;Publishing,data visualisation;information retrieval;matrix decomposition;medical information systems;pattern clustering;Web sites,biomedical scientific papers;document clustering;nonnegative matrix factorization;ensemble clustering;coocurrence matrix;visualization;information retrieval;bioengineering laboratory;Web page,,1,,11,,13-Oct-09,,,IEEE,IEEE Conferences
An Efficient Strategy for Face Clustering use in Video Surveillance System,視頻監控系統中人臉聚類的有效策略,M. Asad; R. Mustafa; M. S. Hossain,"University of Chittagong, Chattogram, Bangladesh; University of Chittagong, Chattogram, Bangladesh; University of Chittagong, Chattogram, Bangladesh","2019 Joint 8th International Conference on Informatics, Electronics & Vision (ICIEV) and 2019 3rd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",7-Oct-19,2019,,,12,17,"The importance of knowledge discovery from data has been increased dramatically with the increase of data over the past few years. A small video file contains more information compared to text documents and other media files such as audio, images. For this reason, extracting useful information from video i.e. automated video surveillance system has become a hot research issue. The presence of human in different frame of a video is a common scenario. In the security based application, identification of the human from the videos is an important issue. Face pattern is the most widely used parameter to recognize a person. A system with the ability of gathering the information about the presence of the same person in different frame of a video is highly demanding. In this study we have proposed a clustering algorithm which is variation of Hierarchical Agglomerative Clustering algorithm that is able to cluster face image of human. It tries to cluster based on both similarities and dissimilarities. Our proposed algorithm performs better compared to traditional Hierarchical Agglomerative Clustering algorithm in terms of accuracy and time complexity.",,978-1-7281-0788-2,10.1109/ICIEV.2019.8858532,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858532,Computer Vision;Face Clustering;Video Surveillance,Face;Clustering algorithms;Video surveillance;Face detection;Training;Correlation,computational complexity;data mining;face recognition;pattern clustering;security of data;video surveillance,video surveillance system;knowledge discovery;video file;hierarchical agglomerative clustering algorithm;face clustering;person recognition;time complexity,,,,40,,7-Oct-19,,,IEEE,IEEE Conferences
Improving the quality of labels for self-organising maps using fine-tuning,使用微調提高自組織地圖的標籤質量,E. Schweighofer; A. Rauber; M. Dittenbach,"Inst. of Public Int. Law, Wien Univ., Vienna, Austria; NA; NA",12th International Workshop on Database and Expert Systems Applications,7-Aug-02,2001,,,804,808,"Vector representation of legal documents is still the best way for computing classification clusters and labelling of its contents. A very special problem occurs with self organising maps: strong clusters tend to dominate neighbouring smaller clusters in terms of their weight vector structure, which influences the labels extracted from these. This unwelcome side-effect can be overcome efficiently with a dedicated fine-tuning phase at the end of the training process, in which the neighbourhood radius of the training function is set to zero. Experiments with our text collection show the great improvement of the quality of labelling.",,0-7695-1230-5,10.1109/DEXA.2001.953155,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953155,,Law;Legal factors;World Wide Web;Labeling;HTML;Internet;Software libraries;Boolean functions;Search engines;Information retrieval,law administration;document handling;self-organising feature maps;learning (artificial intelligence);classification;pattern clustering,self-organising maps;fine-tuning;legal documents;classification clusters;labelling;weight vector structure;vector representation;learning process,,,,22,,7-Aug-02,,,IEEE,IEEE Conferences
"En route to data mining in legal text corpora: clustering, neural computation, and international treaties",在合法文本語料庫中進行數據挖掘的途徑：聚類，神經計算和國際條約,D. Merkl; E. Schweighofer,"Dept. of Comput. Sci., R. Melbourne Inst. of Technol., Vic., Australia; NA","Database and Expert Systems Applications. 8th International Conference, DEXA '97. Proceedings",6-Aug-02,1997,,,465,470,"The huge amount of data in legal information systems requires a new generation of techniques and tools to assist lawyers in analyzing data and finding critical nuggets of useful knowledge. A promising approach for data mining in legal text corpora is classification. What we are looking for are powerful methods for the exploration of such libraries whereby the detection of similarities between documents is the overall goal. These methods may be used to gain insight in the inherent structure of the various items contained in a text archive. In this paper, we present the results from a case study in legal document classification based on an experimental document archive comprising important treaties in public international law. The essentials of our approach are the usage of a vector space document representation and the utilization of an unsupervised artificial neural network for document classification.",,0-8186-8147-0,10.1109/DEXA.1997.617333,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617333,,Data mining;Law;Legal factors;Information retrieval;Information analysis;Organizing;Computer science;Multimedia databases;Data analysis;Libraries,law administration;data analysis;knowledge acquisition;very large databases;classification;document handling;pattern classification;neural nets,data mining;legal text corpora;data clustering;neural computation;international treaties;legal information systems;lawyers;data analysis;legal document classification;document similarity detection;text archive;case study;document archive;public international law;vector space;document representation;unsupervised artificial neural network,,8,,36,,6-Aug-02,,,IEEE,IEEE Conferences
Enhancing Wikipedia search results using Text Mining,使用文本挖掘增強Wikipedia搜索結果,K. D. C. G. Kapugama; S. A. S. Lorensuhewa; M. A. L. Kalyani,"Department of Computer Science, Faculty of Science, University of Ruhuna, Sri Lanka; Department of Computer Science, Faculty of Science, University of Ruhuna, Sri Lanka; Department of Computer Science, Faculty of Science, University of Ruhuna, Sri Lanka",2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer),26-Jan-17,2016,,,168,175,"Wikipedia is an online encyclopedia which contains millions of articles related to different subject domains. Wikipedia also has a search page itself to display the links corresponding to Wikipedia articles for a given user query input. This search result page displays the search results according to the relevance order, without any content based grouping. This paper presents an experimental deduction of a search result clustering methodology to group the links, returned by the search result page for a particular keyword, based on the contents of the HTML documents, represented by the links and label these resulted groups meaningfully. The proposed methodology is based on the concepts and theories of Text Mining. Grouping of search results makes easy and efficient for the user in finding the desired Wikipedia document. It is also possible to view the different applications and usages of a given keyword very quickly. This work identifies the best clustering algorithm for document clustering and investigates the ways to determine optimum number of clusters to have a better grouping and label the groups. We evaluate our proposed method by conducting several experiments and the results indicate that our method has a higher precision and recall.",2472-7598,978-1-5090-6078-8,10.1109/ICTER.2016.7829915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829915,Search Result Clustering;Text Mining;Wikipedia,Encyclopedias;Internet;Electronic publishing;Clustering algorithms;Algorithm design and analysis;Text mining,data mining;hypermedia markup languages;pattern clustering;query processing;text analysis;Web sites,Wikipedia search result page;text mining;online encyclopedia;user query input;HTML documents;Wikipedia document;document clustering,,3,,12,,26-Jan-17,,,IEEE,IEEE Conferences
Automatic script identification from document images using cluster-based templates,使用基於群集的模板從文檔圖像自動識別腳本,J. Hochberg; P. Kelly; T. Thomas; L. Kerns,"Los Alamos Nat. Lab., NM, USA; NA; NA; NA",IEEE Transactions on Pattern Analysis and Machine Intelligence,6-Aug-02,1997,19,2,176,181,We describe an automated script identification system for typeset document images. Templates for each script are created by clustering textual symbols from a training set. Symbols from new images are compared to the templates to find the best script. Our current system processes thirteen scripts with minimal preprocessing and high accuracy.,1939-3539,,10.1109/34.574802,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=574802,,Text analysis;Optical character recognition software;Natural languages;Shape;Laboratories;Typesetting;Image analysis;Character recognition;Indexing;Postal services,optical character recognition,automatic script identification;document images;cluster-based templates;typeset document images;textual symbol clustering,,119,,10,,6-Aug-02,,,IEEE,IEEE Journals
Tri-layer-cluster Generation Model for Activity Prediction,活動預測的三層集群生成模型,D. Zhu; Y. Fukazawa; J. Ota,"Res. into Artifacts, Center for Eng. (RACE), Univ. of Tokyo, Chiba, Japan; Services & Solution Dev. Dept., NTTDOCOMO, Inc., Tokyo, Japan; Res. into Artifacts, Center for Eng. (RACE), Univ. of Tokyo, Chiba, Japan",2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),23-Dec-13,2013,1,,359,366,"We propose a topic model capable of generating tri-layer clusters, each of which is composed of a topic layer, an activity layer and a word layer. The objective is to better predict activities involved in documents by considering general topics of the activities for clustering. The proposed model is a supervised topic model based on the Latent Dirichlet Allocation (LDA). As a follow-up study of word-pair generation LDA (wpLDA) model, the model introduces the topic-specific activity distribution as an external input, with an activity node inserted into the main generation thread. In addition, we refer to D. Ramage et al.'s one-to-one correspondence to directly learn word-activity tags. An experiment was conducted to prove the feasibility of this model. We chose ten top-listed activities from the wish clusters obtained by the previous wpLDA research, and used each as the key words to extract thirty tweets for training and five for testing, respectively, tagging the tweets with the corresponding activities. By applying the proposed model, we obtained the expected tri-layer clusters in the training phase. Then, in the testing phase, we utilized the activity-specific word distribution derived from the training results to learn the activities of the testing documents. The Stanford Classifier was put forward as the control group, and the activity prediction accuracy demonstrates that the proposed model exhibits the superiority in multi-activity prediction.",,978-0-7695-5145-6,10.1109/WI-IAT.2013.51,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690037,LDA model;tri-layer cluster;activity prediction;wpLDA,Testing;Training;Predictive models;Accuracy;Computational modeling;Conferences;Electronic mail,document handling;pattern classification;pattern clustering,tri-layer-cluster generation model;activity prediction;topic layer;activity layer;word layer;supervised topic model;latent Dirichlet allocation;word-pair generation LDA model;topic-specific activity distribution;Stanford classifier,,,,25,,23-Dec-13,,,IEEE,IEEE Conferences
A novel spoken document retrieval system using Auto Associative Neural Network based keyword spotting,一種基於自動聯想神經網絡的關鍵詞檢索新語音文檔檢索系統,J. Sangeetha; S. Jothilakshmi,"Department of Computer Science & Engineering, Annamalai University, Chidambaram, India; Department of Computer Science & Engineering, Annamalai University, Chidambaram, India",2015 IEEE 9th International Conference on Intelligent Systems and Control (ISCO),1-Oct-15,2015,,,1,6,"This paper formulates a novel approach to spoken document information retrieval for instinctive speech corpora. The conventional method for this problem is to make use of an Automatic Speech Recognizer (ASR) integrated with the typical information retrieval method. However, ASRs tend to produce transcripts of spontaneous speech with momentous word error rate, which is a negative aspect of standard retrieval system. To prevail over such a constraint, we propose a method for spoken document retrieval based on spoken keyword spotting using Auto Associative Neural Networks (AANN). The proposed work concerns the exploit of the distribution capturing capability of an auto associative neural network for spoken keyword detection. It involves sliding a frame-based keyword template along the audio documents and by means of confidence score acquired from the normalized squared error of AANN to competently search for a match. This work provides a new spoken keyword spotting algorithm based spoken documents clustering. The experimental results recommend that the proposed method is promising for retrieving relevant documents of a spoken query as a key.",,978-1-4799-6480-2,10.1109/ISCO.2015.7282280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7282280,Document Retrieval;Spoken Keyword Spotting;Mel frequency cepstral coefficients;Auto associative neural networks;Confidence Score,Speech;Hidden Markov models;Mel frequency cepstral coefficient;Feature extraction;Filter banks;Neural networks;Data models,information retrieval;neural nets;speech recognition,spoken document retrieval system;auto associative neural network;instinctive speech corpora;spoken keyword detection;frame-based keyword template;audio documents;confidence score;normalized squared error;AANN;spoken keyword spotting algorithm;spoken documents clustering;spoken query,,1,,29,,1-Oct-15,,,IEEE,IEEE Conferences
What drives social sentiment? An entropic measure-based clustering approach towards identifying factors that influence social sentiment polarity,是什麼推動了社會情緒？基於熵度量的聚類方法，用於識別影響社會情感極性的因素,D. N. Sotiropoulos; C. D. Kounavis; P. Kourouthanassis; G. M. Giaglis,NA; NA; NA; NA,"IISA 2014, The 5th International Conference on Information, Intelligence, Systems and Applications",18-Aug-14,2014,,,361,373,"Analyzing the public sentiment over social media streams constitutes an extremely demanding task mainly due to the difficulties that are imposed by the wide spectrum of discussion topics that underlie a given collection of posts. This paper addresses the problem of determining the underlying semantic factors that influence the social sentiment polarity in a given corpus of posts through the utilization of an entropic measure-based clustering approach. Extant studies examine the semantic structure of social network data primarily through topic modeling or sentiment analysis methods. The novelty of our approach lies upon the utilization of a semantically-aware clustering procedure that effectively combines topic modeling and sentiment analysis algorithms. Our approach extends the fundamental assumption behind traditional sentiment analysis methods, according to which sentiment can be associated with low level document features such as words, phrases or sentences. We argue that sentiment can be associated with higher level entities such as the semantic axes that span a given volume of posts, thus performing sentiment analysis at the topic level. Our experimentation provides strong evidence that combining topic modeling and sentiment analysis results by a semantically-aware clustering procedure can reveal the distribution of the overall public sentiment on the underlying semantic axes.",,978-1-4799-6171-9,10.1109/IISA.2014.6878830,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6878830,Sentiment Analysis;Topic Modelling;Entropic Measure-based Clustering;Support Vector Machines,Abstracts;Information retrieval,information analysis;pattern clustering;social networking (online),entropic measure-based clustering approach;social sentiment polarity;public sentiment;social media streams;semantic factors;entropic measure-based clustering approach;social network data;sentiment analysis methods;semantically-aware clustering procedure;sentiment analysis algorithms;topic modeling;sentiment analysis methods;low level document features,,1,,17,,18-Aug-14,,,IEEE,IEEE Conferences
Multioriented and curved text lines extraction from Indian documents,從印度文檔中提取多向彎曲的文本行,U. Pal; P. P. Roy,"Comput. Vision & Pattern Recognition Unit, Indian Stat. Inst., Kolkata, India; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",19-Jul-04,2004,34,4,1676,1684,"There are printed artistic documents where text lines of a single page may not be parallel to each other. These text lines may have different orientations or the text lines may be curved shapes. For the optical character recognition (OCR) of these documents, we need to extract such lines properly. In this paper, we propose a novel scheme, mainly based on the concept of water reservoir analogy, to extract individual text lines from printed Indian documents containing multioriented and/or curve text lines. A reservoir is a metaphor to illustrate the cavity region of a character where water can be stored. In the proposed scheme, at first, connected components are labeled and identified either as isolated or touching. Next, each touching component is classified either straight type (S-type) or curve type (C-type), depending on the reservoir base-area and envelope points of the component. Based on the type (S-type or C-type) of a component two candidate points are computed from each touching component. Finally, candidate regions (neighborhoods of the candidate points) of the candidate points of each component are detected and after analyzing these candidate regions, components are grouped to get individual text lines.",1941-0492,,10.1109/TSMCB.2004.827613,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315751,,Optical character recognition software;Character recognition;Reservoirs;Graphics;Shape;Water resources;Text recognition;Text analysis;Nearest neighbor searches;Clustering algorithms,text analysis;optical character recognition;document image processing;feature extraction,optical character recognition;OCR;water reservoir analogy;printed Indian document;curve text lines;metaphor;touching component;reservoir base-area;candidate point;artistic document recognition;Bangla script;Devnagari script;document analysis;text line extraction,"Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;India;Information Storage and Retrieval;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Printing;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique",37,,19,,19-Jul-04,,,IEEE,IEEE Journals
"Research on collaborative innovation mechanism of the industrial cluster ?China titanium valley in Baoji??,Y. Han; F. Luo""",寶雞“華納鈦谷”產業集群協同創新機制研究韓F.羅”,"School of Management, Xi'an University of Architecture and Technology, Xi'an, China; School of Management, Xi'an University of Architecture and Technology, Xi'an, China","2013 6th International Conference on Information Management, Innovation Management and Industrial Engineering",9-Jan-14,2013,2,,611,614,"Industrial cluster ?China titanium valley in Baoji??as the research object, this study adopts the case study method to establish the collaborative innovation mechanism model of industry cluster based on the synergy and innovation theory after the research documents about collaborative innovation both in domestic and abroad is reviewed. The function of government, knowledge collaboration, organization collaboration and innovation service platform play an important role in this mechanism model. The research conclusions are as follows: the enterprise is the real main body of collaborative innovation, governments' guidance and promotion play an important role in collaborative innovation of industrial cluster, collaborative innovation is not only the synergy among enterprise university and institution, but also needs the deep cooperation of governments, intermediary agencies, industry association, and other organizations.",2155-1472,978-1-4799-0245-3,10.1109/ICIII.2013.6703227,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703227,"Collaborative innovation mechanism;case study;""China titanium valley in Baoji??industrial cluster",Technological innovation;Collaboration;Titanium;Industries;Government;Educational institutions,innovation management;knowledge management;organisational aspects;public administration,collaborative innovation mechanism;industrial cluster;China titanium valley;Baoji;research documents;knowledge collaboration;organization collaboration;innovation service platform;government guidance;government promotion;intermediary agencies;industry association,,,,16,,9-Jan-14,,,IEEE,IEEE Conferences,
Expectation-maximization algorithm for topic modeling on big data streams,大數據流主題建模的期望最大化算法,W. Romsaiyud,"School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand","2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",12-Dec-16,2016,,,1,7,"Expectation-Maximization (EM) is typically used to compute maximum likelihood estimates given incomplete samples and estimated the parameters. We proposed a new algorithm for generating an extension Dynamic Topic Model (exDTM)-in a time-based manner and based on the distribution of documents topics on Spark. The proposed algorithm can be applied in clustering documents from data streams for threat cyberbullying detection of the collected datasets that continuously change over time. In particular, the algorithm includes two main methods. The first method introduces a clustering method rooted in content-based coded dialogues collected from several data sources. This method measures the centroid for each cluster with EM. The second method calculates the joint distribution of the latent variables of the documents for finding the posterior distribution, and generates a final model using a variational Bayesian inference over a sequential time. In this experiment, authentic datasets collected from year - 2010 to 2011 - using corpus-wide patterns of words-were analyzed and studied. In order to enhance the reliability and computation time, the methods were applied on real-life settings where cyberbullying features and user-based features had experienced.",,978-1-5090-1496-5,10.1109/UEMCON.2016.7777818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777818,Big Data;Data Streaming;Topic Models;Expectation-Maximization (EM),Data models;Computational modeling;Sparks;Clustering algorithms;Heuristic algorithms;Algorithm design and analysis;Probabilistic logic,Bayes methods;Big Data;document handling;expectation-maximisation algorithm;inference mechanisms;pattern clustering;security of data,user-based features;cyberbullying features;corpus-wide patterns;authentic datasets;variational Bayesian inference;posterior distribution;EM;content-based coded dialogues collection;threat cyberbullying detection;data streams;clustering documents;Spark;document topic distribution;exDTM;extension dynamic topic model;expectation-maximization algorithm;Big Data streams,,2,,21,,12-Dec-16,,,IEEE,IEEE Conferences
Feature Extraction in Text Clustering Based on Theme,基於主題的文本聚類中的特徵提取,N. Shi; K. Jing; J. Xu; Y. Duan; C. Li,"Coll. of Comput. Sci. & Commun. Eng., China Univ. of Pet., Dongying; Coll. of Comput. Sci. & Commun. Eng., China Univ. of Pet., Dongying; Coll. of Comput. Sci. & Commun. Eng., China Univ. of Pet., Dongying; Coll. of Comput. Sci. & Commun. Eng., China Univ. of Pet., Dongying; Geophys. Res. Inst. of Shengli Oil Field",2008 International Symposium on Intelligent Information Technology Application Workshops,30-Dec-08,2008,,,632,635,"A new method is proposed, which refers to feature extraction based on oil theme of concept hierarchy to improve the weights between the high-frequency words and low-frequency words in the documents, and we use hash technology to improve the limitations of the theme of concept hierarchy. The method can identify the theme of texts accurately, and enhance the characteristic expression of texts. To a certain extent, it has resolved the semantic problem in specific areas.",,978-0-7695-3505-0,10.1109/IITA.Workshops.2008.180,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732018,Text Clustering;Theme of Concept Hierarchy;Feature Extraction;Weight,Feature extraction;Petroleum;Frequency;Information technology;Application software;Educational institutions;Computer science;Ontologies;Degradation;Petrochemicals,feature extraction;pattern clustering;petroleum industry;text analysis,text clustering;feature extraction;oil theme;concept hierarchy;petrochemical industry,,,,5,,30-Dec-08,,,IEEE,IEEE Conferences
Query-relevant document representation for text clustering,與查詢相關的文檔表示形式，用於文本聚類,M. Makrehchi,"Thomson Reuters, 610 Opperman Dr., Eagan, MN 55123",2010 Fifth International Conference on Digital Information Management (ICDIM),10-Dec-10,2010,,,132,138,"In text categorization, one well-known document representation is bag-of-words. Although it is simple and popular, it ignores semantics, underlying linguistic information, and word correlations. In this paper, a new representation for text data is proposed which is called Bag-Of-Queries (BOQ). First, a taxonomy of the terms in the local vocabulary is extracted. Extracting a taxonomy is performed by learning term dependencies using an information theoretic inclusion index. Next, the taxonomy is partitioned to generate a set of correlated terms or bag of queries. Since every two partitions belong to different concepts, they are considered semantically orthogonal queries. This provides a new space of orthogonal features, which is necessary for an efficient categorization. Finally, instead of using terms as features, we use them to build a set of queries. Documents are ranked in response to the queries using a similarity measure. The similarity indices are considered as new features in a vector space model representation. The proposed approach outperforms bag of word based clustering. It also extracts new non-redundant features and at the same time reduces dimensionality.",,978-1-4244-7573-5,10.1109/ICDIM.2010.5664205,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664205,,Taxonomy;Vocabulary;Semantics;Indexes;Text categorization;Mutual information,indexing;learning (artificial intelligence);pattern clustering;query processing;text analysis;vocabulary,query-relevant document representation;text clustering;text categorization;bag-of-words;linguistic information;word correlations;bag-of-queries;local vocabulary;learning;information theoretic inclusion index;semantically orthogonal queries;vector space model representation,,1,,16,,10-Dec-10,,,IEEE,IEEE Conferences
Visual exploration of relationships between document clusters,視覺探索文檔集群之間的關係,I. Jusufi; A. Kerren; J. Liu; B. Zimmer,"Linnaeus University, Department of Computer Science, ISOVIS Group, V瓣xj繹, Sweden; Linnaeus University, Department of Computer Science, ISOVIS Group, V瓣xj繹, Sweden; Linnaeus University, Department of Computer Science, ISOVIS Group, V瓣xj繹, Sweden; Linnaeus University, Department of Computer Science, ISOVIS Group, V瓣xj繹, Sweden",2014 International Conference on Information Visualization Theory and Applications (IVAPP),8-Oct-15,2014,,,195,203,"The visualization of networks with additional attributes attached to the network elements is one of the ongoing challenges in the information visualization domain. Such so-called multivariate networks regularly appear in various application fields, for instance, in data sets which describe friendship networks or co-authorship networks. Here, we focus on networks that are based on text documents, i.e., the network nodes represent documents and the edges show relationships between them. Those relationships can be derived from common topics or common co-authors. Attached attributes may be specific keywords (topics), keyword frequencies, etc. The analysis of such multivariate networks is challenging, because a deeper understanding of the data provided depends on effective visualization and interaction techniques that are able to bring all types of information together. In addition, automatic analysis methods should be used to support the analysis process of potentially large amounts of data. In this paper, we present a visualization approach that tackles those analysis problems. Our implementation provides a combination of new techniques that shows intra-cluster and inter-cluster relations while giving insight into the content of the cluster attributes. Hence, it facilitates the interactive exploration of the networks under consideration by showing the relationships between node clusters in context of network topology and multivariate attributes.",,978-9-8975-8132-8,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294417,Network Visualization;Multivariate Data;Clustering;Document Visualization;Text Visualization;Interaction;Visual Analytics,Data visualization;Layout;Visualization;Tag clouds;Prototypes;Image color analysis;Network topology,,,,,,32,,8-Oct-15,,,IEEE,IEEE Conferences
Text line processing for high-confidence skew detection in image documents,用於圖像文檔中高可信度偏斜檢測的文本行處理,D. Rosner; C. Boiangiu; A. ?tef?nescu; N. 籠?pu?; A. Olteanu,"Computer Science Department, Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Splaiul Independentei 313, Sector 6, 060042, Romania; Computer Science Department, Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Splaiul Independentei 313, Sector 6, 060042, Romania; Computer Science Department, Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Splaiul Independentei 313, Sector 6, 060042, Romania; Computer Science Department, Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Splaiul Independentei 313, Sector 6, 060042, Romania; Computer Science Department, Faculty of Automatic Control and Computers, University Politehnica of Bucharest, Splaiul Independentei 313, Sector 6, 060042, Romania",Proceedings of the 2010 IEEE 6th International Conference on Intelligent Computer Communication and Processing,21-Oct-10,2010,,,129,132,"Skew detection and correction is an important step in automated content conversion systems, on which overall system performance is dependent. Although there are many working solutions at the present time, the search for an algorithm that can achieve good error rates in a fast running time and on different layout types is still open, so new solutions for skew detection are needed. The paper at hand presents a neighbor clustering based approach that has the classical advantages of this class of algorithms - the speed, but delivers better accuracy, comparable with that of Hough based solutions.",,978-1-4244-8230-6,10.1109/ICCP.2010.5606448,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606448,skew detection;deskew;Hough transform;connected component;neighbor clustering,Pixel;Accuracy;Clustering algorithms;Estimation;Nearest neighbor searches;Heuristic algorithms;Algorithm design and analysis,document image processing;text analysis,text line processing;high-confidence skew detection;image documents;skew correction;automated content conversion systems;neighbor clustering based approach,,2,,10,,21-Oct-10,,,IEEE,IEEE Conferences
Statistical Single-Document Summarization for Chinese News Articles,中文新聞文章的統計單文檔摘要,J. Wang; J. Yang,"Dept. of Comput. Sci. & Inf. Eng., Nat. Taipei Univ. of Technol., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taipei Univ. of Technol., Taipei, Taiwan",2012 26th International Conference on Advanced Information Networking and Applications Workshops,19-Apr-12,2012,,,183,188,"Given huge amount of daily news articles, it would be helpful to users if the news reading time can be reduced. In this paper, we focus on single-document summarization for Chinese news articles with statistical methods. First, new vocabularies are collected from news articles, and verified with online translation services. These are included as the auxiliary lexicon. Then, statistical word segmentation is done by calculating the relative frequency of overlapping word n-grams. Finally, the sentence importance is estimated as the weighted sum of n-gram scores, and the top-ranked sentences are selected as the summary. The experimental results showed that generated summaries can be effectively clustered in the same group as the original news articles. A great reduction in storage size can be observed while preserving suitable similarity with the original document. This shows the potential of our proposed approach in news summarization. Further investigation is needed to verify in other document domains.",,978-1-4673-0867-0,10.1109/WAINA.2012.132,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185120,single-document summarization;news summarization;word segmentation;text mining,Google;Meteorology;Measurement;Moon;Semantics;Feature extraction;Pragmatics,document handling;electronic publishing;language translation;statistical analysis;word processing,statistical single-document summarization;Chinese news articles;statistical method;online translation service;auxiliary lexicon;statistical word segmentation;word n-gram overlapping;sentence importance;n-gram score;top-ranked sentence;group clustering;storage size reduction;news summarization,,2,,12,,19-Apr-12,,,IEEE,IEEE Conferences
Fast CNN-Based Document Layout Analysis,基於CNN的快速文檔佈局分析,D. A. Borges Oliveira; M. P. Viana,"IBM Research Brazil, Rua Tutoia, 1157, Para覺so, Sao Paulo, Brazil; IBM Res. Brazil Rua Tut怷ia, Sao Paulo, Brazil",2017 IEEE International Conference on Computer Vision Workshops (ICCVW),22-Jan-18,2017,,,1173,1180,"Automatic document layout analysis is a crucial step in cognitive computing and processes that extract information out of document images, such as specific-domain knowledge database creation, graphs and images understanding, extraction of structured data from tables, and others. Even with the progress observed in this field in the last years, challenges are still open and range from accurately detecting content boxes to classifying them into semantically meaningful classes. With the popularization of mobile devices and cloud-based services, the need for approaches that are both fast and economic in data usage is a reality. In this paper we propose a fast one-dimensional approach for automatic document layout analysis considering text, figures and tables based on convolutional neural networks (CNN). We take advantage of the inherently one-dimensional pattern observed in text and table blocks to reduce the dimension analysis from bi-dimensional documents images to 1D signatures, improving significantly the overall performance: we present considerably faster execution times and more compact data usage with no loss in overall accuracy if compared with a classical bidimensional CNN approach.",2473-9944,978-1-5386-1034-3,10.1109/ICCVW.2017.142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8265351,,Databases;Computer architecture;Training;Layout;Text analysis;Image segmentation;Two dimensional displays,document image processing;feature extraction;feedforward neural nets;image retrieval;pattern clustering;text analysis,information extraction;mobile devices;content boxes;structured data;images understanding;specific-domain knowledge database creation;cognitive computing;automatic document layout analysis;fast CNN;compact data usage;bi-dimensional documents images;dimension analysis;table blocks;text;one-dimensional pattern;one-dimensional approach;cloud-based services,,2,,21,,22-Jan-18,,,IEEE,IEEE Conferences
The PARIS Algorithm for Determining Latent Topics,確定潛在主題的PARIS算法,M. Aharon; I. Cohen; A. Itskovitch; I. Marhaim; R. Banner,"Hewlett-Packard Israel Labs., Israel; Hewlett-Packard Israel Labs., Israel; Hewlett-Packard Israel Labs., Israel; Hewlett-Packard Israel Labs., Israel; Hewlett-Packard Israel Labs., Israel",2010 IEEE International Conference on Data Mining Workshops,20-Jan-11,2010,,,1092,1099,"We introduce a new method for discovering latent topics in sets of objects, such as documents. Our method, which we call PARIS (for Principal Atoms Recognition In Sets), aims to detect principal sets of elements, representing latent topics in the data, that tend to appear frequently together. These latent topics, which we refer to as `atoms', are used as the basis for clustering, classification, collaborative filtering, and more. We develop a target function which balances compression and low error of representation, and the algorithm which minimizes the function. Optimization of the target function enables an automatic discovery of the number of atoms, representing the dimensionality of the data, and the atoms themselves, all in a single iterative procedure. We demonstrate PARIS's ability to discover latent topics, even when those are arranged hierarchically, on synthetic, documents and movie ranking data, showing improved performance compared to existing algorithms, such as LDA, on text analysis and collaborative filtering tasks.",2375-9259,978-1-4244-9244-2,10.1109/ICDMW.2010.187,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693416,machine learning;automatic discovery;keyword extraction;document clustering;set representation;topic extraction;latent concepts,Cost function;Dictionaries;Data models;Algorithm design and analysis;Atomic measurements;Noise;Matching pursuit algorithms,data structures;document handling;iterative methods;optimisation;pattern clustering;set theory,PARIS algorithm;latent topics determination;principal atoms recognition in sets;target function;optimization;iterative procedure;document clustering;set representation;keyword extraction,,1,,15,,20-Jan-11,,,IEEE,IEEE Conferences
A Neural Networks-Based graph algorithm for cross-document coreference resolution,基於神經網絡的圖算法，用於跨文檔共引用解析,S. He; Y. Dong; H. Wang,"School of Information Engineering of Beijing University of Posts and Telecommunications, 100876, China; France Telecom R&D Center, 100080, Beijing, China; France Telecom R&D Center, 100080, Beijing, China",2008 International Conference on Natural Language Processing and Knowledge Engineering,2-May-09,2008,,,1,9,"Cross-document coreference resolution, which is an important subtask in natural language processing systems, focus on the problem of determining if two mentions from different documents refer to the same entity in the world. In this paper we present a two-step approach, employing a classification and clusterization phase. In a novel way, the clusterization is produced as a graph cutting algorithm, namely, neural networks-based BestCut (NBCut). To our knowledge, our system is the first that employs a statistical model in graph partitioning. We evaluate our approach on ACE 2008 cross-document coreference resolution data sets and obtain encouraging result, indicating that on named noun phrase coreference task, the approach holds promise and achieves competitive performance.",,978-1-4244-4515-8,10.1109/NLPKE.2008.4906800,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906800,Maximum-Entropy;Min-Cut BestCut;Neural-Networks NBCut,Neural networks;Clustering algorithms;Telecommunications;Research and development;Natural language processing;Noise measurement;Performance analysis;Entropy;Helium;Partitioning algorithms,document handling;graph theory;natural language processing;neural nets;pattern classification;pattern clustering;statistical analysis,natural language processing systems;classification phase;clusterization phase;graph cutting algorithm;neural networks-based BestCut;statistical model;graph partitioning;ACE 2008 cross-document coreference resolution data sets;named noun phrase coreference task,,,,11,,2-May-09,,,IEEE,IEEE Conferences
A survey on semantic similarity between words in semantic web,語義網中詞間語義相似度的調查,P. Ilakiya; M. Sumathi; S. Karthik,"Department of Computer Science Engineering (UG&PG), SNS College of Technology, Coimbatore; Department of Computer Science Engineering (UG&PG), SNS College of Technology, Coimbatore; Department of Computer Science Engineering (UG&PG), SNS College of Technology, Coimbatore","2012 International Conference on Radar, Communication and Computing (ICRCC)",7-Feb-13,2012,,,213,216,"Measuring the semantic similarity between words is an important component in various tasks on the web such as relation extraction, community mining, document clustering, and automatic metadata extraction. Despite the usefulness of semantic similarity measures in these applications, accurately measuring semantic similarity between two words (or entities) remains a challenging task. This survey propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search engine for two words. Specifically, this technique defines various word co-occurrence measures using page counts and integrates those with lexical patterns extracted from text snippets. To identify the numerous semantic relations that exist between two given words, a novel pattern extraction algorithm and a pattern clustering algorithm are proposed. The optimal combination of page counts-based co-occurrence measures and lexical pattern clusters is learned using support vector machines.",,978-1-4673-2756-5,10.1109/ICRCC.2012.6450580,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6450580,Semantic Similarity;Relation Extraction;Pattern Clustering;Page Count;Snippet,Semantics;Search engines;Semantic Web;Databases;Web search;Support vector machines;Educational institutions,pattern clustering;search engines;semantic Web;support vector machines;text analysis,semantic web;relation extraction;community mining;document clustering;automatic metadata extraction;semantic similarity measure;semantic similarity estimation;text snippet;Web search engine;word cooccurrence measure;semantic relation;pattern extraction algorithm;pattern clustering algorithm;page counts-based cooccurrence measure;lexical pattern cluster;support vector machine,,4,,13,,7-Feb-13,,,IEEE,IEEE Conferences
Supervised Key Terms Clustering for Regulatory Monitoring,監管關鍵詞的監督聚類,Y. Zou; A. P. Waldo,"Advanced Data Analytics Decernis LLC,Rockville,MD,U.S.A; Decernis LLC,Rockville,MD,U.S.A",2019 IEEE International Conference on Big Data (Big Data),24-Feb-20,2019,,,4352,4356,"Multivariate analysis, clustering, and classification are common machine learning challenges in food, beverage and consumer product regulation documentations. Using selective key terms from a food, beverage chemical and consumer product lexicon to perform supervised key term clustering helps on both predicting the relevance of regulation documents to their subjects and also explaining the classification and document ranking decisions. The results can also be used as feedback to improve the lexicon.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006607,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006607,text classification;correlation matrix;lexicon creation;data clustering;international regulatory compliance,Correlation;Consumer products;Chemicals;Additives;Monitoring;Machine learning;Standards,learning (artificial intelligence);pattern clustering,regulatory monitoring;consumer product regulation documentations;selective key terms;beverage chemical;consumer product lexicon;supervised key term clustering;regulation documents;document ranking decisions;multivariate analysis,,,,11,,24-Feb-20,,,IEEE,IEEE Conferences
Advanced approaches to speaker diarization of audio documents,語音文檔的說話人二分頻的高級方法,K. Markov,"Human Interface Lab, School of Computer Science and Engineering, The University of Aizu, Fukushima, Japan",2009 Joint Conferences on Pervasive Computing (JCPC),25-Feb-10,2009,,,179,184,"Speaker diarization is the process of annotating an audio document with information about the speaker identity of speech segments along with their start and end time. Assuming that audio input consists of speech only or that non-speech segments have been already identified by another method, the task of speaker diarization is to find 聶who spoke when聶. Since there is no prior information about the number of speakers, the main approach is to apply segment clustering. According to the clustering algorithm used, speaker diarization systems can be divided into two groups: (1) based on agglomerative clustering, and (2) based on on-line clustering. Agglomerative clustering is an off-line approach and is used in most of the current systems because it gives accurate results and can be fine tuned by performing several processing passes over the data. This, however, comes at the cost of high computational load which increases exponentially with the number of segments and the requirement of having the whole audio document available in advance. In contrast, on-line clustering based systems have almost constant computational load, work on-line in real time with small latency, but are generally less accurate than off-line systems. As we show in this paper, when using advanced on-line learning methods and original design, on-line systems can make less errors than off-line systems and can even work faster than real time with very low latency.",,978-1-4244-5227-9,10.1109/JCPC.2009.5420194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420194,Speaker diarization;Speaker segmentation;On-line GMM learning,Broadcasting;Loudspeakers;Humans;Real time systems;Delay;Natural languages;Computer science;Speech processing;Clustering algorithms;Computational efficiency,audio databases;pattern clustering;speaker recognition;speech processing,speaker diarization;audio documents;agglomerative clustering;latency;speech segmentation,,,,22,,25-Feb-10,,,IEEE,IEEE Conferences
A fuzzy category based aggregation technique: A mutual approach for clustering and query processing and its application to web mining,基於模糊類別的聚合技術：聚類和查詢處理的相互方法及其在Web挖掘中的應用,M. K. Sridharan; M. Chitra,"Department of Computer Science and Engineering, Anna University of Technology, Coimbatore, India; Department of Information Technology, Sona College of Technology, Salem, Tamil Nadu, INDIA","2012 Third International Conference on Computing, Communication and Networking Technologies (ICCCNT'12)",31-Dec-12,2012,,,1,7,"Web mining, an indispensable branch of data mining is to mine large web data repositories in order to produce results that can be used in these design tasks. Owing to lack of heterogeneity in structure of web data there needed for the evolution of web mining. Web content mining is analogous but different from data mining and text mining additionally the amount of data/information on the Web is huge and still growing rapidly. The issues allied with web content mining is the critical decision-making process. In our previous approach the prime demerits are that First top two ranking sentences are taken to be considered for retrieval of information; consequently due to the redundancy fact clustering accuracy was to bare minimum level. In our proposed approach the documents are clustered according to category based on Fuzzy category based Aggregation Algorithm. Resultantly User pertinent documents are retrieved based on query weighting methodology. The simulation results portrays that our proposed approach facilitates effectively when applied with large set of documents moreover illustrates superior optimization results in terms of accuracy too.",,,10.1109/ICCCNT.2012.6396088,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6396088,Data Mining;Text Mining;Content Mining;Clustering;Query Processing;etc,Barium;Heuristic algorithms;Customer relationship management;Navigation;Computer architecture;Accuracy;Tagging,data mining;decision making;fuzzy set theory;Internet;pattern clustering;query processing;text analysis,fuzzy category based aggregation technique;query processing;data mining;Web data repositories;Web content mining;text mining;decision-making process;ranking sentence;information retrieval;clustering accuracy;document clustering;query weighting methodology,,,,21,,31-Dec-12,,,IEEE,IEEE Conferences
A Load-Balancing Self-Organizing Incremental Neural Network,負載平衡自組織增量神經網絡,H. Zhang; X. Xiao; O. Hasegawa,"Imaging Science and Engineering Laboratory, Tokyo Institute of Technology, Yokohama, Japan; Imaging Science and Engineering Laboratory, Tokyo Institute of Technology, Yokohama, Japan; Imaging Science and Engineering Laboratory, Tokyo Institute of Technology, Yokohama, Japan",IEEE Transactions on Neural Networks and Learning Systems,20-May-17,2014,25,6,1096,1105,"Clustering is widely used in machine learning, feature extraction, pattern recognition, image analysis, information retrieval, and bioinformatics. Online unsupervised incremental learning is an important branch of data clustering. However, accurately separating high-density overlapped areas in a network has a direct impact on the performance of the clustering algorithm. In this paper, we propose a load-balancing self-organizing incremental neural network (LB-SOINN) to achieve good clustering results and demonstrate that it is more stable than an enhanced SOINN (E-SOINN). LB-SOINN has all the advantages of E-SOINN, such as robustness to noise and online unsupervised incremental learning. It overcomes the shortcomings of the topology structure generated by E-SOINN, such as dependence on the sequence of the input data, and avoids the turbulence that occurs when separating a composite class into subclasses. Furthermore, we also introduce a distance combination framework to obtain good performance for high-dimensional space-clustering tasks. Experiments involving both artificial and real world data sets indicate that LB-SOINN has superior performance in comparison with E-SOINN and other methods.",2162-2388,,10.1109/TNNLS.2013.2287884,Japan Science and Technology Agency's CREST project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657814,Document clustering;incremental learning;load-balancing;self-organizing neural network.;Document clustering;incremental learning;load-balancing;self-organizing neural network,Vectors;Euclidean distance;Smoothing methods;Training;Classification algorithms;Learning systems,pattern clustering;self-organising feature maps;unsupervised learning,load-balancing self-organizing incremental neural network;machine learning;feature extraction;pattern recognition;image analysis;information retrieval;bioinformatics;unsupervised incremental learning;data clustering;LB-SOINN;E-SOINN;input data sequence;distance combination framework,,22,,18,,7-Nov-13,,,IEEE,IEEE Journals
Color text extraction from camera-based images: the impact of the choice of the clustering distance,從基於攝像機的圖像中提取彩色文本：選擇聚類距離的影響,C. Mancas-Thillou; B. Gosselin,"Faculte Polytechnique de Mons, Belgium; Faculte Polytechnique de Mons, Belgium",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,312,316 Vol. 1,"Character recognition has a continuous importance for several years and recently, new challenges appeared with camera-based pictures. This paper deals with text extraction for color natural scenes images. Many papers try to combine several color spaces or to choose the best one for a particular database. We show that the main problem is not in the choice of color spaces for generic text extraction but in the choice of clustering distances to handle alt degradations present in this kind of images. Comparative results are given using a public database.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.76,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575560,,Character recognition;Optical character recognition software;Image color analysis;Image databases;Degradation;Data mining;Robustness;Text analysis;Image segmentation;Layout,optical character recognition;document image processing;visual databases;text analysis;image segmentation;natural scenes;feature extraction;image colour analysis;pattern clustering,color text extraction;camera-based images;clustering distance;character recognition;camera-based pictures;color natural scenes images,,18,,14,,16-Jan-06,,,IEEE,IEEE Conferences
Discovering Program Topoi via Hierarchical Agglomerative Clustering,通過分層聚集群集發現程序Topoi,C. Ieva; A. Gotlieb; S. Kaci; N. Lazaar,"Department of Software Engineering, Simula Research Laboratory, Oslo, Norway; Department of Software Engineering, Simula Research Laboratory, Oslo, Norway; Laboratoire d?Informatique, de Robotique et de Micro矇lectronique de Montpellier, University of Montpellier, Montpellier, France; Laboratoire d?Informatique, de Robotique et de Micro矇lectronique de Montpellier, University of Montpellier, Montpellier, France",IEEE Transactions on Reliability,30-Aug-18,2018,67,3,758,770,"In long lifespan software systems, specification documents can be outdated or even missing. Developing new software releases or checking whether some user requirements are still valid becomes challenging in this context. This challenge can be addressed by extracting high-level observable capabilities of a system by mining its source code and the available source-level documentation. This paper presents feature extraction and traceability (FEAT), an approach that automatically extracts topoi, which are summaries of the main capabilities of a program, given under the form of collections of code functions along with an index. FEAT acts in two steps: first, clustering: by mining the available source code, possibly augmented with code-level comments, hierarchical agglomerative clustering groups similar code functions. In addition, this process gathers an index for each function. Second, entry point selection: functions within a cluster are then ranked and presented to validation engineers as topoi candidates. We implemented FEAT on top of a general-purpose test management and optimization platform and performed an experimental study over 15 open-source software projects amounting to more than 1 M lines of codes proving that automatically discovering topoi is feasible and meaningful on realistic projects.",1558-1721,,10.1109/TR.2018.2828135,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359317,Clustering;program analysis;program topos;software maintenance;source code mining,Feature extraction;Data mining;Software systems;Clustering methods;Open source software;Principal component analysis,data mining;feature extraction;optimisation;pattern clustering;public domain software;software maintenance,high-level observable capabilities;available source-level documentation;FEAT acts;available source code;code-level comments;hierarchical agglomerative clustering groups similar code functions;validation engineers;topoi candidates;general-purpose test management;optimization platform;long lifespan software systems;specification documents;software releases;user requirements;open-source software projects,,1,,29,,15-May-18,,,IEEE,IEEE Journals
Online Balancing Two Independent Criteria upon Placements and Deletions,在線平衡放置和刪除的兩個獨立條件,S. S. H. Tse,"Istanbul University, Istanbul",IEEE Transactions on Parallel and Distributed Systems,26-Jun-13,2013,24,8,1644,1650,"We study the online bicriteria load balancing problem in this paper. We choose a system of distributed homogeneous file servers located in a cluster as the scenario and propose an online approximate solution for balancing their loads and required storage spaces upon placements and deletions. By placement (resp. deletion), we mean to insert a document into (resp. remove a document from) a server system. The main technique is to keep two global quantities large enough. To the best of our knowledge, the technique is novel, and the result is the first one, in the literature. Our result works for any sequences of document placements and deletions. For each deletion, a limited number of documents are reallocated. The load and storage space bounds are 1.5 to 4 times those in the best existing result for sole placements. We refer sole placements to those placement algorithms that do not allow any reallocation and replication. The time complexity, for each operation, is O(logMN), where M is the number of servers, and N is the number of existing documents in the servers, plus the reallocation cost for document deletion. The price for handling document deletion is almost totally reflected by the reallocation cost, and the higher bounds of load and storage spaces, while the O(logN) additive term in the time complexity serves as the remainder.",1558-2183,,10.1109/TPDS.2012.253,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547634,Approximate;distributed;online algorithm;load balancing;scheduling;document placement;deletion;reallocation;distributed and parallel computing,Online services;Algorithm design and analysis;Parallel computing;Load management;Document handling,computational complexity;document handling;queueing theory;resource allocation,independent criteria online balancing;document placement;document deletion;online bicriteria load balancing problem;distributed homogeneous file servers;online approximate solution;storage spaces;server system;sole placements;time complexity;reallocation cost;load space,,2,,14,,26-Jun-13,,,IEEE,IEEE Journals
Using semantic relatedness measures with dynamic self-organizing maps for improved text clustering,將語義相關性度量與動態自組織映射結合使用以改善文本聚類,N. Nathawitharana; D. Alahakoon; D. De Silva,"La Trobe Business School, La Trobe University, Bundoora Victoria 3086 Australia; La Trobe Business School, La Trobe University, Bundoora Victoria 3086 Australia; La Trobe Business School, La Trobe University, Bundoora Victoria 3086 Australia",2016 International Joint Conference on Neural Networks (IJCNN),3-Nov-16,2016,,,2662,2671,Growing volumes of text and increasing expectations on the complexity of analysis entail advanced approaches to text mining. Unsupervised text clustering is an efficient approach to determine structural groupings in a text corpus without the impact of external bias. The information content of such structural groupings needs to be enhanced by integrating semantics into the cluster outcomes. This integration can eventuate at different stages of the clustering process where semantic relatedness measures can be used for the integration. In this paper we propose a novel method of semantics integration to cluster separation in a dynamic self-organizing map algorithm. We demonstrate the effectiveness of the proposed method and the value of semantics integration for cluster separation with empirical results from two benchmark text datasets.,2161-4407,978-1-5090-0620-5,10.1109/IJCNN.2016.7727533,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727533,,Semantics;Heuristic algorithms;Clustering algorithms;Text mining;Self-organizing feature maps;Tagging;Context,data mining;pattern clustering;self-organising feature maps;semantic networks;text analysis;unsupervised learning,semantic relatedness measures;dynamic self-organizing maps;text mining;unsupervised text clustering;structural groupings;information content;semantics integration;cluster separation;text document clustering,,,,53,,3-Nov-16,,,IEEE,IEEE Conferences
Near-duplicate detection using GPU-based simhash scheme,使用基於GPU的simhash方案進行近重複檢測,X. Feng; H. Jin; R. Zheng; L. Zhu,"Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China",2014 International Conference on Smart Computing,19-Feb-15,2014,,,223,228,"With the rapid growth of data, near-duplicate documents bearing high similarity are abundant. Elimination of near-duplicates can reduce storage cost and improve the quality of search indexes in data mining. A challenging problem is to find near-duplicate records in large-scale collections efficiently. There have already been several efforts on implementing near-duplicate detection on different architectures. In this paper, a new implementation, using a special hash function namely simhash, is proposed to identify near-duplicate documents on CUDA enabled devices. Two mechanisms are designed to achieve higher performance, including swapping and dynamic allocating. Experimental results show that our parallel implementation outperforms the serial CPU version, achieving up to 18 times.",,978-1-4799-5711-8,10.1109/SMARTCOMP.2014.7043862,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043862,Near-duplicate detection;Similarity;Simhash;Hamming distance;Graphics Processing Units,Graphics processing units;Fingerprint recognition;Instruction sets;Hamming distance;Dynamic scheduling;Kernel;Computer architecture,cryptography;data mining;document handling;graphics processing units;parallel architectures,near-duplicate detection;GPU-based simhash scheme;storage cost reduction;search index quality improvement;data mining;near-duplicate records;large-scale collections;hash function;near-duplicate document identification;CUDA enabled devices;swapping;dynamic allocation;graphics processing units,,1,,20,,19-Feb-15,,,IEEE,IEEE Conferences
Incremental entity fusion from linked documents,鏈接文檔中的增量實體融合,P. Malhotra; P. Agarwal; G. Shroff,"TCS Research, Tata Consultancy Services Ltd., Sector 63, Noida, Uttar Pradesh, India; TCS Research, Tata Consultancy Services Ltd., Sector 63, Noida, Uttar Pradesh, India; TCS Research, Tata Consultancy Services Ltd., Sector 63, Noida, Uttar Pradesh, India",17th International Conference on Information Fusion (FUSION),7-Oct-14,2014,,,1,8,"In many government applications, especially for intelligence and law-enforcement, we often find that information about entities, such as persons or even companies, are available in disparate data sources. For example, information distributed across passports, driving licences, bank accounts, and income tax documents that need to be resolved and fused to reveal a consolidated profile of an individual. In this paper we describe an algorithm to fuse documents that are highly likely to belong to the same entity by exploiting inter-document references in addition to attribute similarity. Our technique uses a combination of iterative graph-traversal, locality-sensitive hashing, iterative match-merge, and graph-clustering to discover unique entities based on a document corpus. Further, new sets of documents can be added incrementally while having to re-process only a small subset of a previously fused entity-document collection. We present performance and quality results via both Bayesian likelihood fusion as well as using Support Vector Machines to demonstrate benefit of using inter-document references, both to improve accuracy as well as for detecting attempts at deliberate obfuscation.",,978-8-4901-2355-3,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916082,,Boolean functions;Bayes methods;Support vector machines;Databases;Licenses;Fuses;Silicon,Bayes methods;document handling;file organisation;graph theory;iterative methods;merging;pattern matching;sensor fusion;support vector machines,incremental entity fusion;government applications;law-enforcement;interdocument references;attribute similarity;iterative graph-traversal;locality-sensitive hashing;iterative match-merge;graph-clustering;unique entities discover;document corpus;Bayesian likelihood fusion;support vector machines;obfuscation;document fusion,,,,26,,7-Oct-14,,,IEEE,IEEE Conferences
Mining biomedical data from hypertext documents,從超文本文檔中挖掘生物醫學數據,S. Salahuddin; R. M. Rahman,"Department of Electrical Engineering and Computer Science, North South University, Bashundhara, Dhaka, Bangladesh; Department of Electrical Engineering and Computer Science, North South University, Bashundhara, Dhaka, Bangladesh",14th International Conference on Computer and Information Technology (ICCIT 2011),9-Mar-12,2011,,,417,422,"Data mining is a process of discovering useful information from a database and analysis of extracted information. Text mining uses many techniques of data mining. It primarily deals with unstructured data. Web mining is an extension of text mining since it deals with unstructured data. Data mining relates to find data from ?static databases??which contains ?structured??data where as, web mining plays with data that are ?dynamic??and ?unstructured?? In this papers our goal is to mine biomedical data from hypertext documents (e.g., mining data from web contents) using text mining techniques with the help of ?biomedical ontology?? Web data repositories are the hypertext documents. Texts in the Hypertext documents are unstructured and they contain Hypertext Markup Language (HTML) tags, scripting languages, images, audios, videos, URLs etc. We collect a number of documents using Google crawler and preprocess the hypertext documents and extract the text data. Next, we identify whether a word is a biomedical entity or not by using a biomedical database the ?Unified Medical Language System (UMLS) metathesaurus?? The mapping of biomedical entity from the metathesaurus will be done based on keyword query. Then we apply the result to re-rank the web documents to find most relevant documents. We conclude that the more occurrence of a biomedical entity in a page, the more relevant the page is, and thus, we can re-rank the documents to find the most relevant documents by using text mining technique.",,978-1-61284-908-9,10.1109/ICCITechn.2011.6164825,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6164825,Datamining;Biomedical ontology;classification;performance analysis;document clustering,Engines;Communities;Unified modeling language,data mining;database management systems;hypermedia markup languages;Internet;medical information systems;ontologies (artificial intelligence);query processing;text analysis,biomedical data mining;hypertext documents;text mining;extracted information analysis;Web mining;static databases;biomedical ontology;Web data repositories;hypertext markup language tags;scripting languages;URL;Google crawler;unified medical language system metathesaurus;keyword query;Web documents,,,,12,,9-Mar-12,,,IEEE,IEEE Conferences
A recommendation-based web content mining model for an university community,基於建議的大學社區Web內容挖掘模型,M. D. Duss獺n-Sarria; E. Le籀n-Guzm獺n,"Universidad Nacional de Colombia - Departamento de Ingenier穩a de Sistemas y Computati籀n, Grupo de investigati籀n MIDAS; Universidad Nacional de Colombia - Departamento de Ingenier穩a de Sistemas y Computati籀n, Grupo de investigati籀n MIDAS",2012 7th Colombian Computing Congress (CCC),31-Dec-12,2012,,,1,6,"This article presents a recommendation-based web content mining model applied to the navigation data from an university community. The recommendation is based on an offline module that does the grouping of the web documents using a vector space model and the Bisecting KMeans algorithm, and an online module that selects the closest cluster and documents of the query document (actual navigation). Tasks for preprocessing the web documents, recommendation strategies, experiments, and a supervised validation are presented. The results suggests that the relationship between the query document and recommendations are good for near half of those polled.",,978-1-4673-1476-3,10.1109/ColombianCC.2012.6398010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398010,Web Recommender Systems;Data Mining;Web Content Mining;Clustering,Internet;HTML;Vectors;Java;Data mining;Software;Computational modeling,computer aided instruction;data mining;document handling;educational institutions;Internet;pattern clustering;query processing;recommender systems,recommendation-based Web content mining model;university community;navigation data;Web documents;vector space model;kmeans algorithm;online module;closest cluster;query document;recommendation strategies;supervised validation,,,,22,,31-Dec-12,,,IEEE,IEEE Conferences
HLDA based text clustering,基於HLDA的文本聚類,P. Liu; L. Li; W. Heng; B. Wang,"Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China; Center for Intelligence Science and Technology, School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing 100876, China",2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems,14-Nov-13,2012,3,,1465,1469,"LDA (Latent Dirichlet Allocation) topic model has been applied into many applications in recent years. But LDA has a shortcoming that it cannot deal with various changes of data set well, which has become a limitation for its applications. Hierarchical Latent Dirichlet Allocation (hLDA) is a generalization of LDA and it can adapt itself to the growing data set automatically. hLDA can mine latent topics from a large amount of discrete data and organize these topics into a hierarchy, in which the topics of higher level are more abstractive while the topics of lower level are more specific. This hierarchy could achieve a deeper semantic model which is similar with human mind. Given a set of documents, hLDA generates a prior distribution of Bayesian nonparametrics using a nested Chinese restaurant process (nCRP)[1]. The documents sharing similar topics are organized into a cluster of path. hLDA learns the distribution of topics using a method of Bayesian posterior inference. This paper tries to study hLDA model in details and apply it into the application of Chinese text clustering. Experiments have shown that hLDA is a very promising model for text clustering.",2376-595X,978-1-4673-1857-0,10.1109/CCIS.2012.6664628,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664628,text clustering;hierarchical latent dirichlet allocation (HLDA);nested chinese restaurant process (ncrp);Bayesian nonparametrics,Entropy;Bayes methods;Markov processes;Resource management;Fitting;Educational institutions;Large scale integration,Bayes methods;natural language processing;nonparametric statistics;pattern clustering;statistical distributions;text analysis,HLDA based text clustering;hierarchical Latent Dirichlet allocation;LDA topic model;discrete data;semantic model;Bayesian nonparametric distribution;nested Chinese restaurant process;nCRP;document sharing;topic distribution;Bayesian posterior inference;Chinese text clustering,,2,,13,,14-Nov-13,,,IEEE,IEEE Conferences
Identifying language groups within multilingual cybercriminal forums,在多語言網絡犯罪論壇中識別語言組,V. Benjamin; H. Chen,"Department of Information Systems, Arizona State University, Tempe, 85287, United States of America; Department of Management Information Systems, The University of Arizona, Tucson, 85721, United States of America",2016 IEEE Conference on Intelligence and Security Informatics (ISI),17-Nov-16,2016,,,205,207,"Online cybercriminal communities exist in various geopolitical regions, including America, China, Russia, and more. Some multilingual forums exist where cybercriminals of differing geopolitical origin interact and exchange hacking knowledge and cybercriminal assets. Researchers can study such forums to better understand the global cybercriminal supply chain and cybercrime trends. However, little work has focused on identifying members of different language groups and geopolitical origin within such forums. One challenge is the necessity of a technique that scales across multiple languages. We are motivated to explore computational techniques that support automated and scalable categorization of cybercriminal forum participants into varying language groups. In particular, we make use of Paragraph Vectors, a state-of-the-art neural network language model to generate fixed-length vector representations (i.e., document embeddings) of messages posted by forum participants. Results indicate Paragraph Vectors outperforms traditional n-gram frequency approaches for generating document embeddings that are useful for clustering cybercriminals into language groups.",,978-1-5090-3865-7,10.1109/ISI.2016.7745471,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745471,Cybersecurity;Cybecrminal community;Multilingual;Neural network;Language modeling,Computer crime;Sociology;Statistics;Neural networks;Supply chains;Computational modeling,computer crime;document handling;natural language processing;neural nets;pattern clustering;supply chains;vectors,language group identification;multilingual cybercriminal forums;online cybercriminal communities;geopolitical regions;hacking knowledge;cybercriminal assets;global cybercriminal supply chain;cybercrime trends;computational techniques;cybercriminal forum participants;paragraph vectors;neural network language model;fixed-length vector representation generation;n-gram frequency approaches;document embedding generation;cybercriminal clustering,,3,,7,,17-Nov-16,,,IEEE,IEEE Conferences
Text mining: Challenges and future directions,文本挖掘：挑戰和未來方向,A. Akilan,"Department of Information Technology, Annai College of Arts and Science, Kumbakonam",2015 2nd International Conference on Electronics and Communication Systems (ICECS),18-Jun-15,2015,,,1679,1684,"In today's world, the amount of stored information has been enormously increasing day by day which is generally in the unstructured form and cannot be used for any processing to extract useful information, so several techniques such as summarization, classification, clustering, information extraction and visualization are available for the same which comes under the category of text mining. Text Mining can be defined as a technique which is used to extract interesting information or knowledge from the text documents. Text mining, also known as text data mining or knowledge discovery from textual databases, refers to the process of extracting interesting and non-trivial patterns or knowledge from text documents. Regarded by many as the next wave of knowledge discovery, text mining has very high commercial values.",,978-1-4799-7225-8,10.1109/ECS.2015.7124872,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7124872,Text mining;data mining;knowledge discovery,Text mining;Knowledge discovery;Semantics;Information retrieval;Internet;Databases,data mining;document handling;pattern classification;pattern clustering,summarization;classification;clustering;information extraction;information visualization;text documents;text data mining;knowledge discovery;textual databases,,12,,6,,18-Jun-15,,,IEEE,IEEE Conferences
Pseudo-Supervised Clustering for Text Documents,文本文件的偽監督群集,M. Maggini; L. Rigutini; M. Turchi,"Universit? di Siena, Italy; NA; NA",IEEE/WIC/ACM International Conference on Web Intelligence (WI'04),4-Apr-05,2004,,,363,369,"Effective solutions for Web search engines can take advantage of algorithms for the automatic organization of documents into homogeneous clusters. Unfortunately, document clustering is not an easy task especially when the documents share a common set of topics, like in vertical search engines. In this paper we propose two clustering algorithms which can be tuned by the feedback of an expert. The feedback is used to choose an appropriate basis for the representation of documents, while the clustering is performed in the projected space. The algorithms are evaluated on a dataset containing papers from computer science conferences. The results show that an appropriate choice of the representation basis can yield better performance with respect to the original vector space model.",,0-7695-2100-2,10.1109/WI.2004.10138,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1410827,,Search engines;Clustering algorithms;Feedback;Computer science;Frequency;Web search;Text processing;Navigation;Application software;Clustering methods,,,,1,,15,,4-Apr-05,,,IEEE,IEEE Conferences
Simple yet effective classification model for skewed text categorization,簡單而有效的分類模型，用於偏斜文本分類,M. Suhil; D. S. Guru; L. N. Raju; H. S. Gowda,"Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysuru, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysuru, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysuru, India; Department of Studies in Computer Science, University of Mysore, Manasagangotri, Mysuru, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",3-Nov-16,2016,,,904,910,"In this paper, the problem of skewness in text corpora for effective classification is addressed. A method of converting an imbalanced text corpus into a more or less balanced one is presented through an application of a classwise clustering algorithm. Further, to avoid curse of dimensionality, the chi-squared feature selection is employed. Nevertheless, each cluster of documents has been given a single vector representation by the use of a vector of interval-valued data which accomplishes a compact representation of text data thereby requiring a less memory for storage. A suitable symbolic classifier is used to match a query document against stored interval valued vectors. The superiority of the model has been demonstrated by conducting series of experiments on two benchmarking imbalanced corpora viz., Reuters-21578 and TDT2. In addition, a comparative analysis of the results of the proposal model versus that of the state of the art models on Reuters 21578 dataset indicates that the proposed model outperforms several contemporary models.",,978-1-5090-2029-4,10.1109/ICACCI.2016.7732160,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732160,Feature Selection;Skewed Text Data;Clustering;Symbolic Data Representation;Text Classification,Text categorization;Classification algorithms;Training;Clustering algorithms;Analytical models;Informatics;Computational modeling,data structures;pattern classification;pattern clustering;text analysis,classification model;skewed text categorization;text corpora;imbalanced text corpus;classwise clustering algorithm;chi-squared feature selection;document cluster;interval-valued data;symbolic classifier;query document matching;Reuters 21578 dataset,,3,,33,,3-Nov-16,,,IEEE,IEEE Conferences
Co-clustering Documents and Words Using Bipartite Isoperimetric Graph Partitioning,使用二等圖圖分區對文檔和單詞進行聚類,M. Rege; M. Dong; F. Fotouhi,"Wayne State University, USA; Wayne State University, USA; Wayne State University, USA",Sixth International Conference on Data Mining (ICDM'06),8-Jan-07,2006,,,532,541,"In this paper, we present a novel graph theoretic approach to the problem of document-word co-clustering. In our approach, documents and words are modeled as the two vertices of a bipartite graph. We then propose isoperimetric co-clustering algorithm (ICA) - a new method for partitioning the document-word bipartite graph. ICA requires a simple solution to a sparse system of linear equations instead of the eigenvalue or SVD problem in the popular spectral co-clustering approach. Our extensive experiments performed on publicly available datasets demonstrate the advantages of ICA over spectral approach in terms of the quality, efficiency and stability in partitioning the document-word bipartite graph.",2374-8486,978-0-7695-2701-7,10.1109/ICDM.2006.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053079,,Bipartite graph;Partitioning algorithms;Independent component analysis;Equations;Eigenvalues and eigenfunctions;Stability;Data mining;Random variables;Mutual information;Machine vision,eigenvalues and eigenfunctions;graph theory;pattern clustering;text analysis,bipartite isoperimetric graph partitioning;graph theory;document-word coclustering;isoperimetric coclustering;sparse system;linear equation;eigenvalue,,33,,26,,8-Jan-07,,,IEEE,IEEE Conferences
A novel approach for ontology-based dimensionality reduction for web text document classification,基於本體的Web文本文檔分類降維的新方法,M. K. Elhadad; K. Badran; G. I. Salama,"Computer department, Military Technical College, Cairo, Egypt; Computer department, Military Technical College, Cairo, Egypt; Computer department, Military Technical College, Cairo, Egypt",2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),29-Jun-17,2017,,,373,378,"Dimensionality reduction of feature vector size plays a vital role in enhancing the text processing capabilities; it aims in reducing the size of the feature vector used in the mining tasks (classification, clustering... etc.). This paper proposes an efficient approach to be used in reducing the size of the feature vector for web text document classification process. This approach is based on using WordNet ontology, utilizing the benefit of its hierarchal structure, to eliminate words from the generated feature vector that has no relation with any of WordNet lexical categories; this leads to the reduction of the feature vector size without losing information on the text. For mining tasks, the Vector Space Model (VSM) is used to represent text documents and the Term Frequency Inverse Document Frequency (TFIDF) is used as a term weighting method. The proposed ontology based approach was evaluated against the Principal component analysis (PCA) approach using several experiments. The experimental results reveal the effectiveness of our proposed approach against other traditional approaches to achieve a better classification accuracy, F-measure, precision, and recall.",,978-1-5090-5507-4,10.1109/ICIS.2017.7960021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960021,Dimensionality reduction;Principal component analysis;Feature Extraction;Feature Selection;WordNet;Ontology;web text documents classification;Semantic similarity;Vector Space Model;Term Frequency Inverse Document Frequency,Feature extraction;Ontologies;Classification algorithms;Semantics;Algorithm design and analysis;Principal component analysis;Databases,data mining;Internet;ontologies (artificial intelligence);pattern classification;principal component analysis;text analysis;vectors,PCA;principal component analysis;TFIDF;term frequency inverse document frequency;VSM;vector space model;WordNet lexical categories;WordNet ontology;mining tasks;feature vector;Web text document classification;ontology-based dimensionality reduction,,5,,33,,29-Jun-17,,,IEEE,IEEE Conferences
Topic model based behaviour modeling and clustering analysis for wireless network users,無線網絡用戶基於主題模型的行為建模和聚類分析,B. Leng; J. Liu; H. Pan; S. Zhou; Z. Niu,"National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China",2015 21st Asia-Pacific Conference on Communications (APCC),25-Feb-16,2015,,,410,415,"User behaviour analysis based on traffic log in wireless networks can be beneficial to many fields in real life: not only for commercial purposes, but also for improving network service quality and social management. We cluster users into groups marked by the most frequently visited websites to find their preferences. In this paper, we propose a user behaviour model based on Topic Model from document classification problems. We use the logarithmic TF-IDF (term frequency - inverse document frequency) weighing to form a high-dimensional sparse feature matrix. Then we apply LSA (Latent semantic analysis) to deduce the latent topic distribution and generate a low-dimensional dense feature matrix. K-means++, which is a classic clustering algorithm, is then applied to the dense feature matrix and several interpretable user clusters are found. Moreover, by combining the clustering results with additional demographical information, including age, gender, and financial information, we are able to uncover more realistic implications from the clustering results.",,978-4-8855-2301-4,10.1109/APCC.2015.7412547,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412547,traffic log;user behaviour modeling;clustering analysis;topic model,Clustering algorithms;Internet;Analytical models;Algorithm design and analysis;Sparse matrices;Mobile communication;Wireless networks,radio networks;telecommunication traffic,topic model based behaviour modeling;clustering analysis;wireless network users;user behaviour analysis;traffic log;network service quality;social management;logarithmic TF-IDF;term frequency-inverse document frequency weighing;high-dimensional sparse feature matrix;latent semantic analysis;low-dimensional dense feature matrix;interpretable user clusters,,3,,14,,25-Feb-16,,,IEEE,IEEE Conferences
Enhancing an Incremental Clustering Algorithm for Web Page Collections,增強網頁集合的增量聚類算法,G. Shaw; Y. Xu,NA; NA,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,9-Oct-09,2009,3,,81,84,"With the size and state of the Internet today, a good quality approach to organizing this mass of information is of great importance. Clustering web pages into groups of similar documents is one approach, but relies heavily on good feature extraction and document representation as well as a good clustering approach and algorithm. Due to the changing nature of the Internet, resulting in a dynamic dataset, an incremental approach is preferred. In this work we propose an enhanced incremental clustering approach to develop a better clustering algorithm that can help to better organize the information available on the Internet in an incremental fashion. Experiments show that the enhanced algorithm outperforms the original histogram based algorithm by up to 7.5%.",,978-0-7695-3801-3,10.1109/WI-IAT.2009.236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284938,Incremental clustering;Similarity Histogram Clustering;Web pages,Clustering algorithms;Web pages;Intelligent agent;Internet;Feature extraction;Histograms;Spatial databases;Conferences;Australia;Organizing,,,,,,6,,9-Oct-09,,,IEEE,IEEE Conferences
ICDAR 2019 Time-Quality Binarization Competition,ICDAR 2019時間質量二值化競賽,R. Dueire Lins; E. Kavallieratou; E. B. Smith; R. B. Bernardino; D. M. d. Jesus,Universidade Federal Rural de Pernambuco; University of the Eagian Mytilene; Boise State University; UFPE; UFPE,2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,1539,1546,"The ICDAR 2019 Time-Quality Binarization Competition assessed the performance of seventeen new together with thirty previously published binarization algorithms. The quality of the resulting two-tone image and the execution time were assessed. Comparisons were on both in ""real-world"" and synthetic scanned images, and in documents photographed with four models of widely used portable phones. Most of the submitted algorithms employed machine learning techniques and performed best on the most complex images. Traditional algorithms provided very good results at a fraction of the time.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00248,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978097,Binarization;Documents;Algorithms;Quality evaluation;Performance evaluation;Historical documents,Clustering algorithms;Machine learning algorithms;Image segmentation;Generators;Gray-scale;Feature extraction;Standards,document image processing;learning (artificial intelligence),ICDAR 2019 Time-Quality Binarization Competition;execution time;resulting two-tone image quality;machine learning techniques,,,,45,,3-Feb-20,,,IEEE,IEEE Conferences
Semi-supervised Clustering Using Bayesian Regularization,使用貝葉斯正則化的半監督聚類,Z. Xu; R. Akella; M. Ching; R. Tang,"Univ. of California, Santa Cruz; Univ. of California, Santa Cruz; NA; NA",Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007),31-Mar-08,2007,,,361,366,"Text clustering is most commonly treated as a fully automated task without user supervision. However, we can improve clustering performance using supervision in the form of pairwise (must-link and cannot-link) constraints. This paper introduces a rigorous Bayesian framework for semi-supervised clustering which incorporates human supervision in the form of pairwise constraints both in the expectation step and maximization step of the EM algorithm. During the expectation step, we model the pairwise constraints as random variables, which enable us to capture the uncertainly in constraints in a principled manner. During the maximization step, we treat the constraint documents as prior information, and adjust the probability mass of model distribution to emphasize words occurring in constraint documents by using Bayesian regularization. Bayesian conjugate prior modeling makes the maximization step more efficient than gradient search methods in the traditional distance learning. Experimental results on several text datasets demonstrate significant advantages over existing algorithms.",2375-9259,978-0-7695-3019-2,10.1109/ICDMW.2007.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476692,,Bayesian methods;Clustering algorithms;Humans;USA Councils;Computer aided instruction;Text mining;Partitioning algorithms;Data mining;Conferences;Random variables,Bayes methods;data mining;expectation-maximisation algorithm;pattern clustering;random processes;statistical distributions;text analysis,semisupervised clustering;Bayesian regularization;text clustering;must-link constraint;cannot-link constraint;pairwise constraints;EM algorithm;random variables;constraint documents;probability mass;model distribution;Bayesian conjugate prior modeling;data mining,,3,,12,,31-Mar-08,,,IEEE,IEEE Conferences
Clustering Synonymous English and Chinese Keywords for Cross-Language Queries,跨語言查詢的同義詞英漢同義聚類,R. Chen; C. Huang; Y. Huang,"Department of Information Management, Chaoyang University of Technology, Taichung 41349, Taiwan. E-MAIL: crching@mail.cyut.edu.tw; Computer Center, Chienkuo Technology University, Changhua 50094, Taiwan. E-MAIL: chungyi@ctu.edu.tw; Department of Computer Science and Information Engineering, Tunghai University, Taichung 40704, Taiwan. E-MAIL: ylhuang@thu.edu.tw",2007 International Conference on Machine Learning and Cybernetics,29-Oct-07,2007,4,,1875,1880,"In this paper, we propose an automatic clustering method to find synonymous terms including cross-language keywords from Chinese and English thesis documents. First, Chinese and English keyword pairs were collected from an existing database. Then, the system calculates the support and confidence values of the keyword pairs. Next, high confidence and support values are selected for keyword pairs. Subsequently, keyword pairs are merged by applying a clustering algorithm to various keyword pairs with similar meanings which are clustered into the same subset. Finally, effective applications can be applied based the subsets of collected words including cross-language or synonymous queries. The experimental results achieved 98.4% precision identifying correct terms from 1220 keyword pair clusters from the collected subsets. The primary experimental results show that the system can provide effective information for users when making queries online.",2160-1348,978-1-4244-0972-3,10.1109/ICMLC.2007.4370454,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370454,Synonymous terms;Keyword pairs;Cross-language;Keyword clustering,Data mining;Databases;Clustering algorithms;Internet;Abstracts;Machine learning;Cybernetics;Natural languages;Text categorization;Information management,natural language processing;pattern clustering;query processing;text analysis,synonymous keywords clustering;English keywords;Chinese keywords;cross-language queries;automatic clustering;keyword pairs,,1,,14,,29-Oct-07,,,IEEE,IEEE Conferences
The conversion from similarity for mobile life-log to euclidean distance,從移動生活日誌的相似性到歐氏距離的轉換,Ye-Teng An; Chung-Nam Pak; Zhi-Yong Feng; Sung-Nam Kim; Hyok-Chol Choe,"School of Computer Science, Tianjin University, China; School of Computer Science, Tianjin University, China; School of Computer Science, Tianjin University, China; Col. of Information Sci., Kim Chaek Univ. of Technol., D.P.R.K; Col. of Information Sci., Kim Chaek Univ. of Technol., D.P.R.K",2013 International Conference on Machine Learning and Cybernetics,8-Sep-14,2013,3,,1087,1091,"Mobile devices are widely used due to their convenience. However, information from Mobile devices always contains the semantic contents, which are hard to be analyzed by the Data Mining methods such as clustering. Therefore, some researchers calculate the relative similarity based on k-means clustering or association rules. However, without the consideration of features of non-Euclidean data, and the impossibility of converting the relative similarity to Euclidean space, the accuracy is low. In this paper, we calculate the similarity and Term Frequency Inverse Document Frequency (TF-IDF) based on analysis of mobile life log data, and then convert the similarity with the non-Euclidean features into the distance on the Euclidean space by pseudo-Euclidean embedding. As a result, we realize the data mining for non-Euclidean data. We apply the proposed method in collecting data that reflect the life-patterns of students from mobile devices. Experimental comparisons with real life-patterns of students show the feasibility and effectiveness of the proposed method.",2160-1348,978-1-4799-0260-6,10.1109/ICMLC.2013.6890755,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890755,Data Mining;Mobile Data;Similarity;pseudo-Euclidean embedding;TF-IDF;non-Euclidean,Eigenvalues and eigenfunctions;Abstracts;Batteries;Bluetooth;Global Positioning System;Digital audio players;Mobile communication,data mining;document handling;mobile computing;pattern clustering,mobile life-log;euclidean distance;mobile devices;semantic contents;data mining method;relative similarity;k-means clustering;association rules;nonEuclidean data;term frequency inverse document frequency;TF-IDF;mobile life log data;pseudo-Euclidean embedding,,,,14,,8-Sep-14,,,IEEE,IEEE Conferences
Temporal Anomaly Detection in Social Media,社交媒體中的時間異常檢測,J. Skryzalin; R. Field; A. Fisher; T. Bauer,"Sandia National Laboratories,Albuquerque,NM,87123; Sandia National Laboratories,Albuquerque,NM,87123; Sandia National Laboratories,Albuquerque,NM,87123; Sandia National Laboratories,Albuquerque,NM,87123",2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),16-Apr-20,2017,,,505,509,"In this work, we approach topic tracking and meme trending in social media with a temporal focus; rather than analyzing topics, we aim to identify time periods whose content differs significantly from normal. We detail two approaches. The first is an information-theoretic analysis of the distributions of terms emitted during each time period. In the second, we cluster the documents from each time period and analyze the tightness of each clustering. We also discuss a method of combining the scores created by each technique, and we provide ample empirical analysis of our methodology on various Twitter datasets.",2473-991X,978-1-4503-4993-2,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069127,,Twitter;Market research;Coherence;Classification algorithms;Computational modeling;Government,document handling;information theory;pattern clustering;security of data;social networking (online),topic tracking;meme trending;social media;temporal focus;information-theoretic analysis;temporal anomaly detection;document clustering;Twitter datasets,,,,17,,16-Apr-20,,,IEEE,IEEE Conferences
"Also by the same author: AKTiveAuthor, a citation graph approach to name disambiguation",也是由同一位作者：AKTiveAuthor提出的，一種用於消除歧義的引用圖方法,N. R. Shadbolt; D. M. Mcrae-spencer,"University of Southampton, Highfield, Southampton, UK; University of Southampton, Highfield, Southampton, UK",Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06),5-Mar-07,2006,,,53,54,"The desire for definitive data and the semantic Web drive for inference over heterogeneous data sources requires co-reference resolution to be performed on those data. In particular, name disambiguation is required to allow accurate publication lists, citation counts and impact measures to be determined. This paper describes a graph-based approach to author disambiguation on large-scale citation networks. Using self-citation, co-authorship and document source analyses, AKTiveAuthor clusters papers, achieving precision of 0.997 and recall of 0.818 over a test group of eight surname clusters",,1-59593-354-9,10.1145/1141753.1141762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4119097,metadata analysis;name disambiguation;self-citation,Computer science;Uniform resource locators;Permission;Semantic Web;Drives;Particle measurements;Large-scale systems;Text analysis;Automatic testing;Information systems,citation analysis;document handling;graph theory;information retrieval;pattern clustering;semantic Web,AKTiveAuthor;citation graph;name disambiguation;semantic Web;author disambiguation;large-scale citation networks;self-citation;document source analyses;surname clusters,,7,,4,,5-Mar-07,,,IEEE,IEEE Conferences
Data exchange technology between electronic documents and relation databases,電子文檔與關係數據庫之間的數據交換技術,A. A. Blazhko; S. Marulin; V. Kalashnikova,"Odessa National Polytechnic University, av. Shevchenko, 1, Odessa Ukraine; Odessa National Polytechnic University, av. Shevchenko, 1, Odessa Ukraine; Odessa National Polytechnic University, av. Shevchenko, 1, Odessa Ukraine",Proceedings of the 6th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems,10-Nov-11,2011,2,,624,628,A general conception of automated transfer of the content of electronic documents to the database of information system is presented. Technology of automated generation of templates of documents of XLS formats and mechanism of reverse transfer of factful data to the database of information system is developed.,,978-1-4577-1425-2,10.1109/IDAACS.2011.6072843,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072843,databases;electronic documents;clusterization,Databases;Clustering algorithms;Information systems;Finite element methods;Vectors;Classification algorithms;Filling,document handling;electronic data interchange;information systems;relational databases,data exchange technology;electronic documents;relation databases;automated transfer;information system;template generation;XLS formats;reverse transfer,,,,10,,10-Nov-11,,,IEEE,IEEE Conferences
Partial contour matching for document pieces with content-based prior,具有基於內容的先驗的文檔片段的局部輪廓匹配,F. Richter; C. X. Ries; S. Romberg; R. Lienhart,"Multimedia Computing and Computer Vision Lab, University of Augsburg, Germany; Multimedia Computing and Computer Vision Lab, University of Augsburg, Germany; Multimedia Computing and Computer Vision Lab, University of Augsburg, Germany; Multimedia Computing and Computer Vision Lab, University of Augsburg, Germany",2014 IEEE International Conference on Multimedia and Expo (ICME),8-Sep-14,2014,,,1,6,In this paper we present a method for aligning shredded document pieces based on outer contours and content-based prior information. Our approach relies on domain-specific knowledge that document pieces must complement each other when aligned correctly. Building on this intuition we propose a variant of MSAC (M-estimator SAmple Consensus) to estimate an hypothesis that recovers the spatial relationship between pairs of pieces. To do so we first approximate their boundaries by polygons from which we define consensus sets between fragments. Each consensus set provides multiple hypotheses for aligning one piece onto the other. An optimal hypothesis is identified by applying a two-stage procedure in which we discard locally inconsistent hypotheses before verifying the remainder for global consistency.,1945-788X,978-1-4799-4761-4,10.1109/ICME.2014.6890237,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890237,Document analysis;MSAC;partial contour matching,Shape;Geometry;Detectors;Approximation algorithms;Buildings;Heuristic algorithms;Clustering algorithms,approximation theory;content-based retrieval;document image processing;estimation theory;image matching,partial contour matching;shredded document piece alignment;content-based prior information;domain specific knowledge;MSAC;M-estimator sample consensus;spatial relationship recovery;polygon boundary approximation;optimal hypothesis estimation;global consistency,,4,,13,,8-Sep-14,,,IEEE,IEEE Conferences
A comparison of clustering methods for writer identification and verification,用於作者識別和驗證的聚類方法的比較,M. Bulacu; L. Schomaker,"Artificial Intelligence Inst., Groningen Univ., Netherlands; Artificial Intelligence Inst., Groningen Univ., Netherlands",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,1275,1279 Vol. 2,"An effective method for writer identification and verification is based on assuming that each writer acts as a stochastic generator of ink-trace fragments, or graphemes. The probability distribution of these simple shapes in a given handwriting sample is characteristic for the writer and is computed using a common codebook of graphemes obtained by clustering. In previous studies we used contours to encode the graphemes, in the current paper we explore a complementary shape representation using normalized bitmaps. The most important aim of the current work is to compare three different clustering methods for generating the grapheme codebook: k-means Kohonen SOM 1D and 2D. Large scale computational experiments show that the proposed method is robust to the underlying shape representation used (whether contours or normalized bitmaps), to the size of codebook used (stable performance for sizes from 10/sup 2/ to 2.5 /spl times/ 10/sup 3/) and to the clustering method used to generate the codebook (essentially the same performance was obtained for all three clustering methods).",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575747,,Clustering methods;Shape;Probability distribution;Robustness;Stochastic processes;Distributed computing;Large-scale systems;Forensics;Artificial intelligence;Databases,handwriting recognition;pattern clustering,clustering methods;writer identification;writer verification;stochastic generator;ink-trace fragments;graphemes;probability distribution;handwriting sample;shape representation;normalized bitmaps;grapheme codebook;k-means Kohonen SOM 1D;k-means Kohonen SOM 2D,,18,,14,,16-Jan-06,,,IEEE,IEEE Conferences
Application of Complex Adaptive Systems Theory to Ica Wine and Grape Cluster,複雜自適應系統理論在伊卡葡萄酒和葡萄集群中的應用,J. C. M. Alvarez,"Pontificia Universidad Cat籀lica del Per繳, Engineering Dept., Lima, Peru",PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology,15-Oct-07,2007,,,641,644,"The enterprises agglomerations are becoming an interesting study topic, so several authors are sure that the interactions and geographic proximity have a strong impact in the competition and innovation of the enterprises of the clusters (T. Altenburg and J. Meyer-Stamer), (P. Krugman, 1999), (M. Porter, 2004). So it is also possible to talk about ""learning clusters"", knowledge accumulated, and knowledge constructed in a collective form. For the study of these shapes of post-fordists production organizations and for the many variables that they present new frameworks are necessary, one of them is the complex adaptive systems theory. The model of the complex adaptive systems permits to evaluate multiple interactions between different agents and the impact of the agent's action in the system. This focus permits to find news and unexpected models from the interactions between the parts ofthe open systems. The present paper has the aim of studying: the dynamic interaction, the knowledge transfer, the learning, and the evolution of the Ica wine cluster in Peru with that model. In this context, the research questions are: what is the dynamic of the interactions between enterprises of the cluster?, how is the knowledge transferred?, how does the cluster learn?, and how is the technological evolution of this cluster? The methodology is the elaboration of a theoretical model that integrates the concepts of characteristics of the clusters and complex adaptive systems, then accordingly this theoretical model is elaborated a questionnaire, after that the questionnaire is applied to a wine and grape cluster, finally the main conclusions and recommendations are obtained. The results of the research will be an important document for the actions, strategic decisions and improvement of the interactions between cluster agents.",2159-5119,978-1-8908-4315-1,10.1109/PICMET.2007.4349380,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349380,,Adaptive systems;Independent component analysis;Pipelines;Technological innovation;Footwear;Companies;Knowledge transfer;Space technology;Shape;Production systems,adaptive systems;agriculture;knowledge management;organisational aspects;pattern clustering;strategic planning;wine industry,complex adaptive systems theory;lca wine;grape cluster;enterprises agglomerations;geographic proximity;enterprises competition;enterprises innovation;learning clusters;post-fordists production organizations;dynamic interaction;knowledge transfer;strategic decisions,,,,15,,15-Oct-07,,,IEEE,IEEE Conferences
Language model adaptation in speech recognition using document maps,使用文檔圖進行語音識別中的語言模型調整,K. Lagus; M. Kurimo,"Neural Networks Res. Centre, Helsinki Univ. of Technol., Finland; Neural Networks Res. Centre, Helsinki Univ. of Technol., Finland",Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing,7-Nov-02,2002,,,627,636,We present speech experiments that were carried out to evaluate a topically focusing language model in large vocabulary speech recognition. An ordered topical clustering is first computed as a self-organized mapping of a large document collection. Language models are then trained for each text cluster or for several neighboring clusters. The obtained organized collection of language models is efficiently utilized in continuous speech recognition to concentrate on the model that corresponds closest to the current topic of discussion. The speech recognition experiments are carried out on a novel Finnish speech database. A property of Finnish that is particularly challenging for speech recognition is the extremely fast vocabulary growth that makes many of the standard word-based language modeling methods impractical for large vocabulary tasks.,,0-7803-7616-1,10.1109/NNSP.2002.1030074,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1030074,,Natural languages;Adaptation model;Speech recognition;Vocabulary;Probability;Intelligent networks;Neural networks;Speech analysis;Databases;Ultraviolet sources,speech recognition;natural languages;text analysis;pattern clustering;self-organising feature maps;learning (artificial intelligence);statistical analysis;probability;parameter estimation,language model adaptation;speech recognition;document maps;topical clustering;self-organized mapping;document collection;text cluster;Finnish speech database;vocabulary growth,,,,16,,7-Nov-02,,,IEEE,IEEE Conferences
Forum Summarization Using Topic Models and Content-Metadata Sensitive Clustering,使用主題模型和內容元數據敏感聚類的論壇摘要,J. Krishnamani; Y. Zhao; R. Sunderraman,"Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA; Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA; Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA",2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),23-Dec-13,2013,3,,195,198,"The advent of the Internet and improvements in data sharing and storage, have resulted in an explosion of textual data. But, complete assimilation of such massive amounts of data in its raw form is a daunting task. Automated text mining methods such as text summarization present the user with a condensed version of data containing only key information. This is especially useful in the case of online user forums that contain a large number of posts spread out across several threads. Document summarization methods have been extensively studied and several methods have been developed in the recent past. This paper aims at developing a new method for automatic summarization of online forums by using topic models and content/metadata sensitive clustering.",,978-0-7695-5145-6,10.1109/WI-IAT.2013.182,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690727,document summarization;text mining;information retrieval;topic models,Message systems;Clustering algorithms;Conferences;Educational institutions;Abstracts;Joints;Mathematical model,data mining;pattern clustering;statistical analysis;text analysis;Web sites,forum summarization;topic models;content-metadata sensitive clustering;data sharing;data storage;textual data;data assimilation;text mining methods;text summarization;online user forums;document summarization methods,,2,,11,,23-Dec-13,,,IEEE,IEEE Conferences
Latent Dirichlet Allocation Based Semantic Clustering of Heterogeneous Deep Web Sources,基於潛在狄利克雷分配的異構深網源語義聚類,U. Noor; A. Daud; A. Manzoor,"Dept. of Comput. Sci. & Software Eng., Int. Islamic Univ., Islamabad, Pakistan; Dept. of Comput. Sci. & Software Eng., Int. Islamic Univ., Islamabad, Pakistan; Dept. of Comput. Sci. & Software Eng., Int. Islamic Univ., Islamabad, Pakistan",2013 5th International Conference on Intelligent Networking and Collaborative Systems,15-Oct-13,2013,,,132,138,"Over the years a critical increase in the mass of the web has been observed. Among that a large part comprises of online subject-specific databases, hidden behind query interface forms known as deep web. Existing search engines are unable to completely index this highly relevant information due to its large volume. To access deep web content, the research community has proposed to organize it using machine learning techniques. Clustering is one of the key solutions to organize the deep web databases. Existing clustering methods do not encounter semantic relevance among deep web forms. In this paper, we propose a novel method DWSemClust to cluster deep web databases based on the semantic relevance found among deep web forms by employing a generative probabilistic model Latent Dirichlet Allocation (LDA) for modeling content representative of deep web databases. A document comprises of multiple topics, the task of LDA is to cluster words present in the document into ""topics"". The purpose of the parameter estimation process in the underlying model is to discover the document's topic and tell about its proportionate distribution in documents. Deep web has a sparse topic distribution. Due to this reason we have proposed to use LDA that is supposed to be a good clustering solution for the sparse distribution of topics. Further we employ a rich set of metadata as our content representative that comprises of form contents (single attribute/ multiple attributes) and page contents. Experimental results show that our proposed method clearly outperforms the existing non-semantics based clustering methods.",,978-0-7695-4988-0,10.1109/INCoS.2013.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630398,Latent Dirichlet Allocation;Deep Web Mining;Soft Clustering;Topic Models;Semantics,Semantics;Entropy;Databases;Clustering algorithms;Resource management;Web sites;Vocabulary,Internet;learning (artificial intelligence);meta data;pattern clustering;statistical analysis,latent Dirichlet allocation;semantic clustering;heterogeneous deep Web sources;query interface form;search engines;machine learning techniques;deep Web database;semantic relevance;DWSemClust method;parameter estimation process;sparse topic distribution;meta data;form contents;page contents,,2,,21,,15-Oct-13,,,IEEE,IEEE Conferences
Pattern document weight discovery for text classification mining,用於文本分類挖掘的模式文檔權重發現,S. Brindha; K. Prabha; S. Sukumaran,"Department of Computer Science Erode Arts and Science College, Erode, Tamilnadu, India; Computer Science, Periyar University PG Extension Center, Dharmapuri, Tamilnadu, India; Department of Computer Science, Erode Arts and Science College, Erode, Tamilnadu, India",2016 International Conference on Communication and Electronics Systems (ICCES),30-Mar-17,2016,,,1,5,"The quality of discovered related features in text documents are describing based on user preferences. For the reason that of large scale terms and data patterns. Most existing popular text mining and classification methods have adopted term-based approaches. Most of the problems are occurred in polysemy and synonmy. Over the years, there has been repeatedly held the hypothesis that pattern-based methods should achieve better than term-based ones. Big challenge is how to effectively use large scale patterns vestiges a hard problem in text mining. In this paper, the robustness is used to discuss the characteristics of a model for describing its training sets is distorted or the application environment is altered. A new model robust if it still provides satisfactory performance regardless of having its training sets are altered or changed. To make a breakthrough in this challenging issue, this paper presents a pioneering model for weight feature discovery. It discovers both positive and negative patterns in text documents as at a higher level features and deploy them over low-level features. The terms also classify into categories and updates term weights depends on their specificity and their distributions in patterns. Significant experiments using this model on RCV1, TREC topics and Reuters-21578 significant experiments using this model on RCV1, TREC topics and Reuters-21578 demonstrate that the proposed model significantly outperforms both the state of the term-based methods and the pattern based methods.",,978-1-5090-1066-0,10.1109/CESYS.2016.7889833,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889833,Clustering;Classification;Pattern Mining;Term Frequency;Weights,Text categorization;Training;Taxonomy;Feature extraction;Classification algorithms;Text mining,data mining;pattern classification;pattern clustering;text analysis,clustering;Reuters-21578;TREC topics;RCV1;weight feature discovery;training sets;synonmy;polysemy;text classification methods;user preferences;text documents;text classification mining;data pattern document weight discovery,,1,,20,,30-Mar-17,,,IEEE,IEEE Conferences
Weighted-Gradient Features for Handwritten Line Segmentation,手寫線分割的加權梯度功能,V. Khare; P. Shivakumara; B. J. Navya; G. C. Swetha; D. S. Guru; U. Pal; T. Lu,"Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Studies in Computer Science, University of Mysore, Karnataka, India; Department of Studies in Computer Science, University of Mysore, Karnataka, India; Department of Studies in Computer Science, University of Mysore, Karnataka, India; Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, Kolkata, India; National Key Lab for Novel Software Technology, Nanjing University, Nanjing, China",2018 24th International Conference on Pattern Recognition (ICPR),29-Nov-18,2018,,,3651,3656,"Text line segmentation from handwritten documents is challenging when a document image contains severe touching. In this paper, we propose a new idea based on Weighted-Gradient Features (WGF) for segmenting text lines. The proposed method finds the number of zero crossing points for every row of Canny edge image of the input one, which is considered as the weights of respective rows. The weights are then multiplied with gradient values of respective rows of the image to widen the gap between pixels in the middle portion of text and the other portions. Next, k-means clustering is performed on WGF to classify middle and other pixels of text. The method performs morphological operation to obtain word components as patches for the result of clustering. The patches in both the clusters are matched to find common patch areas, which helps in reducing touching effect. Then the proposed method checks linearity and non-linearity iteratively based on patch direction to segment text lines. The method is tested on our own and standard datasets, namely, Alaei, ICDAR 2013 robust competition on handwriting context and ICDAR 2015-HTR, to evaluate the performance. Further, the method is compared with the state of art methods to show its effectiveness and usefulness.",1051-4651,978-1-5386-3788-3,10.1109/ICPR.2018.8546259,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546259,Zero crossing points;Gradient values;k-means clustering;Handwritten text line analysis;Text line segmentation,Image segmentation;Image edge detection;Linearity;Writing;Morphological operations;Handwriting recognition;Image restoration,document image processing;edge detection;feature extraction;handwritten character recognition;image classification;image matching;image segmentation;iterative methods;pattern clustering;text detection,k-means clustering;segment text lines;Canny edge image;Weighted-Gradient Features;document image;handwritten documents;text line segmentation;handwritten line segmentation,,,,14,,29-Nov-18,,,IEEE,IEEE Conferences
Query-Focused Multi-document Summarization Using Keyword Extraction,使用關鍵字提取的針對查詢的多文檔摘要,L. Ma; T. He; F. Li; Z. Gui; J. Chen,"Dept. of Comput. Sci., Huazhong Normal Univ., Wuhan; Dept. of Comput. Sci., Huazhong Normal Univ., Wuhan; Dept. of Comput. Sci., Huazhong Normal Univ., Wuhan; Dept. of Comput. Sci., Huazhong Normal Univ., Wuhan; Dept. of Comput. Sci., Huazhong Normal Univ., Wuhan",2008 International Conference on Computer Science and Software Engineering,22-Dec-08,2008,1,,20,23,"This paper proposes a strategy of the summary sentence selection for query-focused multi-document summarization through extracting keywords from relevant document set. It calculates the query related feature and the topic related feature for every word in relevant document set, then obtains the importance of the word by combining the two features. The score of candidate sentence is computed through the importance of words which they contains, and the modified MMR technology is used to adjust the score of the candidate sentence, then the candidate sentence with the highest score is selected as the summary sentence, till the length of the summary is enough. Experimental result shows that our method performs very well in DUC 2005 corpus and DUC 2006 corpus.",,978-0-7695-3336-0,10.1109/CSSE.2008.1323,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721681,multi-document summarization;keyword extraction;summary sentence selection,Data mining;Computer science;Software engineering;Computerized monitoring;Explosions;Web sites;Clustering algorithms;Algorithm design and analysis;Statistics;Robustness,query processing;text analysis,query-focused multidocument summarization;keyword extraction;summary sentence selection;query related feature;topic related feature,,3,,7,,22-Dec-08,,,IEEE,IEEE Conferences
An algorithm for automatic Web-page clustering using link structures,使用鏈接結構的自動網頁聚類算法,D. Mukhopadhyay; S. R. Sing,"Dept. of Comput. Sci. & Eng., St. Thomas Coll. of Eng. & Technol., Kolkata, India; NA","Proceedings of the IEEE INDICON 2004. First India Annual Conference, 2004.",22-Aug-05,2004,,,472,477,"Web contains a large collection of heterogeneous documents. As a result, finding set of related pages from Web is currently facing one of the most crucial problems. The low precision Web search engines like Excite, Alta Vista etc. coupled with the ranked list presentation make it harder for users to find the information they are looking for. In this paper, we have proposed a methodology to cluster related pages using co-citations without manual study and/or predefined categories. These clusters are used to classify random pages in the Universe.",,0-7803-8909-3,10.1109/INDICO.2004.1497798,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1497798,,Clustering algorithms;Web pages;Search engines;Web search;Computer science;Ink;Text categorization;Large-scale systems;Educational institutions;Motion pictures,search engines;citation analysis;Internet;text analysis,automatic Web-page clustering;link structure;heterogeneous document;precision Web search engine;ranked list presentation;cocitation,,2,,14,,22-Aug-05,,,IEEE,IEEE Conferences
Adapted Clustering Based on Maximal Spanning Tree in Collaborative Editing Systems,協同編輯系統中基於最大生成樹的自適應聚類,G. Wang; B. Jiang,"Zhejiang Gongshang Univ., Hangzhou; Zhejiang Gongshang Univ., Hangzhou",2007 International Conference on Convergence Information Technology (ICCIT 2007),7-Jan-08,2007,,,470,475,"Collaborative applications across mobile end and immobile end became more and more popular recently; however, the control over user interface became more intricate or flexible. The clustering of editing objects could give users not only a clear document organization, but also a smart management of operation authority. In this paper we presents an adapted clustering algorithm based on maximal spanning tree that could adjust user power and interface dynamically. The algorithm reduced the load of designer and promoted the upgrade of system. It had been tested in our prototype and proved to be valuable for the application development in heterogeneous collaborative editing systems.",,0-7695-3038-9,10.1109/ICCIT.2007.274,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4420304,,Collaboration;Clustering algorithms;Personal digital assistants;Educational institutions;User interfaces;Heuristic algorithms;Algorithm design and analysis;System testing;Prototypes;Collaborative work,groupware;text editing;trees (mathematics);user interfaces,adapted clustering;maximal spanning tree;user interface;editing objects;document organization;operation authority;heterogeneous collaborative editing system,,,,12,,7-Jan-08,,,IEEE,IEEE Conferences
Soft Geodesic Kernel K-Means,軟測地線K均值,J. Kim; K. Shim; S. Choi,"Digital Content Research Division, ETRI, Korea. jh.kim@etri.re.kr; Digital Content Research Division, ETRI, Korea. shimkh@etri.re.kr; Department of Computer Science, POSTECH, Korea. seunglin@postech.ac.kr","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",4-Jun-07,2007,2,,II-429,II-432,"In this paper we present a kernel method for data clustering, where the soft k-means is carried out in a feature space, instead of input data space, leading to soft kernel k-means. We also incorporate a geodesic kernel into the soft kernel k-means, in order to take the data manifold structure into account. The method is referred to as soft geodesic kernel k-means. In contrast to k-means, our method is able to identify clusters that are not linearly separable. In addition, soft responsibilities as well as geodesic kernel, improve the clustering performance, compared to kernel k-means. Numerical experiments with toy data sets and real-world data sets (UCI and document clustering), confirm the useful behavior of the proposed method.",2379-190X,1-4244-0727-3,10.1109/ICASSP.2007.366264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4217437,Pattern clustering methods;pattern classification;unsupervised learning,Kernel;Clustering algorithms;Partitioning algorithms;Computer science;Pattern clustering;Machine learning algorithms;Unsupervised learning;Pattern recognition;Machine learning;Data mining,data handling;differential geometry,soft geodesic kernel k-means;data clustering;data manifold structure;toy data sets;real-world data sets;UCI;document clustering,,15,,10,,4-Jun-07,,,IEEE,IEEE Conferences
Using relation similarity on open information extraction-based event template extraction,在基於開放信息提取的事件模板提取中使用關係相似性,A. Romadhony; D. H. Widyantoro; A. Purwarianti,"School of Electrical Engineering and Informatics Bandung, Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics Bandung, Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics Bandung, Institute of Technology, Bandung, Indonesia",2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS),9-Mar-17,2016,,,341,346,"Automatic template extraction has been studied intensively in order to perform information extraction without predefined template. Several existing studies utilized the similar preprocessing techniques which are applied in Open Information Extraction (Open IE) paradigm system. We investigate the use of Open IE results to build the automatic event template extraction. In this study, we adapt the clustering based approach for template extraction, and propose to add the relation similarity information in the clustering function. We compare the clusters quality of the Open IE based system and non-Open IE based system and also with the use of relation similarity function using document classification metric. The experimental result shows that the performance of Open IE based system is comparable with the non-Open IE based system and the relation similarity information is able to improve the clusters quality.",,978-1-5090-4629-4,10.1109/ICACSIS.2016.7872791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872791,Open Information Extraction;Event Template Extraction;Relation Similarity;Cluster Evaluation;phrase-to-phrase,Information retrieval;Data mining;Filtering;Clustering methods;Data preprocessing;Semantics;Training data,classification;document handling;pattern clustering,document classification metric;clustering based approach;Open IE paradigm system;automatic template extraction;event template extraction;open information extraction;relation similarity,,,,15,,9-Mar-17,,,IEEE,IEEE Conferences
A Rough Concept Recognition Approach for Information Retrieval Based on Latent Semantic Analysis,基於潛在語義分析的信息檢索粗糙概念識別方法,Y. Wang; Y. Guo; L. Li,"Center for intelligence science and technology research, Beijing University of Posts and Telecommunications, Beijing 100876, China, soapjoy@gmail.com; Center for intelligence science and technology research, Beijing University of Posts and Telecommunications, Beijing, 100876, China, guoyanhui@m165.com; Center for intelligence science and technology research, Beijing University of Posts and Telecommunications, Beijing 100876, China, lilei@nlu.caai.cn",2007 International Conference on Natural Language Processing and Knowledge Engineering,29-Oct-07,2007,,,90,95,"This paper presents an information retrieval approach which uses a rough concept clustering in conjunction with Latent Semantic Analysis(LSA) to provide better document retrieval results matched to queries. The conceptual context defined in this article can be local, so no domain expert has to be involved in this approach. Our experiment consists of word clustering by similarity and rough concept recognition, associated to a basic LSA retrieval system. Our information retrieval process is illustrated through our experimentation model and results are compared in two different aspects. Experiment results show that retrieval performance benefit can be gained from this approach and further performance benefits can also be obtained according to the further work, which needs researching about parameter settings and algorithm development.",,978-1-4244-1610-3,10.1109/NLPKE.2007.4368016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4368016,Latent Semantic Analysis(LSA);Concept Clustering Information Retrieval(IR),Information retrieval;Information analysis;Content based retrieval;Clustering algorithms;Target recognition;Ontologies;Partitioning algorithms;Performance analysis;Clustering methods;Matrix decomposition,information retrieval;pattern clustering;word processing,information retrieval approach;rough concept recognition approach;latent semantic analysis;word clustering,,,,13,,29-Oct-07,,,IEEE,IEEE Conferences
Hierarchical Clustering of Large-Scale Short Conversations Based on Domain Ontology,基於領域本體的大規模短會話層次聚類,Y. Wang; B. Guo,"Inf. Syst. & Manage. Sch., Nat. Univ. of Defense Technol., Changsha, China; Inf. Syst. & Manage. Sch., Nat. Univ. of Defense Technol., Changsha, China",2008 International Symposium on Computer Science and Computational Technology,30-Dec-08,2008,1,,126,130,"With the rapid development of the Internet and communication technology, huge data is accumulated. Short text such as conversation in chatting room and email is common in such data. It is useful to cluster such short documents to get the structure of the data or to help building other data mining applications. But most of the current clustering algorithms can not get acceptable clustering accuracy since key words appear with a low frequency in short documents. It is also difficult to process high-dimensional text data in very large databases. In this paper, we propose a hierarchical clustering algorithm which uses domain ontology to improve clustering accuracy. This clustering algorithm is also parallel and frequent-concept based which makes it scalable to very large high-dimensional text data. Our experimental study shows that this algorithm is more accurate than other hierarchical clustering algorithms when clustering short conversations. Furthermore, this algorithm has good scalability and it can be used to process even huge data.",,978-1-4244-3746-7,10.1109/ISCSCT.2008.210,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731390,Hierarchical Clustering;Short Conversations;Domain Ontology,Large-scale systems;Ontologies;Clustering algorithms;Internet;Communications technology;Buildings;Data mining;Frequency;Databases;Scalability,data mining;electronic mail;ontologies (artificial intelligence);pattern clustering;text analysis,hierarchical clustering;large-scale short conversations;domain ontology;Internet;chatting room;email;data mining;very large databases;text data,,,,18,,30-Dec-08,,,IEEE,IEEE Conferences
Integration of cluster ensemble and text summarization for gene expression analysis,集成群集集成和文本摘要以進行基因表達分析,Xiaohua Hu,"Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA",Proceedings. Fourth IEEE Symposium on Bioinformatics and Bioengineering,26-Jul-04,2004,,,251,258,"Generating high quality gene clusters and identifying the underlying biological mechanism of the gene cluster are the important goals of clustering gene expression analysis. To get high quality cluster results, most of the current approaches rely on choosing the best cluster algorithm whose design biases and assumptions meet the underlying distribution of the data set. There are two issues for this approach: (1) usually the underlying data distribution of the gene expression data sets is unknown, and (2) there are so many clustering algorithms available and it is very challenging to choose the proper one. To provide a textual summary of the gene clusters, the most explored approach is the extractive approach that essentially builds upon techniques borrowed from the information retrieval, in which the objective is to provide terms to be used for query expansion, and not to act as a stand alone summary for the entire document sets. Another drawback is that the clustering quality and cluster interpretation are treated as two isolated research problems and are studied separately. But cluster quality and cluster interpretation are closely related and must be addressed in a coherent and unified way. It is essential to have relatively high quality clusters first, in order to get a correct, informative biological explanation of the gene cluster, otherwise, the biological explanation will be incorrect or misleading, no matter how good or robust the text summarization technique is. Based on this consideration, we design and develop a unified system GE-Miner (gene expression miner) to address these challenging issues in a principled and general manner by integrating cluster ensemble and text summarization and provide an environment for comprehensive gene expression data analysis. Experimental results demonstrate that our system can obtain high quality clusters and provide concise and informative textual summary for the gene clusters.",,0-7695-2173-8,10.1109/BIBE.2004.1317351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317351,,Gene expression;Clustering algorithms;Algorithm design and analysis;Data mining;Information analysis;Information retrieval;Data analysis;Genomics;Bioinformatics;Educational institutions,genetics;molecular biophysics;biology computing;query formulation;data mining;DNA;pattern clustering,cluster ensemble integration;text summarization;gene expression analysis;information retrieval;query expansion;cluster quality;cluster interpretation;GE-Miner;gene expression miner,,5,,34,,26-Jul-04,,,IEEE,IEEE Conferences
Application of immune network metaphor to keyword map-based topic stream visualization,免疫網絡隱喻在基於關鍵詞圖的主題流可視化中的應用,Y. Takama; T. Hori,"Tokyo Metropolitan Inst. of Technol., Japan Sci. & Technol. Corp., Tokyo, Japan; NA",Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No.03EX694),18-Aug-03,2003,2,,770,775 vol.2,"A method to find topic distribution from a sequence of document sets is proposed. As the Web becomes one of the most important information resources for us, the interaction between a human and a Web interface should be considered from various viewpoints, besides that of document retrieval. In this paper, we focus on the extraction and visualization of topic distribution over the Web. The proposed clustering method employs the immune network model, in which the property of memory cell is used to find the topical relation among document sets. The effectiveness of the proposed method is also shown by applying it to two sequences of online news articles. Furthermore, keyword map-based information visualization system is developed to visualize the topic streams found from a sequence of document sets.",,0-7803-7866-0,10.1109/CIRA.2003.1222278,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222278,,Visualization;Clustering methods;Plastics;Immune system;Information resources;Humans;Information retrieval;Biological system modeling;Data mining;HTML,pattern clustering;information retrieval;information resources;Web sites;data visualisation;user interfaces,immune network metaphor;keyword map based topic stream visualization;topic distribution;document sets;information resources;Web interface;document retrieval;clustering method;immune network model;memory cell,,6,,12,,18-Aug-03,,,IEEE,IEEE Conferences
An Ontology-Based Text-Mining Method to Cluster Proposals for Research Project Selection,基於本體的文本挖掘方法對研究項目選擇的提案進行聚類,J. Ma; W. Xu; Y. Sun; E. Turban; S. Wang; O. Liu,"Department of Information Systems, City University of Hong Kong, Kowloon, Hong Kong; School of Information, Renmin University of China, Beijing, China; School of Management, Xi'an Jiaotong University, Xi'an, China; Department of Information Systems, City University of Hong Kong, Kowloon, Hong Kong; Institute of Systems Science, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China; School of Accounting and Finance, The Hong Kong Polytechnic University, Kowloon , Hong Kong","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",16-Apr-12,2012,42,3,784,790,"Research project selection is an important task for government and private research funding agencies. When a large number of research proposals are received, it is common to group them according to their similarities in research disciplines. The grouped proposals are then assigned to the appropriate experts for peer review. Current methods for grouping proposals are based on manual matching of similar research discipline areas and/or keywords. However, the exact research discipline areas of the proposals cannot often be accurately designated by the applicants due to their subjective views and possible misinterpretations. Therefore, rich information in the proposals' full text can be used effectively. Text-mining methods have been proposed to solve the problem by automatically classifying text documents, mainly in English. However, these methods have limitations when dealing with non-English language texts, e.g., Chinese research proposals. This paper presents a novel ontology-based text-mining approach to cluster research proposals based on their similarities in research areas. The method is efficient and effective for clustering research proposals with both English and Chinese texts. The method also includes an optimization model that considers applicants' characteristics for balancing proposals by geographical regions. The proposed method is tested and validated based on the selection process at the National Natural Science Foundation of China. The results can also be used to improve the efficiency and effectiveness of research project selection processes in other government and private research funding agencies.",1558-2426,,10.1109/TSMCA.2011.2172205,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171866,Clustering analysis;decision support systems;ontology;research project selection;text mining,Proposals;Ontologies;Vectors;Clustering algorithms;Humans;Educational institutions;Sun,data mining;government data processing;ontologies (artificial intelligence);optimisation;pattern classification;pattern clustering;pattern matching;project management;research and development;text analysis,government agency;private research funding agency;manual matching;text document classification;English;nonEnglish language text;ontology-based text mining approach;Chinese text;optimization model;National Natural Science Foundation;China;research project selection process;research proposal clustering;grouping proposals,,37,,45,,19-Mar-12,,,IEEE,IEEE Journals
Spatio-temporal Clustering for Grouping in Online Handwriting Document Layout Analysis with GRU-RNN,基於GRU-RNN的在線手寫文檔佈局分析中的分組時空聚類,S. Polotskyi; O. Radyvonenko; I. Degtyarenko; I. Deriuga,"Samsung R&D Institute Ukraine (SRK),Kyiv,Ukraine; Samsung R&D Institute Ukraine (SRK),Kyiv,Ukraine; Samsung R&D Institute Ukraine (SRK),Kyiv,Ukraine; Samsung R&D Institute Ukraine (SRK),Kyiv,Ukraine",2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR),25-Nov-20,2020,,,276,281,"One of the document analyzer's essential functions is dividing input documents into stroke groups that belong to the particular type of handwriting. In this paper, we analyze existing approaches for clustering input strokes of the handwritten document and present a new one that establishes a novel state of the art result. It is the first approach where a grouping of input strokes is considered as a clustering problem and standard metrics for it are provided. This research could be used as a baseline for further ones in the clustering of handwritten documents.",,978-1-7281-9966-5,10.1109/ICFHR2020.2020.00058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257742,Layout analysis;Clustering;Grouping,Iron;Layout;Feature extraction;Indexes;Clustering algorithms;Text analysis;Task analysis,,,,,,18,,25-Nov-20,,,IEEE,IEEE Conferences
Similarity Algorithm Based on Weighted Hierarchical Structure of XML Document,基於XML文檔加權層次結構的相似性算法。,X. Sun; H. Cheng; H. Wang,"Sch. of Comput. Sci., Changshu Inst. of Technol., Changshu, China; Sch. of Comput. Sci., Changshu Inst. of Technol., Changshu, China; Dept. of Comput. Sci., Hubei Univ. of Educ., Wuhan, China",2009 WASE International Conference on Information Engineering,21-Aug-09,2009,2,,143,145,"A similarity algorithm based on weighted hierarchical structure of XML document is brought forward. The algorithm can calculate the similarity among XML documents efficiently according to hierarchical structure. It can be powerful enough to distinguish the similar structural documents. Experimental results prove that the algorithm reduces the complexity and has fairly high performance. The approach presented in this paper can be used in many applications, such as clustering, structural extracting and change checking of XML documents, etc.",,978-0-7695-3679-8,10.1109/ICIE.2009.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211452,XML;similarity;hierarchical structure,XML;Computer science;Data mining;Sun;Clustering algorithms;Application software;Computer science education;Internet;Web sites;Information retrieval,XML,similarity algorithm;weighted hierarchical structure;XML document,,,,5,,21-Aug-09,,,IEEE,IEEE Conferences
Two-layer classification and distinguished representations of users and documents for grouping and authorship identification,用戶和文檔的兩層分類和可區分的表示形式，用於分組和作者身份識別,H. Mohtasseb; A. Ahmed,"School of Computer Science, University of Lincoln, UK; School of Computer Science, University of Lincoln, UK",2009 IEEE International Conference on Intelligent Computing and Intelligent Systems,28-Dec-09,2009,1,,651,657,"Most studies on authorship identification reported a drop in the identification result when the number of authors exceeds 20-25. In this paper, we introduce a new user representation to address this problem and split classification across two layers. There are at least 3 novelties in this paper. First, the two-layer approach allows applying authorship identification over larger number of authors (tested over 100 authors), and it is extendable. The authors are divided into groups that contain smaller number of authors. Given an anonymous document, the primary layer detects the group to which the document belongs. Then, the secondary layer determines the particular author inside the selected group. In order to extract the groups linking similar authors, clustering is applied over users rather than documents. Hence, the second novelty of this paper is introducing a new user representation that is different from document representation. Without the proposed user representation, the clustering over documents will result in documents of author(s) distributed over several clusters, instead of a single cluster membership for each author. Third, the extracted clusters are descriptive and meaningful of their users as the dimensions have psychological backgrounds. For authorship identification, the documents are labelled with the extracted groups and fed into machine learning to build classification models that predicts the group and author of a given document. The results show that the documents are highly correlated with the extracted corresponding groups, and the proposed model can be accurately trained to determine the group and the author identity.",,978-1-4244-4754-1,10.1109/ICICISYS.2009.5357676,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357676,authorship identification;similarity detection;personal blogs;users lexicon and representation;keywords extraction,Power system restoration;Power system simulation;Power system modeling;Genetic engineering;Power engineering and energy;Power system analysis computing;Genetic algorithms;Simulated annealing;Power generation;Marine vehicles,document handling;learning (artificial intelligence),two-layer classification;authorship identification;user representation;document representation;cluster membership;machine learning,,4,,25,,28-Dec-09,,,IEEE,IEEE Conferences
An Extractive Multi-document Summarization Technique Based on Fuzzy Logic Approach,基於模糊邏輯方法的多文檔提取摘要技術,E. S. L. Tsoumou; L. Lai; S. Yang; M. L. Varus,"Sch. of Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Inf. Eng., Wuhan Univ. of Technol., Wuhan, China",2016 International Conference on Network and Information Systems for Computers (ICNISC),12-Jun-17,2016,,,346,351,With the expansion of World Wide Web services due to the growing explosion of information during these recent years automatic summarization has become primordial to provide efficient mechanisms to resume and present effective textual information. This technology can summarize multiple or single documents to get a summary. In this paper we develop method based on Fuzzy ontology extraction technique. We focus this research only in the purpose to introduce a Multi-Document Summarization Technique for Gabonese Documents. And in the purpose to realize a comparative study between our general approach and current techniques we adopt Timestamp as the comparative method. This research approach successfully generates a summary with greater information.,,978-1-4673-8838-2,10.1109/ICNISC.2016.081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946008,Gabonese documents;extraction;abstraction;multi-documents summarization;Fuzzy,Feature extraction;Ontologies;Fuzzy logic;Data mining;Engines;Clustering algorithms;Mathematical model,fuzzy logic;Internet;natural language processing;ontologies (artificial intelligence);text analysis,extractive multidocument summarization technique;fuzzy logic approach;World Wide Web services;automatic summarization;textual information;fuzzy ontology extraction technique;Gabonese documents;timestamp,,,,7,,12-Jun-17,,,IEEE,IEEE Conferences
Assessment of thresholding algorithms for document processing,評估文檔處理的閾值算法,B. Sankur; A. T. Abak; U. Baris,"Dept. of Electr. & Electron. Eng., Bogazici Univ., Istanbul, Turkey; NA; NA",Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348),6-Aug-02,1999,1,,580,584 vol.1,"The thresholding technique used can critically affect the performance of subsequent operations such as page segmentation and character recognition. A taxonomy of thresholding methods has been proposed where the major categories are listed as entropy-based, histogram shape-based, object attribute-based, clustering based, adaptive, and object similarity-based. Furthermore a comprehensive performance evaluation of thresholding algorithms in the context of document analysis and character recognition systems has been performed. A large class of thresholding algorithms have been comparatively evaluated using shape distortion, false alarm and missprobability, and edge discrepancy measures. Both simulated documents and bitmaps of ground-truthed documents are used.",,0-7803-5467-2,10.1109/ICIP.1999.821696,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821696,,Clustering algorithms;Character recognition;Shape measurement;Taxonomy;Histograms;Text analysis;Performance analysis;Algorithm design and analysis;Performance evaluation;Distortion measurement,software performance evaluation;document image processing;character recognition;image recognition,thresholding technique;document processing;thresholding methods;performance evaluation;document analysis;character recognition,,4,,44,,6-Aug-02,,,IEEE,IEEE Conferences
Application of Webpage Optimization for Clustering System on Search Engine V Google Study,網頁優化在搜索引擎V集群系統中的應用Google Study,T. F. Lin; Y. P. Chi,"Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan","2014 International Symposium on Computer, Consumer and Control",30-Jun-14,2014,,,698,701,"As of March 2012, there are over six hundred and forty million active websites [5], amid such immense data in the ocean of network; user's browsing behavior to retrieve information will affect the level of webpage exposure directly. Benjamin Edelman, Michael Ostrovsky and Michael Schwarz reckoned that once obtained a more advanced ranking on search engine, one can obtain higher click through rate [1]. The objective of this study - ?Application of webpage optimization for clustering system on search engine - Google study??is to utilize the technologies of TF-IDF, K-means clustering and indexing quality examination to identify the combination of key words that will benefit search engine optimization. The study demonstrated that it can effectively enhance the website's advancement of ranking on search engine, increase website's exposure level and click through rate.",,978-1-4799-5277-9,10.1109/IS3C.2014.186,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845978,data mining;search engine;SEO;EC;eMarketing,Search engines;Optimization;Google;Couplings;Context;HTML;Testing,data mining;indexing;information retrieval;pattern clustering;search engines;statistical analysis;Web sites,Web page optimization;Google study;user browsing behavior;k-means clustering system;indexing quality examination;TF-IDF technology;search engine optimization;Web site exposure level;click through rate;information retrieval;term frequency inverse document frequency technology;data mining,,1,,6,,30-Jun-14,,,IEEE,IEEE Conferences
Topical Cluster Discovery in Semistructured Healthcare Data,半結構化醫療數據中的主題集群發現,G. Costa; R. Ortale,"ICAR, Rende, Italy; ICAR, Rende, Italy",2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI),13-Jan-19,2018,,,772,775,"We propose an approach to clustering XML-based corpora of healthcare documents by their latent topic similarity. Our approach is a two-step process. Initially, the latent topic distributions of the input healthcare documents are inferred, by performing collapsed Gibbs sampling and parameter estimation under an XML topic model. Subsequently, the inferred distributions are grouped through established clustering techniques.",,978-1-5386-7325-6,10.1109/WI.2018.00014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8609689,Clustering;Healthcare Data,XML;Medical services;Adaptation models;Probability distribution;Vegetation;Probabilistic logic;Random variables,health care;medical information systems;parameter estimation;pattern clustering;XML,established clustering techniques;topical cluster discovery;semistructured healthcare data;XML-based corpora;latent topic similarity;two-step process;latent topic distributions;input healthcare documents;parameter estimation;XML topic model;inferred distributions,,,,35,,13-Jan-19,,,IEEE,IEEE Conferences
Locality Sensitive Hashing based incremental clustering for creating affinity groups in Hadoop ??HDFS - An infrastructure extension,基於局部敏感哈希的增量集群，用於在Hadoop HDFS中創建相似性組-基礎結構擴展,A. Kala Karun; K. Chitharanjan,"Department of Computer Science and Engineering, Sree Chithra Thirunal College of Engineering, Thiruvananthapuram, Kerala, India; Department of Computer Science and Engineering, Sree Chithra Thirunal College of Engineering, Thiruvananthapuram, Kerala, India","2013 International Conference on Circuits, Power and Computing Technologies (ICCPCT)",13-Jun-13,2013,,,1243,1249,"Apache's Hadoop is an open source framework for large scale data analysis and storage. It is an open source implementation of Google's Map/Reduce framework. It enables distributed, data intensive and parallel applications by decomposing a massive job into smaller tasks and a massive data set into smaller partitions such that each task processes a different partition in parallel. Hadoop uses Hadoop distributed File System (HDFS) which is an open source implementation of the Google File System (GFS) for storing data. Map/Reduce application mainly uses HDFS for storing data. HDFS is a very large distributed file system that assumes commodity hardware and provides high throughput and fault tolerance. HDFS stores files as a series of blocks and are replicated for fault tolerance. The default block placement strategy doesn't consider the data characteristics and places the data blocks randomly. Customized strategies can improve the performance of HDFS to a great extend. Applications using HDFS require streaming access to the files and if the related files are placed in the same set of data nodes, the performance can be increased. This paper is discussing about a method for clustering streaming data to the same set of data nodes using the technique of Locality Sensitive Hashing. The method utilizes the compact bitwise representation of document vectors called fingerprints created using the concept of Locality Sensitive Hashing to increase the data processing speed and performance. The process will be done without affecting the default fault tolerant properties of Hadoop and requires only minimal changes to the Hadoop framework.",,978-1-4673-4922-2,10.1109/ICCPCT.2013.6528999,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528999,Hadoop;HDFS;Fingerprint;Locality Sensitive Hashing,,data analysis;data structures;distributed databases;document handling;network operating systems;pattern clustering;public domain software;random processes;software fault tolerance,locality sensitive hashing;incremental clustering;affinity group creation;open source framework;large scale data analysis;large scale data storage;Google MapReduce framework;data intensive applications;parallel applications;distributed applications;massive data set;Hadoop distributed file system;Google file system;GFS;large distributed file system;fault tolerance;default block placement strategy;random data block placement;customized strategies;HDFS performance improvement;streaming file access;data nodes;streaming data clustering;bitwise document vector representation;fingerprints;data processing speed;data processing performance,,2,,20,,13-Jun-13,,,IEEE,IEEE Conferences
A text clustering algorithm based on category resolve power,基於類別分辨能力的文本聚類算法,Faguo Zhou; Fan Zhang; Bingru Yang,"School of Mechanical Electronic & Information Engineering, China University of Mining & Technology, Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining & Technology, Beijing, China; School of Information Engineering, University of Science & Technology Beijing, China",The 2nd International Conference on Information Science and Engineering,17-Jan-11,2010,,,3494,3497,"As an unsupervised machine learning technology, document clustering has been widely used in many fields, such as Information Retrieval (IR) and Text Categorization (TC). But, because of the bag of words used in document clustering as document index, the feature space of corpus must be high dimension space. This problem makes a negative effect to the efficiency and precision of text clustering. Based on category resolve power, a new feature selection function is constructed. Through integration between this function and document clustering algorithm, a high-powered text clustering algorithm is presented. Experiments on a universal corpus show that it has a good performance.",2160-1291,978-1-4244-7618-3,10.1109/ICISE.2010.5690994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690994,text clustering;feature selection;dimension reduction;result evaluaton,Clustering algorithms;Algorithm design and analysis;Classification algorithms;Partitioning algorithms;Text categorization;Machine learning;Accuracy,,,,,,5,,17-Jan-11,,,IEEE,IEEE Conferences
"A Fast, Feature-based Cluster Algorithm for Information Retrieval",快速的基於特徵的信息檢索聚類算法,M. Mehlitz; C. Bauckhage; S. Albayrak,"Technical University Berlin, DAI-Lab, Berlin, Germany. martin.mehlitz@dai-labor.de; Deutsche Telekom Laboratories, Berlin, Germany. christian.bauckhage@telekom.de; Technical University Berlin, DAI-Lab, Berlin, Germany. sahin.albayrak@dai-labor.de",2007 IEEE International Conference on Information Reuse and Integration,4-Sep-07,2007,,,335,341,"The Internet is a vast resource of information. Unfortunately, finding and accessing this information is often a very cumbersome task even with existing information platforms. Searching on the WWW suffers from the fact that almost every word is ambiguous to a certain degree in the information-rich environment of the Internet. Clustering search results is a way to solve this problem. This paper introduces a novel, fast way to cluster documents based on frequent term sets.",,1-4244-1499-7,10.1109/IRI.2007.4296643,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296643,,Clustering algorithms;Information retrieval;Internet;Search engines;Laboratories;World Wide Web;Clustering methods;Matrix decomposition;Singular value decomposition;Web pages,information retrieval;Internet;pattern clustering,information retrieval;Internet;document clustering,,,,19,,4-Sep-07,,,IEEE,IEEE Conferences
Automated cyberbullying detection using clustering appearance patterns,使用聚類外觀模式自動進行網絡欺凌檢測,W. Romsaiyud; K. na Nakornphanom; P. Prasertsilp; P. Nurarak; P. Konglerd,"School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand; School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand; School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand; School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand; School of Science and Technology, Sukhothai Thammathirat Open University, Chaengwattana Rd., Bangpood, Pakkret, Nonthaburi, 11120, Thailand",2017 9th International Conference on Knowledge and Smart Technology (KST),27-Mar-17,2017,,,242,247,Cyberbullying is an activity of sending threatening messages to insult person. To prevent cyber victimization from the activity is challenging. This paper enhanced the Na簿ve Bayes classifier for extracting the words and examining loaded pattern clustering. The algorithm included two main methods: (1) creating partitions by iteratively relocating from entire datasets into clusters using k-mean clustering and (2) capturing any specific partition with the frequency of words with multinomial model feature vector and drawing the probability of words occurring in a document for predicting the eight classes. The proposed method resulted in increasing accuracy and reliability of an experiment.,,978-1-4673-9077-4,10.1109/KST.2017.7886127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886127,Cyberbullying Prevention;Big Data Streaming;Na簿ve Bayes classifier;Pattern Clustering,Data mining;Twitter;Feature extraction;Classification algorithms;Data models;Clustering algorithms;Predictive models,pattern classification;pattern clustering;probability;social aspects of automation;vectors;word processing,cyberbullying detection;na簿ve Bayes classifier;word extraction;pattern clustering;k-mean clustering;multinomial model feature vector;word probability,,4,,29,,27-Mar-17,,,IEEE,IEEE Conferences
Discovering Communities with Self-Adaptive k Clustering in Microblog Data,通過微博數據中的自適應k聚類發現社區,H. Ting; D. Peng; L. Cao,"NA; Sch. of Opt.-Electr. & Comput. Eng., Univ. of Shanghai for Sci. & Technol., Shanghai, China; Sch. of Opt.-Electr. & Comput. Eng., Univ. of Shanghai for Sci. & Technol., Shanghai, China",2012 Second International Conference on Cloud and Green Computing,14-Feb-13,2012,,,383,390,"Nowadays, microblogging has been a popular social network service whose population has incredibly increased in past few years. Many business companies regard microblogging service as an indispensable medium to directly obtain timely opinions from customers and potential customers. A community in social network refers to a crowd of people having similar interests or paying their attention on same things. User community recognition in microblogging social network service is very important for identifying hot topics or users' interests which are very helpful for companies to improve their marketing strategies. However, the massive non-structural tweet data brings tremendous challenge for efficiently mining the valuable communities hidden in it. Tweet data is characterized as containing massive information, being involved in large fields, short-length and non-structure. This makes tweets quite different from the conventional text documents. In order to analyze the data more effectively, in this paper, we propose a set of techniques to preprocess tweets, such as word identification, categories matching and data standardization. An unsupervised learning method has been presented to automatically cluster microblog users into different communities. In the method, an optimized CLARANS algorithm has been developed according to the characteristics of microblog data. During the process of clustering, the interactive relationship between tweets is also exploited to improve the clustering quality. In addition, a self-adaptive k strategy is employed to make the proposed approach more applicable. In order to investigate the performance of our approach from different aspects, we conducted a series of experiments with the microblog data collected from SINA Weibo.",,978-1-4673-3027-5,10.1109/CGC.2012.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6382845,microblogging;clustering;adaptive k;community recognition;social network,Communities;Clustering algorithms;Social network services;Data mining;Algorithm design and analysis;Probability;Market research,data mining;pattern clustering;pattern matching;social networking (online);unsupervised learning,community discovery;self-adaptive k-clustering;microblog data;microblogging;social network service;user community recognition;data mining;tweet data;tweet preprocessing;word identification;category matching;data standardization;unsupervised learning method;CLARANS algorithm;clustering quality;Sina Weibo,,,,11,,14-Feb-13,,,IEEE,IEEE Conferences
Finding text Stroke width variety in city maps,在城市地圖中查找文本筆劃寬度變化,A. Ghafari-Beranghar; E. Kabir; K. Kangarloo,"Islamic Azad University: Department of Engineering, Science and Research Branch Tehran, Iran; Tarbiat Modares University: Department of Engineering, Tehran, Iran; Islamic Azad University, Center Branch Tehran, Iran",2013 8th Iranian Conference on Machine Vision and Image Processing (MVIP),31-Mar-14,2013,,,275,278,"The Stroke width is an important and stable feature to describe the texts in the document images. In this paper, we propose a method for finding stroke width variety in city map images. Since in city maps the graphics lines and text labels are usually overlap with each other, it is difficult to find the stroke width in such images. On the other hand, texts are printed in a variety of widths. Knowing the major text stroke width is a prior knowledge before map processing like text extraction from graphics lines. In the proposed method, we find the candidate connected components that have significant stroke-width information. Then we locally assign a minimum stroke width to each pixel. For each candidate component, stroke width is determined. By clustering stroke width of components, we find major stroke widths. The experimental results on several varieties of city maps are reported and shown to be promising.",2166-6784,978-1-4673-6184-2,10.1109/IranianMVIP.2013.6779994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779994,Stroke width;text stroek;map;ocr,Cities and towns;Graphics;Feature extraction;Transforms;Histograms;Estimation;Educational institutions,cartography;computer graphics;document image processing;pattern clustering;text analysis,text stroke width variety;document images;city map images;graphics lines;text labels;map processing;text extraction;stroke-width information;stroke width clustering,,,,8,,31-Mar-14,,,IEEE,IEEE Conferences
Empirical analysis on key influential factors on cultural hegemony construction based on big data,基於大數據的文化霸權建設關鍵影響因素的實證分析,Y. Zhong,"Civil Engineering College, Fuzhou University, Fuzhou, China",2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),18-Jun-18,2018,,,238,242,"Cultural hegemony is an authority with which the subject of cultural hegemony leads group members to form specific values, ideals and faiths, etc. by means of cultural leadership. According to Huang Cui's empirical research methodology of quantification of policy documents and through analysis of the quantity of releases, social network clustering and co-word analysis of 10986 culture-related policies published, this paper reaches a conclusion that the power of the subject of cultural hegemony, the power structure of the subject of cultural hegemony, and the main culture and information dissemination of cultural hegemony are the key influential factors on cultural hegemony construction.",,978-1-5386-4301-3,10.1109/ICCCBDA.2018.8386519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8386519,big data;cultural hegemony;influential factor,Cultural differences;Social network services;Law;Government;Industries;Statistical analysis,Big Data;cultural aspects;document handling;information dissemination;pattern clustering;social networking (online),information dissemination;cultural hegemony construction;cultural leadership;10986 culture-related policies;Big Data;policy document quantification;social network clustering;coword analysis,,,,4,,18-Jun-18,,,IEEE,IEEE Conferences
UDeKAM: A methodology for acquiring declarative structured knowledge from unstructured knowledge resources,UDeKAM：一種從非結構化知識資源獲取聲明性結構化知識的方法,M. Ali; S. Lee; B. H. Kang,"Department of Computer Science and Engineering, Kyung Hee University, Yongin, 446-701, Republic of Korea; Department of Computer Science and Engineering, Kyung Hee University, Yongin, 446-701, Republic of Korea; School of Engineering and ICT, University of Tasmania, Hobart, Australia",2016 International Conference on Machine Learning and Cybernetics (ICMLC),23-Feb-17,2016,1,,177,182,"An effective knowledge representation has always proved its importance for mankind intelligence. Among various kinds of knowledge, declarative knowledge has a vital role in medical domain and is critical for health-care safety and quality. A large volume of declarative knowledge is hidden in multiple knowledge resources such as clinical notes, standard guidelines etc. that can play an important role in decision support systems as well as in health and wellness applications after structured transformation. In this paper, an Unstructured Declarative Knowledge Acquisition Methodology, called UDeKAM, is proposed that acquires and constructs the declarative structured knowledge from unstructured knowledge resources using Documents Clustering, Topic Modeling, and Controlled Natural Language processing techniques. The proposed methodology is designed for different domains to serve a variety of applications. It is an ongoing work and for the realization of UDeKAM, a diabetes scenario is explained through example.",2160-1348,978-1-5090-0390-7,10.1109/ICMLC.2016.7860897,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860897,Resource classification;Topic modeling;Controlled natural language;Declarative knowledge;Ontological model,Natural languages;Guidelines;Load modeling;Diabetes;Biological system modeling;Engines;Analytical models,document handling;health care;knowledge acquisition;knowledge representation;medical computing;natural language processing;pattern clustering,UDeKAM;unstructured declarative knowledge acquisition methodology;unstructured knowledge resource;knowledge representation;mankind intelligence;medical domain;health care;document clustering;topic modelling;controlled natural language processing,,,,18,,23-Feb-17,,,IEEE,IEEE Conferences
Enhancing Text Clustering Using Concept-based Mining Model,使用基於概念的挖掘模型增強文本聚類,S. Shehata; F. Karray; M. Kamel,"University of Waterloo, Canada; University of Waterloo, Canada; University of Waterloo, Canada",Sixth International Conference on Data Mining (ICDM'06),8-Jan-07,2006,,,1043,1048,"Most of text mining techniques are based on word and/or phrase analysis of the text. The statistical analysis of a term (word or phrase) frequency captures the importance of the term within a document. However, to achieve a more accurate analysis, the underlying mining technique should indicate terms that capture the semantics of the text from which the importance of a term in a sentence and in the document can be derived. A new concept-based mining model that relies on the analysis of both the sentence and the document, rather than, the traditional analysis of the document dataset only is introduced. The proposed mining model consists of a concept-based analysis of terms and a concept-based similarity measure. The term which contributes to the sentence semantics is analyzed with respect to its importance at the sentence and document levels. The model can efficiently find significant matching terms, either words or phrases, of the documents according to the semantics of the text. The similarity between documents relies on a new concept-based similarity measure which is applied to the matching terms between documents. Experiments using the proposed concept-based term analysis and similarity measure in text clustering are conducted. Experimental results demonstrate that the newly developed concept-based mining model enhances the clustering quality of sets of documents substantially.",2374-8486,978-0-7695-2701-7,10.1109/ICDM.2006.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053150,,Data mining;Text mining;Natural language processing;Statistical analysis;Text analysis;Humans;Unsupervised learning;Clustering methods;Frequency measurement;Entropy,computational linguistics;data mining;pattern clustering;pattern matching;text analysis,text clustering;concept-based mining model;phrase analysis;word analysis;statistical analysis;document dataset analysis;concept-based similarity measure;sentence semantics;term matching;natural language processing,,20,,13,,8-Jan-07,,,IEEE,IEEE Conferences
Machine Learning Trend Anticipation by Text Mining Methodology Based on SSCI Database,基於SSCI數據庫的文本挖掘方法對機器學習趨勢的預測,J. K. Chiang; W. Wu; W. Liao; C. Yin,"Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan; Dept. of Manage. Inf. Syst., Nat. Chengchi Univ., Taipei, Taiwan","2009 Fifth International Joint Conference on INC, IMS and IDC",13-Nov-09,2009,,,612,617,"This paper is providing an introduction to the text mining methodology. There are many different researches which applying machine learning to improve its management application efficiency in various domains. This research is utilizing text mining technology, including ""two step auto-clustering"", ""glossaries aggregation"", ""TF-IDF"" and so on, which collecting the homogeneous glossaries from articles, guiding to the literature cluster analysis based on the Social Science Citation Index (SSCI) database. The result discovered that the research domains of artificial intelligence, document pattern and financial related are the most prosperous fields on machine learning application, it is leading by information technology development progressing, Web 2.0 is also a boost to research morale. All of these will become a power for important developing direction on machine learning in near future.",,978-1-4244-5209-5,10.1109/NCM.2009.382,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331782,Text Mining;Machine Learnin;Two step auto-clustering,Machine learning;Text mining;Databases;Management information systems;Terminology;Clustering methods;Clustering algorithms;Documentation;Technology management;Visualization,citation analysis;data mining;glossaries;Internet;learning (artificial intelligence);pattern clustering;text analysis,machine learning trend anticipation;text mining methodology;SSCI database;management application efficiency;two step auto-clustering;glossaries aggregation;TF-IDF;homogeneous glossaries;literature cluster analysis;Social Science Citation Index;artificial intelligence;document pattern;financial domain;information technology development;Web 2.0,,1,,27,,13-Nov-09,,,IEEE,IEEE Conferences
Classification of Semantic Documents Based on WordNet,基於WordNet的語義文檔分類,B. Shi; L. Fang; J. Yan; P. Wang; C. Dong,"Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol., Beijing, China; Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol., Beijing, China; Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol., Beijing, China; Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol., Beijing, China; Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol., Beijing, China","2009 International Conference on E-Learning, E-Business, Enterprise Information Systems, and E-Government",28-Dec-09,2009,,,173,176,"There are a lot benefits to enable intelligent agent understanding the information from semantic Web. It enhances the efficiency of information usage and at the same time, suffices the need of users. Semantic documents contain adequate semantic information which helps understanding. However, discrepancy between ontology which is an interpreter of semantic document prevents the share of knowledge. In this paper, we proposed a uniform representation for the content, which include concepts and relations, of semantic documents based on WordNet. First, disambiguation is preceded within the key words in a document for the purpose of mapping them to concepts. Then we present the whole document in the form of concept graph that Levenshtein Distance could be applied for making a classification of documents. We have empirical result that this methodology makes a promising raise in accuracy.",2161-9239,978-0-7695-3907-2,10.1109/EEEE.2009.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5359268,documents classification;word disambiguation;wordnet,Semantic Web;Frequency;Ontologies;Indexing;Intelligent agent;Clustering methods;Electronic learning;Information systems;Electronic government;Educational institutions,graph theory;ontologies (artificial intelligence);semantic Web;word processing,semantic documents;WordNet;semantic Web;semantic information;ontology;knowledge share;concept graph;Levenshtein distance;document classification;disambiguation,,2,,10,,28-Dec-09,,,IEEE,IEEE Conferences
M2VSM: Extension of vector space model by introducing Meta keyword,M2VSM：通過引入Meta關鍵字擴展向量空間模型,Y. Takama; T. Ishibashi,"Graduate School of System Design, Tokyo Metropolitan University, Japan; Tokyo Metropolitan Inst. Tech., Japan",2008 World Automation Congress,9-Dec-08,2008,,,1,6,"This paper proposes an extended vector space model (VSM), which is called M2VSM (meta keyword-based modified VSM). When conventional VSM is applied to document clustering, it is difficult to adjust the granularity of cluster in terms of topic. In order to solve the problem, M2VSM considers meta keywords such as adjectives and adverbs, as additional value of indexing terms. The similarity between documents is calculated by considering the matching of meta keywords for each index term, which makes it possible to cluster documents with various granularities in terms of topic. Experimental results show that clustering results by M2VSM match the results by test subjects in both rough and detailed clustering.",2154-4824,978-1-889335-38-4,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4698970,Vector space model (VSM);Text mining;Meta keyword;Clustering,Testing;Indexing;Information retrieval;Data mining;Frequency;Text mining;Information processing;Deductive databases;Information technology;Space technology,data mining;indexing;pattern clustering;text analysis,meta keyword-based modified vector space model;document clustering;adjectives;adverbs;indexing terms;text mining,,1,,8,,9-Dec-08,,,IEEE,IEEE Conferences
Local blur correction for document images,文檔圖像的局部模糊校正,V. C. Kieu; F. Cloppet; N. Vincent,"LIPADE laboratory, Paris Descartes University, 75006, France; LIPADE laboratory, Paris Descartes University, 75006, France; LIPADE laboratory, Paris Descartes University, 75006, France",2016 23rd International Conference on Pattern Recognition (ICPR),24-Apr-17,2016,,,4059,4064,"Image quality is a hot topic since most of image analysis and recognition systems are sensitive to degradation. In this paper, we are interested in the quality of document images to recover text content with as few errors as possible. One of the defects that degrades the Optical Character Recognition rate (OCR) is blur. Therefore, we propose to detect blur and to attenuate its effect on an OCR result. As blur is non-uniform on the document area, we propose a local approach. No prior model is chosen for blur that may be of various natures. Thanks to a local clustering based on a novel blur feature, we build a debluring adapted to the heterogeneous blur. As a result, the blurred image is locally corrected according to the blur type. This method focuses on the ease of character segmentation, then makes OCR more efficient. The experiments are carried out on a public database DIQA together with the use of an OCR. The obtained results show that the proposed method gives an overall improvement of 11% in OCR rate.",,978-1-5090-4847-2,10.1109/ICPR.2016.7900269,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900269,,Optical character recognition software;Deconvolution;Transient analysis;Histograms;Kernel;Image analysis;Databases,document image processing;feature extraction;image restoration;image segmentation;optical character recognition;pattern clustering,local blur correction;image analysis;image recognition systems;document image quality;text content recovery;optical character recognition rate;OCR rate;blur detection;local clustering;blur feature;image debluring;heterogeneous blur;character segmentation;DIQA public database,,2,,29,,24-Apr-17,,,IEEE,IEEE Conferences
Document image segmentation using wavelet scale-space features,使用小波尺度空間特徵的文檔圖像分割,M. Acharyya; M. K. Kundu,"Machine Intelligence Unit, Indian Stat. Inst., Calcutta, India; Machine Intelligence Unit, Indian Stat. Inst., Calcutta, India",IEEE Transactions on Circuits and Systems for Video Technology,6-Feb-03,2002,12,12,1117,1127,"An efficient and computationally fast method for segmenting text and graphics part of document images based on textural cues is presented. We assume that the graphics part have different textural properties than the nongraphics (text) part. The segmentation method uses the notion of multiscale wavelet analysis and statistical pattern recognition. We have used M-band wavelets which decompose an image into M/spl times/M bandpass channels. Various combinations of these channels represent the image at different scales and orientations in the frequency plane. The objective is to transform the edges between textures into detectable discontinuities and create the feature maps which give a measure of the local energy around each pixel at different scales. From these feature maps, a scale-space signature is derived, which is the vector of features at different scales taken at each single pixel in an image. We achieve segmentation by simple analysis of the scale-space signature with traditional k- means clustering. We do not assume any a priori information regarding the font size, scanning resolution, type of layout, etc. of the document in our segmentation scheme.",1558-2205,,10.1109/TCSVT.2002.806812,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175448,,Image segmentation;Graphics;Image edge detection;Image storage;Wavelet analysis;Pattern analysis;Pattern recognition;Frequency;Energy measurement;Pixel,document image processing;image segmentation;wavelet transforms;feature extraction;pattern recognition;pattern clustering;filtering theory;channel bank filters,document image segmentation;wavelet scale-space features;textural cues;graphics;textural properties;multiscale wavelet analysis;statistical pattern recognition;filter bank;M-band wavelet filters;bandpass channels;image decomposition;detectable discontinuities;feature maps;local energy;pixel;scale-space signature;k-means clustering,,48,,32,,6-Feb-03,,,IEEE,IEEE Journals
A Machine Learning Based Topic Exploration and Categorization on Surveys,基於機器學習的調查主題探索和分類,C. P. George; D. Z. Wang; J. N. Wilson; L. M. Epstein; P. Garland; A. Suh,"Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Methodology, SurveyMonkey, Palo Alto, CA, USA; Dept. of Methodology, SurveyMonkey, Palo Alto, CA, USA; Dept. of Methodology, SurveyMonkey, Palo Alto, CA, USA",2012 11th International Conference on Machine Learning and Applications,10-Jan-13,2012,2,,7,12,"This paper describes an automatic topic extraction, categorization, and relevance ranking model for multi-lingual surveys and questions that exploits machine learning algorithms such as topic modeling and fuzzy clustering. Automatically generated question and survey categories are used to build question banks and category-specific survey templates. First, we describe different pre-processing steps we considered for removing noise in the multilingual survey text. Second, we explain our strategy to automatically extract survey categories from surveys based on topic models. Third, we describe different methods to cluster questions under survey categories and group them based on relevance. Last, we describe our experimental results on a large group of unique, real-world survey datasets from the German, Spanish, French, and Portuguese languages and our refining methods to determine meaningful and sensible categories for building question banks. We conclude this document with possible enhancements to the current system and impacts in the business domain.",,978-1-4673-4651-1,10.1109/ICMLA.2012.132,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406717,topic modeling;survey clustering;fuzzy clustering;categorization,Large scale integration;Computational modeling;Buildings;Vocabulary;Clustering algorithms;Education;Noise,document handling;fuzzy set theory;learning (artificial intelligence);pattern clustering,machine learning based topic exploration;survey categorization;relevance ranking model;multilingual surveys;topic modeling;fuzzy clustering;automatically generated question;survey categories;question banks;category-specific survey templates;multilingual survey text;German languages;Spanish languages;French languages;Portuguese languages;question banks,,1,,13,,10-Jan-13,,,IEEE,IEEE Conferences
XML clustering by principal component analysis,通過主成分分析進行XML集群,Jianghui Liu; J. T. L. Wang; W. Hsu; K. G. Herbert,"Coll. of Comput. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; Coll. of Comput. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; NA; NA",16th IEEE International Conference on Tools with Artificial Intelligence,10-Jan-05,2004,,,658,662,"XML is increasingly important in data exchange and information management. A large amount of efforts have been spent in developing efficient techniques for storing, querying, indexing and accessing XML documents. In This work we propose a new approach to clustering XML data. In contrast to previous work, which focused on documents defined by different DTDs, the proposed method works for documents with the same DTD. Our approach is to extract features from documents, modeled by ordered labeled trees, and transform the documents to vectors in a high-dimensional Euclidean space based on the occurrences of the features in the documents. We then reduce the dimensionality of the vectors by principal component analysis (PCA) and cluster the vectors in the reduced dimensional space. The PCA enables one to identify vectors with co-occurrent features, thereby enhancing the accuracy of the clustering. Experimental results based on documents obtained from Wisconsin's XML data bank show the effectiveness and good performance of the proposed techniques.",1082-3409,0-7695-2236-X,10.1109/ICTAI.2004.122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374250,,XML;Principal component analysis;Data mining;Clustering algorithms;Feature extraction;Information retrieval;Filters;Educational institutions;Computer science;Information management,XML;pattern clustering;principal component analysis;data mining,XML clustering;principal component analysis;data exchange;information management;document management;DTD;high-dimensional Euclidean space;co-occurrent features,,5,,17,,10-Jan-05,,,IEEE,IEEE Conferences
Clustering Sinhala News Articles Using Corpus-Based Similarity Measures,使用基於語料庫的相似性度量將僧伽羅語新聞文章聚類,P. Nanayakkara; S. Ranathunga,"Department of Computer Science and Engineering, University of Moratuwa, Katubedda, 10400; Department of Computer Science and Engineering, University of Moratuwa, Katubedda, 10400",2018 Moratuwa Engineering Research Conference (MERCon),30-Jul-18,2018,,,437,442,"News aggregators help readers to handle large numbers of news items in a convenient manner by collecting them into a single place with meaningful groupings. Such news aggregators/clusters are available for English and some other popular languages. However, no such tools are available for Sinhala language. To address this void, this paper presents a system to collect news articles published across the web and group related articles using corpus-based similarity measures. Despite the simplicity of the technique and morphological richness of Sinhala, we achieved very promising results that prove the viability of the presented technique.",,978-1-5386-4417-1,10.1109/MERCon.2018.8421890,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421890,document clustering;Corpus-based similarity measurement;Sinhala,Clustering algorithms;Weight measurement;Partitioning algorithms;Standards;Filtering;Heuristic algorithms;Semantics,Internet;natural language processing;pattern clustering,news items;Sinhala language;Sinhala news articles;news aggregator-clusters;English;corpus-based similarity measurement,,1,,17,,30-Jul-18,,,IEEE,IEEE Conferences
Document image segmentation using Gabor wavelet and kernel-based methods,使用Gabor小波和基於核的方法進行文檔圖像分割,Yu-Long Qiao; Zhe-Ming Lu; Chun-Yan Song; Sheng-He Sun,"Dept. of Autom. Test & Control, Harbin Inst. of Technol., China; Dept. of Autom. Test & Control, Harbin Inst. of Technol., China; NA; NA",2006 1st International Symposium on Systems and Control in Aerospace and Astronautics,8-May-06,2006,,,5 pp.,455,"The document image segmentation is an important component in the document image understanding. kernel-based methods have demonstrated excellent performances in a variety of pattern recognition problems. This paper applies kernel-based methods and Gabor wavelet to the document image segmentation. The feature image are derived from Gabor filtered images. Taking the computational complexity into account, we subject the sampled feature image to spectral clustering algorithm (SCA). The clustering results serve as training samples to train a support vector machine (SVM). The initial segmentation is obtained by assigning class labels to pixels of the feature image with the trained SVM. A proper post-processing is used to improve the segmentation result. Several representative document images scanned from popular newspapers and journals are employed to verify the effectiveness of our algorithm",,0-7803-9395-3,10.1109/ISSCAA.2006.1627662,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1627662,,Image segmentation;Gabor filters;Signal processing algorithms;Clustering algorithms;Support vector machines;Wavelet transforms;Pixel;Partitioning algorithms;Image coding;Frequency,computational complexity;feature extraction;Gabor filters;image segmentation;pattern clustering;support vector machines,document image segmentation;Gabor wavelet method;kernel method;pattern recognition problems;filtered images;computational complexity;spectral clustering algorithm;support vector machine,,4,,19,,8-May-06,,,IEEE,IEEE Conferences
A context sensitive document indexing approach for Information Retrieval,用於信息檢索的上下文相關文檔索引方法,M. Vanishree; R. Sudha,"M.E Computer & Communication, PSNA College of Engineering & Technology, Dindigul, Tamilnadu, India; Information Technology Department, PSNA College of Engineering & Technology, Dindigul, Tamilnadu, India",2014 International Conference on Communication and Signal Processing,10-Nov-14,2014,,,395,398,"Information retrieval is used to retrieve the information according to the user query. In the existing model the information retrieval is done by analyzing the whole document to answer a query and the terms related to the query are extracted. The indexing weight is applied to all the terms and finally it provides the response to the user. In the existing model they did not take the context into consideration so the information cannot be retrieved efficiently. In this paper, we propose a context-sensitive document indexing approach for information retrieval. The content carrying terms and background terms are separated by using lexical association. Indexing weight is calculated for content carrying terms. The term having the highest indexing weight is considered as the most salient sentence and these sentences are extracted and the document summarization is done. Then according to the user query the information is retrieved, the query is considered as the keyword. Then this keyword is matched with the summarized document. Once the keyword is matched, the particular sentences were extracted by using indexing algorithm. Finally these sentences are provided as the responses for the user. By using this approach, the information retrieval can be done effectively.",,978-1-4799-3358-7,10.1109/ICCSP.2014.6949870,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949870,Content carrying term and background term;sentences splitting;indexing approach;keyword matching,Indexing;Clustering algorithms;Heating;Analytical models;Algorithm design and analysis;Context;Gases,indexing;information retrieval,context sensitive document indexing;information retrieval;user query;indexing weight;content carrying terms;background terms;lexical association;document summarization,,,,9,,10-Nov-14,,,IEEE,IEEE Conferences
Character Recognition under Severe Perspective Distortion,嚴重視角畸變下的字符識別,P. Zhou; L. Li; C. L. Tan,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,676,680,"Perspective deformation is one of the main issues needed to be addressed in real-scene character recognition. An effective recognition approach, which is able to handle severe perspective deformation, is to employ cross ratio spectrum and dynamic time warping techniques. However, this solution suffers from a time complexity of O(n4). In this paper, a clustering based indexing method is proposed to index cross ratio spectra and thus expedite the recognition. Cross ratio spectra of all templates are clustered. A query is compared with the centroid of each cluster instead of spectra of all templates. Our method is 40 times faster than the previous method, and has archived about 15-time speed up while preserving almost the same recognition accuracy in the real scene character recognition experiment.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277477,Character Recognition;Dynamic Time Warping;Clustering,Character recognition;Indexing;Iterative algorithms;Text analysis;Layout;Shape;Clustering algorithms;Heuristic algorithms;Prototypes;Optical character recognition software,computational complexity;optical character recognition;pattern clustering,character recognition;severe perspective distortion;perspective deformation;cross ratio spectrum;dynamic time warping technique;time complexity;clustering based indexing method;cluster centroid;recognition accuracy,,6,,15,,2-Oct-09,,,IEEE,IEEE Conferences
Text classification with enhanced semi-supervised fuzzy clustering,帶有增強型半監督模糊聚類的文本分類,G. Keswani; L. O. Hall,"Dept. of Comput. Sci., Univ. of South Florida, Tampa, FL, USA; Dept. of Comput. Sci., Univ. of South Florida, Tampa, FL, USA",2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),7-Aug-02,2002,1,,621,626 vol.1,"Given the increasing volume of information available on the Web, it is important to meaningfully organize online documents. Hence, the design of efficient and accurate text classification systems is of interest. In this paper, we explore a framework, in which we improve the performance of a base classifier, by clustering unlabeled data with labeled data using probabilistic and fuzzy approaches. We have used expectation maximization and semi-supervised fuzzy c-means for clustering the unlabeled data with labeled data. The naive Bayes classifier was the base classifier utilizing both the original labeled data and then additional data labeled through clustering. Utilizing unlabeled data from semi-supervised fuzzy clustering results in an improved classifier.",,0-7803-7280-8,10.1109/FUZZ.2002.1005064,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1005064,,Text categorization;Clustering algorithms;Machine learning algorithms;Classification algorithms;Electronic mail;Computer science;Semisupervised learning;Web sites;Feeds;Databases,text analysis;classification;pattern clustering;fuzzy set theory;information resources;Internet;probability;Bayes methods,text classification;enhanced semi-supervised fuzzy clustering;World Wide Web;online document organization;unlabeled data clustering;labeled data;probabilistic approach;fuzzy approach;expectation maximization;semi-supervised fuzzy c-means;naive Bayes classifier,,3,,13,,7-Aug-02,,,IEEE,IEEE Conferences
WICER: a weighted inter-cluster edge ranking for clustered graphs,WICER：集群圖的加權集群間邊緣排名,D. Padmanabhan; P. Desikan; J. Srivastava; K. Riaz,"Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA",The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),17-Oct-05,2005,,,522,528,"Several algorithms based on link analysis have been developed to measure the importance of nodes on a graph such as pages on the World Wide Web. PageRank and HITS are the most popular ranking algorithms to rank the nodes of any directed graph. But, both these algorithms assign equal importance to all the edges and nodes, ignoring the semantically rich information from nodes and edges. Therefore, in the case of a graph containing natural clusters, these algorithms do not differentiate between inter-cluster edges and intra-cluster edges. Based on this parameter, we propose a weighted inter-cluster edge ranking for clustered graphs that weighs edges (based on whether it is an inter-cluster or an intra-cluster edge) and nodes (based on the number of clusters it connects). We introduce a parameter '/spl alpha/' which can be adjusted depending on the bias desired in a clustered graph. Our experiments were two fold. We implemented our algorithm to a relationship set representing legal entities and documents and the results indicate the significance of the weighted edge approach. We also generated biased and random walks to quantitatively study the performance.",,0-7695-2415-X,10.1109/WI.2005.166,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1517903,,Clustering algorithms;Algorithm design and analysis;Web sites;Computer science;Law;Legal factors;Web mining;Data analysis;Web search;Weight measurement,graph theory;random processes;Internet,weighted inter-cluster edge ranking;clustered graphs;random walks;World Wide Web;Web page ranking algorithms,,5,,8,,17-Oct-05,,,IEEE,IEEE Conferences
Clustering Method for Social Network Annotations,社交網絡註釋的聚類方法,J. J. Astrain; F. Echarte; A. Cordoba; J. Villadangos,"Dept. de Ing. Mat. e Inf., Univ. Publica de Navarra, Pamplona, Spain; Dept. de Salud, Gobierno de Navarra, Pamplona, Spain; Dept. de Ing. Mat. e Inf., Univ. Publica de Navarra, Pamplona, Spain; Dept. de Ing. Mat. e Inf., Univ. Publica de Navarra, Pamplona, Spain",IEEE Latin America Transactions,22-Apr-10,2010,8,1,88,93,"Folksonomies are a widely used tool of collaboratively creating and managing tags to annotate and categorize Internet resources (Web 2.0). The process of annotation and tag management by users of social networks is extremely easy and simple; however, it involves serious problems of navigation and search unlike what happens with taxonomies, thesauri and ontologies. The use of fuzzy similarity measures allows the correct identification of syntactic variations when tag lengths are greater or equal than five symbols, been inadequate for smaller length tags. This article presents a method that combines both fuzzy similarity and cosine measures in order to provide a proper classification of tags even with smaller tag lengths. This method allows the proper classification of the 95% of the syntactic variations of tags analyzed in the experiments.",1548-0992,,10.1109/TLA.2010.5453951,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453951,folksonomies;fuzzy similarity;social network;social tagging;tag annotation;tag clustering,Clustering methods;Social network services;Length measurement;Collaborative tools;Resource management;Internet;Navigation;Taxonomy;Thesauri;Ontologies,document handling;fuzzy set theory;Internet;pattern clustering;social networking (online),clustering method;social network annotation;Internet resources;Web 2.0;tag management;fuzzy similarity measure;syntactic variation;cosine measure,,,,24,,22-Apr-10,,,IEEE,IEEE Journals
Audio Keywords Discovery for Text-Like Audio Content Analysis and Retrieval,音頻關鍵詞發現，可進行類似文本的音頻內容分析和檢索,L. Lu; A. Hanjalic,"Microsoft Res. Asia, Beijing; NA",IEEE Transactions on Multimedia,26-Dec-07,2008,10,1,74,85,"Inspired by classical text document analysis employing the concept of (key) words, this paper presents an unsupervised approach to discover (key) audio elements in general audio documents. The (key) audio elements can be considered the equivalents of the text (key) words, and enable content-based audio analysis and retrieval following the analogy to the proven text analysis theories and methods. Since general audio signals usually show complicated and strongly varying distribution and density in the feature space, we propose an iterative spectral clustering method with context-dependent scaling factors to decompose an audio data stream into audio elements. Using this clustering method, temporal signal segments with similar low-level features are grouped into natural clusters that we adopt as audio elements. To detect those audio elements that are most representative for the semantic content, that is, the key audio elements, two cases are considered. First, if only one audio document is available for analysis, a number of heuristic importance indicators are defined and employed to detect the key audio elements. For the case that multiple audio documents are available, more sophisticated measures for audio element importance, including expected term frequency (ETF), expected inverse document frequency (EIDF), expected term duration (ETD) and expected inverse document duration (EIDD), are proposed. Our experiments showed encouraging results regarding the quality of the obtained (key) audio elements and their potential applicability for content-based audio document analysis and retrieval.",1941-0077,,10.1109/TMM.2007.911304,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407815,Audio content mining;audio element;audio keywords;content-based audio analysis;key audio element;knowledge discovery,Content based retrieval;Text analysis;Hidden Markov models;Clustering methods;Support vector machines;Support vector machine classification;Iterative methods;Streaming media;Frequency measurement;Signal analysis,audio signal processing;content-based retrieval;data mining;iterative methods;pattern clustering;text analysis,unsupervised audio keywords discovery;content-based audio document analysis;content-based audio document retrieval;text document analysis;iterative spectral clustering method;context-dependent scaling factor;audio data stream;temporal signal segment;expected term frequency;expected inverse document frequency;expected term duration;expected inverse document duration,,18,,24,,26-Dec-07,,,IEEE,IEEE Journals
Document clustering based on concept lattice,基於概念格的文檔聚類,L. Kovics; P. Baranyi,"Dept. of Inf. Technol., Univ. of Miskolc, Hungary; NA","IEEE International Conference on Systems, Man and Cybernetics",6-Feb-03,2002,7,,6 pp. vol.7,,"There is in our days an increasing interest on application of concept lattices for data mining, for representation of the concept hierarchy generated from the underlying data set. The paper represents a document query system that clusters the documents into groups using a generated concept lattice In the first phase. The users can start the query by entering a set of keywords. The system return the concepts closest to the query vector. The users get a list of neighbouring concepts too, thus they can select the way to navigate to reach the result document set. The final result is produced using an iterative relevance refinement feedback method.",1062-922X,0-7803-7437-1,10.1109/ICSMC.2002.1175673,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175673,,Lattices;Feedback;Data mining;Identity-based encryption;Data analysis;Cost function;Databases;Navigation;Paper technology;Power generation economics,relevance feedback;data mining;matrix algebra;set theory,document clustering;concept lattice;data mining;document query system;query vector;neighbouring concepts;iterative relevance refinement feedback method,,2,,10,,6-Feb-03,,,IEEE,IEEE Conferences
Two-stage framework for a topology-based projection and visualization of classified document collections,兩階段框架，用於基於拓撲的分類文檔集合的投影和可視化,P. Oesterling; G. Scheuermann; S. Teresniak; G. Heyer; S. Koch; T. Ertl; G. H. Weber,University of Leipzig; University of Leipzig; University of Leipzig; University of Leipzig; University of Stuttgart; University of Stuttgart; Lawrence Berkeley National Laboratory,2010 IEEE Symposium on Visual Analytics Science and Technology,10-Dec-10,2010,,,91,98,"During the last decades, electronic textual information has become the world's largest and most important information source. Daily newspapers, books, scientific and governmental publications, blogs and private messages have grown into a wellspring of endless information and knowledge. Since neither existing nor new information can be read in its entirety, we rely increasingly on computers to extract and visualize meaningful or interesting topics and documents from this huge information reservoir. In this paper, we extend, improve and combine existing individual approaches into an overall framework that supports topologi-cal analysis of high dimensional document point clouds given by the well-known tf-idf document-term weighting method. We show that traditional distance-based approaches fail in very high dimensional spaces, and we describe an improved two-stage method for topology-based projections from the original high dimensional information space to both two dimensional (2-D) and three dimensional (3-D) visualizations. To demonstrate the accuracy and usability of this framework, we compare it to methods introduced recently and apply it to complex document and patent collections.",,978-1-4244-9487-3,10.1109/VAST.2010.5652940,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652940,H.5.2 [INFORMATION INTERFACES AND PRESENTATION]: User Interfaces-Theory and methods;I.5.3 [Pattern Recognition]: Clustering-Algorithms,Clouds;Data visualization;Topology;Layout;Context;Optimization;Electronic mail,classification;computational geometry;data visualisation;document handling;information resources,topology-based projection;visualization;classified document collections;electronic textual information;information source;daily newspapers;books;publications;blogs;private messages;tf-idf document-term weighting method;high dimensional information space;patent collections,,16,,25,,10-Dec-10,,,IEEE,IEEE Conferences
Dynamic Clustering Analysis of Expert Instances with XML Structural Based on the Particle Swarm Algorithm,基於粒子群算法的XML結構專家實例動態聚類分析,J. Zeng,NA,2010 International Conference on Management and Service Science,16-Sep-10,2010,,,1,3,"This article dynamically analyzes large scale XML documents of experts' instances according to thought of particle swarm and algorithm of dynamical cluster. It can quickly find out the same or similar experts' instances in Long-distance Medical System, so it improved the efficiency and accuracy of the querying, and provided us a new thinking for future instance studying in Long-distance Medical System.",,978-1-4244-5325-2,10.1109/ICMSS.2010.5577291,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5577291,,XML;Clustering algorithms;Heuristic algorithms;Particle swarm optimization;Classification algorithms;Algorithm design and analysis;Accuracy,data analysis;medical information systems;particle swarm optimisation;statistical analysis;XML,dynamic clustering analysis;expert instance;particle swarm algorithm;XML document;long-distance medical system;extensible markup language,,,,7,,16-Sep-10,,,IEEE,IEEE Conferences
A Multiscale Latent Dirichlet Allocation Model for Object-Oriented Clustering of VHR Panchromatic Satellite Images,VHR全色衛星圖像面向對象聚類的多尺度潛在Dirichlet分配模型,H. Tang; L. Shen; Y. Qi; Y. Chen; Y. Shu; J. Li; D. A. Clausi,"State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada",IEEE Transactions on Geoscience and Remote Sensing,22-Feb-13,2013,51,3,1680,1692,"A novel model is presented to address the problem of semantic clustering of geo-objects in very high resolution panchromatic satellite images. The proposed model combines a probabilistic topic model with a multiscale image representation into an automatic framework by embedding both document and scale selections. The probabilistic topic model is used to characterize the statistical distributions of both intraclass appearance and inter-class coherence of geo-objects within documents, i.e., squared sub-images. Because the bag-of-words assumption involved in the probabilistic topic models does not consider the spatial coherence between topic labels, the multiscale image representation is designed to provide a self-adaptive spatial regularization for various geo-object categories. By introducing scale and document selections, the automatic framework integrates the probabilistic topic model and the multiscale image representation to ensure that words on a site should be allocated the same topic label no matter what documents they reside in. Consequently, unlike the traditional method of applying topic models for analyzing satellite images, the process of explicitly generating a set of documents before modeling and then combining multiple labels for a word on a given site is unnecessary. Gibbs sampling is adopted for parameter estimation and image clustering. Extensive experimental evaluations are designed to first analyze the effect of parameters in the proposed model and then compare the results of our model with those of some state-of-the-art methods for three different types of images. The results indicate that the proposed algorithm consistently outperforms these exiting state-of-the-art methods in all of the experiments.",1558-0644,,10.1109/TGRS.2012.2205579,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6257475,Latent Dirichlet allocation (LDA);object-oriented clustering;probabilistic topic models;scale space theory,Object oriented modeling;Satellites;Probabilistic logic;Indexes;Biological system modeling;Lattices;Entropy,geophysical image processing;geophysical techniques;image representation;pattern clustering,multiscale latent Dirichlet allocation model;object-oriented clustering;VHR panchromatic satellite images;geo-object semantic clustering;multiscale image representation;geo-object inter-class coherence;probabilistic topic models;geo-object categories;self-adaptive spatial regularization;satellite images;Gibbs sampling;image clustering;state-of-the-art methods,,29,,40,,2-Aug-12,,,IEEE,IEEE Journals
Emergent document semantics,緊急文檔語義,D. V. Sreenath; W. I. Grosky,"Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA; NA","3rd International Symposium on Image and Signal Processing and Analysis, 2003. ISPA 2003. Proceedings of the",10-May-04,2003,1,,266,271 Vol.1,"It is well known that interpretation depends on the context, whether for a work of art, a piece of literature, or a natural language utterance. This paper addresses the dynamic context of a collection of linked multimedia documents, of which the Web is a perfect example. Contextual document semantics emerge through identification of various users' browsing paths though this multimedia collection. In this paper, we present techniques that use multimedia information as part of this determination. Some implications of our approach are presented in the problem of defining the document semantics of a Webpage as well as the semantics that emerges through its use.",,953-184-061-X,10.1109/ISPA.2003.1296906,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296906,,Natural languages;Computer science;Art;Information analysis;Text analysis;Uniform resource locators;Web mining;Image retrieval;Content based retrieval;Feature extraction,semantic Web;document handling;information retrieval;multimedia systems,document semantics;multimedia documents;contextual document semantics;semantic Web;semantic clustering;information retrieval,,,,22,,10-May-04,,,IEEE,IEEE Conferences
Quality and performance evaluation of the algorithms KMART and FCM for fuzzy clustering and categorization,模糊聚類和分類的KMART和FCM算法的質量和性能評估,M. Roj?ek; I. ?ern獺k; R. Janiga,"Department of Informatics, Faculty of Education, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1A, 034 01 Ru鱉omberok, Slovakia; Department of Informatics, Faculty of Education, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1A, 034 01 Ru鱉omberok, Slovakia; Department of Informatics, Faculty of Education, Catholic University in Ru鱉omberok, Hrabovsk獺 cesta 1A, 034 01 Ru鱉omberok, Slovakia",2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES),23-Nov-17,2017,,,285,290,"In this paper we present a comparison of two fuzzy text document clustering algorithms. First of them is KMART algorithm, neural network based on Fuzzy ART neural network and the second algorithm is Fuzzy C-Means based on K-Means algorithm. The paper is aimed to compare the quality and performance of both algorithms on the set of real, contextually similar text documents falling into more categories at the same time.",,978-1-4799-7678-2,10.1109/INES.2017.8118571,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118571,,Clustering algorithms;Training;Subspace constraints;Neural networks;Algorithm design and analysis;Hardware;Performance evaluation,ART neural nets;fuzzy neural nets;fuzzy set theory;pattern clustering;text analysis,performance evaluation;fuzzy clustering;categorization;fuzzy text document;KMART algorithm;contextually similar text documents;fuzzy ART neural network;FCM algorithm;fuzzy C-mean-based-on K-mean algorithm,,,,15,,23-Nov-17,,,IEEE,IEEE Conferences
TopicPie: An Interactive Visualization for LDA-Based Topic Analysis,TopicPie：基於LDA的主題分析的交互式可視化,Y. Yang; J. Wang; W. Huang; G. Zhang,"Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China",2016 IEEE Second International Conference on Multimedia Big Data (BigMM),18-Aug-16,2016,,,25,32,"LDA-based topic analysis is widely used in text mining field. Considering the large scale of web documents, document clusters are usually analyzed instead of single ones. However, the existing visualizations of LDA-based clustering do not intuitively present contents of hot topics while maintaining the relationships between the topics and the document clusters. In this paper, we propose an integrated interactive visualization method that provides intuitive and effective views for topic popularity, topic contents, document clusters, and relationships between topics and document clusters. In this way, users can quickly identify the topic-based patterns. We show an experimental evaluation by comparing the tabular representation and our visualization. The results show that our method can significantly facilitate the topic analysis, particularly in the field of Chinese culture study.",,978-1-5090-2179-6,10.1109/BigMM.2016.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544992,Latent Dirichlet Allocation;LDA;topic model;text mining;clustering;visualization;visual analytics,Conferences;Multimedia communication;Big data,data mining;data visualisation;pattern clustering,LDA-based topic analysis;text mining field;Web documents;document clusters;LDA-based clustering;integrated interactive visualization method;topic-based patterns;tabular representation;Chinese culture study,,3,,18,,18-Aug-16,,,IEEE,IEEE Conferences
Hierarchical-Hyperspherical Divisive Fuzzy C-Means (H2D-FCM) Clustering for Information Retrieval,信息檢索的分層超球面除法模糊C均值（H2D-FCM）聚類,G. Bordogna; G. Pasi,NA; NA,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,9-Oct-09,2009,1,,614,621,"In this paper an original soft hierarchical Fuzzy Clustering algorithm is proposed, named Hierarchical Hyper-spherical Divisive Fuzzy C-Means (H2D-FCM), with the following characteristics: it generates a ?soft??hierarchy in which a document can belong to several child clusters of a node, and the clusters in the same hierarchical level are more specific (general) than the clusters in the upper (lower) level. The proposed algorithm is a divisive algorithm based on a modified bisective K-Means, applying a modified probabilistic Fuzzy C Means algorithm to divide each node into child-nodes. The algorithm determines the proper number of cluster to generate at the first level based on an entropy measure and decides if a node can be further split based on a ?density??measure. The paper presents the algorithm and its evaluations on two standard collections.",,978-0-7695-3801-3,10.1109/WI-IAT.2009.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5284910,Fuzzy Hierarchical clustering;documents clustering;Fuzzy C Means Algorithm;unsupervised hierarchical categorization;Vector Space Model.,Information retrieval;Clustering algorithms;Intelligent agent;Character generation;Density measurement;Content based retrieval;Information filtering;Conferences;Entropy;Unsupervised learning,,,,13,,39,,9-Oct-09,,,IEEE,IEEE Conferences
A watermarking scheme for natural language documents,自然語言文件的水印方案,J. Gu; Y. Cheng,"Department of Information Management, Hunan College of Finance and Economics, Changsha, 410205, China; Software department, Changsha Social Work College, Changsha, 410004, China",2010 2nd IEEE International Conference on Information Management and Engineering,3-Jun-10,2010,,,461,464,"In this paper, a novel scheme of text digital watermarking was presented based on components computing, in order to protect copyright of digital text documents, prevent them from illegal copying and diffusing. In the process of abstracting the text content to vector cluster, and thus the embedding and detecting of text digital watermarking was realized. Results of experiments showed that the watermark is robust against copying, cutting and format adjusting.",,978-1-4244-5263-7,10.1109/ICIME.2010.5477622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477622,watermarking;illegal copying;robust,Watermarking;Natural languages;Frequency;Educational institutions;Robustness;Clustering algorithms;Information management;Finance;Protection;ISO standards,natural language processing;text analysis;watermarking,text digital watermarking scheme;natural language documents;protect copyright;digital text documents,,,,8,,3-Jun-10,,,IEEE,IEEE Conferences
Efficient online spherical k-means clustering,高效的在線球形k均值聚類,Shi Zhong,"Dept. of Comput. Sci. & Eng., Florida Atlantic Univ., Boca Raton, FL, USA","Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",27-Dec-05,2005,5,,3180,3185 vol. 5,"The spherical k-means algorithm, i.e., the k-means algorithm with cosine similarity, is a popular method for clustering high-dimensional text data. In this algorithm, each document as well as each cluster mean is represented as a high-dimensional unit-length vector. However, it has been mainly used in hatch mode. Thus is, each cluster mean vector is updated, only after all document vectors being assigned, as the (normalized) average of all the document vectors assigned to that cluster. This paper investigates an online version of the spherical k-means algorithm based on the well-known winner-take-all competitive learning. In this online algorithm, each cluster centroid is incrementally updated given a document. We demonstrate that the online spherical k-means algorithm can achieve significantly better clustering results than the batch version, especially when an annealing-type learning rate schedule is used. We also present heuristics to improve the speed, yet almost without loss of clustering quality.",2161-4407,0-7803-9048-2,10.1109/IJCNN.2005.1556436,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556436,,Clustering algorithms;Frequency;Annealing;Computer science;Data engineering;Scheduling algorithm;Data mining;Information retrieval;Information filtering;Information filters,pattern clustering;vectors;unsupervised learning,online spherical k-means clustering;k-means algorithm;cosine similarity;high-dimensional text data;cluster mean vector;winner-take-all competitive learning;annealing-type learning rate schedule,,37,,25,,27-Dec-05,,,IEEE,IEEE Conferences
A two-stage codebook building method using fast WAN,使用快速WAN的兩階段代碼簿構建方法,F. Balado Pumarino; O. W. Marquez Florez,"Dept. of Commun. Technol., Vigo Univ., Spain; NA",Proceedings 10th International Conference on Image Analysis and Processing,6-Aug-02,1999,,,1047,1050,"Pattern-matching based document compression systems rely on finding a small set of patterns that can be used to represent the whole document. When analyzing and comparing this kind of system two factors have to be considered: the compression rate attained and the speed and associated complexity of the codebook building. In order to reduce the computational burden of the pattern matching operation while keeping a good compression ratio, we propose a new fast algorithm to carry out a WAN (weighted AND-NOT) matching process. Thus, codebook building is performed in two stages: the first step is based on FWAN (fast WAN) with a loose threshold; in the second one a more accurate but slower method (CTM, EPM) is applied over the initial approximate codebook. This screening greatly reduces the search space for the clustering procedure implicit in obtaining the library without altering the compression ratio. Experimental results show a very good speed performance for this new algorithm: at least three times faster than the usual WAN.",,0-7695-0040-4,10.1109/ICIAP.1999.797735,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=797735,,Wide area networks;Pattern matching;Clustering algorithms;Image coding;Communications technology;Pressing;Libraries;Propagation losses;Image reconstruction,table lookup;pattern matching;data compression;computational complexity;search problems;document image processing;image coding;feature extraction;image matching,two-stage codebook building method;fast WAN;pattern-matching based document compression;compression rate;computational burden;WAN matching process;weighted AND-NOT;CTM;EPM;initial approximate codebook;search space;clustering procedure;textual image compression,,,,10,,6-Aug-02,,,IEEE,IEEE Conferences
Text clustering approach based on maximal frequent term sets,基於最大頻繁項集的文本聚類方法,C. Su; Q. Chen; X. Wang; X. Meng,"Intelligence Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China; Intelligence Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China; Intelligence Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China; Intelligence Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China","2009 IEEE International Conference on Systems, Man and Cybernetics",4-Dec-09,2009,,,1551,1556,"Classical text clustering algorithms are usually based on vector space model or its variants. Because of the high computing complexity and the difficulty of controlling clustering results, this kind of approaches are hard to be applied for the purpose of the large scale text clustering. Clustering algorithms based on frequent term sets make use of relationship among documents and their shared frequent term sets to achieve high accuracy and effectiveness in clustering. But since the number of frequent terms is usually too large to reach the efficiency requirement for large collection texts clustering, this paper proposes a novel text clustering approach based on maximal frequent term sets (MFTSC). This approach firstly mines maximal frequent term sets from text set and then clusters texts by following steps: at first, the maximal frequent term sets are clustered based on the criterion of k-mismatch; then texts are clustered according to term sets clustering results; finally, we categorize the left texts uncovered in previous step into produced text clusters Be compared with existing approaches, our experimental results show an average gain of 10% on F-Measure score with better performance on scalability and efficiency.",1062-922X,978-1-4244-2793-2,10.1109/ICSMC.2009.5346313,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346313,Text Clustering;Text mining;Frequent Term Sets;Maximal Frequent Term Sets,Clustering algorithms;Space technology;Large-scale systems;Data mining;Scalability;Cybernetics;USA Councils;Performance gain;Text mining;Motion pictures,pattern clustering;set theory;text analysis,maximal frequent term sets;text clustering algorithm;vector space model;text set;k-mismatch;F-Measure score,,3,,14,,4-Dec-09,,,IEEE,IEEE Conferences
Registration of camera captured documents under non-rigid deformation,非剛性變形下攝像機捕獲文件的配準,V. G. Edupuganti; V. A. Agarwal; S. Kompalli,"Dept. of Computer Science, New Jersey Inst. of Technology, Newark, New Jersey, USA; Stanford University, Stanford, California, USA; Hewlett-Packard Labs, Bangalore, India",CVPR 2011,22-Aug-11,2011,,,385,392,"Document registration is a problem where the image of a template document whose layout is known is registered with a test document image. Given the registration parameters, layout of the template image is superimposed on the test document. Registration algorithms have been popular in applications, such as forms processing where the superimposed layout is used to extract relevant fields. Prior art has been designed to work with scanned documents under affine transformation. We find that the proliferation of camera captured images makes it necessary to address camera noise such as non-uniform lighting, clutter, and highly variable scale/resolution. The absence of a scan bed also leads to challenging non-rigid deformations being seen in paper images. Prior approaches in point pattern based registration like RANdom SAmple Consensus (RANSAC), and Thin Plate Spline-Robust Point Matching (TPS-RPM)form the basis of our work. We propose enhancements to these methods to enable registration of cell phone and camera captured documents under non-rigid transformations. We embed three novel aspects into the framework: (i) histogram based uniformly transformed correspondence estimation, (ii) clustering of points located near the regions of interest (ROI) to select only close by regions for matching, (iii) validation of the registration in RANSAC and TPS-RPM algorithms for non-rigid registration. We consider Scale Invariant Feature Transform (SIFT) and Speeded-Up Robust Features (SURF) as our features. Results are reported as comparing prior art with our method on a dataset that will be made publicly available.",1063-6919,978-1-4577-0395-9,10.1109/CVPR.2011.5995625,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995625,,Layout;Cameras;Geometry;Annealing;Algorithm design and analysis;Robustness;Histograms,document image processing;image registration,camera captured document registration;template document;registration parameters;template image;test document;relevant field extraction;affine transformation;camera noise;nonuniform lighting;clutter;highly variable scale;highly variable resolution;nonrigid deformations;paper images;point pattern;RANdom SAmple Consensus;RANSAC;thin plate spline-robust point matching;cell phone;nonrigid transformations;regions of interest;nonrigid registration;scale invariant feature transform;SIFT;speeded-up robust features;SURF,,2,,15,,22-Aug-11,,,IEEE,IEEE Conferences
Unsupervised clustering technique to harness ideas from an Ideas Portal,無監督的聚類技術，可利用Ideas Portal中的想法,A. De; S. K. Kopparapu,"Tata Consultancy Services, Maharashtra, INDIA; TCS Innovation Labs - Mumbai, Yantra Park, Thane (West), Maharashtra, INDIA","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",21-Oct-13,2013,,,1563,1568,"Supervised learning techniques have long been used to analyze unstructured natural language text documents. However, supervised learning techniques are not only computationally intensive but also often require large training corpora. Supervised techniques often fail when such training corpora is either (a) not available or (b) when available, is not statistically significant to enable learning. In many practical scenarios, unsupervised learning techniques become de-facto since the training corpus is not available. In this paper we first describe an unsupervised text analysis technique and demonstrate its usefulness in addressing a real life application to harness ideas from aggregating ideas posted on our company Ideas Portal website.",,978-1-4673-6217-7,10.1109/ICACCI.2013.6637413,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637413,Unsupervised Learning;Document Clustering,Tag clouds;Portals;Training;Natural languages;Text analysis;Companies;Blogs,natural language processing;pattern clustering;portals;text analysis;unsupervised learning;Web sites,unsupervised clustering technique;unstructured natural language text documents;unsupervised learning techniques;unsupervised text analysis technique;Ideas portal Web site,,3,,16,,21-Oct-13,,,IEEE,IEEE Conferences
Finding english and translated Arabic documents similarities using GHSOM,使用GHSOM查找英文和翻譯的阿拉伯文文檔的相似性,A. Selamat; H. H. Ismail,"Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia; Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia",2008 International Conference on Computer and Communication Engineering,29-Jul-08,2008,,,460,465,"The idea of finding similar news across Arabic and English sources is that to provide the audience with multiple views of the broadcasted news because reading the news from a single source may not always reflects on what happening around the world due different background, cultures and opinions of the readers and writers. To achieve this goal there are many techniques have been used to cluster the documents with similar themes. In this paper, we analyze the similarity of the views on the news written in the news translations form Arabic and English texts using self-organizing map (SOM). However, we have found there are some difficulties in SOM that affect its performance. In order to improve the problems of performance, we have used a growing hierarchical self-organizing map (GHSOM). The main advantage of such a mapping is the ease by which a user gains an idea regarding the structure of the data by analyzing the map. Thousands of news documents have been collected from Arabic and English news sources from the Web in order to train both algorithms. Form experiments, the results show that using GHSOM is better in terms of clustering documents with the same opinions.",,978-1-4244-1691-2,10.1109/ICCCE.2008.4580647,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580647,,Broadcasting;Clustering algorithms;Computer science;Information systems;Data analysis;Web pages;Cultural differences;Humans;Genetic algorithms;Support vector machines,broadcasting;natural language processing;self-organising feature maps;text analysis,English language;Arabic language;document similarity;broadcasted news;news translation;growing hierarchical self-organizing map;news document;news sources;World Wide Web;document clustering,,1,,20,,29-Jul-08,,,IEEE,IEEE Conferences
Complex and degraded color document image binarization,複雜且退化的彩色文檔圖像二值化,S. Mysore; M. K. Gupta; S. Belhe,"D.Y. Patil College of Engineering, Akurdi, Pune 411044, India; Center for Development of Advanced Computing, Pune 411007, India; Center for Development of Advanced Computing, Pune 411007, India",2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN),15-Sep-16,2016,,,157,162,We present a document binarization scheme that is intended at consistently binarizing a range of degraded color document images. The proposed solution makes use of a mean-shift algorithm based segmentation applied at different scales of the image and a contrast enhanced version of the popular Niblack's thresholding method. The solution has been evaluated using standard metrics used in a prominent binarization competition and has also been subject to an end-to-end evaluation by use in an OCR system. The proposed solution was found to perform at par or better than existing state of the art binarization solutions and was found to always be more consistent in performance than the state of the art.,,978-1-4673-9197-9,10.1109/SPIN.2016.7566680,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566680,Color Document Binarization;Mean-shift Filtering;Niblack Thresholding;Contrast,Image color analysis;Image segmentation;Color;Gray-scale;Optical character recognition software;Character recognition;Clustering algorithms,image colour analysis;image segmentation,degraded color document image binarization;complex color document image binarization;document binarization scheme;mean-shift algorithm based segmentation;Niblack's thresholding method;contrast enhanced version;binarization competition;OCR system;binarization solutions,,6,,32,,15-Sep-16,,,IEEE,IEEE Conferences
Keywords Similarity Based Topic Identification for Indonesian News Documents,基於關鍵詞相似度的印尼新聞文獻主題識別,A. Fuddoly; J. Jaafar; N. Zamin,"Dept. of Comput. & Inf. Sci., Univ. Teknol. PETRONAS, Tronoh, Malaysia; Dept. of Comput. & Inf. Sci., Univ. Teknol. PETRONAS, Tronoh, Malaysia; Dept. of Comput. & Inf. Sci., Univ. Teknol. PETRONAS, Tronoh, Malaysia",2013 European Modelling Symposium,31-Mar-14,2013,,,14,20,"Topic identification (TID) is a technique associated with labelling a set of textual documents with a meaningful label representing its content. TID for online news presents different problems from TID for other corpora, such as the large data volume and the frequently updated topic. Moreover, the number of developing methods for Indonesian corpus is rather small. Brace well's algorithm has been proven effective in identifying topics in English and Japanese corpora with high accuracy. This paper implements a method for TID based on Brace well's keywords similarity algorithm and the top-n keywords selection for Indonesian news documents. The top-n method is utilized to improve Brace well's performance within Indonesian corpus, and to reduce the dimension of dataset during training. The combination is aimed to reduce the heavy computation problem and to explore the possibility of a new emerging topic which possibly has to be created. The method consists of two stages: training and classification. It studies the keywords of the training dataset then calculates the similarity between testing and training articles' keywords. The algorithm produced accuracy as high as 95.22% on onlineand95.26% on offline environment, 84% against human evaluation, and an average of 2.96 seconds computational time.",,978-1-4799-2578-0,10.1109/EMS.2013.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779815,topic identification;Indonesian text documents;information retrieval;news domain;Bracewell Algorithm,Accuracy;Vectors;Training;Databases;Clustering algorithms;Equations;Mathematical model,electronic publishing;information retrieval;natural language processing;pattern classification;text analysis,keyword similarity based topic identification;textual documents;online news;TID;Indonesian corpus;Indonesian news documents;Japanese corpora;English corpora;Bracewell's keywords similarity algorithm;top-n keywords selection method;Indonesian news documents;Bracewell's performance;classification;training dataset;article keyword testing,,,,24,,31-Mar-14,,,IEEE,IEEE Conferences
A WordNet-Based Semantic Model for Enhancing Text Clustering,基於WordNet的增強文本聚類的語義模型,S. Shehata,"Univ. of Waterloo, Waterloo, ON, Canada",2009 IEEE International Conference on Data Mining Workshops,28-Dec-09,2009,,,477,482,"Most of text mining techniques are based on word and/or phrase analysis of the text. The statistical analysis of a term (word or phrase) frequency captures the importance of the term within a document. However, to achieve a more accurate analysis, the underlying mining technique should indicate terms that capture the semantics of the text from which the importance of a term in a sentence and in the document can be derived. Incorporating semantic features from the WordNet lexical database is one of many approaches that have been tried to improve the accuracy of text clustering techniques. A new semantic-based model that analyzes documents based on their meaning is introduced. The proposed model analyzes terms and their corresponding synonyms and/or hypernyms on the sentence and document levels. In this model, if two documents contain different words and these words are semantically related, the proposed model can measure the semantic-based similarity between the two documents. The similarity between documents relies on a new semantic-based similarity measure which is applied to the matching concepts between documents. Experiments using the proposed semantic-based model in text clustering are conducted. Experimental results demonstrate that the newly developed semantic-based model enhances the clustering quality of sets of documents substantially.",2375-9259,978-1-4244-5384-9,10.1109/ICDMW.2009.86,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360452,,Data mining;Clustering methods;Text mining;Frequency;Spatial databases;Clustering algorithms;Conferences;Statistical analysis;Natural language processing;Unsupervised learning,data mining;statistical analysis;text analysis,WordNet lexical database;semantic model;text clustering;text mining;statistical analysis;term frequency;document analysis;semantic-based similarity,,17,,12,,28-Dec-09,,,IEEE,IEEE Conferences
Tracking Aspects in News Documents,跟?新聞文檔中的方面,M. Inoue; T. Miura,"Dept..of Adv. Sci., HOSEI Univ., Koganei, Japan; Dept..of Adv. Sci., HOSEI Univ., Koganei, Japan",2018 IEEE International Conference on Data Mining Workshops (ICDMW),10-Feb-19,2018,,,1152,1159,"In this investigation, we discuss aspect tracking, i.e., how to identify tracking story-lines of document topics. Because a huge amount of fragment information occurs, it is difficult to see what they mean and how they go within topics by hand. Here we are addressing this type of problem using stochastic models. Our basic idea is that we consider state transitions as internal structure of stories based on HMM and we extract some story-lines as aspects of topics by probabilistic likelihood. We will use KL divergence to screen topics. Also, we discuss topic detection, i.e., how to construct clusters of events to discuss the same topic.",2375-9259,978-1-5386-9288-2,10.1109/ICDMW.2018.00165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637483,Topic Detection and Tracking;Aspect Tracking;Hidden Markov Model (HMM);Backward Viterbi Algorithm,Hidden Markov models;Markov processes;Maximum likelihood estimation;Training data;Viterbi algorithm;Mathematical model,document handling;hidden Markov models;pattern clustering,state transitions;screen topics;topic detection;news documents;aspect tracking;tracking story-lines;document topics;fragment information;stochastic models,,,,11,,10-Feb-19,,,IEEE,IEEE Conferences
Using topic-concept based Clustering for Genomic Information Retrieval in TREC,使用基於主題概念的聚類進行TREC中的基因組信息檢索,T. Jiang; T. He; F. Li,"Department of Computer Science, HuaZhong Normal University, WuHan China; Department of Computer Science, HuaZhong Normal University, WuHan China; National Engineering Research, Center for E-learning, WuHan China",2010 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW),28-Jan-11,2010,,,189,192,"In this paper, a topic-concepts Clustering method for the task of Genomic Information Retrieval in TREC is presented. The main idea is that all the documents are clustered according to the 36 queries, then, for each query, only the documents of the same cluster are considered as related documents and used to generate the accurate answer. It converts the queries and documents to concepts; then, expands the queries by using the pseudo-relevance feedback technique, and the expanded queries are considered initial centers to cluster all the documents; finally, the documents in each cluster are re-ranked respectively to mine the result for the corresponding query. The results of comparative experiments on the TREC 2007 Genomics indicate that this method is effective.",,978-1-4244-8304-4,10.1109/BIBMW.2010.5703797,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703797,Genomic Information Retrieval;Universal Medical Language System;Topic-Concepts;Clustering,Bioinformatics;Genomics;Information retrieval;Data mining;Unified modeling language;Conferences,bioinformatics;genomics;information retrieval,genomic information retrieval task;topic-concepts Clustering method;pseudorelevance feedback technique;TREC 2007 genomics,,,,8,,28-Jan-11,,,IEEE,IEEE Conferences
Topic Navigation Generation Using Topic Extraction and Clustering,使用主題提取和聚類生成主題導航,Z. Chengzhi; Z. Qingguo,"Dept. of Inf. Manage., Nanjing Univ. of Sci. & Technol., Nanjing; Tsinghua Tongfang Knowledge Network Technol. Co., Ltd., Beijing",2008 International Symposium on Knowledge Acquisition and Modeling,30-Dec-08,2008,,,333,339,"Topic digital library is a special domain digital library based on topic features. This paper is to introduce a new approach to build topic navigation in the topic digital library using topic extraction and clustering. Topic digital library is an important application of knowledge service and it is a special domain digital library based on topic or concept features. Firstly, documents in a special domain are automatically produced by document classification approach. It integrates the rule-based and statistical method to classify the documents in the large-scale collection. Then, the keywords of each document are extracted using the machine learning approach. The keywords are used to cluster the documents subset. The clustered result is the taxonomy of the subset. Lastly, the taxonomy is modified to the hierarchical structure for user navigation by manual adjustments. The topic digital library is constructed after combining the full-text retrieval and hierarchical navigation function. The construction method of topic digital library is significant to build the topic databases or special resource databases.",,978-0-7695-3488-6,10.1109/KAM.2008.88,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732840,Topic Navigation;Topic Extraction;Topic Clustering;Topic Digital Library,Navigation;Software libraries;Data mining;Web and internet services;Machine learning;Taxonomy;Databases;Artificial intelligence;Semantic Web;Knowledge acquisition,digital libraries;information retrieval;pattern classification;pattern clustering;text analysis,topic navigation generation;topic extraction;topic digital library;topic clustering;knowledge service;document classification;machine learning;hierarchical structure;full-text retrieval;hierarchical navigation function,,,,21,,30-Dec-08,,,IEEE,IEEE Conferences
Clustering Web Pages Based on Structure and Style Similarity (Application Paper),基於結構和?式相似度的網頁聚類（應用論文）,T. Gowda; C. A. Mattmann,"Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA",2016 IEEE 17th International Conference on Information Reuse and Integration (IRI),19-Dec-16,2016,,,175,180,"We consider cluster analysis task on web pages based on various techniques to group the pages. While grouping the web pages based on the semantic meaning expressed in the content is required for some applications, we focus on clustering based on the web page structure and style for applications like categorization, cleaning, schema detection and automatic extractions. This paper describes some of the applications of similarity measures and a clustering technique to group the web pages into clusters. The structural similarity of HTML pages is measured by using Tree Edit Distance measure on DOM trees. The stylistic similarity is measured by using Jaccard similarity on CSS class names. An aggregated similarity measure is computed by combining structural and stylistic measures. A clustering method is then applied to this aggregated similarity measure to group the documents.",,978-1-5090-3207-5,10.1109/IRI.2016.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785739,jaccard;similarity;tika;metadata;clustering;documents,Web pages;Vegetation;HTML;Cascading style sheets;World Wide Web;Data models;Videos,hypermedia markup languages;Internet;pattern clustering;trees (mathematics),cluster analysis task;semantic meaning;Web page structure;Web page style;categorization;cleaning;schema detection;automatic extractions;structural similarity;HTML pages;tree edit distance measure;DOM trees;stylistic similarity;hyper text markup language,,6,,17,,19-Dec-16,,,IEEE,IEEE Conferences
Document processing methods for Telugu and other South East Asian scripts,泰盧固語和其他東南亞文字的文檔處理方法,Atul Negi; V. S. R. Sowri; K. Mohan Rao,"Dept. of CIS, Hyderabad Univ., India; NA; NA",2004 IEEE Region 10 Conference TENCON 2004.,23-May-05,2004,B,,132,135 Vol. 2,"It is observed that in several South East Asian scripts, a single character consists of two or more connected components. In these scripts the complex arrangement of connected components leads to problems such as touching characters and difficulty in identifying words and text line boundaries. In the present work we propose a method to extract text lines by clustering of connected components, based upon their spatial properties. Those components, with abnormal properties and which are not identified by an OCR, are sent for character segmentation. For character segmentation we describe ""Drop Fall"" and ""Whitestream"" methods. The methods presented here are applicable to any language (script) that requires connected component based processing.",,0-7803-8560-8,10.1109/TENCON.2004.1414549,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1414549,,Optical character recognition software,natural languages;image segmentation;document image processing,document processing method;Telugu;South East Asian script;text line boundary;spatial property;character segmentation;drop fall;Whitestream method;connected component based processing,,,,9,,23-May-05,,,IEEE,IEEE Conferences
NS-IMMC: A Method of Generating the Cause-and-Effect of News Topic,NS-IMMC：一種產生新聞話題因果的方法,Z. Wang; X. Zhou,"Huaihua Vocational & Tech. Coll., Huaihua; Hunan Univ. of Techonogy, Zhuzhou",2008 International Symposium on Information Science and Engineering,30-Dec-08,2008,1,,330,335,"On the base of NS-IMMC, this paper propose a new method of generating the cause-and-effect of news topic. The new method choose representative sentences for news documents according to the specialty of news structure (NS, News structure), and then utilizes IMMC (Improved Min-Max clustering) to classify these representative sentences to generate multi-documents summary which represents the topic cause-and-effect. The experimental results prove that excerpts generated by the new method has an all-around content covering and a strong logic, and shows the development of the topic exactly.",2160-1291,978-0-7695-3494-7,10.1109/ISISE.2008.325,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732229,,Clustering methods;Internet;Compression algorithms;Data mining;Clustering algorithms;Information science;Educational institutions;Character generation;Kernel,cause-effect analysis;classification;document handling;minimax techniques;pattern clustering;publishing,NS-IMMC;cause-and-effect;news topic;news documents;news structure;improved min-max clustering;representative sentences;multidocuments summary,,,,8,,30-Dec-08,,,IEEE,IEEE Conferences
Structure-preserving document image compression,保留結構的文檔圖像壓縮,O. E. Kia; D. S. Doermann,"Center for Autom. Res., Maryland Univ., College Park, MD, USA; NA",Proceedings of 3rd IEEE International Conference on Image Processing,6-Aug-02,1996,1,,193,196 vol.1,"Maintaining a document in image form is often preferable in order to avoid the high cost of manual conversion or the introduction of large numbers of errors by automatic OCR and/or graphics interpretation. The large volume of data in the image can be greatly reduced by using compression techniques. Text-intensive document images typically have a great deal of redundancy in the bitmap representations of symbols, and we make use of that redundancy for compression by clustering components, representing each cluster by a template and encoding the error. Our method is novel in modeling the error associated with each cluster and in preserving structure, an important component for readability and processing.",,0-7803-3259-8,10.1109/ICIP.1996.559466,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559466,,Image coding;Redundancy;Image storage;Degradation;Propagation losses;Automation;Educational institutions;Costs;Image converters;Optical character recognition software,data compression;image coding;image representation;coding errors,structure-preserving document image compression;automatic OCR;graphics interpretation;compression techniques;data reduction;text-intensive document images;redundancy;bitmap representations;symbols;template;error encoding;error modeling;text components clustering,,2,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Multiresolution recognition of handwritten numerals with wavelet transform and multilayer cluster neural network,小波變換和多層聚類神經網絡的手寫數字多分辨率識別,Seong-Whan Lee; Young-Joon Kim,"Dept. of Comput. Sci., Korea Univ., Seoul, South Korea; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,1010,1013 vol.2,"In this paper, we propose a new scheme for multiresolution recognition of totally unconstrained handwritten numerals using wavelet transform and a simple multilayer cluster neural network. The proposed scheme consists of two stages: a feature extraction stage for extracting multiresolution features with wavelet transform, and a classification stage for classifying totally unconstrained handwritten numerals with a simple multilayer cluster neural network. In order to verify the performance of the proposed scheme, experiments with unconstrained handwritten numeral database of Concordia University of Canada, that of Electro-Technical Laboratory of Japan, and that of Electronics and Telecommunications Research Institute of Korea were performed. The error rates were 3.20%, 0.83%, and 0.75%, respectively. These results showed that the proposed scheme is very robust in terms of various writing styles and sizes.",,0-8186-7128-9,10.1109/ICDAR.1995.602073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=602073,,Handwriting recognition;Multi-layer neural network;Wavelet transforms;Neural networks;Feature extraction;Spatial databases;Laboratories;Error analysis;Robustness;Writing,feature extraction;handwriting recognition;wavelet transforms;multilayer perceptrons;feedforward neural nets,multiresolution recognition;handwritten numerals;wavelet transform;multilayer cluster neural network;totally unconstrained handwritten numerals;feature extraction;classification;handwritten numeral database;error rates,,21,,4,,6-Aug-02,,,IEEE,IEEE Conferences
A language model based on semantically clustered words in a Chinese character recognition system,漢字識別系統中基於語義聚類詞的語言模型,Hsi-Jian Lee; Cheng-Huang Tung,"Dept. of Comput. Sci. & Inf. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,1,,450,453 vol.1,"This paper presents a new method for clustering the words in a dictionary into word groups, which are applied in a Chinese character recognition system with a language model to describe the contextual information. The Chinese synonym dictionary Tong2yi4ci2 ci2lin2 providing the semantic features is used to train the weights of the semantic attributes of the character-based word classes. The weights of the semantic attributes are next updated according to the words of the behavior dictionary, which has a rather complete word set. Then, the updated word classes are clustered into m groups according to the semantic measurement by a greedy method. The words in the behavior dictionary can finally be assigned into the m groups. The parameter space for bigram contextual information of the character recognition system is m/sup 2/. From the experimental results, the recognition system with the proposed model has shown better performance than that of a character-based bigram language model.",,0-8186-7128-9,10.1109/ICDAR.1995.599033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599033,,Natural languages;Character recognition;Dictionaries;Context modeling;Error correction;Computer science;Postal services;Random access memory,character recognition;computational linguistics,Chinese character recognition;language model;semantically clustered words;character recognition system;Chinese synonym dictionary;Tong2yi4ci2 ci2lin2;semantic attributes;behavior dictionary,,,,6,,6-Aug-02,,,IEEE,IEEE Conferences
Towards automatic column-based data object clustering for multilingual databases,邁向多語言數據庫的基於列的自動數據對象聚類,W. M. S. Yafooz; S. Z. Z. Abidin; N. Omar,"Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, MALAYSIA; Faculty of Communication and Media Studies, Universiti Teknologi MARA, Shah Alam, Selangor, MALAYSIA; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, MALAYSIA","2011 IEEE International Conference on Control System, Computing and Engineering",26-Apr-12,2011,,,415,420,"The amount of data in all computer applications is growing tremendously. As a result, the organization of the huge data is crucial. Recently, many researchers consider clustering as one of the important approaches in handling data for wide range of research domains. The examples include Topic Detection and Tracking (TDT), Multilingual Document Clustering, Multilingual News Clustering, Text Clustering and Web Record. Normally, data clustering is time consuming and challenging since they involve heavy programming or scripting. In online news, data clustering analysis is very much needed as the nature of the news across the globe is dynamically changing in every second. The news can come from any web sources in the form of multilingual news. This paper proposes system architecture for an automatic data object clustering in multilingual database for online news, web record and text mining. The architecture provides an overview of a virtual scheme that handles data objects within the database tables as part of the database management system. The proposed technique architecture will provide the platform for quick extraction, data arrangement, data grouping based on pattern similarities. Thus, it will improve query processing performance in multilingual databases without the need to code or script for interface programming. This is the first attempt to apply the data clustering technique prior to data extraction in any database application in the form of semi-structured and structured data (web record).",,978-1-4577-1642-3,10.1109/ICCSCE.2011.6190562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190562,Clustering;Column-based;Attributes Data;Multilingual;Database;Web Record;Online news,Data mining;Query processing;Computer architecture;Distributed databases;Object oriented modeling;Web pages,data analysis;data mining;database management systems;electronic publishing;Internet;natural language processing;pattern clustering;query processing;text analysis,automatic column-based data object clustering;multilingual database;data handling;data clustering analysis;Web sources;online news;Web record;text mining;virtual scheme;database management system;database tables;data arrangement;data grouping;data extraction;pattern similarities;query processing performance;interface programming;semistructured data;structured data;multilingual news clustering,,8,,56,,26-Apr-12,,,IEEE,IEEE Conferences
Automated Keyword Extraction using Support Vector Machine from Arabic News Documents,使用支持向量機從阿拉伯新聞文檔中自動提取關鍵字,B. Armouty; S. Tedmori,"Computer Science Department, Princess Sumaya University for Technology, Amman, Jordan; Computer Science Department, Princess Sumaya University for Technology, Amman, Jordan",2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT),20-May-19,2019,,,342,346,"Keyword extraction is an indispensable step for many natural language processing and information retrieval applications such as; text summarization and search engine optimization. Keywords hold the most important information describing the content of a document. With the increasing volume and variety of unlabeled documents on the Internet, the need for automatic keyword extraction methods increases. Even though keyword extraction can be used in many applications, Arabic research in the field still lacking. In this paper, a supervised learning technique that uses statistical features and Support Vector Machine classifier was implemented and applied to extract the keywords from Arabic news documents. The proposed supervised learning approach achieved a precision of 0.77 and a recall of 0.58.",,978-1-5386-7942-5,10.1109/JEEIT.2019.8717420,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717420,Keyword Extraction;Arabic Language;Natural Language Processing;Support Vector Machine;Feature Extraction;Downsampling,Feature extraction;Support vector machines;Internet;Linguistics;Natural language processing;Clustering algorithms;Distributed databases,information retrieval;Internet;learning (artificial intelligence);natural language processing;pattern classification;search engines;statistical analysis;support vector machines;text analysis,Arabic news documents;natural language processing;information retrieval applications;text summarization;search engine optimization;unlabeled documents;automatic keyword extraction methods;support vector machine classifier;Internet;statistical features;supervised learning technique;Arabic research,,2,,17,,20-May-19,,,IEEE,IEEE Conferences
Using Incremental PLSI for Threshold-Resilient Online Event Analysis,使用增量PLSI進行閾值彈性在線事件分析,T. Chou; M. C. Chen,"Acad. Sinica Taiwan, Nankang; Acad. Sinica Taiwan, Nankang",IEEE Transactions on Knowledge and Data Engineering,31-Jan-08,2008,20,3,289,299,"The goal of online event analysis is to detect events and track their associated documents in real time from a continuous stream of documents generated by multiple information sources. Unlike traditional text categorization methods, event analysis approaches consider the temporal relations among documents. However, such methods suffer from the threshold-dependency problem, so they only perform well for a narrow range of thresholds. In addition, if the contents of a document stream change, the optimal threshold (that is, the threshold that yields the best performance) often changes as well. In this paper, we propose a threshold-resilient online algorithm, called the incremental probabilistic latent semantic indexing (IPLSI) algorithm, which alleviates the threshold-dependency problem and simultaneously maintains the continuity of the latent semantics to better capture the story line development of events. The IPLSI algorithm is theoretically sound and empirically efficient and effective for event analysis. The results of the performance evaluation performed on the topic detection and tracking (TDT)-4 corpus show that the algorithm reduces the cost of event analysis by as much as 15 percent ~ 20 percent and increases the acceptable threshold range by 200 percent to 300 percent over the baseline.",1558-2191,,10.1109/TKDE.2007.190702,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378375,Web mining;Clustering;Knowledge life cycles;Probabilistic algorithms;Web mining;Clustering;Knowledge life cycles;Probabilistic algorithms,Algorithm design and analysis;Event detection;Information analysis;Performance analysis;Text analysis;Clustering algorithms;Text categorization;Indexing;Change detection algorithms;Performance evaluation,document handling;electronic publishing;statistical analysis,incremental PLSI;threshold-resilient online event analysis;associated documents;multiple information sources;threshold-dependency problem;document stream;incremental probabilistic latent semantic indexing,,42,,22,,31-Jan-08,,,IEEE,IEEE Journals
Binarization of Degraded Document Images Based on Combination of Contrast Images,基於對比度圖像組合的降級文檔圖像二值化,A. W. A. Arruda; C. A. B. Mello,"Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil",2014 14th International Conference on Frontiers in Handwriting Recognition,15-Dec-14,2014,,,615,620,"This paper introduces a novel algorithm for binarization of gray scale images of degraded documents based on the creation of three different structural contrast images, the representation of these features in a 2D feature space that is further partitioned. A weak bi-level image and a strong bi-level image are created and combined to produce the final image. Experiments were conducted with the application of the new algorithm to DIBCO 2011 and H-DIBCO 2012 datasets achieving satisfactory results.",2167-6445,978-1-4799-4334-0,10.1109/ICFHR.2014.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6981087,binarization;degraded documents;contrast image,Image color analysis;Ink;Classification algorithms;Clustering algorithms;Partitioning algorithms;Proposals;Image reconstruction,document image processing;image colour analysis;image representation,degraded document images;contrast images;gray scale images binarization;structural contrast image;2D feature space;bilevel image;DIBCO 2011;H-DIBCO 2012 datasets,,4,,20,,15-Dec-14,,,IEEE,IEEE Conferences
Non-negative Matrix Factorization with sparse features,具有稀疏特徵的非負矩陣分解,K. Kimura; T. Yoshida,"Graduate School of Information Science and Technology, Hokkaido University, N-14 W-9, Sapporo 060-0814, Japan; Graduate School of Information Science and Technology, Hokkaido University, N-14 W-9, Sapporo 060-0814, Japan",2011 IEEE International Conference on Granular Computing,5-Jan-12,2011,,,324,329,"We propose an approach for Non-negative Matrix Factorization (NMF) with sparseness constraints on feature vectors. It has been believed that the non-negativity constraint in NMF contributes to making the learned features sparse, and some approaches incorporated additional sparseness constraints. However, previous approaches have not considered the sparsity of features explicitly. Our approach explicitly incorporates the notion of sparsity of features, in terms of independence of features and correlation of features. The proposed notion of sparsity is formalized as regularization terms in the framework of NMF, and learning algorithms with multiplicative update rules are proposed. The proposed approach is evaluated in terms of document clustering over well-known benchmark datasets. The results are encouraging and show that the proposed approach improves the clustering performance, while sustaining relatively good quality of data approximation.",,978-1-4577-0371-3,10.1109/GRC.2011.6122616,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122616,Non-negative Matrix Factorization;Sparse Coding;Localized representation;Clustering,Clustering algorithms;Sparse matrices;Correlation;Approximation methods;Vectors;Approximation algorithms;Feature extraction,constraint handling;correlation methods;document handling;learning (artificial intelligence);matrix decomposition;pattern clustering;set theory,nonnegative matrix factorization;sparse feature vector;sparseness constraint;nonnegativity constraint;feature correlation;regularization term;NMF;learning algorithm;document clustering performance;benchmark datasets;data approximation,,,,13,,5-Jan-12,,,IEEE,IEEE Conferences
Word distributed representation based text clustering,基於Word分佈式表示的文本聚類,Shan Feng; R. Liu; Qinlong Wang; Ruisheng Shi,"School of Information and Communication Engineering, BUPT, China; School of Information and Communication Engineering, BUPT, China; School of Information and Communication Engineering, BUPT, China; Education Ministry Key Laboratory of Trustworthy Distributed Computing and Service, BUPT, China",2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems,6-Aug-15,2014,,,389,393,"The fast growth of Internet web documents has posed new challenges on how to efficiently and accurately manage and retrieve the textual collections, text clustering plays a significant role. Traditional document clustering is an unsupervised categorization of a given document collection based on vector space model, which is a high sparse vector. In this paper, we propose a means to fight the existing shortcomings with a word vector in distributed representation which is obtained from a neural probabilistic language model. To improve the representation of document vector and enhance the accuracy of text clustering, we first computing semantic similarities between words using word embedded vector, and then expanding the keywords of each document. The experiment results show the method can improve the accuracy of clustering.",2376-595X,978-1-4799-4719-5,10.1109/CCIS.2014.7175766,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175766,Word distributed representation;Keyword extension;NMF,Semantics;Recruitment,distributed processing;pattern clustering;text analysis,text clustering;word distributed representation;Internet Web documents;textual collections;unsupervised categorization;sparse vector;vector space model;neural probabilistic language model;document vector representation,,1,,22,,6-Aug-15,,,IEEE,IEEE Conferences
Interpreting clustering results through cluster labeling,通過集群標籤解釋集群結果,O. Maqbool; H. A. Babri,"Lahore Univ. of Manage. Sci., Pakistan; NA","Proceedings of the IEEE Symposium on Emerging Technologies, 2005.",19-Dec-05,2005,,,429,434,"Software architecture refers to the overall structure of a software system, and is defined by the components (sub-systems) within a software system and their interactions with one another. Quite often, there is little documentation describing a software system's architecture, especially in the case of legacy software systems. Thus techniques must be employed for recovering the architecture from the software's source code. Given the size and complexity of legacy systems, researchers have started exploring the use of automated techniques for architecture recovery. A technique that has shown promising results is clustering. Clusters that are obtained as a result of the clustering process represent sub-systems within a software system, but are nor easy to interpret until they are given appropriate names. In this paper, we present a cluster labeling scheme based on identifiers. As the clustering process proceeds, keywords are ranked using the inverse document frequency ranking scheme. Results of experiments conducted on a test system demonstrate that our labeling approach is effective. We also compare the clustering results of the complete algorithm and the weighted combined algorithm based on labels of the clusters produced by them during clustering.",,0-7803-9247-7,10.1109/ICET.2005.1558920,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1558920,,Labeling;Software systems;Computer architecture;Documentation;Software architecture;Technology management;Frequency;System testing;Costs;Manuals,software architecture;system documentation;software maintenance,cluster labeling;software architecture;system;documentation;source code;architecture recovery;clustering process;identifiers;inverse document frequency ranking scheme;weighted combined algorithm,,8,,18,,19-Dec-05,,,IEEE,IEEE Conferences
A divide-and-conquer approach to Latent Perceptual Indexing of audio for large Web 2.0 applications,大型Web 2.0應用程序的音頻潛在感知索引的分而治之方法,S. Sundaram; S. Narayanan,"Deutsche Telekom Laboratories, Quality and Usability Lab, TU-Berlin, Germany; Signal Analysis and Interpretation Lab (SAIL), Dept. of Electrical Engineering-Systems, Univ. of Southern California (USC), Los Angeles, USA",2009 IEEE International Conference on Multimedia and Expo,18-Aug-09,2009,,,466,469,"In the recently proposed latent perceptual indexing of audio, a collection of clips is indexed using unit-document frequency measures between a set of reference clusters as units and the clips as the documents. The reference units are derived by clustering the bag-of-feature vectors extracted from the whole audio library using an unsupervised clustering technique. Indexing is achieved through reduced-rank approximation (using singular-value decomposition) of the unit-document co-occurrence measure matrix that is obtained for the given set of reference clusters and the collection of audio clips. In our initial investigation, the k-means algorithm was used to derive the reference units. In this paper, we attempt to reduce the computation load requirements for the k-means algorithm and singular-value decomposition by randomly splitting the training data into smaller sized parts instead of working on it as a whole. We present results of classification experiments on the BBC sound effects library and our results indicate this approach can significantly reduce the computation time without significant loss in classification performance.",1945-788X,978-1-4244-4290-4,10.1109/ICME.2009.5202535,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5202535,content based audio retrieval;audio classification;clustering;Latent Perceptual Indexing;Web 2.0 applications,Indexing;Clustering algorithms;Measurement units;Libraries;Content based retrieval;Frequency measurement;Matrix decomposition;Training data;Large scale integration;Concurrent computing,audio databases;content-based retrieval;divide and conquer methods;Internet;multimedia systems;pattern clustering;singular value decomposition;unsupervised learning,divide-and-conquer approach;latent perceptual indexing;audio indexing;unit-document frequency;bag-of-feature vectors;audio library;unsupervised clustering technique;reduced-rank approximation;unit-document cooccurrence measure matrix;audio clips;k-means algorithm;singular-value decomposition;randomly splitting,,1,,15,,18-Aug-09,,,IEEE,IEEE Conferences
Castsearch - Context Based Spoken Document Retrieval,Castsearch-基於上下文的語音文檔檢索,L. L. Molgaard; K. W. Jorgensen; L. K. Hansen,"Informatics and Mathematical Modelling, Technical University of Denmark Richard Petersens Plads, Building 321, DK-2800 Kongens Lyngby, Denmark; Informatics and Mathematical Modelling, Technical University of Denmark Richard Petersens Plads, Building 321, DK-2800 Kongens Lyngby, Denmark; Informatics and Mathematical Modelling, Technical University of Denmark Richard Petersens Plads, Building 321, DK-2800 Kongens Lyngby, Denmark","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",4-Jun-07,2007,4,,IV-93,IV-96,"The paper describes our work on the development of a system for retrieval of relevant stories from broadcast news. The system utilizes a combination of audio processing and text mining. The audio processing consists of a segmentation step that partitions the audio into speech and music. The speech is further segmented into speaker segments and then transcribed using an automatic speech recognition system, to yield text input for clustering using non-negative matrix factorization (NMF). We find semantic topics that are used to evaluate the performance for topic detection. Based on these topics we show that a novel query expansion can be performed to return more intelligent search results. We also show that the query expansion helps overcome errors of the automatic transcription.",2379-190X,1-4244-0727-3,10.1109/ICASSP.2007.367171,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218045,Audio Retrieval;Document Clustering;Non-negative Matrix Factorization;Text Mining,Broadcasting;Indexing;Speech analysis;Text mining;Automatic speech recognition;Mel frequency cepstral coefficient;Streaming media;Informatics;Mathematical model;Music information retrieval,document handling;matrix decomposition;query formulation;speaker recognition;speech processing,castsearch;context based spoken document retrieval;broadcast news;audio processing;text mining;speaker segments;automatic speech recognition system;non-negative matrix factorization;query expansion;automatic transcription,,4,,9,,4-Jun-07,,,IEEE,IEEE Conferences
TF-IDF method in ranking keywords of Instagram users' image captions,TF-IDF方法對Instagram用戶圖像標題的關鍵詞進行排名,B. A. Kuncoro; B. H. Iswanto,"Information Technology, Bina Nusantara University, Jakarta, Indonesia; Department of Physics, Jakarta State University, Indonesia",2015 International Conference on Information Technology Systems and Innovation (ICITSI),24-Mar-16,2015,,,1,5,"Instagram is one of the popular social media applications used by a wide range of people around the world. The significant growth of active Instagram users affects the size of Instagram data. The more number of users, the larger and more various Instagram data is posted. In line with its popularity, in recent years many researchers begin to study and analyze it for various purposes, such as detecting event photos based on location, clustering the photo content, advertising strategies based on user types, and so on. As of now there are three types of data available in Instagram which are text, image, and video. In this paper we propose Term-Frequency and Inverse Document Frequency (TF-IDF) method to rank keywords of top twenty most followed Instagram users based on image captions of Instagram. The objective of this research is to automatically know the main idea of Instagram users based on 50 recent image captions posted. In our experiments, TF-IDF has been successfully implemented to reveal a set of keywords with its ranking. The highest ranking of keyword is indeed the main topic of a user, indicated by the value of TF-IDF. The result of study indicates that TF-IDF method is very useful to find and rank the keywords of Instagram users image captions. In the future research, the ranking keywords are needed in solving classification and clustering tasks as feature extractions.",,978-1-4673-6664-9,10.1109/ICITSI.2015.7437705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437705,Instagram;text mining;Term-Frequency and Inverse Document Frequency;social media,Media;Twitter;Tagging;Text mining;Information technology;Electronic mail;Cultural differences,advertising;classification;feature extraction;image processing;pattern clustering;social networking (online);user interfaces,TF-IDF method;ranking keywords;Instagram;users image captions;social media applications;photo content;advertising strategies;term-frequency;inverse document frequency;classification;clustering tasks;feature extraction,,8,,9,,24-Mar-16,,,IEEE,IEEE Conferences
Name disambiguation in author citations using a K-way spectral clustering method,使用K-way譜聚類方法在作者引用中消除歧義,C. L. Giles; H. Zha; H. Han,"Pennsylvania State University, University Park, PA; Pennsylvania State University, University Park, PA; Yahoo! Inc., Sunnyvale, CA and The Pennsylvania State University, University Park, PA",Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '05),5-Mar-07,2005,,,334,343,"An author may have multiple names and multiple authors may share the same name simply due to name abbreviations, identical names, or name misspellings in publications or bibliographies (citations). This can produce name ambiguity which can affect the performance of document retrieval, web search, and database integration, and may cause improper attribution of credit. Proposed here is an unsupervised learning approach using K-way spectral clustering that disambiguates authors in citations. The approach utilizes three types of citation attributes: co-author names, paper titles, and publication venue titles. The approach is illustrated with 16 name datasets with citations collected from the DBLP database bibliography and author home pages and shows that name disambiguation can be achieved using these citation attributes",,1-58113-876-8,10.1145/1065385.1065462,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118563,feature selection;name disambiguation;spectral clustering;unsupervised learning,Clustering methods;Bibliographies;Information retrieval;Computer science;Web search;Unsupervised learning;Permission;Statistics;Supervised learning;Databases,citation analysis;pattern clustering;unsupervised learning,name disambiguation;author citations;K-way spectral clustering method;name abbreviations;name misspellings;name ambiguity;document retrieval;Web search;database integration;unsupervised learning approach;citation attributes;co-author names;paper titles;publication venue titles;name datasets;DBLP database bibliography;author home pages,,28,,53,,5-Mar-07,,,IEEE,IEEE Conferences
Multi-View Robust Feature Learning for Data Clustering,用於數據聚類的多視圖魯棒特徵學習,L. Zhao; T. Zhao; T. Sun; Z. Liu; Z. Chen,"School of Software Technology, Dalian University of Technology, Dalian, China; School of Software Technology, Dalian University of Technology, Dalian, China; Department of Natural Resources Information, Dalian Natural Resources Affairs Service Center, Dalian, China; First Affiliated Hospital of Dalian Medical University, Dalian, China; School of Software Technology, Dalian University of Technology, Dalian, China",IEEE Signal Processing Letters,12-Oct-20,2020,27,,1750,1754,"Multi-view feature learning can provide basic information for consistent grouping, and is very common in practical applications, such as judicial document clustering. However, it is a challenge to combine multiple heterogeneous features to learn a comprehensive description of data samples. To solve this problem, many methods explore the correlation between various features across views by assuming that all views share the same semantic information. Inspired by this, in this paper we propose a new multi-view robust feature learning (MRFL) method. In addition to projecting features from different views to a shared semantic subspace, our approach also learns the irrelevant information of data space to capture the feature dependencies between views in potential common subspaces. Therefore, the MRFL can obtain flexible feature associations hidden in multi-view data. A new objective function is designed to derive, and solve the effective optimization process of MRFL. Experiments on real-world multi-view datasets show that the proposed MRFL method is superior to the state-of-the-art multi-view learning methods.",1558-2361,,10.1109/LSP.2020.3026943,National Key Research and Development Program of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206140,Data Clustering;multi-view data;nonnegative matrix factorization;robust feature learning,Optimization;Matrix decomposition;Learning systems;Clustering algorithms;Noise measurement;Aerospace electronics;Sun,feature extraction;learning (artificial intelligence);pattern clustering,multiview data;real-world multiview datasets;MRFL method;multiview feature learning;judicial document clustering;multiple heterogeneous features;data samples;semantic information;multiview robust feature learning method;projecting features;shared semantic subspace;irrelevant information;data space;feature dependencies;potential common subspaces;flexible feature associations,,,,20,IEEE,25-Sep-20,,,IEEE,IEEE Journals
Overlap regulation for additive overlapping clustering methods,加性重疊聚類方法的重疊規則,M. Ismail Maiza; C. Ben N'cir; N. Essoussi,"LARODEC, ISG-University of Tunis; LARODEC, ESEN-University of Manouba; LARODEC, FSEGN-University of Carthage",2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS),25-Aug-16,2016,,,1,6,"Overlapping Clustering is an important technique in machine learning which aims to organize data into a set of non-disjoint groups rather than the disjoint one which is the case of conventional clustering methods. Several machine learning applications require that data object be assigned to one or several groups resulting in non-disjoint partitioning of data such as document clustering where each document can discuss one or many topics and then must be assigned to one or several groups. This paper presents a new partitional overlapping clustering method based on the additive model of overlaps. Compared to existing methods which build clusters with fixed size of overlaps, the proposed method gives users the ability to regulate this size. Experiments performed on simulated and real datasets show the performance of the proposed regulation principle to control the size of overlaps among groups.",2151-1357,978-1-4799-8710-8,10.1109/RCIS.2016.7549335,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549335,,Additives;Clustering methods;Clustering algorithms;Optimization;Linear programming;Mathematical model;Electronic mail,learning (artificial intelligence);pattern clustering,additive overlapping clustering method;machine learning;data partitioning;overlaps additive model;overlap regulation principle,,,,20,,25-Aug-16,,,IEEE,IEEE Conferences
On Text Clustering with Side Information,具有輔助信息的文本聚類,C. C. Aggarwal; Y. Zhao; P. S. Yu,"IBM T. J. Watson Res. Center, Hawthorne, NY, USA; Univ. of Illinois at Chicago, Chicago, IL, USA; Univ. of Illinois at Chicago, Chicago, IL, USA",2012 IEEE 28th International Conference on Data Engineering,2-Jul-12,2012,,,894,904,"Text clustering has become an increasingly important problem in recent years because of the tremendous amount of unstructured data which is available in various forms in online forums such as the web, social networks, and other information networks. In most cases, the data is not purely available in text form. A lot of side-information is available along with the text documents. Such side-information may be of different kinds, such as the links in the document, user-access behavior from web logs, or other non-textual attributes which are embedded into the text document. Such attributes may contain a tremendous amount of information for clustering purposes. However, the relative importance of this side-information may be difficult to estimate, especially when some of the information is noisy. In such cases, it can be risky to incorporate side-information into the clustering process, because it can either improve the quality of the representation for clustering, or can add noise to the process. Therefore, we need a principled way to perform the clustering process, so as to maximize the advantages from using this side information. In this paper, we design an algorithm which combines classical partitioning algorithms with probabilistic models in order to create an effective clustering approach. We present experimental results on a number of real data sets in order to illustrate the advantages of using such an approach.",2375-026X,978-0-7695-4747-3,10.1109/ICDE.2012.111,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228142,,Clustering algorithms;Context;Approximation methods;Noise measurement;Partitioning algorithms;Probabilistic logic;Coherence,Internet;pattern clustering;probability;social networking (online);text analysis,text clustering;side information;unstructured data;online forums;social networks;information networks;text documents;user-access behavior;Web logs;nontextual attributes;classical partitioning algorithms;probabilistic models,,12,,33,,2-Jul-12,,,IEEE,IEEE Conferences
A low-cost parallel K-means VQ algorithm using cluster computing,使用集群計算的低成本並行K均值VQ算法,A. de S. Britto; P. S. L. de Souza; R. Sabourin; S. R. S. de Souza; D. L. Borges,Pontificia Univ. Catolica do Parana; NA; NA; NA; NA,"Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,839,843,,,0-7695-1960-1,10.1109/ICDAR.2003.1227780,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227780,,Clustering algorithms;Concurrent computing;Hidden Markov models;Vector quantization;Master-slave;Computational modeling;Predictive models;Handwriting recognition;Databases;Pattern recognition,,,,1,,7,,8-Sep-03,,,IEEE,IEEE Conferences
Word image matching using dynamic time warping,使用動態時間扭曲的單詞圖像匹配,T. M. Rath; R. Manmatha,"Center for Intelligent Inf. Retrieval, Univ. of Massachusetts, Amherst, MA, USA; Center for Intelligent Inf. Retrieval, Univ. of Massachusetts, Amherst, MA, USA","2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",15-Jul-03,2003,2,,II,II,"Libraries and other institutions are interested in providing access to scanned versions of their large collections of handwritten historical manuscripts on electronic media. Convenient access to a collection requires an index, which is manually created at great labor and expense. Since current handwriting recognizers do not perform well on historical documents, a technique called word spotting has been developed: clusters with occurrences of the same word in a collection are established using image matching. By annotating ""interesting"" clusters, an index can be built automatically. We present an algorithm for matching handwritten words in noisy historical documents. The segmented word images are preprocessed to create sets of 1-dimensional features, which are then compared using dynamic time warping. We present experimental results on two different data sets from the George Washington collection. Our experiments show that this algorithm performs better and is faster than competing matching techniques.",1063-6919,0-7695-1900-8,10.1109/CVPR.2003.1211511,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1211511,,Image matching;Image segmentation;Indexing;Information retrieval;Handwriting recognition;Optical character recognition software;Character recognition;Libraries;Image recognition;Clustering algorithms,document image processing;image matching;handwriting recognition;character recognition;computer vision;image segmentation;indexing;feature extraction;pattern clustering;library automation,word image matching;dynamic time warping;library;scanned version;handwritten historical manuscript;electronic media;collection index;handwriting recognition;word spotting;word occurrence cluster;cluster annotation;handwritten word matching;noisy historical document;word image segmentation;image preprocessing;data set;George Washington collection,,234,,11,,15-Jul-03,,,IEEE,IEEE Conferences
Off-line Farsi / arabic handwritten word recognition using vector quantization and hidden Markov model,使用矢量量化和隱馬爾可夫模型的離線波斯語/阿拉伯語手寫單詞識別,B. Vaseghi; S. Alirezaee; M. Ahmadi; R. Amirfattahi,"Engineering Department, Islamic Azad University, Najafabad Branch, Iran; Engineering Department, University of Zanjan, Iran; ECE Department, University of Windsor, Ontario, Canada; ECE Department, Isfahan University of Technology, Iran",2008 IEEE International Multitopic Conference,6-Feb-09,2008,,,575,578,"In this paper a Farsi handwritten word recognition system for reading city names in postal addresses is presented. The method is based on vector quantization (VQ) and hidden Markov model (HMM). The sliding right to left window is used to extract the proper features(we have proposed four features). After feature extraction, K-means clustering is used for generation a codebook and VQ generates a codeword for each word image. In the next stage, HMM is trained by Baum Welch algorithm for each city name. A test image is recognized by finding the best match (likelihood) between the image and all of the HMM words models using forward algorithm. Experimental results show the advantages of using VQ/HMM recognizer engine instead of conventional discrete HMM.",,978-1-4244-2823-6,10.1109/INMIC.2008.4777804,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777804,Off-line character recogntion;HMM;VQ,Handwriting recognition;Vector quantization;Hidden Markov models;Cities and towns;Feature extraction;Image recognition;Engines;Pattern recognition;Character recognition;Clustering algorithms,document image processing;feature extraction;handwritten character recognition;hidden Markov models;natural language processing;pattern clustering;vector quantisation,offline Farsi-Arabic handwritten word recognition;vector quantization;hidden Markov model;feature extraction;K-means clustering;codebook generation;Baum Welch algorithm;forward algorithm;VQ-HMM recognizer engine,,2,,7,,6-Feb-09,,,IEEE,IEEE Conferences
Grouping of Customer Opinions Written in Natural Language Using Unsupervised Machine Learning,使用無監督機器學習以自然語言編寫的客戶意見分組,F. Darena; J. ika; K. Burda,"Dept. of Inf., Mendel Univ., Brno, Czech Republic; NA; Dept. of Inf., Mendel Univ., Brno, Czech Republic",2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing,18-Mar-13,2012,,,265,270,"Among one of the current and most topical tasks in the area of textual documents processing belongs the problem of automatic categorization. Clustering as the most common form of unsupervised learning enables automatic grouping of unlabeled documents into subsets called clusters. In this paper, the authors are concerned with results of clustering of very large electronic real-world data collections containing customers' reviews written freely, in English as a natural language. The reviews are automatically clustered into two groups that should contain either positive or negative reviews. The paper focuses on the analysis why certain reviews are assigned wrongly to a group containing mostly reviews of a different class. The assignment of a review into a certain cluster is based on its properties, i.e., on the words that appeared in the review. Thus, words appearing in incorrectly categorized reviews were analyzed. It was found that words that are important from the correct classification viewpoint (and thus bearing some sentiment) are often similarly important as the words in a different set than expected, therefore do not take effect as misleading information unlike words that are much more or quite insignificant.",,978-1-4673-5026-6,10.1109/SYNASC.2012.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481040,cluster mining;incorrect categorization;textual data;similarity;customer opinion,Dictionaries;Clustering algorithms;Vectors;Natural languages;Entropy;Training;Prediction algorithms,natural language processing;pattern classification;pattern clustering;text analysis;unsupervised learning,customer opinion grouping;natural language;unsupervised machine learning;textual documents processing;automatic categorization problem;automatic unlabeled document grouping;clusters;electronic real-world data collections;customers reviews;English;classification viewpoint,,2,,14,,18-Mar-13,,,IEEE,IEEE Conferences
An ontology-based text-mining method to develop intelligent information system using cluster based approach,基於聚類的基於本體的文本挖掘方法開發智能信息系統,K. Rajput; N. Kandoi,"Dept. of Computer Engineering, SSGMCE, Shegaon, India; Dept. of Computer Sci. & Engineering, SSGMCE, Shegaon, India",2017 International Conference on Inventive Systems and Control (ICISC),16-Oct-17,2017,,,1,6,"Text clustering is an important task. Generally, text document clustering methods attempt to segregate the documents into groups where each group represents some topic if the topics are same then they are belonging in the same group if the topics are different then new group can be created with the help of cluster and that different topic store in new group. In this paper has present an clustering on ontology based text mining for grouping paper or proposals and assigning that grouped proposal to reviewers systematically. It facilitates text-mining and some text extraction techniques to cluster approach based on their similarities and then to assign them to reviewer and obtain text summarization. A principled approach is proposed to develop an intelligent information system by analyzing the unstructured repair verbatim data. Construct the fault diagnosis ontology for find out the fault so that extract the irrelevant information. The three text mining technique can be used for creating intelligent information system. The proposed method is that analysis of text summarization and plagiarism analysis and find out the efficiency i.e. time complexity and increase the performance of system using cluster based approach.",,978-1-5090-4715-4,10.1109/ICISC.2017.8068581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068581,Data Mining;fault analysis;fault diagnosis;information retrieval;text processing,Ontologies;Text mining;Proposals;Fault diagnosis;Clustering algorithms;Information systems,data mining;information systems;ontologies (artificial intelligence);pattern clustering;text analysis,ontology-based text-mining method;intelligent information system;text document clustering;text extraction;text summarization;unstructured repair verbatim data;fault diagnosis ontology;plagiarism analysis;time complexity,,,,14,,16-Oct-17,,,IEEE,IEEE Conferences
Implementation of Fuzzy C-Means algorithm and TF-IDF on English journal summary,Fuzzy C-Means算法和TF-IDF在英文期刊摘要中的實現,M. Irfan; Jumadi; W. B. Zulfikar; Erik,"Department of Informatics, UIN Sunan Gunung Djati Bandung, Jl. AH Nasution No. 105, Bandung, West Java, Indonesia; Department of Informatics, UIN Sunan Gunung Djati Bandung, Jl. AH Nasution No. 105, Bandung, West Java, Indonesia; Department of Informatics, UIN Sunan Gunung Djati Bandung, Jl. AH Nasution No. 105, Bandung, West Java, Indonesia; Department of Informatics, UIN Sunan Gunung Djati Bandung, Jl. AH Nasution No. 105, Bandung, West Java, Indonesia",2017 Second International Conference on Informatics and Computing (ICIC),5-Feb-18,2017,,,1,5,"Text Summary is a process of distilling important information from various sources to produce a summary used by personal users or for certain works. Usually the method of summary is studied in language lesson, unfortunately the foreign language proficiency of Indonesian particularly on English is very low. Hence, to make summary of English texts will be difficult and automatic summary tool is needed. The application is able to summarize a text by input the file text in pdf format. Furthermore, to make us sure how important sentences in a document Weighting every single sentence will be as the best way. Weighting is used by TF-IDF method. Fuzzy C Means Method is used in Weighting resulting sentences is divided into two groups; first, important groups (High Sentence Weight) and second, unimportant groups (Low Sentence Weight).",,978-1-5386-2985-7,10.1109/IAC.2017.8280646,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280646,Automatic Text Summary Tool;Term Frequency-Inverse Document Frequency;Fuzzy C-means,Clustering algorithms;Frequency measurement;Mathematical model;Linear programming;Manuals;Filtering;Tools,electronic publishing;fuzzy set theory;pattern clustering;text analysis,English journal summary;Text Summary;language lesson;English texts;automatic summary tool;pdf format;document Weighting;file text;Low Sentence Weight;High Sentence Weight;Weighting resulting sentences;Fuzzy C Means Method;TF-IDF method,,,,16,,5-Feb-18,,,IEEE,IEEE Conferences
An Intuitive Graphic Environment for Navigation and Classification of Multimedia Documents,多媒體文檔導航和分類的直觀圖形環境,M. Campanella; R. Leonardi; P. Migliorati,"Signals and Communications Lab - DEA University of Brescia, 25123, Brescia, ITALY; Signals & Commun. Lab-DEA, Brescia Univ.; Signals & Commun. Lab-DEA, Brescia Univ.",2005 IEEE International Conference on Multimedia and Expo,24-Oct-05,2005,,,743,746,"In this work, we propose an intuitive graphic framework for the effective visualization of MPEG-7 low-level features, in the context of classification and annotation of audio-visual documents. This graphic tool is proposed to facilitate the access to the content, and to improve a quick understanding of the semantics associated to the considered document. The main visualization paradigm employed consists in representing a 2D feature space in which the shots of the audiovisual document are located. In another window, the same shots are drawn in a temporal bar that gives the users also the information related to the time domain. In the main window, shots with similar content fall near each other, and the proposed tool offers various functionalities for automatically and semi-automatically finding and annotating shot clusters in the feature space. The use of the proposed system to analyze the content of few video sequences has shown very interesting capabilities",1945-788X,0-7803-9331-7,10.1109/ICME.2005.1521530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1521530,,Graphics;Navigation;Visualization;MPEG 7 Standard;Multimedia systems;Content based retrieval;Context;Video sequences;Indexing;Standards development,data compression;data visualisation;feature extraction;formal logic;image classification;image representation;image sequences;multimedia communication;video coding,intuitive graphic environment;multimedia document;MPEG-7 low-level feature;classification context;audio-visual document;visualization paradigm;2D feature space representation;shot cluster annotation;video sequence,,2,,11,,24-Oct-05,,,IEEE,IEEE Conferences
Integrating rich document representations for text classification,集成豐富的文檔表示形式以進行文本分類,S. Jiang; J. Lewris; M. Voltmer; H. Wang,"Data Science Institute, University of Virginia; Data Science Institute, University of Virginia; Data Science Institute, University of Virginia; Computer Science, University of Virginia",2016 IEEE Systems and Information Engineering Design Symposium (SIEDS),13-Jun-16,2016,,,303,308,"This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2-3 percent.",,978-1-5090-0970-1,10.1109/SIEDS.2016.7489319,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489319,Natural Language Processing;Text Classification;Text Mining;Word2vec,Neural networks;Logistics;Conferences;Text categorization;Measurement;Semantics;Benchmark testing,data mining;inference mechanisms;natural language processing;neural nets;pattern classification;text analysis,unstructured text data;rich-document representation integration;machine learning text classification problem improvement;document classification performance;word vector representations;inference;word vector clustering abilities;neural network;text sources;premarked binary outcomes;benchmark metric;feature space;area under-the-curve,,6,,10,,13-Jun-16,,,IEEE,IEEE Conferences
SVD: a novel content-based representation technique for Web documents,SVD：一種新穎的基於內容的Web文檔表示技術,W. L. Chue; L. H. Chen,"Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint",4-May-04,2003,3,,1840,1844 vol.3,"Most users typically express their information need via short queries to search engines and they often have to physically sift through the search results based on relevance ranking, making the process of relevance judgement time-consuming. The structure of the Web is increasingly being used to improve organisation, search and analysis of information on the Web. In this paper, we describe how the Web structure together with our novel summarisation techniques can be applied to better represent knowledge in actual Web documents via the proposed semantic virtual documents (SVD). We will also outline our experimental design to evaluate the effectiveness of the proposed SVD with a prototype system called iSEARCH (intelligent search and review of cluster hierarchy) for Web content retrieval and mining. The experimental results confirm that the novel technique show promising qualities in content-based representation for Web documents to enhance Web content retrieval and mining.",,0-7803-8185-8,10.1109/ICICS.2003.1292785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1292785,,Knowledge representation;Content based retrieval;Internet;Search engines;Information analysis;Design for experiments;Prototypes;Information retrieval;Intelligent systems;Visualization,Internet;virtual reality;content-based retrieval;data mining,SVD;semantic virtual documents;content-based representation technique;Web documents;communications techniques;SEARCH;intelligent search and review of cluster hierarchy;Web content retrieval;Web content mining,,2,,11,,4-May-04,,,IEEE,IEEE Conferences
Document block identification using a neural network,使用神經網絡識別文檔塊,C. Strouthopoulos; N. Papamarkos,"Dept. of Electr. & Comput. Eng., Democritus Univ. of Thrace, Xanthi, Greece; NA",Proceedings of 13th International Conference on Digital Signal Processing,6-Aug-02,1997,2,,999,1002 vol.2,"This paper describes a new method that clusters the content of a mixed type document in text or nontext areas. The proposed approach is based on a new set of textural features combined with a two stage neural network classifier. The neural network classifier consists of a principal components analyzer and a Kohonen self organized feature map. Document blocks are classified as text, graphics and halftones or to secondary subclasses corresponding to special cases of the primal classes. The proposed method can identify text regions included in graphics or even overlapped regions, that is, regions that cannot be separated with horizontal and vertical cuts. The performance of the method was extensively tested on a variety of documents with very promising results.",,0-7803-4137-6,10.1109/ICDSP.1997.628532,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=628532,,Neural networks;Graphics;Layout;Circuits;Laboratories;Coils;Robustness;Automatic testing;Databases;Histograms,document image processing;image segmentation;self-organising feature maps,document block identification;mixed type document;text areas;nontext areas;clustering;textural features;two-stage neural network classifier;principal components analyzer;PCA;Kohonen self organized feature map;graphics;halftones;secondary subclasses;segmentation,,,,16,,6-Aug-02,,,IEEE,IEEE Conferences
Finding structure in diversity: a hierarchical clustering method for the categorization of allographs in handwriting,在多樣性中尋找結構：一種用於手寫筆跡分類的分層聚類方法,L. Vuurpijl; L. Schomaker,"Inst. for Cognition & Inf., Nijmegen Univ., Netherlands; NA",Proceedings of the Fourth International Conference on Document Analysis and Recognition,6-Aug-02,1997,1,,387,393 vol.1,"The paper introduces a variant of agglomerative hierarchical clustering techniques. The new technique is used for categorizing character shapes (allographs) in large data sets of handwriting into a hierarchical structure. Such a technique may be used as the basis for a systematic naming scheme of character shapes. Problems with existing methods are described and the proposed method is explained. After application of the method to a very large set of characters, separately for all the letters of the alphabet, relevant clusters are identified and given a unique name. Each cluster represents an allograph prototype.",,0-8186-7898-4,10.1109/ICDAR.1997.619876,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=619876,,Clustering methods;Shape;Writing;Databases;Educational institutions;Cognition;Helium;Prototypes;Taxonomy;Clustering algorithms,character sets;character recognition,allograph categorization;handwriting;agglomerative hierarchical clustering techniques;character shape categorization;large data sets;systematic naming scheme;character shapes;character set,,15,,12,,6-Aug-02,,,IEEE,IEEE Conferences
Clustering based feature selection using Extreme Learning Machines for text classification,使用Extreme Learning Machines進行基於聚類的特徵選擇以進行文本分類,R. K. Roul; S. Gugnani; S. M. Kalpeshbhai,"Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, India - 403726; Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, India - 403726; Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, India - 403726",2015 Annual IEEE India Conference (INDICON),31-Mar-16,2015,,,1,6,"The expansion of the dynamic Web increases the digital documents, which has attracted many researchers to work in the field of text classification. It is an important and well studied area of machine learning with a variety of modern applications. A good feature selection is of paramount importance to increase the efficiency of the classifiers working on text data. Choosing the most relevant features out of what can be an incredibly large set of data, is particularly important for accurate text classification. This paper is a motivation in that direction where we propose a new clustering based feature selection technique that reduces the feature size. Traditional k-means clustering technique along with TF-IDF and Wordnet helps us to form a quality and reduced feature vector to train the Extreme Learning Machine (ELM) and Multi-layer ELM (ML-ELM) which have been used as the classifiers for text classification. The experimental work has been carried out on 20-Newsgroups and DMOZ datasets. Results on these two standard datasets demonstrate the efficiency of our approach using ELM and ML-ELM as the classifiers over the state-of-the-art classifiers.",2325-9418,978-1-4673-7399-9,10.1109/INDICON.2015.7443788,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443788,Classification;ELM;K-Means;ML-ELM;SVM;Wordnet,Training;Clustering algorithms;Frequency measurement;Support vector machines;Neural networks;Computational modeling;Computer science,feature selection;Internet;learning (artificial intelligence);pattern clustering;text analysis;vectors,clustering based feature selection;extreme learning machines;text data classification;dynamic Web;digital documents;machine learning;k-means clustering technique;TF-IDF;Wordnet;feature vector;multilayer ELM;ML-ELM;DMOZ datasets,,4,,16,,31-Mar-16,,,IEEE,IEEE Conferences
Clustering XML Elements for Efficient Code Generation,集群XML元素以高效生成代碼,S. Y. M. Nadvi; R. M. Rahman,NA; NA,2011 10th IEEE/ACIS International Conference on Computer and Information Science,28-Nov-11,2011,,,91,96,"Several works [1,2,3] have been reported in literature to cluster XML documents and/or XML Schemas based on structure similarity, semantic similarity, edit distance similarity etc. Our current attempt clusters the elements presented in an XML Schema based on conversion cost that incurs to transform one object to the other. Our strategy would help a dynamic XML compiler to reproduce classes for new elements in C# programming language by modifying existing classes rather generating those from the very beginning.",,978-1-4577-0141-2,10.1109/ICIS.2011.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086454,clustering;XML Schema;conversion cost;XML document;outlier,Prototypes;XML;Clouds;Middleware;Meteorology;Temperature distribution,,,,,,9,,28-Nov-11,,,IEEE,IEEE Conferences
Query-Based Multi-Document Summarization Using Non-Negative Semantic Feature and NMF Clustering,使用非負語義特徵和NMF聚類的基於查詢的多文檔摘要,S. Park; B. Cha,"Dept. of Comput. Eng., Univ. of Honam, Gwangju; Dept. of Comput. Eng., Univ. of Honam, Gwangju",2008 Fourth International Conference on Networked Computing and Advanced Information Management,12-Sep-08,2008,2,,609,614,"In this paper, a novel summarization method, which uses non-negative matrix factorization (NMF) and NMF clustering, is introduced to extract meaningful sentences from query-based multi-documents. The proposed method decomposes a sentence into the linear combination of sparse non-negative semantic features so that it can represent a sentence as the sum of a few semantic features that are comprehensible intuitively. It can improve the quality of document summaries because it can avoid extracting the sentences whose similarities with query are high but are meaningless by using the similarity between the query and the semantic features. Besides, it uses NMF clustering to remove noises so that it can avoid the biased inherent semantics of the documents to be reflected in summaries. Also it can ensure the coherence of summaries by using the rank score of sentences with respect to semantic features. The experimental results demonstrate that the proposed method has better performance than other methods using the thesaurus, the LSA, the K-means, and the NMF.",,978-0-7695-3322-3,10.1109/NCM.2008.246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624213,,Feature extraction;Matrix decomposition;Thesauri;Noise;Equations;Data mining;Coherence,matrix decomposition;pattern clustering;query processing;text analysis,query-based multidocument summarization;NMF clustering;nonnegative matrix factorization;sparse nonnegative semantic feature,,3,,21,,12-Sep-08,,,IEEE,IEEE Conferences
Extending Skel to Support the Development and Optimization of Next Generation I/O Systems,擴展Skel以支持下一代I / O系統的開發和優化,J. Logan; J. Y. Choi; M. Wolf; G. Ostrouchov; L. Wan; N. Podhorszki; W. Godoy; S. Klasky; E. Lohrmann; G. Eisenhauer; C. Wood; K. Huck,"Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Oak Ridge Nat. Lab., Oak Ridge, TN, USA; Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA; Univ. of Oregon, Eugene, OR, USA; Univ. of Oregon, Eugene, OR, USA",2017 IEEE International Conference on Cluster Computing (CLUSTER),25-Sep-17,2017,,,563,571,"As the memory and storage hierarchy get deeper and more complex, it is important to have new benchmarks and evaluation tools that allow us to explore the emerging middleware solutions to use this hierarchy. Skel is a tool aimed at automating and refining this process of studying HPC I/O performance. It works by generating application I/O kernel/benchmarks as determined by a domain-specific model. This paper provides some techniques for extending Skel to address new situations and to answer new research questions. For example, we document use cases as diverse as using Skel to troubleshoot I/O performance issues for remote users, refining an I/O system model, and facilitating the development and testing of a mechanism for runtime monitoring and performance analytics. We also discuss data oriented extensions to Skel to support the study of compression techniques for Exascale scientific data management.",2168-9253,978-1-5386-2326-8,10.1109/CLUSTER.2017.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048970,I/O benchmarking;Mini Applications;Generative Programming;High Performance I/O;Scientific Data Management;I/O performance;Skel;Adios;Runtime performance monitoring;Runtime performance analytics;Data Compression,Tools;Benchmark testing;Generators;Computational modeling;Hardware;Bandwidth;Data models,computer peripheral equipment;middleware,storage hierarchy;memory hierarchy;data oriented extensions;exascale scientific data management;compression techniques;performance analytics;runtime monitoring;remote users;domain-specific model;I/O kernel-benchmarks;middleware solutions;evaluation tools;HPC performance;Skel;next generation I/O systems,,2,,33,,25-Sep-17,,,IEEE,IEEE Conferences
On-line new event detection using time window strategy,使用時間窗策略進行在線新事件檢測,R. Xu; W. Peng; J. Xu; X. Long,"M School of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; M School of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; M School of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; M School of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China",2011 International Conference on Machine Learning and Cybernetics,12-Sep-11,2011,4,,1932,1937,"New Event Detection is the task of automatically detecting novel events from a temporally-ordered stream of news story documents. Traditionally, on-line new event detection system determines whether the incoming document contains a new event based on the history of all processed documents. With the improvement of processed documents, the efficiency of on-line new event detection will decrease. In this paper, we apply a time window strategy to new event detection. A group of new incoming documents in the time window are firstly clustered to obtain candidate topics. Next, these candidate topics are compared with the previously identified topics to determine whether a new topic is detected. The first story of the new detected topic, in temporal order, is regarded as a new event. By analyzing the news story documents within the time window in group, the new event detection efficiency is improved. Furthermore, the evaluations show that the time window processing strategy is helpful to improve the accuracy of new event detection.",2160-1348,978-1-4577-0308-9,10.1109/ICMLC.2011.6016957,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016957,New Event Detection;Time Window;Clustering,Event detection;Clustering algorithms;Machine learning;Cybernetics;Real time systems;Vocabulary;Time frequency analysis,document handling;Internet;pattern clustering,on-line new event detection;time window strategy;news story document processing;temporally-ordered stream,,3,,14,,12-Sep-11,,,IEEE,IEEE Conferences
Confronting Sparseness and High Dimensionality in Short Text Clustering via Feature Vector Projections,通過特徵向量投影在短文本聚類中應對稀疏性和高維性,L. Akritidis; M. Alamaniotis; A. Fevgas; P. Bozanis,"School of Science and Technology, Int'l Hellenic University,Thessaloniki,Greece; University of Texas at San Antonio,Department of Electrical and Computer Engineering,San Antonio,USA; University of Thessaly,Department of Electrical and Computer Engineering,Volos,Greece; School of Science and Technology, Int'l Hellenic University,Thessaloniki,Greece",2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),24-Dec-20,2020,,,813,820,"Short text clustering is a popular problem that focuses on the unsupervised grouping of similar short text documents, or entitled entities. Since the short texts are currently being utilized in a vast number of applications, the problem in question has been rendered increasingly significant in the past few years. The high cluster homogeneity and completeness are two among the most important goals of all data clustering algorithms. However, in the context of short texts, their fulfilment is particularly difficult, because this type of data is typically represented by sparse vectors that collectively comprise a very high dimensional space. In this article we introduce VEPHC, a two-stage clustering algorithm designed to confront the sparseness and high dimensionality traits of short texts. During the first stage (or else, the VEP part), the initial feature vectors are projected onto a lower dimensional space by constructing and scoring variable-sized combinations of features (that is, terms). In the second stage (or else, the HC part), VEPHC improves the homogeneity and completeness of the generated clusters through split and merge operations that are based on the similarities of all inter-cluster elements. The experimental evaluation of VEPHC on two real-world datasets demonstrates its superior performance over numerous state-of-the-art clustering algorithms in terms of F1 scores and Normalized Mutual Information.",2375-0197,978-1-7281-9228-4,10.1109/ICTAI50040.2020.00129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288177,short text clustering;text mining;machine learning;unsupervised learning;clustering;data mining,Conferences;Clustering methods;Merging;Clustering algorithms;Tools;Artificial intelligence;Mutual information,,,,,,22,,24-Dec-20,,,IEEE,IEEE Conferences
Analysis of Non-Negative Double Singular Value Decomposition Initialization Method on Eigenspace-based Fuzzy C-Means Algorithm for Indonesian Online News Topic Detection,基於特徵空間的模糊C均值算法用於印度尼西亞在線新聞主題檢測的非負雙奇異值分解初始化方法分析,R. T. Sutrisman; H. Murfi,"Universitas Indonesia, Department of Mathematics, Depok, 16424, Indonesia; Universitas Indonesia, Department of Mathematics, Depok, 16424, Indonesia",2018 6th International Conference on Information and Communication Technology (ICoICT),11-Nov-18,2018,,,55,60,"The rapid increasing of online news in Indonesia creates the need for news analysis to obtain information as fast as possible. Topics are basic components that are often used to analyze data in the textual forms, such as the news article. By using topic modeling, topics can be detected automatically on large news documents which are difficult to perform manually. One of the topic modeling that can be used is the clustering-based method, i.e., Eigenspace-based Fuzzy C-Means (EFCM). The common initialization method of EFCM is random. However, this random initialization usually produces different topics for each run. Therefore, we consider Non-Negative Double Singular Value Decomposition (NNDSVD) as an initialization method of EFCM. Besides the advantage of non-randomness, our simulations show that the NNDSVD method gives better accuracies in term of interpretability score than the random method.",,978-1-5386-4572-7,10.1109/ICoICT.2018.8528791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528791,topic detection;fuzzy c-means;eigenspace;initialization,Clustering algorithms;Matrix decomposition;Singular value decomposition;Portals;Clustering methods;Data models;Coherence,eigenvalues and eigenfunctions;fuzzy set theory;pattern clustering;singular value decomposition;text analysis,random initialization;nonrandomness;NNDSVD method;random method;indonesian online news topic detection;news analysis;textual forms;news article;topic modeling;news documents;clustering-based method;nonnegative double singular value decomposition initialization method;Eigenspace-based fuzzy c-means algorithm,,,,13,,11-Nov-18,,,IEEE,IEEE Conferences
Using Multilayer TMDOM Model to Organize and Retrieve Textual Information,使用多層TMDOM模型組織和檢索文本信息,W. Jiang-ning; T. Hai-yan,"Institute of Systems Engineering, Dalian University of Technology, P.R.China, 116024; Institute of Systems Engineering, Dalian University of Technology, P.R.China, 116024",2006 International Conference on Management Science and Engineering,4-Sep-07,2006,,,146,151,"Infoglut that comes with the development of Internet-connected computers results in difficulties of knowledge management (KM). So making an efficient organization and retrieval of overwhelming amount of Web resources ad hoc electronic document resources becomes more and more crucial. The topic map (TM), as an effective technology that is good at modeling and representing knowledge resources, is adopted in our study aiming to organize and retrieve textual information efficiently. In this paper, a multilayer topic-map-based model is proposed to organize the underlying concepts induced from domain document contents as the hierarchical structure by which the domain textual resources can then be linked to the related concepts in TM. Compared with the traditional statistics-based model, the searching process using TM-based model can be bounded in the narrow topic space, which leads to a high-quality retrieval results in terms of precision and recall. The experiment in the given domain is designed and implemented to evaluate the performance of the proposed model. The experimental results show that the efficient knowledge navigation can be made through building various associations between topics and textual resources",2155-1855,7-5603-2355-3,10.1109/ICMSE.2006.313917,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4104884,Document clustering;Document organization;Document retrieval;TMDOM;Topic map,Nonhomogeneous media;Information retrieval;Navigation;Information resources;Knowledge engineering;Systems engineering and theory;Internet;Knowledge management;Content based retrieval;Buildings,information retrieval;Internet;knowledge management;pattern clustering;text analysis,multilayer TMDOM model;textual information retrieval;Internet-connected computers;knowledge management;Web resources;electronic document resources;knowledge resources;multilayer topic-map-based model;statistics-based model,,,,15,,4-Sep-07,,,IEEE,IEEE Conferences
Semi-Supervised Multiple Disambiguation,半監督多重消歧,K. Ghoorchian; F. Rahimian; S. Girdzijauskas,"Sch. of Electr. Eng., R. Inst. of Technol., Stockholm, Sweden; Sch. of Electr. Eng., R. Inst. of Technol., Stockholm, Sweden; Sch. of Electr. Eng., R. Inst. of Technol., Stockholm, Sweden",2015 IEEE Trustcom/BigDataSE/ISPA,3-Dec-15,2015,2,,88,95,"Determining the true entity behind an ambiguous word is an NP-Hard problem known as Disambiguation. Previous solutions often disambiguate a single ambiguous mention across multiple documents. They assume each document contains only a single ambiguous word and a rich set of unambiguous context words. However, nowadays we require fast disambiguation of short texts (like news feeds, reviews or Tweets) with few context words and multiple ambiguous words. In this research we focus on Multiple Disambiguation (MD) in contrast to Single Disambiguation (SD). Our solution is inspired by a recent algorithm developed for SD. The algorithm categorizes documents by first, transferring them into a graph and then, clustering the graph based on its topological structure. We changed the graph-based document-modeling of the algorithm, to account for MD. Also, we added a new parameter that controls the resolution of the clustering. Then, we used a supervised sampling approach for merging the clusters when appropriate. Our algorithm, compared with the original model, achieved 10% higher quality in terms of F1-Score using only 4% sampling from the dataset.",,978-1-4673-7952-6,10.1109/Trustcom.2015.566,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345479,Graph Algorithms;Parallel Processing;Coreference resolution;Diffusion Clustering,Context;Clustering algorithms;Image color analysis;Electrical engineering;Electronic mail;Algorithm design and analysis;Skin,computational complexity;graph theory;learning (artificial intelligence);optimisation;pattern clustering;sampling methods;text analysis;word processing,semisupervised multiple disambiguation;MD;NP-hard problem;text disambiguation;graph clustering;topological structure;graph-based document-modelling;supervised sampling approach;cluster merging,,,,20,,3-Dec-15,,,IEEE,IEEE Conferences
A data reduction method for efficient document skew estimation based on Hough transformation,基於霍夫變換的高效文檔歪斜估計數據約簡方法,Younki Min; Sung-Bae Cho; Yillbyung Lee,"Dept. of Comput. Sci., Yonsei Univ., Seoul, South Korea; NA; NA",Proceedings of 13th International Conference on Pattern Recognition,6-Aug-02,1996,3,,732,736 vol.3,"Document recognition usually requires several preprocessing steps in which skew estimation and correction are critical to get a useful system. This paper proposes an efficient data reduction method to enhance the performance of document skew estimation by using a Hough transformation. The time complexity of the Hough transformation is O(/spl Theta/N), where N is the number of black pixels in a document and /spl Theta/ is the skew estimation range divided by /spl Delta//spl theta/. We might enhance the performance by reducing N or /spl Theta/. The proposed method uses an efficient data reduction method called the modified version of divided horizontal histograms, which reduces the number of black pixels N, while retaining the skewness of document. In order to show the superiority of the proposed method, we have also performed experiments with scanned documents, comparing the result with those of the usual data reduction methods: vertical run-length and connected component methods.",1051-4651,0-8186-7282-X,10.1109/ICPR.1996.547265,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547265,,Text analysis;Histograms;Clustering algorithms;Computer science;Algorithm design and analysis;Goniometers;Frequency;Image resolution,computational complexity,data reduction method;document skew estimation;Hough transformation;time complexity;black pixels;divided horizontal histograms;vertical run-length;connected component methods,,4,,16,,6-Aug-02,,,IEEE,IEEE Conferences
Unsupervised Construction of Topic-Based Twitter Lists,基於主題的Twitter列表的無監督構造,F. de Villiers; M. Hoffmann; S. Kroon,"Comput. Sci. Div., Stellenbosch Univ., Stellenbosch, South Africa; Comput. Sci. Div., Stellenbosch Univ., Stellenbosch, South Africa; Comput. Sci. Div., Stellenbosch Univ., Stellenbosch, South Africa","2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing",10-Jan-13,2012,,,283,292,"The Twitter lists feature was launched in late 2009 and enables the creation of curated groups containing Twitter users. Each user can be a list author and decide the basis on which other users are added to a list. The most popular lists are those that associate with a topic. Twitter lists can be used as a powerful organisation tool, but its widespread adoption has been limited. The two main obstacles are the initial setup time and the effort of continual curation. In this paper we attempt to solve the first problem by applying unsupervised clustering algorithms to construct topic-based Twitter lists. We consider k-means and affinity propagation (AP) as clustering algorithms and evaluate these algorithms using two document representation techniques. The selected representation techniques are the popular term frequency-inverse document frequency (TF-IDF) and the latent Dirichlet allocation (LDA) topic model. We calculate the similarities for the clustering algorithms using five well-known similarity measures that have been used extensively in the text domain. The adjusted normalised information distance (ANID) was used to compare the clustering result yielded by k-means and affinity propagation. We found that the careful selection of a similarity measure, combined with the LDA topic model can provide a user with a sensible starting point for list creation.",,978-1-4673-5638-1,10.1109/SocialCom-PASSAT.2012.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406257,clustering;Twitter;topics;unsupervised;similarity,Clustering algorithms;Vectors;Entropy;Twitter;Correlation;Indexes;Resource management,pattern clustering;social networking (online);text analysis,unsupervised construction;topic-based Twitter list;Twitter lists feature;curated group;organisation tool;continual curation;unsupervised clustering algorithm;k-means;affinity propagation;AP;document representation technique;term frequency-inverse document frequency;TF-IDF;latent Dirichlet allocation;LDA topic model;similarity measures;text domain;adjusted normalised information distance;ANID;list creation,,,,33,,10-Jan-13,,,IEEE,IEEE Conferences
Cluster tool wafer handler reliability modeling using top-down and bottom-up methodologies,使用自上而下和自下而上的方法對群集工具晶圓處理機進行可靠性建模,P. D. Ashe,"Brooks Autom., Lowell, MA, USA",Proceedings of 1994 IEEE/SEMI Advanced Semiconductor Manufacturing Conference and Workshop (ASMC),6-Aug-02,1994,,,257,260,"Brooks Automation produces automated, vacuum substrate-material-handling products for the semiconductor equipment industry. The Brooks Cluster Tool Wafer Handler is the Cluster Tool manufacturer's Central Wafer Handler (CWH) for PVD, CVD, RTP, etch, and other applications used to manufacture semiconductor devices. Since the Brooks CWH is integrated directly into the customer's Cluster Tool, the CWH reliability requirements are dictated by the Cluster Tool reliability requirements. Typically there is not enough time in the design and development phase to empirically determine the actual system reliability (with confidence). Therefore, reliability modeling is used to determine if the reliability requirements have been achieved. ""Top down"" and ""bottom up"" modeling are two methods used to estimate the reliability of a complex system, such as the CWH. The top down method uses knowledge of the system's failure modes to generate a system reliability estimate. These failure modes are identified by evaluating the system's functionality at the top level, then proceeding down to the lower level subsystems to evaluate their failure modes. The bottom up method uses knowledge of the system's component failure modes (or failure rates) to generate a system reliability estimate. These failure modes are generated by the component type and application. This document is intended to provide project managers and engineers with a systematic way to estimate the reliability of complex systems when reliability demonstration data is not available. The examples presented in this document focus on the Brooks CWH. In this document the author: 1) presents two reliability modeling methods, top down and bottom up, for developing reliability models for the Brooks CWH; 2) provides step-by-step procedures for creating these models; and 3) explains the appropriate application of these models to the CWH.",,0-7803-2053-0,10.1109/ASMC.1994.588264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=588264,,Semiconductor device modeling;Reliability;Semiconductor device manufacture;Manufacturing automation;Manufacturing industries;Atherosclerosis;Etching;Semiconductor devices;Project management;Engineering management,cluster tools,cluster tool;wafer handler reliability;reliability modeling;top-down methodology;bottom-up methodology;material-handling products;semiconductor equipment industry;PVD;CVD;RTP;etching;failure modes,,,,4,,6-Aug-02,,,IEEE,IEEE Conferences
Inexact graph matching for entity recognition in OCRed documents,OCRed文檔中用於實體識別的不精確圖形匹配,N. Kooli; A. Bela簿d,"LORIA - Universit矇 de Lorraine Campus scientifique - BP 239 - 54506 Vandoeuvre-l癡s-Nancy, France; LORIA - Universit矇 de Lorraine Campus scientifique - BP 239 - 54506 Vandoeuvre-l癡s-Nancy, France",2016 23rd International Conference on Pattern Recognition (ICPR),24-Apr-17,2016,,,4071,4076,This paper proposes an entity recognition system in image documents recognized by OCR. The system is based on a graph matching technique and is guided by a database describing the entities in its records. The input of the system is a document which is labeled by the entity attributes. A first grouping of those labels based on a function score leads to a selected set of candidate entities. The entity labels which are locally close are modeled by a structure graph. This graph is matched with model graphs learned for this purpose. The graph matching technique relies on a specific cost function that integrates the feature dissimilarities. The matching results are exploited to correct the mislabeling errors and then validate the entity recognition task. The system evaluation on three datasets which treat different kind of entities shows a variation between 88.3% and 95% for recall and 94.3% and 95.7% for precision.,,978-1-5090-4847-2,10.1109/ICPR.2016.7900271,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900271,entity recognition;local structure;graph matching;mislabeling correction;structure model;graph clustering,Databases;Optical character recognition software;Context;Labeling;Cost function;Pattern recognition;Image recognition,document image processing;image matching;optical character recognition,inexact graph matching;OCRed documents;entity recognition system;image documents;OCR;structure graph;cost function;feature dissimilarities;mislabeling errors,,1,,17,,24-Apr-17,,,IEEE,IEEE Conferences
Workload-aware load balancing for clustered Web servers,群集Web服務器的工作負載感知負載平衡,Qi Zhang; A. Riska; W. Sun; E. Smirni; G. Ciardo,"Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA; NA; NA; NA",IEEE Transactions on Parallel and Distributed Systems,31-Jan-05,2005,16,3,219,233,"We focus on load balancing policies for homogeneous clustered Web servers that tune their parameters on-the-fly to adapt to changes in the arrival rates and service times of incoming requests. The proposed scheduling policy, ADAPTLOAD, monitors the incoming workload and self-adjusts its balancing parameters according to changes in the operational environment such as rapid fluctuations in the arrival rates or document popularity. Using actual traces from the 1998 World Cup Web site, we conduct a detailed characterization of the workload demands and demonstrate how online workload monitoring can play a significant part in meeting the performance challenges of robust policy design. We show that the proposed load, balancing policy based on statistical information derived from recent workload history provides similar performance benefits as locality-aware allocation schemes, without requiring locality data. Extensive experimentation indicates that ADAPTLOAD results in an effective scheme, even when servers must support both static and dynamic Web pages.",1558-2183,,10.1109/TPDS.2005.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388212,Clustered Web servers;self-managing clusters;load balance;locality awareness;workload characterization;static and dynamic pages.,Load management;Web server;Vehicle dynamics;Switches;Robustness;Availability;Service oriented architecture;Scalability;Internet;Sun,resource allocation;Internet;file servers;workstation clusters;Web sites,workload-aware load balancing;clustered Web servers;operational environment;rapid fluctuation;1998 World Cup Web site;online workload monitoring;robust policy design;locality-aware allocation schemes;dynamic Web pages;self-managing clusters,,87,,26,,31-Jan-05,,,IEEE,IEEE Journals
"Self-adaptive GA, quantitative semantic similarity measures and ontology-based text clustering",自適應遺傳算法，定量語義相似性度量和基於本體的文本聚類,C. Zhang; W. Song; C. Li; W. Yu,"Department of Information Management, Nanjing University of Science & Technology, Institute of Sci & Tech Information of China, China; Division of Electronics and Information Engineering, Chonbuk National University, Jeonju, Jeonbuk, Korea; Division of Electronics and Information Engineering, Chonbuk National University, Jeonju, Jeonbuk, Korea; Institute of Scientific & Technical Information of China, Beijing, China",2008 International Conference on Natural Language Processing and Knowledge Engineering,2-May-09,2008,,,1,8,"As the common clustering algorithms use vector space model (VSM) to represent document, the conceptual relationships between related terms which do not co-occur literally are ignored. A genetic algorithm-based clustering technique, named GA clustering, in conjunction with ontology is proposed in this article to overcome this problem. In general, the ontology measures can be partitioned into two categories: thesaurus-based methods and corpus-based methods. We take advantage of the hierarchical structure and the broad coverage taxonomy of Wordnet as the thesaurus-based ontology. However, the corpus-based method is rather complicated to handle in practical application. We propose a transformed latent semantic analysis (LSA) model as the corpus-based method in this paper. Moreover, two hybrid strategies, the combinations of the various similarity measures, are implemented in the clustering experiments. The results show that our GA clustering algorithm, in conjunction with the thesaurus-based and the LSA-based method, apparently outperforms that with other similarity measures. Moreover, the superiority of the GA clustering algorithm proposed over the commonly used k-means algorithm and the standard GA is demonstrated by the improvements of the clustering performance.",,978-1-4244-4515-8,10.1109/NLPKE.2008.4906791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906791,Clustering;ontology;latent semantic analysis;semantic similarity measure;genetic algorithm,Ontologies;Clustering algorithms;Partitioning algorithms;Taxonomy;Genetic algorithms;Iterative algorithms;Information management;Extraterrestrial measurements;Algorithm design and analysis;Web sites,genetic algorithms;ontologies (artificial intelligence);pattern clustering;text analysis;thesauri,self-adaptive genetic algorithm;quantitative semantic similarity measures;ontology-based text clustering;vector space model;thesaurus-based methods;corpus-based methods;latent semantic analysis,,,,20,,2-May-09,,,IEEE,IEEE Conferences
A template-based method for theme information extraction from web pages,基於模板的網頁主題信息提取方法,Gui-Sheng Yin; Guang-Dong Guo; Jing-Jing Sun,"Department of Computer Science and Technology, Harbin Engineering University, China; Department of Computer Science and Technology, Harbin Engineering University, China; Department of Computer Science and Technology, Harbin Engineering University, China",2010 International Conference on Computer Application and System Modeling (ICCASM 2010),4-Nov-10,2010,3,,V3-721,V3-725,"The introducing web page templates and DOM technology can effectively extract simple structured information from web information. In reference to previous research achievements of the foundation, this paper presents a new method of inductive web page templates. This method is able to contain various layout elements of the web page templates. The main research contents include the methods based on edit distance, about DOM document similarity judgment, clustering methods focus on web structure, the extraction methods of web page templates and programming a information extraction engine.",2161-9077,978-1-4244-7237-6,10.1109/ICCASM.2010.5620763,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620763,Web Extraction;Template Method;Page similarity;Web Clustering,Electronic mail;Noise reduction,distributed object management;document handling;information retrieval;Web sites,theme information extraction;Web pages;DOM technology;simple structured information;Web information;inductive Web page templates;edit distance;DOM document similarity judgment;clustering method;Web structure;information extraction engine,,,,9,,4-Nov-10,,,IEEE,IEEE Conferences
Concept - based semantic annotation indexing and retrieval for E- Learning web documents,基於概念的電子學習Web文檔的語義註釋索引和檢索,R. Sarasu; K. K. Thyagharajan,"Computer Science and Engineering, Dhanalaksmi Srinivasan College Of Engineering and Technology, Anna University, Chennai, India; R.M.D Engineering College, Anna University, Chennai, India","2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)",2-Jun-14,2013,,,833,836,"E-learning is the technology enables the people to access the Information anytime and anywhere. E-Learning Is just-in-time learning grown from the new learning requirements. ?Semantic Web??is an enhancement of WWW architecture that provides content with formal semantics. The semantic web technology can be applied in different areas. E-Learning is one of the areas in which can benefitted by semantic web technology. This can be done by two folds, gathering the materials and creating semi-automatic annotation for the e-learning materials. After annotation, with the help of metadata, the annotation-ontology knowledge base defined for the documents and clustering is used when there are same concepts coming under the hierarchy. Whenever the user searches the query, it compared with the defined ontology present in the knowledge base. The user query and the defined ontology are matched the user are presented with the required documents and if not matched the query is refined.",,978-1-4673-6126-2,10.1109/ICGCE.2013.6823549,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823549,Semantic web;clustering;Ontology;indexing,Handheld computers;Decision support systems;Conferences,computer aided instruction;indexing;ontologies (artificial intelligence);query processing;semantic Web,concept-based semantic annotation;semantic annotation indexing;semantic annotation retrieval;e-learning Web documents;electronic learning;just-in-time learning;learning requirements;semantic Web;WWW architecture;World Wide Web;meta data;annotation-ontology knowledge base;user query;defined ontology,,,,13,,2-Jun-14,,,IEEE,IEEE Conferences
Using latent topic features to improve binary classification of spoken documents,使用潛在主題功能改進語音文檔的二進制分類,J. Wintrode,"Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",11-Jul-11,2011,,,5544,5547,"In many topic identification applications, supervised training labels are indirectly related to the semantic content of the documents being classified. For example, many topically distinct emails will all be assigned a single broad category label of ""spam"" or ""not-spam"", and a two-class classifier will lack direct knowledge of the underlying topic structure. This paper examines the degradation of topic identification performance on conversational speech when multiple semantic topics are combined into a single broad category. We then develop techniques using document clustering and Latent Dirchlet Allocation (LDA) to exploit the underlying semantic topics which improve performance over classifiers trained on the single category label by up to 20%.",2379-190X,978-1-4577-0539-7,10.1109/ICASSP.2011.5947615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947615,topic identification;LDA;clustering,Training;Speech recognition;Semantics;Support vector machines;Detectors;Error analysis;Speech,speech recognition,latent topic features;spoken document binary classification;two-class classifier;conversational speech identification performance;latent Dirchlet allocation;LDA,,,,13,,11-Jul-11,,,IEEE,IEEE Conferences
Semi-supervised Learning of Text Classification on Bacterial Protein-Protein Interaction Documents,細菌-蛋白質相互作用文獻的文本分類半監督學習,G. Xu; Z. Niu; P. Uetz; X. Gao; X. Qin; H. Liu,"Coll. of Comput. Sci., Beijing Inst. of Technol., Beijing, China; Coll. of Comput. Sci., Beijing Inst. of Technol., Beijing, China; J. Craig Venter Inst., Rockville, MD, USA; North China Grid Co. Ltd., Beijing, China; Univ. of South China, Hengyang, China; Dept. of Bio3, Georgetown Univ. Med. Center, Washington, DC, USA","2009 International Joint Conference on Bioinformatics, Systems Biology and Intelligent Computing",25-Sep-09,2009,,,263,270,"Protein-protein interaction (PPI) network is essential to understand the fundamental processes governing cell biology. The mining and curation of PPI knowledge is critical for analyzing high-throughput genomics and proteomics data. Several PPI knowledge bases have been generated through expensive manual curation but far from comprehensive. It is desired to have a document classification system which can classify documents as PPI-related or not PPI-related and therefore assist the mining and curation of PPI knowledge. In order to build document classification systems, an annotated corpus is needed where each document in the corpus is tagged with a label (either positive or negative). However, it is usually the case that only a small number of positive documents can be obtained manually or from existing PPI knowledge bases with literature evidences. Meanwhile, there are a large number of unlabeled documents where most of them are not PPI-related. Machine learning based on a small number of positives and a large number of unlabeled documents is called learning from positive and unlabelled documents (LPU) which has been studied in the general domain. A popular approach for LPU is a two-step strategy where the first step is to obtain reliable negative documents (RN) and the second step is to refine RN using various methods such as clustering or boosting. In this paper, we tackle the problem of LPU for PPI document classification and compare three two-step procedures based on a public data set, Reuters-21578. One is to obtain a negative data set by building a machine learning classifier which treats each unlabelled document as negatives and then classifies unlabelled documents. The second procedure is to refine the negative data set iteratively and consider those unlabeled documents always classified as negative as reliable negatives. The third procedure is to augment the negative data set iteratively by including unlabeled documents classified as negative in any iteration. Three machine learning algorithms were deployed for each two-step procedure.",,978-0-7695-3739-9,10.1109/IJCBS.2009.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5260672,protein-protein interaction;semi-supervised learning;text classification,Semisupervised learning;Text categorization;Microorganisms;Proteins;Machine learning;Biological cells;Genomics;Bioinformatics;Proteomics;Boosting,biology computing;genomics;learning (artificial intelligence);microorganisms;proteins;text analysis,semi-supervised learning;text classification;bacterial protein-protein interaction documents;cell biology;high-throughput genomics;proteomics data;document classification system;unlabelled documents;positive documents;reliable negative documents;negative data set;machine learning classifier;two-step procedure,,7,,16,,25-Sep-09,,,IEEE,IEEE Conferences
UML Modeling for Preserving Sensitive Information Based on k-Means Clustering Approach,基於k均值聚類方法的UML敏感信息保存模型,Anurag; D. Arora; U. Kumar,"Department of Computer Science, Amity Institute of Information Technology Amity University Uttar Pradesh, Lucknow Campus; Department of Computer Science, Amity Institute of Information Technology Amity University Uttar Pradesh, Lucknow Campus; Department of Computer Science, Amity Institute of Information Technology Amity University Uttar Pradesh, Lucknow Campus, Birla Institute of Technology, Patna Campus, India",2019 Amity International Conference on Artificial Intelligence (AICAI),29-Apr-19,2019,,,110,117,"This paper proposes the UML modeling approach to facilitate the software engineers in order to implement the cryptographic PPkDC (Privacy Preserving k-means Data Clustering) algorithm based on Object Oriented methodology. Industrial applications for securing sensitive information to be mined have now become the need of different organizations. This paper applies the k-means data clustering approach on to securing sensitive information. The authors have demonstrated the Object Oriented modeling of the proposed approach with the help of visual modeling language platform UML. It applies clustering algorithm on the data encrypted by Pailler homomorphic cryptosystem and UML modeling of the proposed approach has also been given in this research work with the help of different UML sequence, class, activity diagram. Authors have represented the UML approach of software design regarding PPkDC implementation to different data sources. The proposed approach provide the blueprint for constructing and documenting the UML model for the proposed system for providing the cost and time efficient framework for securing sensitive data to be mined accidently or intentionally.",,978-1-5386-9346-9,10.1109/AICAI.2019.8701284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701284,Pailler Cryptosystem;Homomorphic Encryption;Privacy Preserving Data Mining;Object Oriented Modeling;Unified Modeling Language,Unified modeling language;Ciphers;Object oriented modeling;Software;Computational modeling;Servers,cryptography;data privacy;object-oriented methods;pattern clustering;software engineering;Unified Modeling Language,UML modeling approach;cryptographic PPkDC algorithm;UML model;sensitive data;k-means clustering approach;object oriented methodology;sensitive information preservation;privacy preserving k-means data clustering;visual modeling language platform,,,,14,,29-Apr-19,,,IEEE,IEEE Conferences
Personalized text summarization using NMF and cluster refinement,使用NMF和集群優化的個性化文本摘要,S. Park; M. S. Choi; Y. Lee; S. R. Lee,"Institute of Information Science and Engineering Research, Mokop National University, South Korea; Institute of Information Science and Engineering Research, Mokop National University, South Korea; Dept. of Information & Communication Engineering, Mokop National University, South Korea; Dept. of Information Science and Electronics Engineering, Mokop National University, South Korea",ICTC 2011,17-Nov-11,2011,,,213,218,"As accessing text information on the Internet has become popular, the needs for automatic personalized document summarization have increased. In this paper, a personalized document summarization method that uses Non-negative Matrix Factorization (NMF) and cluster refinement is proposed. The proposed method uses NMF with cluster refinement to summarize generic summary so that it can extract sentences covering the major topics of the document. In addition, the method can improve the quality of personalized summaries because the inherent semantics of the documents are well reflected with respect to user interest. The experimental results demonstrate that the proposed method achieves better performance the than other methods.",2162-1241,978-1-4577-1268-5,10.1109/ICTC.2011.6082582,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082582,Personalized text summarization;NMF;cluster Refinement,Semantics;Vectors;Feature extraction;Matrix decomposition;Equations;Coherence;Mathematical model,matrix decomposition;pattern clustering;text analysis,personalized text summarization;cluster refinement;nonnegative matrix factorization;text information;Internet;personalized document summarization;user interest,,,,18,,17-Nov-11,,,IEEE,IEEE Conferences
Horizontal Reduction: Instance-Level Dimensionality Reduction for Similarity Search in Large Document Databases,水平縮減：大型文檔數據庫中相似搜索的實例級維度縮減,M. S. Kim; K. Whang; Y. Moon,"Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol. (KAIST), Daejeon, South Korea; Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol. (KAIST), Daejeon, South Korea; Dept. of Comput. Sci., Kangwon Nat. Univ., Chuncheon, South Korea",2012 IEEE 28th International Conference on Data Engineering,2-Jul-12,2012,,,1061,1072,"Dimensionality reduction is essential in text mining since the dimensionality of text documents could easily reach several tens of thousands. Most recent efforts on dimensionality reduction, however, are not adequate to large document databases due to lack of scalability. We hence propose a new type of simple but effective dimensionality reduction, called horizontal (dimensionality) reduction, for large document databases. Horizontal reduction converts each text document to a few bitmap vectors and provides tight lower bounds of inter-document distances using those bitmap vectors. Bitmap representation is very simple and extremely fast, and its instance-based nature makes it suitable for large and dynamic document databases. Using the proposed horizontal reduction, we develop an efficient k-nearest neighbor (k-NN) search algorithm for text mining such as classification and clustering, and we formally prove its correctness. The proposed algorithm decreases I/O and CPU overheads simultaneously since horizontal reduction (1) reduces the number of accesses to documents significantly by exploiting the bitmap-based lower bounds in filtering dissimilar documents at an early stage, and accordingly, (2) decreases the number of CPU-intensive computations for obtaining a real distance between high-dimensional document vectors. Extensive experimental results show that horizontal reduction improves the performance of the reduction (preprocessing) process by one to two orders of magnitude compared with existing reduction techniques, and our k-NN search algorithm significantly outperforms the existing ones by one to three orders of magnitude.",2375-026X,978-0-7695-4747-3,10.1109/ICDE.2012.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228156,query interface;fuzzy system;user-centric application,Vectors;Large scale integration;Gold;Euclidean distance;Text mining;Complexity theory;Discrete Fourier transforms,data mining;database management systems;pattern clustering;text analysis,instance-level dimensionality reduction;similarity search;large document databases;text mining;text documents;bitmap vectors;inter-document distances;Bitmap representation;k-nearest neighbor search algorithm;horizontal reduction;k-NN search algorithm,,6,,30,,2-Jul-12,,,IEEE,IEEE Conferences
Use of machine learning in big data analytics for insider threat detection,在大數據分析中使用機器學習進行內部威脅檢測,M. Mayhew; M. Atighetchi; A. Adler; R. Greenstadt,"United States Air Force Research Laboratory, Rome, NY, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Drexel University, Philadelphia, PA, USA",MILCOM 2015 - 2015 IEEE Military Communications Conference,17-Dec-15,2015,,,915,922,"In current enterprise environments, information is becoming more readily accessible across a wide range of interconnected systems. However, trustworthiness of documents and actors is not explicitly measured, leaving actors unaware of how latest security events may have impacted the trustworthiness of the information being used and the actors involved. This leads to situations where information producers give documents to consumers they should not trust and consumers use information from non-reputable documents or producers. The concepts and technologies developed as part of the Behavior-Based Access Control (BBAC) effort strive to overcome these limitations by means of performing accurate calculations of trustworthiness of actors, e.g., behavior and usage patterns, as well as documents, e.g., provenance and workflow data dependencies. BBAC analyses a wide range of observables for mal-behavior, including network connections, HTTP requests, English text exchanges through emails or chat messages, and edit sequences to documents. The current prototype service strategically combines big data batch processing to train classifiers and real-time stream processing to classifier observed behaviors at multiple layers. To scale up to enterprise regimes, BBAC combines clustering analysis with statistical classification in a way that maintains an adjustable number of classifiers.",,978-1-5090-0073-9,10.1109/MILCOM.2015.7357562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357562,trust;machine learning;usage patterns;documents;email;chat;TCP;HTTP;support vector machine;insider threat;big data,Access control;Feature extraction;Computer security;Big data;Monitoring;Electronic mail,authorisation;Big Data;data analysis;document handling;learning (artificial intelligence);pattern classification;pattern clustering;trusted computing,machine learning;big data analytics;insider threat detection;enterprise environments;interconnected systems;document trustworthiness;actor trustworthiness;security events;information trustworthiness;behavior-based access control;BBAC;mal-behavior;network connections;HTTP requests;English text exchanges;emails;chat messages;big data batch processing;classifier training;real-time stream processing;clustering analysis;statistical classification,,21,,37,,17-Dec-15,,,IEEE,IEEE Conferences
Use link-based clustering to improve Web search results,使用基於鏈接的群集來改善Web搜索結果,Yitong Wang; M. Kitsuregawa,"Inst. of Ind. Sci., Tokyo Univ., Japan; Inst. of Ind. Sci., Tokyo Univ., Japan",Proceedings of the Second International Conference on Web Information Systems Engineering,7-Aug-02,2001,1,,115,124 vol.1,"While Web search engines can retrieve information on the Web for a specific topic, users have to step a long ordered list in order to locate the needed information, which is often tedious and less efficient. We propose a new link-based clustering approach to cluster search results returned from Web search engines by exploring both co-citation and coupling. Unlike document clustering algorithms in IR that are based on common words/phrases shared among documents, our approach is based on common links shared by pages. We also extend the standard clustering algorithm, K-means, to make it more natural to handle noise and apply it to Web search results. By filtering some irrelevant pages, our approach clusters high quality pages in Web search results into semantically meaningful groups to facilitate users accessing and browsing. Preliminary experiments and evaluations are conducted to investigate its effectiveness. The experimental results show that link-based clustering of Web search results is promising and beneficial.",,0-7695-1393-X,10.1109/WISE.2001.996472,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996472,,Web search;Web pages;Search engines;Clustering algorithms;Information retrieval;Information filtering;Information filters;Internet;Data engineering;Data mining,information resources;Internet;search engines;information retrieval;citation analysis,link-based clustering;Web search results;Web search engines;co-citation;coupling;document clustering;common links;clustering algorithm;K-means;experiments;information retrieval;Internet,,7,,21,,7-Aug-02,,,IEEE,IEEE Conferences
Modeling Network with Topic Model and Triangle Motif,具有主題模型和三角形圖案的建模網絡,X. Bian; K. Zhang,"Sch. of Sci. & Eng. of Comput., Nanjing Sci. & Technol. Univ., Nanjing, China; Sch. of Sci. & Eng. of Comput., Nanjing Sci. & Technol. Univ., Nanjing, China",2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom),21-Jul-16,2015,,,880,886,"This paper describes a hierarchical model based on triangle motif and topic model, considering both network data and node attribute. The attribute of nodes we study here is text, so we choose document network as our research content. We represent the document network with triangle motif, which has good scalability on large amount of data. This representation makes the complexity of our approach grows linearly in the number of documents, and more relational with the max degree of the network. We extend hLDA by incorporating network data, remodeling the hLDA. Using non-parametric Bayesian model, our approach does not need pre-specification of the branch factor at each non-terminal. The model is suitable for large-scale network of academic abstract, web document and related news.",,978-1-4673-7211-4,10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.170,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518349,nCRP;Hierarchical document clustering;Document topic models;Triangle motif,Data models;Computational modeling;Complexity theory;Context;Taxonomy;Computers;Vocabulary,Bayes methods;document handling,topic model;triangle motif;modeling network;network data;node attribute;document network;max degree;Bayesian model;branch factor,,1,,42,,21-Jul-16,,,IEEE,IEEE Conferences
Constructing a self-organizing C++ programming inquiry behavior miner on the forum,在論壇上構建自組織的C ++編程查詢行為挖掘器,S. Tseng; J. Weng; H. Lin; J. Su,"Department of Computer Science, National Chiao Tung University, ROC; Department of Computer Science, National Chiao Tung University, ROC; Department of Computer Science, National Chiao Tung University, ROC; Department of Computer Science, National Chiao Tung University, ROC",2009 IEEE International Conference on Information Reuse & Integration,21-Aug-09,2009,,,372,375,"The Web forum is the popular platform used for C++ program learning in which learners can discuss and solve the encountered problems collaboratively. The process of discussions for specific topics in the forum is most likely the inquiry-based learning. Discovering the students' inquiry behavior models in the forum is helpful for teachers to support the guidance during the inquiry-based learning. In this paper, the clustering analysis is applied to discover the behavior patterns on the forum. Before the clustering, we first apply the divide and conquer mining strategy to classify the documents into purposes and sub-purposes until the number of documents is affordable for clustering analysis. To incrementally maintain the topics of inquiry behavior and efficiently analyze the new inquiry topic, the self-organizing ontology maintenance scheme is proposed. The topics discussed with multiple purposes are identified as inquiry behavior of hot topics. Finally, the experiment of inquiry behavior mining from legacy forums has been evaluated and the experimental result shows the usefulness of the obtained behavior information.",,978-1-4244-4114-3,10.1109/IRI.2009.5211581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5211581,Data mining and knowledge discovery;behavior modeling;forum;programming;inquiry-based learning,Ontologies;Collaboration;Pattern analysis;Electronic learning;Collaborative work;Computer science;Data mining;USA Councils;Clustering algorithms;Taxonomy,C++ language;computer aided instruction;computer science education;data mining;divide and conquer methods;groupware;Internet;ontologies (artificial intelligence);pattern clustering;teaching,selforganizing C++ programming inquiry based learning;data mining;Web forum;computer-supported collaborative learning;student inquiry behavior model;clustering analysis;divide-and-conquer mining strategy;document classification;selforganizing ontology maintenance scheme,,,,10,,21-Aug-09,,,IEEE,IEEE Conferences
A Partitional Clustering Approach to Persian Spell Checking,波斯語拼寫檢查的分區聚類方法,F. H. Kermani; S. Ghanbari,"Department of Technology, Alzahra University, Tehran, Iran; Department of Computer Science and Electronic, University of Essex, UK",2019 5th Conference on Knowledge Based Engineering and Innovation (KBEI),13-Jun-19,2019,,,297,301,"Spell checkers find, suggest and correct incorrect words within documents. Complexities of languages increases the challenge in-hand. Due to the challenges of the Persian language, preprocessing and processing spell checkers requires further analysis and natural language processing. For the past decade, the majority of methods that identify errors in documents have been lexicon-based or probability-based. This paper describes a new approach for Persian spell-checking using a clustered dictionary to save time and storage. Clustering is optimally generated through k-medoids and during processing misspelt words are compared to the centers of the generated clusters rather than the entire dictionary. The proposed spell checker is evaluated using the Faspell spelling error corpus that contains Persian misspellings and is shown to provide suggestions with significant accuracy rates.",,978-1-7281-0872-8,10.1109/KBEI.2019.8734932,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734932,spell checker;partitional clustering;non-word errors;Persian;Natural Language Processing,Dictionaries;Complexity theory;Clustering algorithms;Partitioning algorithms;Natural language processing;Phonetics;Optical character recognition software,natural language processing;pattern clustering;probability;spelling aids,Persian language;natural language processing;lexicon-based;Persian spell-checking;Faspell spelling error corpus;Persian misspellings;clustering approach;probability-based;k-medoids,,,,17,,13-Jun-19,,,IEEE,IEEE Conferences
TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams,TF-ICF：用於動態數據流聚類的新術語加權方案,J. W. Reed; Y. Jiao; T. E. Potok; B. A. Klump; M. T. Elmore; A. R. Hurson,"Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; Oak Ridge National Laboratory, USA; The Pennsylvania State University, USA",2006 5th International Conference on Machine Learning and Applications (ICMLA'06),26-Dec-06,2006,,,258,263,"In this paper, we propose a new term weighting scheme called term frequency-inverse corpus frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods",,0-7695-2735-3,10.1109/ICMLA.2006.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041501,,Computational complexity;Data engineering;Frequency conversion;Machine learning;Software engineering;Laboratories;Computer science;Vectors;Parallel algorithms;Information filtering,computational complexity;pattern clustering;text analysis;unsupervised learning,term weighting scheme;dynamic data stream clustering;term frequency-inverse corpus frequency;machine learning application;unsupervised document clustering,,58,,23,,26-Dec-06,,,IEEE,IEEE Conferences
Sentiment-based search in digital libraries,數字圖書館中基於情感的搜索,C. S. G. Khoo; N. B. Hamzah; S. Chan; J. Na,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '05),5-Mar-07,2005,,,143,144,"Several researchers have developed tools for classifying/ clustering Web search results into different topic areas (such as sports, movies, travel, etc.), and to help users identify relevant results quickly in the area of interest. This study follows a similar approach, but is in the area of sentiment classification - automatically classifying on-line review documents according to the overall sentiment expressed in them. This paper presents a prototype system that has been developed to perform sentiment categorization of Web search results. It assists users to quickly focus on recommended (or non-recommended) information by classifying Web search results into four categories: positive, negative, neutral, and non-review documents, by using an automatic classifier based on a supervised machine learning algorithm, support vector machine (SVM)",,1-58113-876-8,10.1145/1065385.1065416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118529,automatic text classification;digital libraries;sentiment classification,Software libraries;Motion pictures;Web search;Support vector machines;Support vector machine classification;Text categorization;Machine learning algorithms;Information retrieval;Permission;Prototypes,classification;digital libraries;information retrieval;Internet;learning (artificial intelligence);pattern clustering;support vector machines,Web search results clustering;Web search results classification;automatic on-line review document classification;sentiment-based search;digital libraries;nonreview documents;neutral documents;negative documents;positive documents;supervised machine learning algorithm;support vector machine,,1,,4,,5-Mar-07,,,IEEE,IEEE Conferences
An enhanced sensitive information security model,增強的敏感信息安全模型,S. Kaushik; S. Puri,"BIT, Mesra, Ranchi, India; BIT, Mesra, Ranchi, India","2012 International Conference on Computing, Electronics and Electrical Technologies (ICCEET)",24-May-12,2012,,,1055,1060,"Sensitive Information Security (SIS) model provides the strong base to transmit a large volume of sensitive information based textual documents securely and safely. In this paper, a model is proposed to provide sensitive information security using new techniques for dimensionality reduction where two new algorithms Term Similarity Clustering (TSC) and Term Index Clustering (TIC) based on supervised learning are proposed for SIS model. The TSC approach includes the lossy data reduction at the sender side, but makes the system tolerable to maintain integrity by keeping confidentiality at its best level when data is sent over on the unsecured communication channel. The terms or words are extracted from the text documents and they are categorized into Term Clusters (TC) with the use of Knowledge Repository (KR). Otherwise, it uses the KR and then makes a new entry into the appropriate TC with its associated TSC. Then these TCs of reduced dimension are provided the high security and sent over on the unsecured channel. The second approach TIC works differently and provides the lossless data reduction at the sender side with high data integrity and confidentiality during data transmission but includes an overhead of keeping KR at the receiver side. In TIC, instead of keeping the word in the TC, the index of each term, referring the knowledge repository, is placed in the respective TIC. Due to the KR overhead, this approach increases the configuration complexity at the receiver side. As both proposed approaches decrease the space and time complexities, so this paper provides the analytical experimental results of testing a text document of NASA Standards in which TSC gives about 10% space dimension reduction with some information loss, whereas TIC provides around 12% of space reduction with the additional overhead of KR at the receiver side.",,978-1-4673-0212-8,10.1109/ICCEET.2012.6203910,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6203910,information security;knowledge repository;data dimension reduction;similarity clustering;index clustering,NASA,aerospace computing;computational complexity;data integrity;data reduction;learning (artificial intelligence);pattern clustering;security of data;text analysis,sensitive information security model;SIS model;textual documents;dimensionality reduction;term similarity clustering;term index clustering;TIC;supervised learning;TSC approach;knowledge repository;lossless data reduction;data integrity;data confidentiality;KR;configuration complexity;space complexities;time complexities;National Aeronautics and Space Administration,,,,5,,24-May-12,,,IEEE,IEEE Conferences
Fully-Automatic XML Clustering by Structure-Constrained Phrases,通過結構受限短語的全自動XML聚類,G. Costa; R. Ortale,"ICAR Inst., Rende, Italy; ICAR Inst., Rende, Italy",2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),7-Jan-16,2015,,,146,153,"Conventional approaches to XML clustering by content and structure are generally affected by a limitation due to the adoption of the bag-of-word model for the representation of their textual contents. This choice may lead to consider structure-constrained textual items of separate XML documents as related, even though the actual meaning of such items in their respective contexts is different. To overcome such a limitation, we propose XML clustering by structure-constrained phrases. The latter is a previously unexplored method relying on the more accurate bag-of-phrase model of the XML textual content, with which to better preserve the meaning of the structure-constrained content items for improved clustering effectiveness. In order to conduct an in-depth and systematic study of the effectiveness of the proposed method, we develop a parameter-free prototypical approach to XML partitioning, which projects the XML documents into a space of XML features representing fixed-length sequences of adjacent textual items in the context of root-to-leaf paths. Feature selection without any tunable threshold is used to choose a subset of the XML features on the basis of their relevance to clustering, which is assessed through a new scoring scheme. A comparative experimentation on real-world benchmark XML corpora reveals a higher effectiveness than several state-of-the-art competitors.",1082-3409,978-1-5090-0163-7,10.1109/ICTAI.2015.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372130,XML Analysis;XML Topic Models;XML Clustering,XML;Vegetation;Context;Data models;Information retrieval;Benchmark testing;Electronic mail,feature selection;pattern clustering;text analysis;XML,fully-automatic XML clustering;structure-constrained phrases;structure-constrained textual items;XML documents;bag-of-phrase model;XML textual content;structure-constrained content items;parameter-free prototypical approach;XML partitioning;XML features;fixed-length sequences;textual items;root-to-leaf paths;feature selection;scoring scheme;real-world benchmark XML corpora;eXtensible Markup Language,,2,,39,,7-Jan-16,,,IEEE,IEEE Conferences
Towards large scale image similarity discovery model,面向大規模圖像相似性發現模型,H. M. Al-Barhamtoshy,"Faculty of Computing and Information Technology, Jeddah, Saudi Arabia, King Abdulaziz University (KAU)",2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),28-Jul-16,2016,,,1,9,"The paper introduces to the digitization and features extraction processes of large volume of imaging documents and stored as images using mechanisms of big data and cloud technology. So, layout analysis, image representation, feature extraction and transformation huge amount of the prepared document images are presented in this paper. Accordingly, an efficient way reliable and highly clustering functionality of these document images will be focused. Consequently, image-based extractor using ?document image similarity??is the main methodology to apply this paper. Many tasks have been proposed to contribute such idea and also to support retrieval performance. Different methods such as layout analysis, image representation, features vectors creation, similarity measure, accuracy computation and document image retrieval will be presented.",,978-1-4673-8526-8,10.1109/ATSIP.2016.7523047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523047,Image representation;feature descriptor;similarity measure;F-Measure,Feature extraction;Layout;Image representation;Training;Vegetation;Histograms;Detectors,Big Data;cloud computing;document image processing;feature extraction;image matching;image representation;image retrieval;pattern clustering,document image retrieval;feature vector creation;image representation;document image similarity;image-based extractor;clustering functionality;feature extraction;image representation;layout analysis;cloud technology;Big Data;imaging documents;digitization processes;large scale image similarity discovery model,,,,20,,28-Jul-16,,,IEEE,IEEE Conferences
Semi-supervised text categorization with only a few positive and unlabeled documents,半監督文本分類，僅包含少量正面和未標記文檔,F. Lu; Q. Bai,"College of Mathematics and Computer Science, Fuzhou University, China; College of Mathematics and Computer Science, Fuzhou University, China",2010 3rd International Conference on Biomedical Engineering and Informatics,18-Nov-10,2010,7,,3075,3079,"This paper studies a special case of semi-supervised text categorization. We want to build a text classifier with only a set P of labeled positive documents from one class (called positive class) and a set U of a large number of unlabeled documents from both positive class and other diverse classes (called negative class). This kind of semi-supervised text classification is called positive and unlabeled learning (PU-Learning). Although there are some effective methods for PU-Learning, they do not perform very well when the labeled positive documents are very few. In this paper, we propose a refined method to do the PU-Learning with the known technique combining Rocchio and K-means algorithm. Considering the set P may be very small (??%), not only we extract more reliable negative documents from U but also enlarge the size of P with extracting some most reliable positive documents from U. Our experimental results show that the refined method can perform better when the set P is very small.",1948-2922,978-1-4244-6498-2,10.1109/BMEI.2010.5639749,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639749,semi-supervised learning;text categorization;cluster,Classification algorithms;Text categorization;Prototypes;Clustering algorithms;Training;Support vector machines;Web pages,learning (artificial intelligence);text analysis,semisupervised text categorization;unlabeled documents;labeled positive documents;positive class;unlabeled learning;PU-learning;K-means algorithm,,3,,11,,18-Nov-10,,,IEEE,IEEE Conferences
Transfer Learning by Linking Similar Feature Clusters for Sentiment Classification,通過鏈接相似特徵類進行情感分類的轉移學習,W. Zhang; H. Zhang; D. Wang; R. Liu; H. Zhang; X. Jiang; Y. Chen,"Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China; Sch. of Comput. Sci. & Eng., BeiHang Univ., Beijing, China",2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),16-Jan-17,2016,,,1019,1026,"Transfer learning aims to extract the knowledge from a label-rich source domain to enhance the predictive model of a target domain. Previous methods achieve knowledge transfer by detecting a shared low-dimensional feature representation from source domain to target domain. Along this line, many algorithms, e.g., dual transfer learning (DTL), triplex transfer learning (TRi-TL) etc., have been proposed and widely used for text classification. However, we argue that it is difficult for models to distinguish exactly the common concepts or identical concepts across different domains through the existing algorithms, even though source and target domains are related but different. So we propose to use the similar feature clusters as knowledge transfer, that is, we only guarantee the approximate similarity of common word clusters across different domains, rather than the exactly same. Based on the above assumption, the derived association matrices between word clusters and document classes should be slightly different to account for the word clusters variations. To take the above assumptions into account, we propose a novel Nonnegative Matrix Tri-Factorization based transfer learning by linking similar feature clusters (LSF-TL) for sentiment classification, in which an approximate constraint between similar word clusters matrices is added to allow differences while keeping the knowledge transferring function. Besides, LSF-TL also provides the same approximate constraint for the derived clusters association matrices. Then we employ an iterative updating algorithm with sound theoretical proof to find the local optimal solution. Last, we evaluate our method by conducting extensive experiments on Amazon product reviews. The results show that our approach achieves better classification accuracy than the state-of-the-art methods for both Cross-lingual sentiment classification(CLSC) and Cross-lingual cross-domain sentiment classification(CLCDSC) tasks.",2375-0197,978-1-5090-4459-7,10.1109/ICTAI.2016.0157,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814717,Transfer Learning;Non-negative Matrix Tri-factorization;Similar Concept;Domain Adaption;Sentiment Classification,,feature extraction;knowledge acquisition;learning (artificial intelligence);matrix decomposition;pattern clustering;sentiment analysis,CLCDSC tasks;cross-lingual cross-domain sentiment classification;Amazon product reviews;word clusters matrices;LSF-TL;similar feature clusters;nonnegative matrix tri-factorization based transfer learning;document classes;association matrices;text classification;TRi-TL;triplex transfer learning;DTL;dual transfer learning;low-dimensional feature representation;knowledge transfer;target domain predictive model;label-rich source domain;knowledge extraction;feature cluster linking,,2,,23,,16-Jan-17,,,IEEE,IEEE Conferences
Evidence-Based Trust Mechanism Using Clustering Algorithms for Distributed Storage Systems (Short Paper),基於聚類算法的分佈式存儲系統基於證據的信任機制（論文）,G. Traverso; C. G. Cordero; M. Nojoumian; R. Azarderakhsh; D. Demirel; S. M. Habib; J. Buchmann,"Tech. Univ. of Darmstadt, Darmstadt, Germany; Tech. Univ. of Darmstadt, Darmstadt, Germany; Florida Atlantic Univ., Boca Raton, FL, USA; Florida Atlantic Univ., Boca Raton, FL, USA; Tech. Univ. of Darmstadt, Darmstadt, Germany; Tech. Univ. of Darmstadt, Darmstadt, Germany; Tech. Univ. of Darmstadt, Darmstadt, Germany","2017 15th Annual Conference on Privacy, Security and Trust (PST)",30-Sep-18,2017,,,277,282,"In distributed storage systems, documents are shared among multiple Cloud providers and stored within their respective storage servers. In social secret sharing-based distributed storage systems, shares of the documents are allocated according to the trustworthiness of the storage servers. This paper proposes a trust mechanism using machine learning techniques to compute evidence-based trust values. Our mechanism mitigates the effect of colluding storage servers. More precisely, it becomes possible to detect unreliable evidence and establish countermeasures in order to discourage the collusion of storage servers. Furthermore, this trust mechanism is applied to the social secret sharing protocol AS3, showing that this new evidence-based trust mechanism enhances the protection of the stored documents.",,978-1-5386-2487-6,10.1109/PST.2017.00040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476945,Trust management;social secret sharing;applied cryptography;distributed storage systems;cloud computing;and clustering algorithms,Servers;Cryptography;Machine learning;Clustering algorithms;Gaussian distribution;Protocols;Atmospheric measurements,cloud computing;cryptographic protocols;data protection;document handling;learning (artificial intelligence);storage management;trusted computing,distributed storage systems;evidence-based trust values;social secret sharing protocol;cloud providers;storage servers trustworthiness;machine learning techniques;AS3;stored documents protection,,1,,26,,30-Sep-18,,,IEEE,IEEE Conferences
Automatic summarizer for mobile devices using sentence ranking measure,使用句子排名度量的移動設備自動匯總器,Yazhini R.; Vishnu Raja P.,"Department of Computer Science and Engineering, Kongu Engineering College, Erode, India; Department of Computer Science and Engineering, Kongu Engineering College, Erode, India",2014 International Conference on Recent Trends in Information Technology,29-Dec-14,2014,,,1,6,"The modern digital world is immersed in tons of information in the form of electronic documents. To overcome the consequences of system failure, the fault tolerance system uses redundant information too. While surfing in the mobile devices, since they have small display screen. The mobile users prefer to analyse the summarized report, if it is relevant to their requirement, the user may observe it deeper. However, it is difficult to manually summarize the large documents of text. Sentence level clustering can be employed to perform automatic summarization task. The proposed work computes the centroid sentence of each cluster to automatically generate the summary which in turn reduces the manual processing. The integration of automatic summarization process to mobile devices is deployed android mobile application development interface. Experimental evaluation on the sample dataset shows that the clustering algorithm is more effective in retrieval of summarized content in mobile devices.",,978-1-4799-4989-2,10.1109/ICRTIT.2014.6996138,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996138,Fuzzy clustering;mobile devices;sentence ranking,Clustering algorithms;Mobile handsets;Mobile communication;Data mining;Equations;Mathematical model;Algorithm design and analysis,application program interfaces;pattern clustering;smart phones;software fault tolerance;text analysis,document text summarization;mobile devices;sentence ranking measure;electronic documents;fault tolerance system;sentence level clustering;Android mobile application development interface,,2,,20,,29-Dec-14,,,IEEE,IEEE Conferences
Exploring Health-Related Topics in Online Health Community Using Cluster Analysis,使用聚類分析探索在線健康社區中與健康相關的主題,Y. Lu; P. Zhang; S. Deng,"Antai Coll. of Econ. & Manage., Shanghai Jiao Tong Univ., Shanghai, China; Antai Coll. of Econ. & Manage., Shanghai Jiao Tong Univ., Shanghai, China; Antai Coll. of Econ. & Manage., Shanghai Jiao Tong Univ., Shanghai, China",2013 46th Hawaii International Conference on System Sciences,18-Mar-13,2013,,,802,811,"Recently patients are increasingly turning to online health community to share their experiences and exchange healthcare knowledge. Exploring hot topics from online health community helps us better understand their needs and interests in health-related knowledge. However, statistical-based topic analysis employed in previous studies is becoming impractical to process the growing large-scale online data. Automatic topic analysis based on document clustering is an alternative approach but usually produce poor results as a result of lack of domain-specific knowledge. So this paper proposes a novel framework for health-related topic analysis using text clustering integrating medical domain-specific knowledge. Experiment results show that adding medical domain-specific features into feature set could achieve significantly better clustering performance than existing methods. In addition, further analysis reveals that there also exist some significant differences about hot topics among different kinds of disease discussion boards.",1530-1605,978-0-7695-4892-0,10.1109/HICSS.2013.216,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479930,,Communities;Medical diagnostic imaging;Feature extraction;Diseases;Media;Internet;Semantics,health care;pattern clustering;statistical analysis,health related topics;online health community;cluster analysis;healthcare knowledge;hot topics;health related knowledge;statistical based topic analysis;large scale online data;automatic topic analysis;document clustering;health related topic analysis;text clustering integrating medical domain specific knowledge;clustering performance;disease discussion boards,,10,,30,,18-Mar-13,,,IEEE,IEEE Conferences
An efficient XML label based on MapReduce,基於MapReduce的高??效XML標籤,Bowen Wei; Kunfang Song; Minghua Jiang,"School of Math and Computer, Wuhan Textile University, 430200, China; School of Math and Computer, Wuhan Textile University, 430200, China; School of Math and Computer, Wuhan Textile University, 430200, China",2016 World Automation Congress (WAC),6-Oct-16,2016,,,1,6,"In the face of development trend that a single XML document data volume is increasingly big, involved a core problem of XML data query technology is how to effectively solve result that meet certain query semantic, and label used by corresponding document tree based on XML document is the key factor of affecting the query efficiency. This paper, all experimental processing is completed through MapReduce framework, and prefix flow label is taken as object of study. Xwei is proposed in allusion to the problems that redundancy of existing prefix flow label, the length is increased constantly with the increase of the depth of the document tree, specific location cannot be intuitively determined in the document tree, and then, query algorithm efficiency designed by prefix flow label is not high. By means of experiments in the distributed cluster and by contrasting Dewey and ED, the efficiency of Labeling with Xwei is improved by 10% at least, and the query efficiency of Xwei based on the certain query semantics is improved by 33%-75%.",,978-1-8893-3551-3,10.1109/WAC.2016.7583044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583044,Xwei;XML;MapReduce;Dewey;ED,Labeling;XML;Redundancy;Clustering algorithms;Semantics;Computers;Textiles,document handling;parallel processing;query processing;trees (mathematics);XML,XML prefix flow label;MapReduce;XML document data;XML data query technology;document tree,,,,14,,6-Oct-16,,,IEEE,IEEE Conferences
Application of the wavelet and the Hough transform for detecting the skew angle in Arabic printed documents,小波和霍夫變換在阿拉伯文印刷文檔傾斜角檢測中的應用,N. Khorissi; A. Namane; A. Mellit; F. Abdati; Z. A. Bensalama; A. Guessoum,"Signal, Image and Pattern Recognition Group. Department of Electronics, Faculty of Sciences Engineering, Blida University. 09000, Algeria; Signal, Image and Pattern Recognition Group. Department of Electronics, Faculty of Sciences Engineering, Blida University. 09000, Algeria; CUYFM, Department of Electronics, Faculty of Sciences Engineering, M矇d矇a University. Algeria; Signal, Image and Pattern Recognition Group. Department of Electronics, Faculty of Sciences Engineering, Blida University. 09000, Algeria; Signal, Image and Pattern Recognition Group. Department of Electronics, Faculty of Sciences Engineering, Blida University. 09000, Algeria; Signal, Image and Pattern Recognition Group. Department of Electronics, Faculty of Sciences Engineering, Blida University. 09000, Algeria",2007 9th International Symposium on Signal Processing and Its Applications,27-Jun-08,2007,,,1,4,"The purpose of this work is to present a new technique for detecting the skew angle based on wavelet transform analysis (WT) and Hough transform (HT). The application concerns the Arabic document images. The application of the Hough transform presents a good solution for skew angle detection. However, this approach requires an important memory space and high computing time. For this, we have suggested the using of the WT in order to reduce the number of points and the computing time. This technique is based principally on the high band frequency of the wavelet transform. To evaluate the performance of the proposed method, a test of 100 different documents was used. Obtained result indicates that the suggested approach gave good performance, the accuracy results of the skew angle estimation is very good and the computing time is well decreases compared to the alternative methods. The technique has been applied for Arabic image document, but it can be generalized for any documents.",,978-1-4244-0778-1,10.1109/ISSPA.2007.4555586,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4555586,,Wavelet transforms;Frequency;Histograms;Wavelet analysis;Testing;Clustering methods;Pattern recognition;Text analysis;Robustness;Writing,character recognition;edge detection;Hough transforms;wavelet transforms,Hough transform;wavelet transform;skew angle detection;Arabic document images,,5,,12,,27-Jun-08,,,IEEE,IEEE Conferences
Visualizing similar text documents based on 3D dendrogram,基於3D樹狀圖可視化相似的文本文檔,T. Kinoshita; T. Ohkubo; K. Kobayashil; K. Watanabe; Y. Kurihara,"Faculty of Engineering, Hosei University, 3-7-2 Kajinocho, Koganei, Tokyo 184-8584, Japan; Faculty of Engineering, Hosei University, 3-7-2 Kajinocho, Koganei, Tokyo 184-8584, Japan; Faculty of Engineering, Hosei University, 3-7-2 Kajinocho, Koganei, Tokyo 184-8584, Japan; Faculty of Engineering, Hosei University, 3-7-2 Kajinocho, Koganei, Tokyo 184-8584, Japan; Systems Engineering, Seikei University, 3-3-1 Kitamachi-Kichijoji, Musashino, Tokyo 180-8633, Japan",Proceedings of SICE Annual Conference 2010,14-Oct-10,2010,,,1285,1288,"This paper describes a new data visualization method using a three-dimensional dendrogram to And the relationship between similar text documents. For the detection of similarities in documents, we introduce two basic evaluation functions, ""LCS"" and ""SED."" We constructed and developed the proposed algorithm in MATLAB language, and conducted a comparison measurement and examined the evaluations. The validity of the proposed method can be verified by applying actual documents to demonstrate the 3D visualization.",,978-4-907764-36-4,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602599,LCS;SED;dendrogram;3D visualization,Data visualization;Algorithm design and analysis;Three dimensional displays;Clustering algorithms;Visualization;Detection algorithms;Software algorithms,data visualisation;mathematics computing;text analysis,similar text document visuallization;data visualization method;three-dimensional dendrogram;LCS;SED;MATLAB language;shortest edit distance;longest common subsequence,,,,2,,14-Oct-10,,,IEEE,IEEE Conferences
Development of innovation cluster in Mongolia for ICT sector /role of high education system/,在蒙古發展信息通信技術部門的創新集群/高等教育系統的作用/,N. Munkhuu; B. Tsetsgee; T. Narantuya,"SICT MUST, Ulaanbaatar, Mongolia; SICT MUST, Ulaanbaatar, Mongolia; CSMS MUST, Ulaanbaatar, Mongolia",Ifost,3-Oct-13,2013,2,,154,158,"This paper examines the role of regional clusters in regional entrepreneurship. We focus on the distinct influences on growth in the number of start-up firms as well as in employment in these new firms in a given region-industry. Mongolian government has approved several policy documents to enhance national capacity building. Within the framework of these policy documents, some efforts have been made at initial level from creating legal environment for science and technology oriented activities. The National Development Policy and Action plan of Government of Mongolia specified implementation of different large scale projects, such as high-tech innovation cluster for ICT (Silicon Valley) and other sector, launching of national industry, improving capacities of human resources, which can be implemented together and in cooperation with different organizations. Concept of developing innovation cluster in Mongolia aimed to build a knowledge driven society to enhance living standards of Mongolian people.",,978-1-4799-0933-9,10.1109/IFOST.2013.6616877,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616877,,Technological innovation;Industries;TV broadcasting;Educational institutions;Software;Consumer electronics;Communications technology,employment;further education;government policies;innovation management;regional planning;risk management,Mongolian people living standards;human resources;national industry;Silicon Valley;high-tech ICT innovation cluster;National Development Policy and Action plan of Government of Mongolia;science and technology oriented activities;legal environment;national capacity building;policy documents;Mongolian government;region-industry;start-up firms;regional entrepreneurship;regional clusters;high education system;lCT sector;innovation cluster development,,,,6,,3-Oct-13,,,IEEE,IEEE Conferences
Improving Web Page Clustering Through Selecting Appropiate Term Weighting Functions,通過選擇適當的術語加權功能改善網頁聚類,V. Fresno; R. Martinez; S. Montalvo,"ESCET Universidad Rey, Juan Carlos, victor.fresno@urjc.es; E.T.S.I. Inform獺tica, UNED, raquel@lsi.uned.es; ESCET Universidad Rey, Juan Carlos, soto.montalvo@urjc.es",2006 1st International Conference on Digital Information Management,4-Jun-07,2007,,,511,518,"Web page clustering is useful for taxonomy design, information extraction, similarity search, and it can assist to the evaluation and visualization of the results of search engines. Therefore, an accurate clustering is a goal in Web mining and Web information extraction. Besides the particular clustering algorithm, the different term weighting functions applied to the selected features to represent Web pages is a main aspect in clustering task. This paper presents the evaluation of the performance of six different term weighting functions of Web pages, by means of a partitioning clustering algorithm results. Besides, two reduction methods have been applied: (1) the proper function, and (2) removing all features occurring more times than upper thresholds in page and collection, and occurring less times than lower thresholds in page and collection. By means of the experimentation with a collection of Web documents used in clustering research, we have determined that the best results are obtained when the term weighting function based on a fuzzy criteria combination is used.",,1-4244-0682-X,10.1109/ICDIM.2007.369244,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221936,,Web pages;Clustering algorithms;HTML;Frequency;Taxonomy;Data mining;Search engines;Visualization;Web mining;Partitioning algorithms,data mining;feature extraction;information retrieval;Internet;pattern clustering;search engines;text analysis;Web sites,Web page clustering;term weighting function selection;taxonomy design;information extraction;similarity search;search engine;Web mining;Web information extraction;feature selection;Web textual document,,,,30,,4-Jun-07,,,IEEE,IEEE Conferences
Intelligent Text Clustering Based on Semantics Similarity,基於語義相似度的智能文本聚類,M. E. Polus; T. Abbas,"Mustansiriyah University,College of Science,Department of Computer Science,Baghdad,Iraq; Mustansiriyah University,College of Science,Department of Computer Science,Baghdad,Iraq",2020 1st. Information Technology To Enhance e-learning and Other Application (IT-ELA,13-Nov-20,2020,,,60,66,"Clustering text documents have become an increasingly important problem in recent years due to the availability of a huge amount of unstructured data in various forms, such as the web, social networks, and other information networks. It aims to organise and classify large document groups into smaller groups of meaning. This process is crucial because it is challenging to deal with a large amount and an increasing number of digital data. The documents are organised and classified to facilitate faster information retrieval (IR) and try to extract information with semantic knowledge. The process above enables the retrieve information, browsed and understood instead of clustering texts in the traditional way, i.e. compilation of data without descriptive concepts. As textual data has become a diverse set of vocabulary hence, there is an urgent need for text aggregation techniques based on semantic similarity is the primary solution to this problem as it is grouped into groups according to meaning rather than keywords. Several Papers that are using semantic similarity in various scopes. It has been reviewed in this research; some of them which are using similarity based on semantic using document cluster. For developing an effective and efficient clustering methodology to take care of the semantic structure of the text documents, and a compare of between them is presented according to (algorithms, tools, and assessment methods). Finally, extensive study and comparison of work are presented.",,978-1-7281-8233-9,10.1109/IT-ELA50150.2020.9253127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253127,Text Clustering;Semantic Similarity;Ontology;Document Clustering;WordNet;polysemy;synonymy;Vector Space Model(VSM);Tf-idf;k-Means;HAC,,,,,,,46,,13-Nov-20,,,IEEE,IEEE Conferences
A New Normalized Similarity for Discriminating Similar Documents,區分相似文檔的新歸一化相似度,J. Ji; C. Ryu; G. Woo; H. Cho,"Dept. of Comput. Eng., Pusan Nat. Univ., Pusan; Dept. of Comput. Eng., Pusan Nat. Univ., Pusan; Dept. of Comput. Eng., Pusan Nat. Univ., Pusan; Dept. of Comput. Eng., Pusan Nat. Univ., Pusan",2008 Fourth International Conference on Networked Computing and Advanced Information Management,12-Sep-08,2008,2,,108,113,"To find out similar document pairs from a set of documents, computing normalization similarities is inevitable because the sizes of documents are different from documents to documents. However, the normalized similarities proposed up to now are still unreliably sensitive to the size of programs compared. Due to this fact, most previously announced similarity detection tools have difficulties in determining the cutoff threshold to discriminate similar documents from a set of documents. In this paper, we propose a new normalized similarity based on Weibull distribution. To test the effectiveness of the new similarity measure, we applied it in detecting similar program pairs from a set of programs. According to the experiment, the new similarity measure showed very nice characteristics in discriminating the very similar program pairs from other pairs. Also, the proposed normalized similarity is effective in detecting similar documents written in natural languages.",,978-0-7695-3322-3,10.1109/NCM.2008.189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624126,Plagiarism Detection;Programming Contest;ICPC;Weibull,Plagiarism;Weibull distribution;Distance measurement;Symmetric matrices;DNA;Testing;Clustering algorithms,document handling;natural language processing;Weibull distribution,similar document discrimination;normalization similarities;normalized similarities;similarity detection tools;Weibull distribution;natural languages;plagiarism detection,,,,13,,12-Sep-08,,,IEEE,IEEE Conferences
Streaming-data algorithms for high-quality clustering,用於高質量聚類的流數據算法,L. O'Callaghan; N. Mishra; A. Meyerson; S. Guha; R. Motwani,"Dept. of Comput. Sci., Stanford Univ., CA, USA; NA; NA; NA; NA",Proceedings 18th International Conference on Data Engineering,7-Aug-02,2002,,,685,694,"Streaming data analysis has recently attracted attention in numerous applications including telephone records, Web documents and click streams. For such analysis, single-pass algorithms that consume a small amount of memory are critical. We describe such a streaming algorithm that effectively clusters large data streams. We also provide empirical evidence of the algorithm's performance on synthetic and real data streams.",1063-6382,0-7695-1531-2,10.1109/ICDE.2002.994785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=994785,,Clustering algorithms;Computer science;Data analysis;Algorithm design and analysis;Partitioning algorithms;Laboratories;Telephony;Lab-on-a-chip;Telecommunications;Data engineering,data analysis;pattern clustering,streaming data analysis;telephone records;Web documents;click streams;single-pass algorithms;large data stream clusters;streaming-data algorithms;high-quality clustering,,185,,27,,7-Aug-02,,,IEEE,IEEE Conferences
Online Bicriteria Load Balancing for Distributed File Servers,分佈式文件服務器的在線Bicriteria負載平衡,S. Tse,"Computer Engineering Department, Bilkent University, 06800 Ankara, Turkey. Email: sshtse@cs.bilkent.edu.tr",2007 Second International Conference on Communications and Networking in China,7-Mar-08,2007,,,218,222,"We study the online bicriteria load balancing problem in a system of M distributed homogeneous file servers located in a cluster. The load and storage space are assumed to be independent. We propose two online approximate algorithms for balancing the load and required storage space of each server during document placement. Our first algorithm combines the first result in [10] and the upper bound result in [1]. With applying document reallocation, we further obtain improvement and give a smoother tradeoff curve of the upper bounds of load and storage space. This result improves the best existing solutions. The second algorithm is for theoretical purpose. Its existence proves that the bounds for the load and the required storage space of each server, respectively, are strictly better when document reallocation is allowed. It enhances the research in applying document reallocation. The time complexities of both algorithms are O(logM); and the cost of document reallocation should be taken into account.",,978-1-4244-1008-8,10.1109/CHINACOM.2007.4469367,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469367,Load balancing;Scheduling;Document placement;Re-allocation,Load management;File servers;Costs;Clustering algorithms;Upper bound;Financial advantage program;Distributed computing;NP-complete problem;Algorithm design and analysis;Scheduling algorithm,approximation theory;computational complexity;document handling;file organisation;file servers;resource allocation,online bicriteria load balancing;distributed homogeneous file servers;online approximate algorithm;document placement;document reallocation,,,,10,,7-Mar-08,,,IEEE,IEEE Conferences
Enhancing Scientific Collaborations using Community Detection and Document Clustering,使用社區檢測和文檔聚類加強科學合作,I. -M. R?dulescu; C. -O. Truic?; E. -S. Apostol; C. Dobre,"University Politehnica of Bucharest,Faculty of Automatic Control and Computers,Computer Science and Engineering Department,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computers,Computer Science and Engineering Department,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computers,Computer Science and Engineering Department,Bucharest,Romania; University Politehnica of Bucharest,Faculty of Automatic Control and Computers,Computer Science and Engineering Department,Bucharest,Romania",2020 IEEE 16th International Conference on Intelligent Computer Communication and Processing (ICCP),26-Nov-20,2020,,,43,50,"Community detection is the process of extracting community structured subgraphs from community networks. Most research regarding community detection has focused on the network structure without taking the content associated with the nodes into account. In this paper, we propose a new method for enhancing a co-authorship network?s structure using clustering. Specifically, considering the clustering process, we use a sequence with proved performance between the WordNet lemmatizer, Document Embeddings and Spherical K-Means, while choosing the Louvain algorithm for community detection. Thus, we improve the Louvain?s community detection algorithm modularity by interconnecting the author nodes for the articles clustered together. To evaluate our method, we collected a dataset containing articles??abstracts and authors. The experimental results show that our method suggests potential new collaborations by adding vertices to the graph after analysing the textual content.",,978-1-7281-9080-8,10.1109/ICCP51029.2020.9266267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9266267,Community detection;Clustering;Louvain;Spherical K-Means,Clustering algorithms;Collaboration;Partitioning algorithms;Merging;Image edge detection;Pipelines;Mathematical model,,,,,,24,,26-Nov-20,,,IEEE,IEEE Conferences
Manifold Regularized Stochastic Block Model,流形正則化隨機塊模型,T. He; L. Bai; Y. Ong,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),13-Feb-20,2019,,,800,807,"Stochastic block models (SBMs) play essential roles in network analysis, especially in those related to unsupervised learning (clustering). Many SBM-based approaches have been proposed to uncover network clusters, by means of maximizing the block-wise posterior probability that generates edges bridging vertices. However, none of them is capable of inferring the cluster preference for each vertex through simultaneously modeling block-wise edge structure, vertex features, and similarities between pairwise vertices. To fill this void, we propose a novel SBM dubbed manifold regularized stochastic model (MrSBM) to perform the task of unsupervised learning in network data in this paper. Besides modeling edges that are within or connecting blocks, MrSBM also considers modeling vertex features utilizing the probabilities of vertex-cluster preference and feature-cluster contribution. In addition, MrSBM attempts to generate manifold similarity of pairwise vertices utilizing the inferred vertex-cluster preference. As a result, the inference of cluster preference may well capture the comparability in the manifold. We design a novel process for network data generation, based on which, we specify the model structure and formulate the network clustering problem using a novel likelihood function. To guarantee MrSBM learns the optimal cluster preference for each vertex, we derive an effective Expectation-Maximization based algorithm for model fitting. MrSBM has been tested on five sets of real-world network data and has been compared with both classical and state-of-the-art approaches to network clustering. The competitive experimental results validate the effectiveness of MrSBM.",2375-0197,978-1-7281-3798-8,10.1109/ICTAI.2019.00115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995195,Stochastic block model (SBM);generative model;network clustering;community detection;complex network;social network;biological network;document network;manifold regulation;vertex features,,expectation-maximisation algorithm;pattern clustering;probability;stochastic processes;unsupervised learning,manifold regularized stochastic block model;network analysis;unsupervised learning;SBM-based approaches;block-wise posterior probability;block-wise edge structure;pairwise vertices;stochastic model;MrSBM;vertex features;feature-cluster contribution;manifold similarity;network data generation;model structure;network clustering problem;optimal cluster preference;model fitting;real-world network data;expectation-maximization based algorithm;manifold regularized stochastic model;vertex-cluster preference,,1,,32,,13-Feb-20,,,IEEE,IEEE Conferences
Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets,通過對Web片段中的最高相關日期進行聚類來消除隱式時間查詢的歧義,R. Campos; A. M. Jorge; G. Dias; C. Nunes,"LIAAD, INESC TEC, Portugal; LIAAD, INESC TEC, Portugal; HULTECH/GREYC, Univ. of Caen Basse-Normandie, Caen, France; Dept. of Math., Univ. of Beira Interior, Covilha, Portugal",2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology,2-May-13,2012,1,,1,8,"With the growing popularity of research in Temporal Information Retrieval (T-IR), a large amount of temporal data is ready to be exploited. The ability to exploit this information can be potentially useful for several tasks. For example, when querying ""Football World Cup Germany"", it would be interesting to have two separate clusters {1974,2006} corresponding to each of the two temporal instances. However, clustering of search results by time is a non-trivial task that involves determining the most relevant dates associated to a query. In this paper, we propose a first approach to flat temporal clustering of search results. We rely on a second order co-occurrence similarity measure approach which first identifies top relevant dates. Documents are grouped at the year level, forming the temporal instances of the query. Experimental tests were performed using real-world text queries. We used several measures for evaluating the performance of the system and compared our approach with Carrot Web-snippet clustering engine. Both experiments were complemented with a user survey.",,978-1-4673-6057-9,10.1109/WI-IAT.2012.158,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511858,Temporal Information Retrieval;Temporal Clustering;Dating Implicit Temporal Queries;Temporal Query Understanding,,pattern clustering;query processing;search engines;Web sites,implicit temporal query disambiguation;top relevant date clustering;temporal information retrieval;T-IR;temporal data;search result clustering;second order cooccurrence similarity measure approach;real-world text queries;Carrot Web-snippet clustering engine;user survey,,4,,17,,2-May-13,,,IEEE,IEEE Conferences
Iterative Denoising using Jensen-Renyi Divergences with an Application to Unsupervised Document Categorization,使用Jensen-Renyi散度的迭代去噪及其在無監督文檔分類中的應用,D. Karakos; S. Khudanpur; J. Eisner; C. E. Priebe,"Center for Language and Speech Processing, Johns Hopkins University, MD. damianos@jhu.edu; Center for Language and Speech Processing, Johns Hopkins University, MD. khudanpur@jhu.edu; Center for Language and Speech Processing, Johns Hopkins University, MD. eisner@jhu.edu; Department of Applied Mathematics and Statistics, Johns Hopkins University, MD. cep@jhu.edu","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",4-Jun-07,2007,2,,II-509,II-512,"Iterative denoising trees were used by Karakos et al. (2005) for unsupervised hierarchical clustering. The tree construction involves projecting the data onto low-dimensional spaces, as a means of smoothing their empirical distributions, as well as splitting each node based on an information-theoretic maximization objective. In this paper, we improve upon the work of (Karakos et al., 2005) in two ways: (i) the amount of computation spent searching for a good projection at each node now adapts to the intrinsic dimensionality of the data observed at that node; (ii) the objective at each node is to find a split which maximizes a generalized form of mutual information, the Jensen-Renyi divergence; this is followed by an iterative Naive Bayes classification. The single parameter 帢 of the Jensen-Renyi divergence is chosen based on the ""strapping"" methodology, which learns a meta-classifier on a related task. Compared with the sequential information bottleneck method, our procedure produces state-of-the-art results on an unsupervised categorization task of documents from the ""20 Newsgroups"" dataset.",2379-190X,1-4244-0727-3,10.1109/ICASSP.2007.366284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4217457,Unsupervised learning;clustering methods;information theory;text processing,Noise reduction;Statistical distributions;Mutual information;Smoothing methods;Decision trees;Classification tree analysis;Computer vision;Distributed computing;Mathematics;Natural languages,Bayes methods;document image processing;image classification;image denoising;iterative methods;trees (mathematics),iterative denoising trees;Jensen-Renyi divergences;unsupervised document categorization;unsupervised hierarchical clustering;Naive Bayes classification;strapping methodology;meta-classifier;sequential information bottleneck method,,4,,21,,4-Jun-07,,,IEEE,IEEE Conferences
Handwritten ZIP code recognition using lexicon free word recognition algorithm,使用詞典免費單詞識別算法的手寫郵政編碼識別,F. Kimura; Y. Miyake; M. Shridhar,"Fac. of Eng., Mie Univ., Tsu, Japan; NA; NA",Proceedings of 3rd International Conference on Document Analysis and Recognition,6-Aug-02,1995,2,,906,910 vol.2,"The paper describes a new approach to ZIP code recognition using a word recognition algorithm, where a numeral string is recognized as a word. The paper also describes an end to end ZIP code recognition system consisting of tilt/slant correction, line segmentation, word segmentation, ZIP code location, as well as the ZIP code recognition. Evaluation tests are performed using address block image samples collected from United States mail pieces. The results of isolated numeral recognition, manually extracted ZIP code recognition, and end to end ZIP code recognition are presented to show and discuss the advantage of the word recognition based numeral string recognition.",,0-8186-7128-9,10.1109/ICDAR.1995.602048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=602048,,Handwriting recognition;Image segmentation;Character recognition;Testing;Performance evaluation;Postal services;Writing;Clustering algorithms;Error correction;Image recognition,document image processing;image recognition;postal services;image segmentation;edge detection,handwritten ZIP code recognition;lexicon free word recognition algorithm;word recognition algorithm;numeral string recognition;end to end ZIP code recognition system;tilt correction;slant correction;line segmentation;word segmentation;ZIP code location;address block image samples;United States mail pieces;isolated numeral recognition;manually extracted ZIP code recognition,,20,,12,,6-Aug-02,,,IEEE,IEEE Conferences
"Handling Out of Vocabulary in supervised event extraction on Indonesian tweets: Using word representation, word list, word context and document level features",處理印尼推文的受監督事件提取中的詞彙：使用單詞表示，單詞列表，單詞上下文和文檔級別功能,F. Muhammad; A. Purwarianti,"School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia",2016 International Conference on Data and Software Engineering (ICoDSE),1-Jun-17,2016,,,1,6,"Extracting event information from Twitter is still promising since there are many Twitter accounts built just to spread the event information broadly. The most difficult part to extract event information is the Out of Vocabulary (OOV) problem, especially for event name. Here, we tried to enhance the features used for our supervised event extraction. These features include the word representation (skip-gram model and brown cluster), word list (event name and event location), word context and document level feature. By using CRF as the classification algorithm, 4 fold cross validation technique, and 1,300 tweets, the best F-Measure score achieved for OOV cases was 0.6 which is a significant improvement compared to the baseline of 0.445. The enhanced features also improved the F-Measure score for all vocabulary case from 0.693 (baseline) into 0.814 (proposed).",,978-1-5090-5671-2,10.1109/ICODSE.2016.7936113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936113,Skip-Gram Model;Brown Cluster;Event Information Extraction;Out of Vocabulary,Vocabulary,document handling;information retrieval;pattern classification;social networking (online);vocabulary,out of vocabulary handling;supervised event extraction;Indonesian tweets;word representation;word list;word context;document level features;event information extraction;Twitter;OOV problem;event name;skip-gram model;brown cluster;event location;CRF;classification algorithm;cross validation technique;f-measure score,,1,,17,,1-Jun-17,,,IEEE,IEEE Conferences
Text processing by using projective ART neural networks,使用投影ART神經網絡進行文本處理,R. Forg獺?; R. Krakovsk羸,"Dept. of Parallel and Distributed Information Processing, Institute of Informatics Slovak Academy of Sciences, Bratislava, Slovakia; Department of Informatics, Faculty of Education, Catholic University, Ru鱉omberok, Slovakia",2016 New Trends in Signal Processing (NTSP),21-Nov-16,2016,,,1,5,"This paper presents the summary of experience obtained with the modified clustering algorithm of Projective Adaptive Resonance Theory. The algorithm was proposed by authors, and was tested for text processing. Possible usage of the algorithm is exemplified by text document clustering, and generation of keyword dictionaries from text documents.",,978-8-0804-0529-8,10.1109/NTSP.2016.7747780,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747780,Projective Adaptive Resonance Theory;neural network;clustering;text processing,Neurons;Clustering algorithms;Dictionaries;Neural networks;Semantics;Text processing;Adaptive systems,ART neural nets;text analysis,text processing;projective ART neural networks;projective adaptive resonance theory;modified clustering algorithm;text document clustering,,,,20,,21-Nov-16,,,IEEE,IEEE Conferences
Structured clustering with automatic kernel adaptation,具有自動內核適應功能的結構化集群,W. Pan; J. T. Kwok,"Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong",The 2011 International Joint Conference on Neural Networks,3-Oct-11,2011,,,1322,1327,"Clustering is an invaluable data analysis tool in a variety of applications. However, existing algorithms often assume that the clusters do not have any structural relationship. Hence, they may not work well in situations where such structural relationships are present (e.g., it may be given that the document clusters are residing in a hierarchy). Recently, the development of the kernel-based structured clustering algorithm CLUHSIC [9] tries to alleviate this problem. But since the input kernel matrix is defined purely based on the feature vectors of the input data, it does not take the output clustering structure into account. Consequently, a direct alignment of the input and output kernel matrices may not assure good performance. In this paper, we reduce this mismatch by learning a better input kernel matrix using techniques from semi-supervised kernel learning. We combine manifold information and output structure information with pairwise clustering constraints that are automatically generated during the clustering process. Experiments on a number of data sets show that the proposed method outperforms existing structured clustering algorithms.",2161-4407,978-1-4244-9637-2,10.1109/IJCNN.2011.6033377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033377,,Kernel;Clustering algorithms;Manifolds;Vegetation;Loss measurement;Optimization,data analysis;learning (artificial intelligence);matrix algebra;pattern clustering,automatic kernel adaptation;data analysis tool;structural relationship;kernel-based structured clustering algorithm;feature vectors;input kernel matrix;semisupervised kernel learning;pairwise clustering constraints,,,,16,,3-Oct-11,,,IEEE,IEEE Conferences
Text line extraction of curved document images using hybrid metric,使用混合度量提取彎曲文檔圖像的文本行,Z. Huang; J. Gu; G. Meng; C. Pan,"Institute of Automation, Chinese Academy of Sciences (CASIA) No.95, Zhongguancun East Road, Beijing 100190, P.R. China; Institute of Automation, Chinese Academy of Sciences (CASIA) No.95, Zhongguancun East Road, Beijing 100190, P.R. China; Institute of Automation, Chinese Academy of Sciences (CASIA) No.95, Zhongguancun East Road, Beijing 100190, P.R. China; Institute of Automation, Chinese Academy of Sciences (CASIA) No.95, Zhongguancun East Road, Beijing 100190, P.R. China",2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR),9-Jun-16,2015,,,251,255,"This paper proposes a novel approach to extracting text lines from curved document images that are captured from an opened thick and bounded book or a curled document sheet. We first extract the connected components (CCs) in a binary image and then remove the non-textual CCs. Additionally, we estimate the orientation of each CC through local projections and a feature vector is accordingly defined to describe each CC. Furthermore, a hybrid metric is designed based on the distances between CCs and the corresponding minimum spanning tree which can well exploit the overall structure of the curved text lines is constructed. A tree pruning strategy is finally proposed to cluster the CCs into separated text lines. Experimental results on a wide variety of curved document images demonstrate the effectiveness and efficiency of the proposed method.",2327-0985,978-1-4799-6100-9,10.1109/ACPR.2015.7486504,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486504,,Measurement;Estimation;Histograms;Distortion;DH-HEMTs;Radon;Strips,document image processing;feature extraction;text detection;trees (mathematics),curved document image text line extraction;hybrid metric;bounded book;curled document sheet;binary image;connected component extraction;local projection;afeature vector;minimum spanning tree;tree pruning strategy;curved document image;nontextual CC removal,,3,,11,,9-Jun-16,,,IEEE,IEEE Conferences
Non-supervised determination of allograph sub-classes for on-line omni-scriptor handwriting recognition,在線全腳本手寫識別的Allograph子類的無監督確定,L. Prevost; M. Milgram,"LIS Groupe PC, Univ. Pierre et Marie Curie, Paris, France; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,438,441,"We present a novel clustering algorithm dedicated to the determination of character allographs. The ""problem of the allographs"" specific of the dynamic handwriting in omni-scriptor context renders the implementation of ""classical"" clustering algorithms particularly delicate because it introduces the notion of heterogeneous classes characterized by strongly variable example densities. We propose an hybrid clustering algorithm that combines a prototype placement stage and an adaptation stage. The first realizes an under-optimal determination of kernels in the different clusters composing the classes. It is followed by a kernel adaptation stage driving to an optimization of their position. The process drastically reduces the number of references to examine during a k-nn classification while preserving to the classifier a high level of performances. The experience has been driven on an extensive alphabet comprising 80 classes (upper- and lower-case letters, digits and mathematical symbols). Recognition rate, evaluated on near 35000 examples from the UNIPEN database show the sturdiness of the modelization.",,0-7695-0318-7,10.1109/ICDAR.1999.791818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791818,,Handwriting recognition,handwriting recognition;handwritten character recognition;optical character recognition;pattern clustering;word processing,non-supervised determination;allograph sub-classes;online omni-scriptor handwriting recognition;prototype placement stage;character allographs;dynamic handwriting;omni-scriptor context;heterogeneous classes;strongly variable example densities;hybrid clustering algorithm;adaptation stage;under-optimal determination;kernel adaptation stage;k-nn classification;alphabet;mathematical symbols;UNIPEN database,,1,,13,,6-Aug-02,,,IEEE,IEEE Conferences
A Caching policy driven by clusters of high popularity,由高人氣集群驅動的緩存策略,N. M. Markovich; U. R. Krieger,"Institute of Control Sciences, Russian Academy of Sciences, 117997 Moscow, Russia; Faculty WIAI, Otto-Friedrich-University, D-96047 Bamberg, Germany",2016 International Wireless Communications and Mobile Computing Conference (IWCMC),29-Sep-16,2016,,,363,368,"Caching is applied to provide requested documents or Web contents quickly from a short memory. We consider the Cluster Caching Rule policy proposed recently by Markovich [12]. The idea of the rule is to keep only highly popular contents in the cache. Due to dependency in the inter-request process and heavy-tail distributed inter-request times, such frequently requested documents arise in clusters of popularity. The corresponding clusters of documents are loaded in the cache. If the requested document is present in the previous cluster, then it stays further in the cache. Otherwise, it is evicted from the cache. A mixture of m-dependent Markov and Poisson renewal processes is proposed as example of an inter-request time model. We present the hit/miss probabilities of such caching policy and consider cache size estimation.",2376-6506,978-1-5090-0304-4,10.1109/IWCMC.2016.7577085,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577085,Cluster Caching Rule;content popularity;clusters of extreme values;hit/miss probability;extremal index,Markov processes;Indexes;Manganese;Electronic mail;Probabilistic logic;Load modeling;Estimation,cache storage;Markov processes,hit-miss probability;cache size estimation;Poisson renewal process;m-dependent Markov process;popularity cluster;heavy-tail distributed inter-request times;inter-request process;cache content;cluster caching rule policy,,5,,17,,29-Sep-16,,,IEEE,IEEE Conferences
Storing and querying multiversion XML documents using durable node numbers,使用持久節點號存儲和查詢多版本XML文檔,Shu-Yao Chien; V. J. Tsotras; C. Zaniolo; Donghui Zhang,NA; NA; NA; NA,Proceedings of the Second International Conference on Web Information Systems Engineering,7-Aug-02,2001,1,,232,241 vol.1,"Managing multiple versions of XML documents represents an important problem for many traditional applications, such as software configuration control, as well as new ones, such as link permanence of web documents. Research on managing multiversion XML documents seeks to provide efficient and robust techniques for storing, retrieving and querying such documents. In this paper we present a novel approach to version management that achieves these objectives by a scheme based on Durable Node Numbers and timestamps for the elements of XML documents. We first present efficient storage and retrieval techniques for multiversion documents. Then, we explore the indexing and clustering strategies needed to assure efficient support for complex queries on content and on document evolution.",,0-7695-1393-X,10.1109/WISE.2001.996484,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=996484,,XML;Application software;Content management;History;Object oriented databases;Transaction databases;Database systems;Robustness;Indexing;Collaborative work,hypermedia markup languages;query languages;temporal databases;document handling,multiversion XML documents;durable node numbers;software configuration control;link permanence of web documents;version management;Sparse Preorder and Range numbering scheme,,1,,24,,7-Aug-02,,,IEEE,IEEE Conferences
Generation of context-dependent keyword pairs from text by projective ART,通過投影ART從文本生成上下文相關的關鍵字對,R. Krakovsky; R. Forgac,"Department of Informatics, Catholic University in Ruzomberok, Slovakia; Institute of Informatics, Slovak Academy of Sciences, Bratislava, Slovakia",2018 IEEE 16th World Symposium on Applied Machine Intelligence and Informatics (SAMI),26-Mar-18,2018,,,195,200,"The paper deals with the use of Projective Adaptive Resonance Theory neural network (PART) to generate dictionary of context-dependent keyword pairs from text document. The principle of automatic generation of context-dependent keywords pairs is based on a modified algorithm of clustering MA-PART, which was proposed by the authors not long ago to generate the dictionary of single keywords. The proposed model allows clustering in a single iteration process of two words in the input vector. Context dependency of test pair of words is determined on the basis of determination of pair membership within the same cluster.",,978-1-5386-4772-1,10.1109/SAMI.2018.8324838,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324838,,Neurons;Subspace constraints;Neural networks;Clustering algorithms;Mathematical model;Ontologies;Informatics,ART neural nets;iterative methods;pattern clustering;text analysis;vectors,context-dependent keyword pairs;Projective Adaptive Resonance Theory neural network;automatic generation;context-dependent keywords pairs;single keywords;context dependency;text document;projective ART;MA-PART clustering;single iteration process,,,,17,,26-Mar-18,,,IEEE,IEEE Conferences
A Text Topic Mining Algorithm Based on Spatial Propagation Similarity Metric,基於空間傳播相似度量的文本主題挖掘算法,K. Yang; H. Zhang; Z. Chu; L. Sun,"Zhengzhou Normal University, Zhengzhou Henan, 450053; Zhengzhou Normal University, Zhengzhou Henan, 450053; Zhengzhou Normal University, Zhengzhou Henan, 450053; Zhengzhou Normal University, Zhengzhou Henan, 450053",2019 Chinese Control And Decision Conference (CCDC),12-Sep-19,2019,,,4339,4344,"With the arrival of the big data era, a large amount of text data has grown exponentially, and text topic mining has become a challenging task. Text topic knowledge mining based on co-word analysis can not deeply discover the semantic relationship between keywords, while LDA model misses the important information of document keyword structure. Therefore, this paper proposes a text topic mining algorithm based on spatial propagation similarity metrics. Based on the co-occurrence matrix, the spatial propagation similarity metric algorithm is used to calculate the inter-word relationship, and further cluster analysis is carried out, aiming at deeper mining of text themes.",1948-9447,978-1-7281-0106-4,10.1109/CCDC.2019.8832869,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832869,Spatial propagation similarity;co-word analysis;LDA;cluster analysis,Data mining;Measurement;Authentication;Clustering algorithms;Frequency control;Computer architecture,Big Data;data mining;matrix algebra;pattern clustering;text analysis,text topic mining algorithm;spatial propagation similarity metric algorithm;text data;Big Data;co-occurrence matrix;cluster analysis;interword relationship,,,,18,,12-Sep-19,,,IEEE,IEEE Conferences
Evaluating quality of text clustering with ART1,使用ART1評估文本聚類的質量,L. Massey,"R. Mil. Coll. of Canada, Kingston, Ont., Canada","Proceedings of the International Joint Conference on Neural Networks, 2003.",26-Aug-03,2003,2,,1402,1407 vol.2,"Self-organizing large amounts of textual data in accordance to some topics structure is an increasingly important application of clustering. Adaptive resonance theory (ART) neural networks possess several interesting properties that make them appealing in this area. Although ART has been used in several research works as a text clustering tool, the level of quality of the resulting document clusters has not been clearly established yet. In this paper, we present experimental results with binary ART that address this issue by determining how close clustering quality is to an upper bound on clustering quality.",1098-7576,0-7803-7898-9,10.1109/IJCNN.2003.1223901,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223901,,Subspace constraints;Neural networks;Resonance;Text categorization;Testing;Educational institutions;Costs;Stability;Performance evaluation;Upper bound,ART neural nets;text analysis;pattern clustering,text clustering;ART1;adaptive resonance theory;document clusters;clustering quality;neural networks,,5,,32,,26-Aug-03,,,IEEE,IEEE Conferences
DOTS: Detection of Off-Topic Search via Result Clustering,DOTS：通過結果聚類檢測非主題搜索,N. Goharian; A. Platt,"Information Retrieval Laboratory, Illinois Institute of Technology; Information Retrieval Laboratory, Illinois Institute of Technology",2007 IEEE Intelligence and Security Informatics,25-Jun-07,2007,,,145,151,"Often document dissemination is limited to a ""need to know"" basis so as to better maintain organizational trade secrets. Retrieving documents that are off-topic to a user's predefined area of information need (task) via a search engine is potentially a violation of access rights and is a concern to every private, commercial, and governmental organization. Such misuse, defined as ""off-topic access to sensitive data by an authorized user"", is the second most prevalent form of computer crime after viruses per a recent Computer Security Institute/Federal Bureau of Investigation study. We present a content-based off-topic detection approach that uses query result clustering to detect off-topic searches. This approach supports higher detection precision than the state of the art. Multiple methods for picking the ""good"" clusters are proposed, and their effect on the detection rate and precision is evaluated. A high detection precision is critical as a false access violation accusation unfairly and inappropriately subjects the user to scrutiny. Our empirical results show that using clustering query results can significantly reduce such false positives.",,1-4244-1329-X,10.1109/ISI.2007.379547,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4258688,Clustering;Information Retrieval;Off Topic Search;misuse detection,US Department of Transportation;Information retrieval;Search engines;Permission;Humans;Laboratories;Computer crime;Computer viruses;Computer security;Information filtering,computer crime;pattern clustering;query processing;search engines,search engine;document retrieval;computer crime;content-based off-topic detection approach;query result clustering,,1,,26,,25-Jun-07,,,IEEE,IEEE Conferences
An Approach of Service Discovery Based on Service Goal Clustering,基於服務目標聚類的服務發現方法,N. Zhang; J. Wang; K. He; Z. Li,"State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; Sch. of Comput. & Inf. Eng., Henan Univ., Kaifeng, China",2016 IEEE International Conference on Services Computing (SCC),1-Sep-16,2016,,,114,121,"The increasing amount of services published on the Web makes it difficult to discover relevant services for users. Unlike the SOAP-based services that are described by structural WSDL documents, RESTful services, the most popular type of services, are mainly described using short texts. The keyword-based discovery technology for RESTful services adopted by existing service registries is insufficient to obtain accurate services according to user requirements. Moreover, it remains a difficult task for users to specify queries that perfectly reflect their requirements due to the lack of knowledge of their expected service functionalities. In this paper, we propose a goal-oriented service discovery approach, which aims to obtain accurate RESTful services for user functional goals. The approach first groups existing services into clusters using topic models. It then clusters the service goals extracted from the textual descriptions of services by leveraging the topic model trained for services. Based on the service goal clusters, our approach can help users refine their initial queries by recommending similar service goals. Finally, relevant services are obtained by matching the service goals selected by users with those of existing services. Experiments conducted on a real-world service dataset crawled from ProgrammableWeb show the effectiveness of the proposed approach.",,978-1-5090-2628-9,10.1109/SCC.2016.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557443,Service discovery;RESTful services;service goal;topic model;clustering,Semantics;Data models;Skeleton;Computational modeling;Algorithm design and analysis;Computers;Web services,document handling;Internet;service-oriented architecture,service goal clustering;Web;structural WSDL documents;service functionalities;goal oriented service discovery approach;textual descriptions;ProgrammableWeb,,8,,26,,1-Sep-16,,,IEEE,IEEE Conferences
A novel paragraph embedding method for spoken document summarization,一種新的語音文檔摘要段落嵌入方法,K. Chen; S. Liu; B. Chen; H. Wang,"Academia Sinica, Taiwan; Academia Sinica, Taiwan; National Taiwan Normal University, Taiwan; Academia Sinica, Taiwan",2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),19-Jan-17,2016,,,1,6,"Representation learning has emerged as a newly active research subject in many machine learning applications because of its excellent performance. In the context of natural language processing, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as information retrieval and document summarization. However, as far as we are aware, there is only a dearth of research focusing on launching paragraph embedding methods. Extractive spoken document summarization, which can help us browse and digest multimedia data efficiently, aims at selecting a set of indicative sentences from a source document to express the most important theme of the document. A general consensus is that relevance and redundancy are both critical issues in a realistic summarization scenario. However, most of the existing methods focus on determining only the relevance degree between a pair of sentence and document. Motivated by these observations, three major contributions are proposed in this paper. First, we propose a novel unsupervised paragraph embedding method, named the essence vector model, which aims at not only distilling the most representative information from a paragraph but also getting rid of the general background information to produce a more informative low-dimensional vector representation. Second, we incorporate the deduced essence vectors with a density peaks clustering summarization method, which can take both relevance and redundancy information into account simultaneously, to enhance the spoken document summarization performance. Third, the effectiveness of our proposed methods over several well-practiced and state-of-the-art methods is confirmed by extensive spoken document summarization experiments.",,978-9-8814-7682-1,10.1109/APSIPA.2016.7820882,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820882,,Redundancy;Artificial neural networks;Context modeling;Context;Multimedia communication;Streaming media,document handling;information retrieval;natural language processing;speech processing;unsupervised learning;vectors,deduced essence vectors;density peaks clustering summarization method;low-dimensional vector representation;essence vector model;multimedia data;information retrieval;natural language processing;machine learning applications;representation learning;spoken document summarization;novel unsupervised paragraph embedding method,,,,34,,19-Jan-17,,,IEEE,IEEE Conferences
XML Document Clustering Using Common XPath,使用通用XPath的XML文檔集群,Ho-pong Leung; Fu-lai Chung; S. C. F. Chan; R. Luk,"Department of Computing Hong Kong Polytechnic University Hunghom, Hong Kong, China.; NA; NA; NA",International Workshop on Challenges in Web Information Retrieval and Integration,12-Dec-05,2005,,,91,96,"XML is becoming a common way of storing data. The elements and their arrangement in the document?s hierarchy not only describe the document structure but also imply the data?s semantic meaning, and hence provide valuable information to develop tools for manipulating XML documents. In this paper, we pursue a data mining approach to the problem of XML document clustering. We introduce a novel XML structural representation called common XPath (CXP), which encodes the frequently occurring elements with the hierarchical information, and propose to take the CXPs mined to form the feature vectors for XML document clustering. In other words, data mining acts as a feature extractor in the clustering process. Based on this idea, we devise a path-based XML document clustering algorithm called PBClustering which groups the documents according to their CXPs, i.e. their frequent structures. Encouraging simulation results are observed and reported.",,0-7695-2414-1,10.1109/WIRI.2005.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553000,,XML;Data mining;Feature extraction;HTML;Clustering algorithms;Tree data structures;Information retrieval;Indexing;Query processing;Computational efficiency,,,,16,,18,,12-Dec-05,,,IEEE,IEEE Conferences
Learning Sparse Functional Factors for Large-Scale Service Clustering,學習大規模服務集群的稀疏功能因素,Q. Yu; H. Wang; L. Chen,"NA; Southeast Univ., Nanjing, China; Zhejiang Univ., Hangzhou, China",2015 IEEE International Conference on Web Services,17-Aug-15,2015,,,201,208,"The past decade has witnessed a fast growth of web-based services, making discovery of user desired services from a large and diverse service space a fundamental challenge. Service clustering has been demonstrated as a promising solution by automatically detecting functionally similar services so that they can be searched and discovered together. In this way, both the efficiency and accuracy of service discovery can be improved. However, the autonomous nature of service providers leads to highly diverse usage of terms in their respective service descriptions. Furthermore, a typical service description is comprised of very limited terms due to the small number of (and focused) functionalities offered by the service. These unique characteristics make service descriptions different from regular text documents, which poses additional challenges when clustering large-scale services. Recent works show that service clustering can benefit from discovery and use of functionality-related latent factors to represent services as opposed to a large and diverse set of terms. Nonetheless, how to determine the total number of latent functional factors and sparsely assign them to each service description arises as a central challenge, especially for a large service space where there is no easy way to enumerate the types of different functionalities. In this paper, we propose a machine learning method that automatically learns the number of latent functional factors in a service space. It also enforces the sparsity constraint, which allows each service to be represented by a small number of latent functional factors. The sparsity constraint is in line with the fact that most real-world services only provide limited functionalities. We conduct extensive experiments on two sets of real-world service data to demonstrate the effectiveness of the proposed service clustering approach.",,978-1-4673-7272-5,10.1109/ICWS.2015.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7195570,Service clustering;nonparametric learning;latent factor models,Bayes methods;Clustering algorithms;Accuracy;Computational modeling;Web services;Least squares approximations,data mining;learning (artificial intelligence);pattern clustering;statistical analysis;Web services,latent functional factor;sparsity constraint;machine learning method;Web-based service;service clustering;service discovery;service description,,13,,28,,17-Aug-15,,,IEEE,IEEE Conferences
A New Suffix Tree Similarity Measure and Labeling for Web Search Results Clustering,Web搜索結果聚類的新後綴樹相似性度量和標籤,A. Kale; U. Bharambe; M. SashiKumar,"Thadomal Shahani Eng. Coll., P.G. Kher Marg, Bandra, India; Thadomal Shahani Eng. Coll., P.G. Kher Marg, Bandra, India; CDAC Kharghar, Navi Mumbai, India",2009 Second International Conference on Emerging Trends in Engineering & Technology,22-Jan-10,2009,,,856,861,"Due to the enormous size of the web and low precision of user queries, finding the right information from the web can be difficult if not impossible. One approach that tries to solve this problem is using clustering techniques for grouping similar documents together in order to facilitate presentation of results in more compact form and enable thematic browsing of the results set. Web search results clustering is an attempt to apply the idea of clustering to document references (snippets) returned by a search engine in response to a query. Thus, it can be perceived as a way of organizing the snippets into set of meaningful thematic groups. This paper introduces a new similarity criterion for merging which is evaluated for search results returned from actual web search engines.",2157-0485,978-1-4244-5250-7,10.1109/ICETET.2009.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395068,,Labeling;Web search;Search engines;Clustering algorithms;Navigation;Information retrieval;Merging;Educational institutions;Size measurement;Organizing,information retrieval;Internet;pattern clustering;search engines;trees (mathematics),suffix tree similarity measure;web search results clustering;user queries;document references;web search engines,,2,,34,,22-Jan-10,,,IEEE,IEEE Conferences
An Efficient Method of Genetic Algorithm for Text Clustering Based on Singular Value Decomposition,基於奇異值分解的遺傳算法高效文本聚類方法,W. Song; S. C. Park,"Chonbuk National University, Korea; Chonbuk National University, Korea",7th IEEE International Conference on Computer and Information Technology (CIT 2007),21-Nov-07,2007,,,53,58,"In this paper, we propose a method of genetic algorithm (GA) for text clustering based on singular value decomposition technique. The main difficulty in the application of GA to text clustering is its long string representation in high dimensional space. Because the most straightforward and popular approach represents texts with vector space model (VSM), that is, each unique term in the vocabulary represents one dimension. Singular value decomposition (SVD) is a successful technique arising from numerical linear algebra that is used in latent semantic indexing (LSI). Employing the SVD-based document representation, LSI can overcome the problems by using statistically derived conceptual indices instead of individual words and provide a dimension reduced space. Genetic algorithm belongs to search techniques which could automatically exploit the optimal solution for objective or fitness function of an optimization problem. GA can be used in conjunction with the reduced latent semantic structure and improve clustering efficiency and accuracy. Our algorithm is performed on Reuter documents collection. The results show that the performance of SVD-based GA is significantly superior to that of conventional GA in vector space model.",,978-0-7695-2983-7,10.1109/CIT.2007.197,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385056,,Genetic algorithms;Singular value decomposition;Clustering algorithms;Large scale integration;Vocabulary;Indexing;Computational efficiency;Information technology;Genetic engineering;Vectors,genetic algorithms;pattern clustering;singular value decomposition;text analysis,genetic algorithm;text clustering;singular value decomposition;string representation;vector space model;numerical linear algebra;latent semantic indexing;statistical analysis;optimization problem;Reuter document collection,,1,,19,,21-Nov-07,,,IEEE,IEEE Conferences
Search Results Clustering without External Resources,沒有外部資源的搜索結果聚類,C. Staff; J. Azzopardi; C. Layfield; D. Mercieca,"Fac. of Inf. & Commun. Technol., Univ. of Malta, Msida, Malta; Fac. of Inf. & Commun. Technol., Univ. of Malta, Msida, Malta; Fac. of Inf. & Commun. Technol., Univ. of Malta, Msida, Malta; Fac. of Inf. & Commun. Technol., Univ. of Malta, Msida, Malta",2015 26th International Workshop on Database and Expert Systems Applications (DEXA),15-Feb-16,2015,,,276,280,"Our unsupervised Search Results Clustering (SRC) system partitions into clusters the top-n results returned by a search engine. We present the results of experiments with our SRC system that performs incremental clustering on document titles and snippets only and does not use external resources, yet which outperforms the best performers to date on the SemEval-2013 Task 11 gold standard. We include Latent Semantic Analysis (LSA) as an optional step, using the snippets themselves as the background corpus. We demonstrate that better results are achieved by leaving the query terms out of the clustering process, and that currently, the version without LSA outperforms the version with LSA.",2378-3915,978-1-4673-7582-5,10.1109/DEXA.2015.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406306,SemEval-2013 Task 11;K-Means;Unsupervised Clustering;Generalized Dunns Index;Latent Semantic Analysis;Word Sense Induction,Clustering algorithms;Conferences;Semantics;Indexes;Search engines;Standards;Gold,pattern clustering;search engines,LSA;latent semantic analysis;SemEval-2013 Task 11 gold standard;search engine;unsupervised SRC system;external resources;search results clustering,,4,,30,,15-Feb-16,,,IEEE,IEEE Conferences
Collaboratively Tracking Interests for User Clustering in Streams of Short Texts,協作跟?短文本流中的用戶聚類興趣,S. Liang; E. Yilmaz; E. Kanoulas,"Sun Yat-sen University, Guangzhou, Guangdong, China; Department of Computer Science, University College London, London, United Kingdom; University of Amsterdam, Amsterdam, The Netherlands",IEEE Transactions on Knowledge and Data Engineering,10-Jan-19,2019,31,2,257,272,"In this paper, we aim at tackling the problem of user clustering in the context of their published short text streams. Clustering users by short text streams is more challenging than in the case of long documents associated with them as it is difficult to track users' dynamic interests in streaming sparse data. To obtain better user clustering performance, we propose two user collaborative interest tracking models that aim at tracking changes of each user's dynamic topic distributions in collaboration with their followees' dynamic topic distributions, based both on the content of current short texts and the previously estimated distributions. Our models can be either short-term or long-term dependency topic models. Short-term dependency model collaboratively tracks users' interests based on users' topic distributions at the previous time period only, whereas long-term dependency model collaboratively tracks users' interests based on users' topic distributions at multiple time periods in the past. We also propose two collapsed Gibbs sampling algorithms for collaboratively inferring users' dynamic interests for their clustering in our short-term and long-term dependency topic models, respectively. We evaluate our proposed models via a benchmark dataset consisting of Twitter users and their tweets. Experimental results validate the effectiveness of our proposed models that integrate both users' and their collaborative interests for user clustering by short text streams.",1558-2191,,10.1109/TKDE.2018.2832211,Elsevier Foundation; UCL Big Data Institute; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355681,Clustering;topic models;streaming text;twitter,Collaboration;Clustering algorithms;Heuristic algorithms;Context modeling;Twitter;Analytical models;Discrete cosine transforms,Markov processes;Monte Carlo methods;pattern clustering;social networking (online);text analysis,user collaborative interest tracking models;short-term dependency model;long-term dependency topic models;Twitter users;published short text streams;user clustering;users topic distribution;collapsed Gibbs sampling algorithm,,1,,46,,7-May-18,,,IEEE,IEEE Journals
Multi-document Text Summarization for Competitor Intelligence: A Methodology,用於競爭者情報的多文檔文本摘要：一種方法,S. Chakraborti; S. Dey,"Dept. of Inf. Syst., Indian Inst. of Manage., Indore, India; Dept. of Inf. Syst., Indian Inst. of Manage., Indore, India",2014 2nd International Symposium on Computational and Business Intelligence,11-Jun-15,2014,,,97,100,"With increasing adoption of Internet and various social media technologies by companies, the web has become a rich source of information about many aspects of organizational activities. Hence one input towards gathering competitive intelligence is to mine the text sources available on the web for any particular company and use the summarized version of that information for strategic decision making. This paper discusses a methodological approach and an architecture for such summarization system which can be used for gathering competitor intelligence by the companies.",,978-1-4799-7552-5,10.1109/ISCBI.2014.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119542,Text Analysis;Text Summarization;Multi Document Summarization;Topic Detection;Clustering;Decision Support Systems;Competitor Intelligence,Semantics;Ontologies;Business;Standards;Expert systems;Hidden Markov models,competitive intelligence;data mining;Internet;text analysis,multidocument text summarization system;competitor intelligence;Internet;social media technologies;World Wide Web;organizational activities;competitive intelligence;text sources;strategic decision making,,2,,29,,11-Jun-15,,,IEEE,IEEE Conferences
Relationship Network Augmented Web Services Clustering,關係網絡增強型Web服務群集,Y. Cao; J. Liu; M. Shi; B. Cao; X. Zhang; Y. Wang,Hunan University of Science and Technology; Hunan Univeristy of Science and Technology; Hunan University of Science and Technology; Hunan University of Science and Technology; Hunan University of Science and Technology; Macquarie University,2019 IEEE International Conference on Web Services (ICWS),29-Aug-19,2019,,,247,254,"Clustering Web services can promote the quality of services discovery and management within a service repository. Traditional clustering methods primarily focus on using the semantic distance between service features, i.e., latent topics learned from WSDL documents, to measure the service content similarity between Web services. Few works exploited the structural information generated during the usage of Web services, i.e., the service compositing and tagging behaviors. Nowadays, Web services frequently interact (e.g., composition relation and tag sharing relation) with each other to form a complex service relationship network. The rich network relations inherently reflect either positive or negative categorical relevance between services, which can be strong supplement of service semantics in characterizing the functional affinities between services. In this paper, we propose to utilize the services relationship network for augmented services clustering algorithm design. We first learn semantic information from service descriptions based on the widely used Doc2Vec model. Then, we propose a revised K-means algorithm for service clustering that benefits simultaneously from service semantics and network relations, where the service relations are previously preserved in a set of low-dimensional vectors achieved based on a recently proposed network embedding technique. Experiments on a real-world dataset demonstrated that the proposed clustering approach yields an improvement of 6.89% than the state-of-the-art.",,978-1-7281-2717-0,10.1109/ICWS.2019.00050,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818427,Web services;services clustering;services network;network embedding;Semantic mining,,learning (artificial intelligence);pattern clustering;text analysis;Web services,service relations;services discovery;service repository;service features;service content similarity;service compositing;tagging behaviors;complex service relationship network;service semantics;augmented services;service descriptions;network embedding technique;low-dimensional vectors;revised K-means algorithm;Doc2Vec model;semantic information learning;service categorical relevance;composition relation;tag sharing relation;WSDL documents;service management;relationship network augmented Web services clustering,,1,,26,,29-Aug-19,,,IEEE,IEEE Conferences
On the resemblance and containment of documents,關於文件的相似性和包含性,A. Z. Broder,"Digital Systems Res. Center, Palo Alto, CA, USA",Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171),6-Aug-02,1997,,,21,29,"Given two documents A and B we define two mathematical notions: their resemblance r(A, B) and their containment c(A, B) that seem to capture well the informal notions of ""roughly the same"" and ""roughly contained."" The basic idea is to reduce these issues to set intersection problems that can be easily evaluated by a process of random sampling that can be done independently for each document. Furthermore, the resemblance can be evaluated using a fixed size sample for each document. This paper discusses the mathematical properties of these measures and the efficient implementation of the sampling process using Rabin (1981) fingerprints.",,0-8186-8132-2,10.1109/SEQUEN.1997.666900,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666900,,Sampling methods;Web sites;Digital systems;Particle measurements;Fingerprint recognition;Explosions;Algorithm design and analysis;Clustering algorithms;Costs;Testing,random processes;set theory;information retrieval,documents;containment;resemblance;mathematical notions;informal notions;roughly the same;roughly contained;intersection problems;random sampling;fixed size sample;mathematical properties;Rabin fingerprints;World Wide Web;information retrieval,,139,,10,,6-Aug-02,,,IEEE,IEEE Conferences
Knowledge-based document embedding for cross-domain text classification,基於知識的文檔嵌入，用於跨域文本分類,Y. Li; B. Wei; L. Yao; H. Chen; Z. Li,"College of Computer Science and Technology, Zhejiang University, 310027 Hangzhou, China; College of Computer Science and Technology, Zhejiang University, 310027 Hangzhou, China; College of Computer Science and Technology, Zhejiang University, 310027 Hangzhou, China; College of Computer Science and Technology, Zhejiang University, 310027 Hangzhou, China; College of Computer Science and Technology, Zhejiang University, 310027 Hangzhou, China",2017 International Joint Conference on Neural Networks (IJCNN),3-Jul-17,2017,,,1395,1402,"Cross-domain learning text classification aims to train an accurate model for a target domain by using labeled text data from a source domain with different but related data distributions. To narrow the data distribution gap between different domains, most of the previous approaches utilize the bag-of-words model to obtain latent features representation of the text. However, this kind of model loses the word order information and misses the background knowledge of the text. As the result, the conceptual information of the text is ignored to a big extent. In this paper, we propose a novel framework named Document Concept Vector for the cross-domain text classification which leverages both the neural network and the knowledge base in order to produce a high quality representation of the text. Specifically, a raw document is first transformed into a conceptualized document which consists of a set of concepts by utilizing a large taxonomy knowledge base. After that, the conceptualized document is transformed into a document vector through the neural network and the vector is used as the concept level feature of the original document. Finally, we conducted the experiments on two real-world corpora and compared it with both traditional classification algorithms and several state-of-the-art approaches of cross-domain text classification. The results validate the effectiveness of our framework.",2161-4407,978-1-5090-6182-2,10.1109/IJCNN.2017.7966016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966016,,Knowledge based systems;Neural networks;Clustering algorithms;Training;Adaptation models;Companies;Data models,knowledge based systems;learning (artificial intelligence);neural nets;pattern classification;text analysis,cross-domain learning text classification;target domain;labeled text data;source domain;data distributions;latent feature representation;word order information;background knowledge;document concept vector;neural network;high quality text representation;raw document;conceptualized document;large taxonomy knowledge base;knowledge-based document embedding,,,,29,,3-Jul-17,,,IEEE,IEEE Conferences
The allocation of documents in multiprocessor information retrieval systems: an application of genetic algorithms,多處理器信息檢索系統中的文檔分配：遺傳算法的應用,H. T. Siegelmann; O. Frieder,"Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA; NA","Conference Proceedings 1991 IEEE International Conference on Systems, Man, and Cybernetics",6-Aug-02,1991,,,645,650 vol.1,"The determination of an optimal document allocation within a distributed memory, distributed input/output (I/O) multicomputer, the multiprocessor document allocation problem (MDAP), is an NP-complete problem. Obtaining an optimal document allocation, therefore, is computationally intractable, and, hence, heuristic approaches are required. A genetic-algorithm-based approach to MDAP is described. A proof convergence for the algorithm is provided. Some experimental results from a simulation study of the approach are given. The results illustrate the potential of the genetic-algorithm-based approach as a means of tackling MDAP.<>",,0-7803-0233-8,10.1109/ICSMC.1991.169758,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=169758,,Information retrieval;Genetic algorithms;Clustering algorithms;Parallel processing;Application software;Computer science;Concurrent computing;NP-complete problem;Convergence;Databases,genetic algorithms;information retrieval systems;parallel algorithms,distributed I/O multicomputer;multiprocessor information retrieval systems;genetic algorithms;distributed memory;multiprocessor document allocation problem;NP-complete problem;heuristic approaches;proof convergence,,4,,23,,6-Aug-02,,,IEEE,IEEE Conferences
Comparison between fingerprint and winnowing algorithm to detect plagiarism fraud on Bahasa Indonesia documents,指紋和風選算法在印度尼西亞語文獻中檢測竊欺詐的比較,A. T. Wibowo; K. W. Sudarmadi; A. M. Barmawi,"Faculty of Informatics, Telkom Institute of Technology, Bandung, Indonesia; Faculty of Informatics, Telkom Institute of Technology, Bandung, Indonesia; Faculty of Informatics, Telkom Institute of Technology, Bandung, Indonesia",2013 International Conference of Information and Communication Technology (ICoICT),5-Aug-13,2013,,,128,133,"Plagiarism detection has been widely discussed in recent years. Various approaches have been proposed such as the text-similarity calculation, structural-approaches, and the fingerprint. In fingerprint-approaches, small parts of document are taken to be matched with other documents. In this paper, fingerprint and Winnowing algorithm is proposed. Those algorithms are used for detecting plagiarism of scientific articles in Bahasa Indonesia. Plagiarism classification is determined from those two documents by a Dice Coefficient at a certain threshold value. The results showed that the best performance of fingerprint algorithm was 92.8% while Winnowing algorithm's best performance was 91.8%. Level-of-relevance to the topic analysis result showed that Winnowing algorithm has got stronger term-correlation of 37.1% compared to the 33.6% fingerprint algorithm.",,978-1-4673-4992-5,10.1109/ICoICT.2013.6574560,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574560,Fingerprint;Winnowing;Plagiarism Detection;Dice Coefficient,Fingerprint recognition;Plagiarism;Algorithm design and analysis;Accuracy;Classification algorithms;Communications technology;Clustering algorithms,copy protection;fraud;security of data;text analysis,Winnowing algorithm;plagiarism fraud detection;Bahasa Indonesia document;text similarity calculation;scientific articles;plagiarism classification;dice coefficient;fingerprint algorithm;topic analysis,,7,,13,,5-Aug-13,,,IEEE,IEEE Conferences
Keyword selection method for characterizing text document maps,用於表徵文本文檔圖的關鍵字選擇方法,K. Lagus; S. Kaski,"Neural Networks Res. Centre, Helsinki Univ. of Technol., Finland; NA",1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470),6-Aug-02,1999,1,,371,376 vol.1,"Characterization of subsets of data is a recurring problem in data mining. We propose a keyword selection method that can be used for obtaining characterizations of clusters of data whenever textual descriptions can be associated with the data. Several methods that cluster data sets or form projections of data provide an order or distance measure of the clusters. If such an ordering of the clusters exists or can be deduced, the method utilizes the order to improve the characterizations. The proposed method may be applied, for example, to characterizing graphical displays of collections of data ordered (e.g. with SOM algorithm). The method is validated using a collection of 10000 scientific abstracts from the INSPEC database organized on a WEBSOM document map.",0537-9989,0-85296-721-7,10.1049/cp:19991137,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=819749,,,information retrieval,text document maps;data mining;keyword selection;data clusters;INSPEC database;WEBSOM;information retrieval;self organising map;SOM algorithm,,15,,,,6-Aug-02,,,IET,IET Conferences
A topic-based cross-language retrieval model with PLSA and TF-IDF,使用PLSA和TF-IDF的基於主題的跨語言檢索模型,Z. Huo; J. Wu; Y. Lu; C. Li,"School of Data and Computer Science, Sun yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun yat-sen University, Guangzhou, China",2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA),28-May-18,2018,,,340,344,"Cross-language document retrieval is an important research topic in information retrieval. This paper proposes a cross-language retrieval model, which can retrieve English documents for a Chinese query. The model includes two parts: topic-processing and online retrieval. The topic-processing could be executed once the library of literature is updated. It extracts topics from semantics of texts and then clusters documents of different languages with the extracted topics. Three crucial algorithms are adopted in topic-processing: Probability Latent Semantic Analysis (PLSA) analyzes the topics of documents, Term Frequency-Inverse Document Frequency (TFIDF) identifies keywords for the topics, and word co-occurrence method with dictionary-based translation generates the association among words of different languages. Practical retrieval runs whenever a retrieval request is submitted by a user. It achieves our goal of cross-languages retrieval by analyzing the correlation between query terms and topics, and using document correlation results from topic-processing to find appropriate document results.",,978-1-5386-4794-3,10.1109/ICBDA.2018.8367704,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367704,cross-language document retrieval;Probability Latent Semantic Analysis (PLSA);Term Frequency-Inverse Document Frequency (TFIDF);word co-occurrence analysis,Text processing;Correlation;Semantics;Biological system modeling;Computational modeling;Databases;Data models,language translation;natural language processing;probability;query processing;text analysis,topic-processing;Term Frequency-Inverse Document Frequency;retrieval request;cross-languages retrieval;document correlation results;cross-language document retrieval;information retrieval;English documents;online retrieval;topic extraction;Probability Latent Semantic Analysis;topic-based cross-language retrieval model;dictionary-based translation;word association;query terms;Chinese query,,1,,20,,28-May-18,,,IEEE,IEEE Conferences
From video shot clustering to sequence segmentation,從視頻鏡頭聚類到序列分割,E. Veneau; R. Ronfard; P. Bouthemy,"Inst. Nat. de l'Audiovisuel, Bry-sur-Marne, France; NA; NA",Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,6-Aug-02,2000,4,,254,257 vol.4,"Segmenting video documents into sequences from elementary shots to supply an appropriate higher level description of the video is a challenging task. The paper presents a two-stage method. First, we build a binary agglomerative hierarchical time-constrained shot clustering. Second, based on the cophenetic criterion, a breaking distance between shots is computed to detect sequence changes. Various options are implemented and compared. Real experiments have proved that the proposed criterion can be efficiently used to achieve appropriate segmentation into sequences.",1051-4651,0-7695-0750-6,10.1109/ICPR.2000.902907,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902907,,Data mining;Symmetric matrices;Gunshot detection systems;Buildings;Indexing;Layout;Feature extraction;Merging,image sequences;image segmentation;pattern clustering,video shot clustering;sequence segmentation;higher level description;two-stage method;binary agglomerative hierarchical time-constrained shot clustering;cophenetic criterion;breaking distance;sequence changes,,13,,10,,6-Aug-02,,,IEEE,IEEE Conferences
Speaker clustering performance improvement using eigen-voice speaker adaptation,使用特徵聲音說話人自適應來提高說話人聚類性能,M. H. Moattar; M. M. Homayounpour,"Laboratory for Intelligent Signal and speech Processing, Computer Enginerring and IT Dept., Amirkabir University of Technology, Tehran, Iran; Laboratory for Intelligent Signal and speech Processing, Computer Enginerring and IT Dept., Amirkabir University of Technology, Tehran, Iran",2009 14th International CSI Computer Conference,8-Dec-09,2009,,,501,506,One of the most important phases of speaker indexing is speaker clustering which aims to find the number of speakers in a speech document and merge the speech segments corresponding to a single speaker. The most critical source of problem in speaker clustering is the speech segments duration which may be so short that proper segment modeling becomes hard to achieve. An alternative suggestion in these situations is to adapt global models with new data instead of building the speaker models from the ground. In this paper we investigate two adaptation techniques in eigen-voice space for improving clustering performance especially for shorter speech utterances. These techniques were embedded in a clustering framework and evaluated on a set of domestic conversational speech. We have also compared the proposed methods with some other known techniques. The experiments show a considerable improvement in speaker clustering performance.,,978-1-4244-4261-4,10.1109/CSICC.2009.5349629,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349629,,Indexing;Speech processing;Clustering algorithms;Signal processing algorithms;Viterbi algorithm;Signal processing;Covariance matrix;Laboratories;Speech analysis;Labeling,pattern clustering;speech processing,eigen-voice speaker adaptation;speaker clustering;speech document;domestic conversational speech,,1,,25,,8-Dec-09,,,IEEE,IEEE Conferences
Finding related documents via communities in the citation graph,通過引用圖中的社區查找相關文檔,P. Wanjantuk; J. A. Keane,"Dept. of Comput. Eng., Khon Kaen Univ., Thailand; NA","IEEE International Symposium on Communications and Information Technology, 2004. ISCIT 2004.",11-Apr-05,2004,1,,445,450 vol.1,"A body of scientific literature can be represented by a directed graph, which is commonly referred to as the citation graph. This paper describes an implementation of the random walk graph clustering algorithm to identify the communities within the citation graph. Based only on the link structure of the citation graph, the random walk graph clustering algorithm is able to efficiently identity highly topically related communities. The ability to identify community structure in the citation graph clearly has practical application. Communities in the citation graph represent related papers on a single topic. This approach is used as a method for finding related papers to a paper of interest by treating member papers in the same community of papers of interest as related papers. The performance of the random walk graph clustering algorithm was evaluated compared to the method for finding related papers used by ResearchIndex, CCIDF. The experimental results show that the random walk graph clustering algorithm performs better than the CCIDF.",,0-7803-8593-4,10.1109/ISCIT.2004.1412885,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1412885,,Clustering algorithms;Databases;Graph theory;Web sites;Publishing;Delay;Software libraries;Indexing;Internet;Indexes,citation analysis;information retrieval;Web sites;directed graphs;pattern clustering;randomised algorithms;data mining,related documents;citation graph;community structure;related papers;performance;scientific literature;directed graph;random walk graph clustering algorithm;topically related communities,,1,,5,,11-Apr-05,,,IEEE,IEEE Conferences
Using Wikipedia-Based Conceptual Contexts to Calculate Document Similarity,使用基於維基百科的概念上下文來計算文檔相似度,F. Kaiser; H. Schwarz; M. Jakob,"Inst. of Parallel & Distrib. Syst., Univ. Stuttgart, Stuttgart; Inst. of Parallel & Distrib. Syst., Univ. Stuttgart, Stuttgart; Inst. of Parallel & Distrib. Syst., Univ. Stuttgart, Stuttgart",2009 Third International Conference on Digital Society,13-Feb-09,2009,,,322,327,"Rating the similarity of two or more text documents is an essential task in information retrieval. For example, document similarity can be used to rank search engine results, cluster documents according to topics etc. A major challenge in calculating document similarity originates from the fact that two documents can have the same topic or even mean the same, while they use different wording to describe the content. A sophisticated algorithm therefore will not directly operate on the texts but will have to find a more abstract representation that captures the texts' meaning. In this paper, we propose a novel approach for calculating the similarity of text documents. It builds on conceptual contexts that are derived from content and structure of the Wikipedia hypertext corpus.",,978-1-4244-3550-6,10.1109/ICDS.2009.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782896,document similarity;text similarity;semantic relatedness,Search engines;Information retrieval;Wikipedia;Crawlers;Humans;Clustering algorithms;Content based retrieval;Organizing;Advertising;Weight measurement,search engines;vocabulary,Wikipedia-based conceptual contexts;document similarity;search engine results;Wikipedia hypertext corpus,,4,,21,,13-Feb-09,,,IEEE,IEEE Conferences
Adapting the Secretary Hiring Problem for Optimal Hot-Cold Tier Placement Under Top-K Workloads,調整秘書招聘問題，以在Top-K工作負荷下優化熱冷層放置,B. Blamey; F. Wrede; J. Karlsson; A. Hellander; S. Toor,Uppsala University; Uppsala University; AstraZeneca; Uppsala University; Uppsala University,"2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",8-Jul-19,2019,,,576,583,"Top-K queries are an established heuristic in information retrieval. This paper presents an approach for optimal tiered storage allocation under stream processing workloads using this heuristic: those requiring the analysis of only the top-K ranked most relevant documents from a fixed-length stream, stream window, or batch job. Documents are ranked for relevance on a user-specified interestingness function, the top-K stored for further processing. This scenario bears similarity to the classic Secretary Hiring Problem (SHP), and the expected rate of document writes and document lifetime can be modelled as a function of document index. We present parameter-based algorithms for storage tier placement, minimizing document storage and transport costs. We derive expressions for optimal parameter values in terms of tier storage and transport costs a priori, without needing to monitor the application. This contrasts with (often complex) existing work on tiered storage optimization, which is either tightly coupled to specific use cases, or requires active monitoring of application IO load - ill-suited to long-running or one-off operations common in the scientific computing domain. We motivate and evaluate our model with a trace-driven simulation of human-in-the-loop bio-chemical model exploration, and two cloud storage case studies.",,978-1-7281-0912-1,10.1109/CCGRID.2019.00074,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752756,secretary hiring problem;hot cold storage;hybrid storage;tiered storage;interestingness function;top k queries;stream processing;HASTE,,document handling;query processing;storage allocation;storage management,document lifetime;document index;parameter-based algorithms;storage tier placement;transport costs;optimal parameter values;tiered storage optimization;cloud storage case studies;optimal hot-cold tier placement;top-K queries;information retrieval;optimal tiered storage allocation;fixed-length stream;stream window;batch job;user-specified interestingness function;expected rate;stream processing workloads;classic secretary hiring problem;human-in-the-loop biochemical model exploration;top-k workloads;heuristic;document storage minimization;application IO load active monitoring,,2,,32,,8-Jul-19,,,IEEE,IEEE Conferences
SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering,SpreadCluster：通過基於相似性的群集恢復版本化的電子表格,L. Xu; W. Dou; C. Gao; J. Wang; J. Wei; H. Zhong; T. Huang,"State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China",2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),3-Jul-17,2017,,,158,169,"Version information plays an important role in spreadsheet understanding, maintaining and quality improving. However, end users rarely use version control tools to document spreadsheets' version information. Thus, the spreadsheets' version information is missing, and different versions of a spreadsheet coexist as individual and similar spreadsheets. Existing approaches try to recover spreadsheet version information through clustering these similar spreadsheets based on spreadsheet filenames or related email conversation. However, the applicability and accuracy of existing clustering approaches are limited due to the necessary information (e.g., filenames and email conversation) is usually missing. We inspected the versioned spreadsheets in VEnron, which is extracted from the Enron Corporation. In VEnron, the different versions of a spreadsheet are clustered into an evolution group. We observed that the versioned spreadsheets in each evolution group exhibit certain common features (e.g., similar table headers and worksheet names). Based on this observation, we proposed an automatic clustering algorithm, SpreadCluster. SpreadCluster learns the criteria of features from the versioned spreadsheets in VEnron, and then automatically clusters spreadsheets with the similar features into the same evolution group. We applied SpreadCluster on all spreadsheets in the Enron corpus. The evaluation result shows that SpreadCluster could cluster spreadsheets with higher precision (78.5% vs. 59.8%) and recall rate (70.7% vs. 48.7%) than the filename-based approach used by VEnron. Based on the clustering result by SpreadCluster, we further created a new versioned spreadsheet corpus VEnron2, which is much bigger than VEnron (12,254 vs. 7,294 spreadsheets). We also applied SpreadCluster on the other two spreadsheet corpora FUSE and EUSES. The results show that SpreadCluster can cluster the versioned spreadsheets in these two corpora with high precision (91.0% and 79.8%).",,978-1-5386-1544-7,10.1109/MSR.2017.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962365,spreadsheet;evolution;clustering;version,Electronic mail;Feature extraction;Software;Tools;Layout;Fuses;Cloning,configuration management;pattern clustering;software maintenance;software quality;spreadsheet programs,spreadsheet understanding;spreadsheet maintaining;spreadsheet quality;spreadsheet version information;similarity-based clustering;spreadsheet filenames;email conversation;VEnron;Enron Corporation;automatic clustering algorithm;SpreadCluster;Enron corpus;spreadsheet corpora FUSE;spreadsheet corpora EUSES,,7,,43,,3-Jul-17,,,IEEE,IEEE Conferences
Khmer word segmentation based on Bi-directional Maximal Matching for Plaintext and Microsoft Word document,基於雙向最大匹配的純文本和Microsoft Word文檔高棉語分詞,N. Bi; N. Taing,"Royal University of Phnom Penh, Phnom Penh, Cambodia; Royal University of Phnom Penh, Phnom Penh, Cambodia","Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific",16-Feb-15,2014,,,1,9,"One of major key component in Khmer language processing is how to transform Khmer texts into series of separated Khmer words. But unlike in Latin languages such as English or French; Khmer language does not have any explicit word boundary delimiters such as blank space to separate between each word. Moreover, Khmer language has more complex structure to word form which causes Khmer Unicode standard ordering of character components to permit different orders that lead to the same visual representation; exactly looking word, but different character order. Even more, Khmer word could also be a join of two or more Khmer words together. All these complications address many challenges in Khmer word segmentation to determine word boundaries. Response to these challenges and try to improve level of accuracy and performance in Khmer word segmentation, this paper presents a study on Bidirectional Maximal Matching (BiMM) with Khmer Clusters, Khmer Unicode character order correction, corpus list optimization to reduce frequency of dictionary lookup and Khmer text manipulation tweaks. The study also focuses on how to implement Khmer word segmentation on both Khmer contents in Plaintext and Microsoft Word document. For Word document, the implementation is done on currently active Word document and also on file Word document. The study compares the implementation of Bi-directional Maximal Matching (BiMM) with Forward Maximal Matching (FMM) and Backward Maximal Matching (BMM) and also with similar algorithm from previous study. The result of study is 98.13% on accuracy with time spend of 2.581 seconds for Khmer contents of 1,110,809 characters which is about 160,000 of Khmer words.",,978-6-1636-1823-8,10.1109/APSIPA.2014.7041822,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041822,Bi-directional Maximal Matching;Forward Maximal Matching;Backward Maximal Matching;Word Segmentation;Khmer Unicode;Khmer Cluster;Khmer Unicode,Decision support systems;Bidirectional control;Accuracy;Abstracts;Transforms;Standards;Visualization,natural language processing;pattern matching;text analysis;word processing,Khmer word segmentation;bi-directional maximal matching;Microsoft word document;plaintext document;Khmer language processing;BiMM;Khmer clusters;Khmer Unicode character order correction;corpus list optimization;forward maximal matching;backward maximal matching;FMM;BMM,,2,,20,,16-Feb-15,,,IEEE,IEEE Conferences
Decipherment of Historical Manuscript Images,歷史手稿圖像的解密,X. Yin; N. Aldarrab; B. Megyesi; K. Knight,USC/ISI; USC/ISI; Uppsala University; DiDi Labs,2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,78,85,"European libraries and archives are filled with enciphered manuscripts from the early modern period. These include military and diplomatic correspondence, records of secret societies, private letters, and so on. Although they are enciphered with classical cryptographic algorithms, their contents are unavailable to working historians. We therefore attack the problem of automatically converting cipher manuscript images into plaintext. We develop unsupervised models for character segmentation, character-image clustering, and decipherment of cluster sequences. We experiment with both pipelined and joint models, and we give empirical results for multiple ciphers.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978177,decipherment;historical manuscripts;image segmentation;character recognition;unsupervised learning;zero shot learning,Ciphers;Image segmentation;Feature extraction;Gold;Channel models;Optical character recognition software;Decoding,cryptography;document image processing;history;image segmentation;natural language processing;pattern clustering,character segmentation;character-image clustering;cluster sequences;pipelined models;historical manuscript;enciphered manuscripts;secret societies;classical cryptographic algorithms;working historians;cipher manuscript images,,,,18,,3-Feb-20,,,IEEE,IEEE Conferences
A Combined Approach for Filter Feature Selection in Document Classification,文檔分類中過濾器特徵選擇的組合方法,H. N. Le Nguyen; Q. Ho Bao,"Sch. of Inf. Technol., VNUHCM - Univ. of Sci., Ho Chi Minh City, Vietnam; Sch. of Inf. Technol., VNUHCM - Univ. of Sci., Ho Chi Minh City, Vietnam",2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),7-Jan-16,2015,,,317,324,"For a large set of documents, bag-of-words vector can reach thousands of features. Document classification faces many difficulties in high dimensionality of bag-of-words vector. High dimensionality not only increases computation cost but also degrades the accuracy of classification process. The aim of filter feature selection is to remove irrelevant features by selecting a subset of the original feature set. In this paper, we analyze two filter feature selection approaches which are the frequency-based approach and the cluster-based approach. We propose a hybrid filter Feature Selection method for the combination of these approaches, named FCFS, in order to exploit their strong points. We experiment on FCFS and related filter feature selection methods as CMFS, OCFS, CIIC, IG, CHI with two datasets about news and medicine. Regarding Macro-F1, FCFS is superior to the other methods, while FCFS shows comparable and even better performance than the other methods in term of Micro-F1",1082-3409,978-1-5090-0163-7,10.1109/ICTAI.2015.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372152,bag-of-words vector space;filter feature selection;Document classification,Filtering algorithms;Training;Classification algorithms;Information filters;Iron;Filtering theory,document handling;feature selection;pattern classification;set theory,document classification;filter feature selection;bag-of-words vector;FCFS;CMFS;OCFS;CIIC;IG;CHI,,2,,24,,7-Jan-16,,,IEEE,IEEE Conferences
BitCube: a three-dimensional bitmap indexing for XML documents,BitCube：XML文檔的三維位圖索引,J. P. Yoon; V. Raghavan; V. Chakilam,"Centre for Adv. Comput. Studies, Louisiana Univ., Lafayette, LA, USA; NA; NA",Proceedings Thirteenth International Conference on Scientific and Statistical Database Management. SSDBM 2001,7-Aug-02,2001,,,158,167,"We describe a new bitmap indexing based technique to cluster XML documents. XML is a new standard for exchanging and representing information on the Internet. Documents can be hierarchically represented by XML-elements. XML documents are represented and indexed using a bitmap indexing technique. We define the similarity and popularity operations available in bitmap indexes and propose a method for partitioning a XML document set. Furthermore, a 2-dimensional bitmap index is extended to a 3-dimensional bitmap index, called BitCube. We define statistical measurements in the BitCube: mean, mode, standard derivation, and correlation coefficient. Based on these measurements, we also define the slice, project, and dice operations on a BitCube. BitCube can be manipulated efficiently and improves the performance of document retrieval.",1099-3371,0-7695-1218-6,10.1109/SSDM.2001.938548,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=938548,,Indexing;XML;Frequency;Image databases;Content based retrieval;Information retrieval;Object oriented databases;Relational databases;Venus;Internet,database indexing;hypermedia markup languages;Internet;query processing;visual databases,BitCube;three-dimensional bitmap indexing;XML documents;document clustering;Internet;similarity operations;popularity operations;document partitioning;statistical measurements;document retrieval,,8,,13,,7-Aug-02,,,IEEE,IEEE Conferences
Semantics Based Multi-XML Query Algorithm,基於語義的多XML查詢算法,B. Zhang; F. Ye,"Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai; Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai",2008 International Conference on Computer Science and Software Engineering,22-Dec-08,2008,4,,708,712,"Recently, with the rapid spread of computer technology, XML has become the standard for data exchange for a wide variety of applications. A large number of XML and XML schema are generated by different individual or company. The problem addressed in this paper is the execution of XML queries over multiple XML documents. In this paper, we propose an algorithm: SMXQA (semantics based multi-XML query algorithm). In the SMXQA, we parse the DTD/XSD of multi-XML documents to collect the set of functional dependency in it. Then we use method of clustering and pattern matching based on semantic to catch the element with homologous semantic in the set of functional dependency. And we concentrate on make a global materialized view by that element. For query conveniently, we propose a multi-XML query language: MXquery, which is extended from Xquery. At last, we experimentally evaluate the effectiveness of the proposed algorithm.",,978-0-7695-3336-0,10.1109/CSSE.2008.385,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722717,XML;DTD/XSD;MXquery;SMXQA,XML;Data engineering;Clustering algorithms;Database languages;Law;Legal factors;Switches;Computer science;Software engineering;Software algorithms,electronic data interchange;pattern clustering;pattern matching;programming language semantics;XML,semantics based multi-XML query algorithm;data exchange;XML schema;eXtensible Markup Language;multi-XML documents;pattern clustering;pattern matching;functional dependency;MXquery,,,,9,,22-Dec-08,,,IEEE,IEEE Conferences
Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection,信息檢索中神經文檔嵌入的視覺探索：語義和特徵選擇,X. Ji; H. -W. Shen; A. Ritter; R. Machiraju; P. -Y. Yen,"School of Medicine, Washington University, St. Louis, MO, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; School of Medicine, Institute for Informatics, Washington University, St. Louis, MO, USA",IEEE Transactions on Visualization and Computer Graphics,30-Apr-19,2019,25,6,2181,2192,"Neural embeddings are widely used in language modeling and feature generation with superior computational power. Particularly, neural document embedding - converting texts of variable-length to semantic vector representations - has shown to benefit widespread downstream applications, e.g., information retrieval (IR). However, the black-box nature makes it difficult to understand how the semantics are encoded and employed. We propose visual exploration of neural document embedding to gain insights into the underlying embedding space, and promote the utilization in prevalent IR applications. In this study, we take an IR application-driven view, which is further motivated by biomedical IR in healthcare decision-making, and collaborate with domain experts to design and develop a visual analytics system. This system visualizes neural document embeddings as a configurable document map and enables guidance and reasoning; facilitates to explore the neural embedding space and identify salient neural dimensions (semantic features) per task and domain interest; and supports advisable feature selection (semantic analysis) along with instant visual feedback to promote IR performance. We demonstrate the usefulness and effectiveness of this system and present inspiring findings in use cases. This work will help designers/developers of downstream applications gain insights and confidence in neural document embedding, and exploit that to achieve more favorable performance in application domains.",1941-0506,,10.1109/TVCG.2019.2903946,Agency for Healthcare Research and Quality; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667702,Neural document embedding;information retrieval;semantic analysis;feature selection,Semantics;Analytical models;Visual analytics;Medical services;Task analysis;Feature extraction,data analysis;data visualisation;health care;information retrieval;neural nets;text analysis,visual analytics system;healthcare decision-making;biomedical IR;IR application-driven view;semantic vector representations;text conversion;feature selection;salient neural dimensions;neural embedding space;information retrieval;neural document embedding;visual exploration,Cluster Analysis;Humans;Information Storage and Retrieval;Machine Learning;Natural Language Processing;Semantics,6,,39,,15-Mar-19,,,IEEE,IEEE Journals
Global caching mechanisms in clusters of Web servers,Web服務器群集中的全局緩存機制,M. E. Poleggi; B. Ciciani,"Dipt. di Informatica, Sistemi e Produzione, Univ. di Roma, Italy; NA","Proceedings. 10th IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunications Systems",22-Jan-03,2002,,,421,426,"We investigate how to improve Web cluster performance through cooperation protocols for a shared use of the main memories of the server nodes (global caching). Many popular schemes to locate requested documents are based upon locality information sharing (namely, informed cooperation); we also propose a novel approach for document lookup through a query broadcasting mechanism (namely, on-demand cooperation). Moreover, we consider the migration and handoff mechanisms to retrieve successfully located documents. The experimental results show that, in a static Web scenario, the on-demand with handoff scheme performs similarly to the informed with handoff scheme, whereas the on-demand with migration scheme outperforms the informed with migration scheme. On the other hand, in a scenario with some dynamic requests, on-demand cooperation experiences a significant performance degradation with respect to informed cooperation.",1526-7539,0-7695-1840-0,10.1109/MASCOT.2002.1167103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167103,,Web server;Broadcasting;Protocols;Degradation;Costs;Resource management;Local area networks;Internet;Memory;Information retrieval,Internet;file servers;protocols;document handling;query processing;cache storage,global caching;Web server clusters;cooperation protocols;locality information sharing;informed cooperation;document lookup;query broadcasting mechanism;on-demand cooperation;migration mechanism;handoff mechanism;document retrieval;dynamic requests,,1,,12,,22-Jan-03,,,IEEE,IEEE Conferences
Dimensionality reduction by random mapping: fast similarity computation for clustering,通過隨機映射減少維數：用於聚類的快速相似度計算,S. Kaski,"Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland",1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227),6-Aug-02,1998,1,,413,418 vol.1,"When the data vectors are high-dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space. It is therefore necessary to reduce the dimensionality before, for example, clustering the data. If the dimensionality is very high, like in the WEBSOM method which organizes textual document collections on a self-organizing map, then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly. It is demonstrated that the document classification accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the final dimensionality is sufficiently large (about 100 out of 6000). In fact, it can be shown that the inner product (similarity) between the mapped vectors follows closely the inner product of the original vectors.",1098-7576,0-7803-4859-1,10.1109/IJCNN.1998.682302,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=682302,,Vectors;Multidimensional systems;Space technology;Feature extraction;Computer networks;Neural networks;Data analysis;Pattern recognition;Clustering algorithms;Organizing,document image processing;self-organising feature maps;pattern matching;data analysis,dimensionality reduction;random mapping method;similarity computation;clustering;data analysis;pattern recognition;WEBSOM method;data vectors;self-organizing map;document image processing,,122,,11,,6-Aug-02,,,IEEE,IEEE Conferences
On the Applicability of Speaker Diarization to Audio Concept Detection for Multimedia Retrieval,說話人差異化在多媒體檢索音頻概念檢測中的適用性,R. Mertens; P. Huang; L. Gottlieb; G. Friedland; A. Divakaran,"Int. Comput. Sci. Inst., Berkeley, CA, USA; ECE Dept., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Int. Comput. Sci. Inst., Berkeley, CA, USA; Int. Comput. Sci. Inst., Berkeley, CA, USA; SRI Int. Sarnoff, Princeton, NJ, USA",2011 IEEE International Symposium on Multimedia,9-Jan-12,2011,,,446,451,"Recently, audio concepts emerged as a useful building block in multimodal video retrieval systems. Information like ""this file contains laughter"", ""this file contains engine sounds"" or ""this file contains slow music"" can significantly improve purely visual based retrieval. The weak point of current approaches to audio concept detection is that they heavily rely on human annotators. In most approaches, audio material is manually inspected to identify relevant concepts. Then instances that contain examples of relevant concepts are selected -- again manually -- and used to train concept detectors. This approach comes with two major disadvantages: (1) it leads to rather abstract audio concepts that hardly cover the audio domain at hand and (2) the way human annotators identify audio concepts likely differs from the way a computer algorithm clusters audio data -- introducing additional noise in training data. This paper explores whether unsupervized audio segementation systems can be used to identify useful audio concepts by analyzing training data automatically and whether these audio concepts can be used for multimedia document classification and retrieval. A modified version of the ICSI (International Computer Science Institute) speaker diarization system finds segments in an audio track that have similar perceptual properties and groups these segments. This article provides an in-depth analysis on the statistic properties of similar acoustic segments identified by the diarization system in a predefined document set and the theoretical fitness of this approach to discern one document class from another.",,978-1-4577-2015-4,10.1109/ISM.2011.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123387,Audio Clustering;Audio Indexing;Speaker Diarization;Video Indexing,Acoustics;Humans;Engines;Mathematical model;Multimedia communication;Clustering algorithms;Streaming media,audio signal processing;image classification;multimedia systems;pattern clustering;speaker recognition;video retrieval,speaker diarization;audio concept detection;multimedia retrieval;multimodal video retrieval system;visual based retrieval;human annotator;audio material;abstract audio concept;computer algorithm;audio data cluster;unsupervised audio segmentation system;multimedia document classification;multimedia document retrieval;ICSI;International Computer Science Institute,,,,10,,9-Jan-12,,,IEEE,IEEE Conferences
Comparison of Gradient Descent Methods in Online Fuzzy Co-clustering,在線模糊共聚中梯度下降法的比較,K. Kida; S. Ubukata; A. Notsu; K. Honda,"Graduate School of Engineering, Osaka Prefecture University,Sakai,Osaka,Japan,599-8531; Graduate School of Engineering, Osaka Prefecture University,Sakai,Osaka,Japan,599-8531; Graduate School of Humanities and Sustainable System Sciences, Osaka Prefecture University,Sakai,Osaka,Japan,599-8531; Graduate School of Engineering, Osaka Prefecture University,Sakai,Osaka,Japan,599-8531",2019 International Conference on Fuzzy Theory and Its Applications (iFUZZY),16-Apr-20,2019,,,9,14,"Fuzzy co-clustering schemes including Fuzzy Co-Clustering induced by Multinomial Mixture models (FCCMM) are promising approaches for analyzing object-item cooccur-renee information such as document-keyword frequencies and customer-product purchase history transactions. However, such cooccurrence datasets are generally maintained as very large matrices and cannot be dealt with conventional batch algorithms. In order to deal with such problems, online FCCMM (OFC-CMM) that sequentially loads a single object has been proposed. Conventional OFCCMM uses stochastic gradient descent (SGD) to update parameters. SGD generally has drawbacks that convergence is slow and it is susceptible to vibration state and a saddle point. Many improvements on SGD have been proposed such as Momentum SGD, Nesterov's accelerated gradient method, AdaGrad, and Adam. In this study, we introduce various gradient descent methods into OFCCMM and observe their characteristics and performance through numerical experiments.",2377-5831,978-1-7281-0840-7,10.1109/iFUZZY46984.2019.9066194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066194,Fuzzy co-clustering;FCCMM;Online algorithm;gradient descent method,Clustering algorithms;Acceleration;Convergence;Linear programming;Gradient methods;Mixture models;Vibrations,fuzzy set theory;Gaussian processes;gradient methods;mixture models;pattern clustering;stochastic processes,fuzzy coclustering schemes;online fuzzy coclustering;Nesterov's accelerated gradient method;momentum SGD;stochastic gradient descent;conventional OFCCMM;OFC-CMM;online FCCMM;conventional batch algorithms;cooccurrence datasets;customer-product purchase history transactions;document-keyword frequencies;object-item cooccurrence information;multinomial mixture models;gradient descent methods,,,,11,,16-Apr-20,,,IEEE,IEEE Conferences
A Concept-Based ILP Approach for Multi-document Summarization Exploring Centrality and Position,基於概念的ILP方法用於多文檔摘要探索中心和位置,H. Oliveira; R. D. Lins; R. Lima; F. Freitas; S. J. Simske,"University Center of Joao Pessoa, Joao Pessoa, Brazil; Inf. Center, UFPE, Recife, Brazil; Inf. Center, UFPE, Recife, Brazil; Inf. Center, UFPE, Recife, Brazil; Colorado State Univ., Fort Collins, CO, USA",2018 7th Brazilian Conference on Intelligent Systems (BRACIS),16-Dec-18,2018,,,37,42,"Multi-document summarization systems aim to generate a brief text containing the most relevant information from a collection of related documents. The fast and continually growing volume of text data has increasingly drawn the attention from users and researchers to such systems. Aspects such as sentence centrality and position have been extensively studied in multi-document summarization as indicators of content relevancy. Very few works have investigated their efficient integration using global-based optimization approaches, however. This paper proposes a concept-based integer linear programming approach for multi-document summarization of news articles that integrates centrality and position features to filter out the less relevant sentences and measure the importance of concepts (textual fragments) in composing the output summary. The presented approach relies on a centrality-based strategy to perform the sentence clustering process and also to support the sentence ordering step. The benchmarks conducted with four datasets of the Document Understanding Conferences from 2001 to 2004 demonstrate that the proposed approach presents competitive performance compared with other state-of-the-art methods.",,978-1-5386-8023-0,10.1109/BRACIS.2018.00015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575586,"Multi-document summarization, integer linear programming, centrality and position features",Task analysis;Frequency measurement;Optimization;Integer linear programming;Redundancy;Weight measurement;Position measurement,integer programming;pattern clustering;text analysis,global-based optimization approaches;concept-based integer linear programming approach;centrality-based strategy;concept-based ILP approach;multidocument summarization exploring centrality;text data;sentence centrality;sentence position;sentence clustering process,,,,16,,16-Dec-18,,,IEEE,IEEE Conferences
Application of Self-Organizing Map in Aerosol Single Particles Data Clustering,自組織圖在氣溶膠單顆粒數據聚類中的應用,G. Wen; X. Guo; D. Huang; K. Liu,"Intelligent Computing Lab, Institute of Intelligent Machines, Chinese Academy of Sciences, China; Department of Automation, University of Science and Technology of China, China.; Anhui Institute of Optics and Fine Mechanics, Chinese Academy of Sciences, China; Intelligent Computing Lab, Institute of Intelligent Machines, Chinese Academy of Sciences, China. phone: 086-0551-5591108; fax: 086-0551-5592751; dshuang@iim.ac.cn; Intelligent Computing Lab, Institute of Intelligent Machines, Chinese Academy of Sciences, China.",2007 International Joint Conference on Neural Networks,29-Oct-07,2007,,,991,996,"In this paper, self-organizing map (SOM) is used to visualize and cluster the data set of aerosol single particle mass spectrum, which was collected by aerosol time-of-flight mass spectrometry (ATOFMS). In view of the characteristic feature of aerosol particle data, the TF-IDF scheme used widely in document clustering is employed to preprocess. Subsequently for data clustering analysis, a two-level clustering framework is proposed, wherein SOM is firstly used to cluster input data and get the primary results, and then the results are again clustered by semiautomatic k-means algorithm. In order to demonstrate the validity of clustering, the chemical significance for cluster centroid is also investigated, wherein inorganic salts, ""calcium-containing"" particles, biogenic soot particles, and carbonaceous particles etc. are identified.",2161-4407,978-1-4244-1379-9,10.1109/IJCNN.2007.4371093,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4371093,,Aerosols;Machine intelligence;Neural networks;Data visualization;Data analysis;Humans;Content addressable storage;Chemical analysis;Manuals;Mass spectroscopy,aerosols;data handling;geophysics computing;mass spectroscopy;pattern clustering;self-organising feature maps,self-organizing map;aerosol single particles data clustering;aerosol single particle mass spectrum;aerosol time-of-flight mass spectrometry;document clustering;semiautomatic k-means algorithm,,1,,19,,29-Oct-07,,,IEEE,IEEE Conferences
Assignment of term descriptors to clusters,將術語描述符分配給群集,S. K. Bhatia; J. S. Deogun; V. V. Raghavan,"Dept. of Comput. Sci., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci., Nebraska Univ., Lincoln, NE, USA; NA",Proceedings of the 1990 Symposium on Applied Computing,6-Aug-02,1990,,,181,185,"An approach to cluster characterization is developed. The clusters are constructed by using one of the user-oriented techniques. If the clusters have been characterized on the basis of user preferences, the representations of documents contained within the clusters need not be scanned to determine the relevance of a cluster of documents to a query. Since the cluster representations are decided by the users, this approach results in a higher precision and recall in document retrieval. A program to interview a user automatically to elicit the constructs and the repertory grid has been implemented.<>",,0-8186-2031-5,10.1109/SOAC.1990.82165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=82165,,Feedback;Animation,indexing;information retrieval;knowledge acquisition,user interview program;term descriptors;cluster characterization;user-oriented techniques;user preferences;document retrieval;repertory grid,,1,,12,,6-Aug-02,,,IEEE,IEEE Conferences
Building Candidate Monolingual Parallel Corpus from Scientific Papers,通過科學論文構建候選單語平行語料庫,R. Ilyas; D. H. Widiyantoro; M. L. Khodra,"Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia; Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia; Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia",2018 International Conference on Asian Language Processing (IALP),31-Jan-19,2018,,,230,233,"Paraphrase extraction is the method of collecting words, phrases or sentences that have same meaning from free text or structured documents. Candidate monolingual parallel corpus was build from scientific papers by clustering techniques. The corpus contain couples of citation sentences that mention same index. The corpus evaluated using weighting techniques and proximity measurements based on the density of each sentence cluster. The results of this research are expected to simplify paraphrase labeling by language and computer science experts.",,978-1-7281-1175-9,10.1109/IALP.2018.8629246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629246,Corpus;Paraphrase;Scientific Paper,Semantics;Clustering algorithms;Euclidean distance;Task analysis;Filtering;Buildings;Electrical engineering,information retrieval;natural language processing;pattern clustering;statistical analysis;text analysis,sentence cluster;paraphrase labeling;building candidate;scientific papers;paraphrase extraction;free text;structured documents;candidate monolingual parallel corpus;citation sentences;weighting techniques,,3,,9,,31-Jan-19,,,IEEE,IEEE Conferences
Text Clustering by 2D Cellular Automata Based on the N-Grams,基於N-Grams的二維細胞自動機的文本聚類,R. M. Hamou; A. Lehireche; A. C. Lokbani; M. Rahmani,"EEDIS Lab., Evolutionary Eng. & Distrib. Inf. Syst. Lab., Univ. Dr Tahar MOULAY of Saida, Saida, Algeria; Evolutionary Eng. & Distrib. Inf. Syst. Lab., EEDIS Lab., Univ. Djillali Liabes of Sidi Bel Abbes, Sidi Bel Abbes, Algeria; EEDIS Lab., Evolutionary Eng. & Distrib. Inf. Syst. Lab., Univ. Dr Tahar MOULAY of Saida, Saida, Algeria; EEDIS Lab., Evolutionary Eng. & Distrib. Inf. Syst. Lab., Univ. Dr Tahar MOULAY of Saida, Saida, Algeria","2010 First ACIS International Symposium on Cryptography, and Network Security, Data Mining and Knowledge Discovery, E-Commerce and Its Applications, and Embedded Systems",29-Apr-11,2010,,,271,277,"In this article we present a 2D cellular automaton (Class_AC) to solve a problem of text mining in the case of unsupervised classification (clustering). Before to experiment the cellular automaton, we vectorized our data indexing textual documents from the database REUTERS 21,578 by the approach of N-grams. The cellular automaton that we propose in this paper is a grid cell structure with a flat neighborhood arising from this structure (planar). Three functions of transitions were used to vary the automaton with four states for each cell. The results obtained show that the virtual machine parallel computing (Class_AC) effectively includes similar documents on near threshold. Section 1 gives an introduction, Section 2 presents representation of texts based on the n grams, Section 3 describes the cellular automaton for clustering, Section 4 shows the experimentation and comparison results and finally Section 5 gives a conclusion and perspectives.",,978-1-4244-9595-5,10.1109/CDEE.2010.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759331,Data classification;Cellular Automata;biomimetic methods;data mining;clustering and segmentation;unsupervised classification,Automata;Classification algorithms;Text mining;Support vector machine classification;Laboratories;Entropy;Biological system modeling,cellular automata;indexing;parallel processing;pattern clustering;text analysis,text clustering;2D cellular automata;N-grams;unsupervised classification;data indexing;textual documents;virtual machine parallel computing,,8,,19,,29-Apr-11,,,IEEE,IEEE Conferences
Print-Based Digital Content Interfaces for Social Reading Activities,用於社交閱讀活動的基於打印的數字內容接口,D. Turk; J. Brine; K. Kanev,"University of Aizu, Japan; Aizu Univ., Aizu-Wakamatsu; Aizu Univ., Aizu-Wakamatsu",Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06),24-Jul-06,2006,,,832,834,"This paper investigates the application of digitally-enhanced print-based interfaces to support a combined individual and social jigsaw reading activity. The project team aimed to design and develop an activity that required students to read and process complex authentic technical texts. Informed by student feedback, this research project evaluated the benefits of enhancing the activity with a patented Cluster Pattern Interface (CLUSPIreg) (Kamen and Shigeo) to facilitate the students' reading comprehension and improve their multiliteracy skills. CLUSPIreg is a cluster pattern-based encoding scheme that allows digitally encoded information to be embedded in a layer of clustered graphical objects on printed documents. CLUSPIreg can expand the application of multimedia by linking paper-based materials to language learning tools through a point-and-click interface",2161-377X,0-7695-2632-2,10.1109/ICALT.2006.1652570,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1652570,jigsaw method;reading;digitally-enhanced;paper;cluster pattern encoding;group work.,Feedback;Encoding;Assembly;Joining processes;Problem-solving;Testing;Computer science;Local area networks;Switches;Education,computer aided instruction;document handling;graphical user interfaces;linguistics;multimedia systems,print digital content interface;social jigsaw reading activity;complex authentic technical text;student feedback;cluster pattern interface;CLUSPI;student reading comprehension;multiliteracy skill;cluster pattern encoding;digitally encoded information;clustered graphical object;language learning;point-and-click interface,,,,7,,24-Jul-06,,,IEEE,IEEE Conferences
Vesselness for text detection in historical document images,歷史文件圖像中文本檢測的容器,S. Hofmann; M. Gropp; D. Bernecker; C. Pollin; A. Maier; V. Christlein,"Friedrich-Alexander-Universit瓣t Erlangen-N羹rnberg, Pattern Recognition Lab, Martensstra?e 3, 91058 Erlangen, Germany; Friedrich-Alexander-Universit瓣t Erlangen-N羹rnberg, Pattern Recognition Lab, Martensstra?e 3, 91058 Erlangen, Germany; Friedrich-Alexander-Universit瓣t Erlangen-N羹rnberg, Pattern Recognition Lab, Martensstra?e 3, 91058 Erlangen, Germany; University of Graz, Centre for Information Modelling, Elisabethstra?e 59/II, 8010 Graz, Austria; Friedrich-Alexander-Universit瓣t Erlangen-N羹rnberg, Pattern Recognition Lab, Martensstra?e 3, 91058 Erlangen, Germany; Friedrich-Alexander-Universit瓣t Erlangen-N羹rnberg, Pattern Recognition Lab, Martensstra?e 3, 91058 Erlangen, Germany",2016 IEEE International Conference on Image Processing (ICIP),19-Aug-16,2016,,,3259,3263,"Text detection is typically the first step for any text processing such as hand-written text recognition, layout analysis, line detection, or writer identification. This paper describes a new method to detect text in images, particularly in historical document images. For a robust detection, we propose the use of the vesselness filter as a new preprocessing step for text detection. We show, that this step improves the detection rate significantly. At the locations segmented by this filter, SIFT keypoints are detected which are spatially clustered. Overlapping windows from these clusters are subsequently VLAD encoded and classified in text and non-text. We evaluate this approach on a newly created database, where we achieve an F1-score of 92%. Additionally, we demonstrate the effectiveness of this method for line segmentation.",2381-8549,978-1-4673-9961-6,10.1109/ICIP.2016.7532962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532962,text detection;vesselness;historical document analysis;RootSIFT;VLAD,Image segmentation;Pipelines;Layout;Feature extraction;Visualization;Encoding;Text recognition,document image processing;history;image filtering;text detection,text detection;historical document images;text processing;vesselness filter;detection rate improvement;SIFT keypoints detection;overlapping windows;VLAD;F1-score;line segmentation,,,,26,,19-Aug-16,,,IEEE,IEEE Conferences
ClusTex: Information Extraction from HTML Pages,ClusTex：從HTML頁面提取信息,F. Ashraf; R. Alhajj,"University of Calgary, Canada; University of Calgary, Canada; Global University, Lebanon",21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07),27-Aug-07,2007,1,,355,360,"This paper propose ClusTex, a system which employs clustering techniques for automatic information extraction from HTML documents containing semi- structured data. Using domain-specific information provided by the user, ClusTex parses and tokenizes the data from an HTML document, partitions it into clusters containing similar elements, and estimates an extraction rule based on the pattern of occurrence of data tokens. The extraction rule is then used to refine clusters, and finally the output is reported. To demonstrate the effectiveness of this approach, the proposed approach is tested by conducting experiments on the University of Calgary Web-site; the results prove comparable to those reported in the literature.",,978-0-7695-2847-2,10.1109/AINAW.2007.119,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4221085,information extraction;clustering;web pages;HTML documents.,Data mining;HTML;Computer science;Testing;Web pages;Clustering algorithms;Markup languages;Automation;Conferences;Societies,grammars;hypermedia markup languages;information retrieval;Internet;pattern clustering;text analysis,ClusTex;automatic information extraction;HTML Web document;semi structured data;domain-specific information;data parsing;data token;Web page,,2,,6,,27-Aug-07,,,IEEE,IEEE Conferences
It's all about habits: Exploiting multi-task clustering for activities of daily living analysis,一切都與習慣有關：利用多任務聚類進行日常生活分析活動,Y. Yan; E. Ricci; N. Rostamzadeh; N. Sebe,"Department of Information Engineering and Computer Science, University of Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy",2014 IEEE International Conference on Image Processing (ICIP),29-Jan-15,2014,,,1071,1075,"Motivated by applications in areas such as patient monitoring, tele-rehabilitation and ambient assisted living, analyzing activities of daily living is an active research topic in computer vision and image processing. In this paper we address the problem of everyday activity recognition from unlabeled data proposing a novel multi-task clustering (MTC) approach. Our intuition is that, when analyzing activities of daily living, we can take advantage of the fact that people tend to perform the same actions in the same environment (e.g. people working in an office environment use to read and write documents). Thus, even if labels are not available, information about typical activities can be exploited in the learning process. Arguing that the tasks of recognizing activities of specific individuals are related, we resort on multi-task learning and rather than clustering the data of each individual separately, we also look for clustering results which are coherent among related tasks. Extensive experimental results show that our method outperforms several state-of-the-art approaches by up to 11% on the Rochester activities of daily living dataset.",2381-8549,978-1-4799-5751-4,10.1109/ICIP.2014.7025213,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025213,Multi-Task Clustering;Activities of Daily Living Analysis,Kernel;Clustering algorithms;Feature extraction;Algorithm design and analysis;Videos;Optimization;Linear programming,computer vision;feature extraction;image recognition;learning (artificial intelligence);pattern clustering,daily living analysis;patient monitoring;telerehabilitation;ambient assisted living;computer vision;image processing;everyday activity recognition;multitask clustering approach;MTC approach;Rochester activity,,3,,18,,29-Jan-15,,,IEEE,IEEE Conferences
Automatic clustering of part-of-speech for vocabulary divided PLSA language model,詞彙劃分的PLSA語言模型的詞性自動聚類,M. Suzuki; N. Kuriyama; A. Ito; S. Makino,"The University of Tokushima, JAPAN; Grad. School of Engineering, Tohoku University, Sendai, JAPAN; Grad. School of Engineering, Tohoku University, Sendai, JAPAN; Grad. School of Engineering, Tohoku University, Sendai, JAPAN",2008 International Conference on Natural Language Processing and Knowledge Engineering,2-May-09,2008,,,1,7,"PLSA is one of the most powerful language models for adaptation to a target speech. The vocabulary divided PLSA language model (VD-PLSA) shows higher performance than the conventional PLSA model because it can be adapted to the target topic and the target speaking style individually. However, all of the vocabulary must be manually divided into three categories (topic, speaking style, and general category). In this paper, an automatic method for clustering parts-of-speech (POS) is proposed for VD-PLSA. Several corpora with different styles are prepared, and the distance between corpora in terms of POS is calculated. The ""general tendency score"" and ""style tendency score"" for each POS are calculated based on the distance between corpora. All of the POS are divided into three categories using two scores and appropriate thresholds. Experimental results showed the proposed method formed appropriate clusters, and VD-PLSA with acquired categories gave the highest performance of all other models. We applied the VD-PLSA into large vocabulary continuous speech recognition system. VD-PLSA improved the recognition accuracy for documents with lower out-of-vocabulary ratio, while other documents were not improved or slightly descended the accuracy.",,978-1-4244-4515-8,10.1109/NLPKE.2008.4906747,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906747,Vocabulary divided PLSA;general/style tendency score;part-of-speech;language model;speech recognition,Vocabulary;Natural languages;Clustering algorithms;Speech recognition;Training data;Adaptation model;Indium tin oxide;Predictive models;Probability;Statistics,natural language processing;pattern clustering;speech processing;speech recognition,automatic clustering;part-of-speech;vocabulary divided PLSA language model;powerful language model;target speech;target speaking;speaking style;general tendency score;style tendency score;vocabulary continuous speech recognition system;out-of-vocabulary ratio;documents,,1,,10,,2-May-09,,,IEEE,IEEE Conferences
On semantic evaluation of text clustering algorithms,文本聚類算法的語義評估,S. H. Nguyen; W. ?wieboda; H. S. Nguyen,"Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Warsaw Poland; Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Warsaw Poland; Institute of Mathematics, The University of Warsaw, Banacha 2, 02-097, Warsaw Poland",2014 IEEE International Conference on Granular Computing (GrC),15-Dec-14,2014,,,224,229,"In this paper, we investigate the problem of quality analysis of clustering results using semantic annotations given by experts. In previous work we proposed a novel approach to construction of evaluation measure, called SEE (Semantic Evaluation by Exploration), which is an extension of the existing methods such as Rand Index or Normalized Mutual Information. In this paper we present some further extensions as well as some theoretical properties of the of the proposed measure. We illustrate the proposed evaluation method on documents in INFONA document retrieval system. We compare different search result clustering algorithms using the proposed measure.",,978-1-4799-5464-3,10.1109/GRC.2014.6982839,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982839,,Clustering algorithms;Semantics;Decision trees;Mutual information;Indexes;Approximation algorithms,information retrieval;pattern clustering;text analysis,text clustering algorithms;quality analysis;semantic annotations;SEE;semantic evaluation by exploration;INFONA document retrieval system,,1,,16,,15-Dec-14,,,IEEE,IEEE Conferences
Density-based spatial clustering of application with noise algorithm for the classification of solar radiation time series,基於密度的空間聚類與噪聲算法在太陽輻射時間序列分類中的應用,B. Khalil; C. Ali,"Department of electronic, Faculty of technology, Amar Telidji University. LSMF Laboratory, Laghouat, Algeria; Department of electronic, Faculty of technology, Amar Telidji University. LSMF Laboratory, Laghouat, Algeria","2016 8th International Conference on Modelling, Identification and Control (ICMIC)",5-Jan-17,2016,,,279,283,"The study of the dynamic behaviour of the solar radiation is a very important task for PV system efficiency. Hence, we propose in this paper, a time series data mining method to detect the underlying dynamic presents in hourly solar radiation time series. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is used to cluster the solar radiation time series and detect noisy data. Moreover, the proposed method is compared with two unsupervised clustering techniques, k-means and Fuzzy c-means, for the analysis of the measured hourly solar radiation time series. All the algorithms are focused on extracting useful information from the data with the aim of model the time series behaviour and find patterns to be used in PV system applications. This electronic document is a ?live??template and already defines the components of your paper [title, text, heads, etc.] in its style sheet.",,978-0-9567157-7-7,10.1109/ICMIC.2016.7804123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804123,Phase space reconstruction;DBSCAN;k-means;fuzzy c-means;solar radiation;clustering,Clustering algorithms;Time series analysis;Solar radiation;Classification algorithms;Databases;Data mining;Mutual information,data mining;feature extraction;pattern classification;pattern clustering;photovoltaic power systems;power engineering computing;solar power;statistical analysis;time series,density-based spatial clustering of application with noise algorithm;DBSCAN algorithm;solar radiation time series classification;PV system efficiency;photovoltaic system;time series data mining;information extraction,,3,,10,,5-Jan-17,,,IEEE,IEEE Conferences
Exploiting side information in distance dependent Chinese restaurant processes for data clustering,在距離相關的中餐廳流程中利用輔助信息進行數據聚類,Cheng Li; Dinh Phung; S. Rana; S. Venkatesh,"Centre for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Geelong, Australia; Centre for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Geelong, Australia; Centre for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Geelong, Australia; Centre for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Geelong, Australia",2013 IEEE International Conference on Multimedia and Expo (ICME),26-Sep-13,2013,,,1,6,"Multimedia contents often possess weakly annotated data such as tags, links and interactions. The weakly annotated data is called side information. It is the auxiliary information of data and provides hints for exploring the link structure of data. Most clustering algorithms utilize pure data for clustering. A model that combines pure data and side information, such as images and tags, documents and keywords, can perform better at understanding the underlying structure of data. We demonstrate how to incorporate different types of side information into a recently proposed Bayesian nonparametric model, the distance dependent Chinese restaurant process (DD-CRP). Our algorithm embeds the affinity of this information into the decay function of the DD-CRP when side information is in the form of subsets of discrete labels. It is flexible to measure distance based on arbitrary side information instead of only the spatial layout or time stamp of observations. At the same time, for noisy and incomplete side information, we set the decay function so that the DD-CRP reduces to the traditional Chinese restaurant process, thus not inducing side effects of noisy and incomplete side information. Experimental evaluations on two real-world datasets NUS WIDE and 20 Newsgroups show exploiting side information in DD-CRP significantly improves the clustering performance.",1945-788X,978-1-4799-0015-2,10.1109/ICME.2013.6607475,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607475,Side information;multimedia;annotated data;distance dependent Chinese restaurant processes;clustering,Clustering algorithms;Noise measurement;Mutual information;Bayes methods;Multimedia communication;Time measurement;Layout,Bayes methods;multimedia systems;nonparametric statistics;pattern clustering,side information;weakly annotated data;auxiliary information;data clustering algorithms;link data structure;Bayesian nonparametric model;distance dependent Chinese restaurant process;DD-CRP decay function;discrete label subsets;real-world datasets;NUS_WIDE;multimedia contents,,1,,11,,26-Sep-13,,,IEEE,IEEE Conferences
Hyperspherical possibilistic fuzzy c-means for high-dimensional data clustering,用於高維數據聚類的超球可能模糊c均值,Y. Yan; L. Chen,"School of Electric and Electronic Engineering, Nanyang Technological University, Singapore, 639798; School of Electric and Electronic Engineering, Nanyang Technological University, Singapore, 639798","2009 7th International Conference on Information, Communications and Signal Processing (ICICS)",22-Jan-10,2009,,,1,5,"A possibilistic fuzzy c-means (PFCM)[1] has been proposed for clustering unlabeled data. It is a hybridization of possibilistic c-means (PCM) and fuzzy c-means (FCM), therefore it has been shown that PFCM is able to solve the noise sensitivity issue in FCM, and at the same time it helps to avoid coincident clusters problem in PCM with some numerical examples in low-dimensional data sets. In this paper, we conduct further evaluation of PFCM for high-dimensional data and proposed a revised version of PFCM called Hyperspherical PFCM (HPFCM). Modifications have been made in the original PFCM objective function, so that cosine similarity measure could be incorporated in the approach. We apply both the original and revised approaches on six large benchmark data sets, and compare their performance with some of the traditional and recent clustering algorithms for automatic document categorization. Our analytical as well as experimental study show HPFCM is promising for handling complex high dimensional data sets and achieves more stable performance. On the other hand, the remaining problem of PFCM approach is also discussed.",,978-1-4244-4656-8,10.1109/ICICS.2009.5397538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5397538,cosine similairity;high dimensional;possibilisitic fuzzy clustering,Clustering algorithms;Phase change materials;Robust stability;Data engineering;Fuzzy sets;Performance analysis;Robustness;Costs;Fuzzy systems;Web pages,data handling;fuzzy set theory;pattern clustering;probability,possibilistic fuzzy c-means;data clustering;noise sensitivity;possibilistic c-means;fuzzy c-means;high dimensional data;hyperspherical PFCM;cosine similarity,,2,,14,,22-Jan-10,,,IEEE,IEEE Conferences
Experience Co-Creation on Ubiquitous Cultural e-Service Provision: A Case of Taiwan's Hakka Culture,體驗無處不在的文化電子服務提供的共創性-以台灣客家文化為例,Y. Hwang; Y. Hwang; C. Su,"Dept. of Inf. Manage., Nat. United Univ., Miaoli; NA; NA",2008 Fourth International Conference on Networked Computing and Advanced Information Management,12-Sep-08,2008,2,,438,443,"This paper proposes a collaborative experience based travel recommendation service for Hakka culture. This service utilizes mobile devices as platform to share and purify the user's knowledge of Hakka culture. By applying TF-IDF conversion on tags that were extracted from users' experience and past knowledge, in this paper we argue that a better recommendation service can be provided compared to the traditional frequency based service. Possible AI approach for utilizing the TF-IDF data is also proposed to improve the service level by using online adaptive clustering algorithm without increasing the resource requirements.",,978-0-7695-3322-3,10.1109/NCM.2008.181,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624183,,Cultural differences;Mobile handsets;Clustering algorithms;Collaboration;Organizations;Global communication;Clustering methods,artificial intelligence;document handling;groupware;mobile computing;pattern clustering;travel industry,experience co-creation;ubiquitous cultural e-service provision;Taiwan Hakka culture;collaborative experience;travel recommendation service;mobile devices;term frequency-inverse document frequency conversion;artificial intelligence;online adaptive clustering algorithm,,3,,5,,12-Sep-08,,,IEEE,IEEE Conferences
The incubation effects of the industrial cluster on entrepreneurs,產業集群對企業家的孵化效應,Liu Rongzhi; Hu Bei; Zhang Wenhui,"School of Management, Huazhong University of Science and Technology, 430074 Wuhan, China; School of Management, Huazhong University of Science and Technology, 430074 Wuhan, China; School of Management, Huazhong University of Science and Technology, 430074 Wuhan, China",4th International Conference on New Trends in Information Science and Service Science,17-Jun-10,2010,,,683,686,"Practical Experience has shown that the agglomeration and successful rate of the entrepreneurial activity in the industrial clusters was much higher than non-industrial clusters, the aim of this paper is to contribute to a greater understanding of the process about how the industrial cluster promote the entrepreneurial activities functioning as the incubator of the entrepreneurs and enterprises. By analyzing the detailed cases of the leather industrial cluster in Wenzhou in China, we raised the following questions: (1) what are the major parties in the industry cluster that can construct an effective incubation system to enhance the entrepreneurs' growth? (2)What are their functions in providing the resources to facilitate the entrepreneur growth and enterprise development? The empirical bases for the analyses are derived from various sources: historical documents, statistical data, and in-depth interviews with key entrepreneurs in private and public organizations. According to the evidence from Wenzhou, we proposed the model of the industrial cluster incubation system, which identified its major parties and the function of each party.",,978-89-88678-17-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488532,Industrial cluster;Incubation system;Entrepreneurial Activity;Wenzhou,Silicon;Government;Innovation management;Technological innovation;Environmental economics;Technology management;Industrial training;Convergence,innovation management;leather industry;organisational aspects,incubation effects;industrial cluster;entrepreneurial activity;leather industry;Wenzhou China;entrepreneur growth;enterprise development,,,,7,,17-Jun-10,,,IEEE,IEEE Conferences
Document Clustering with Semantic Analysis,具有語義分析的文檔聚類,Yong Wang; J. Hodges,Mississippi State University; NA,Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06),23-Jan-06,2006,3,,54c,54c,"Document clustering generates clusters from the whole document collection automatically and is used in many fields, including data mining and information retrieval. In the traditional vector space model, the unique words occurring in the document set are used as the features. But because of the synonym problem and the polysemous problem, such a bag of original words cannot represent the content of a document precisely. In this paper, we investigate using the sense disambiguation method to identify the sense of words to construct the feature vector for document representation. Our experimental results demonstrate that in most conditions, using sense can improve the performance of our document clustering system. But the comprehensive statistical analysis performed indicates that the differences between using original single words and using senses of words are not statistically significant. In this paper, we also provide an evaluation of several basic clustering algorithms for algorithm selection.",1530-1605,0-7695-2507-5,10.1109/HICSS.2006.129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579400,,Information retrieval;Clustering algorithms;Partitioning algorithms;Databases;Frequency;Thesauri;Computer science;Data engineering;Data mining;Statistical analysis,,,,14,,39,,23-Jan-06,,,IEEE,IEEE Conferences
Tensor-based document retrieval over Neo4j with an application to PubMed mining,Neo4j上基於Tensor的文檔檢索及其在PubMed挖掘中的應用,G. Drakopoulos; A. Kanavos,"Computer Engineering and Informatics Department, University of Patras, Patras 26500 Hellas; Computer Engineering and Informatics Department, University of Patras, Patras 26500 Hellas","2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)",19-Dec-16,2016,,,1,6,"PubMed mining is currently at the epicenter of intense interdisciplinary research. Text mining methodologies provide a way to retrieve and analyze emotionally charged words, punctuation, and syntax. Moreover, they can analyze scientific literature and process document collections. Moving beyond traditional document-term matrix representation, an architecture for content based retrieval from PubMed is proposed whose core is a document-term-author third order tensor. This methodology has been implemented in Python over Neo4j and has been applied to a PubMed document article collection.",,978-1-5090-3429-1,10.1109/IISA.2016.7785366,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785366,PubMed;Entrez;Tensor algebra;Medical information retrieval;Topic clustering;MeSH ontology;Text mining;Neo4j,Tensile stress;Databases;Biomedical imaging;Ontologies;XML;Text mining;Computer architecture,content-based retrieval;data mining;information retrieval systems;matrix algebra;medical computing;tensors;text analysis,tensor-based document retrieval;Neo4j;PubMed document article collection;Python;document-term-author third order tensor;content based retrieval;document-term matrix representation;syntax;punctuation;emotionally charged words;text mining methodologies;PubMed mining,,7,,35,,19-Dec-16,,,IEEE,IEEE Conferences
Cluster-based query expansion using language modeling in the biomedical domain,在生物醫學領域使用語言建模進行基於集群的查詢擴展,X. Xu; X. Hu,"College of Information Science and Technology, Drexel University, Philadelphia, PA 19104, USA; College of Information Science and Technology, Drexel University, Philadelphia, PA 19104, USA",2010 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW),28-Jan-11,2010,,,185,188,"Several techniques have been developed, such as query expansion, cluster-based retrieval and dimensionality reduction to resolve low search precision and low search recall issues. Of these techniques, this paper proposed cluster-based query expansion using language modeling. Through our experiments in TREC genomic track ad-hoc retrieval task, we explore and demonstrate clusters that are created based on the whole collection or the initially returned document results of the original query can be utilized to perform query expansion and eventually improve the overall effectiveness and performance of information retrieval system in the biomedical domain.",,978-1-4244-8304-4,10.1109/BIBMW.2010.5703796,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703796,Informational Retrieval;Query Expansion;Language Modeling;Pseudo-feedback;Query Drift;Cluster;TREC,Bioinformatics;Genomics;Smoothing methods;Atmospheric modeling;Computational modeling;Search engines,bioinformatics;genomics;information retrieval systems;query languages,cluster-based query expansion;language modeling;cluster-based retrieval;TREC genomic track adhoc retrieval task;information retrieval system;biomedical domain,,4,,12,,28-Jan-11,,,IEEE,IEEE Conferences
Decentralized Probabilistic Text Clustering,分散概率文本聚類,O. Papapetrou; W. Siberski; N. Fuhr,"Technical University of Crete, Chania; L3S Research Center, Hannover; University of Duisburg-Essen, Duisburg",IEEE Transactions on Knowledge and Data Engineering,17-Aug-12,2012,24,10,1848,1861,"Text clustering is an established technique for improving quality in information retrieval, for both centralized and distributed environments. However, traditional text clustering algorithms fail to scale on highly distributed environments, such as peer-to-peer networks. Our algorithm for peer-to-peer clustering achieves high scalability by using a probabilistic approach for assigning documents to clusters. It enables a peer to compare each of its documents only with very few selected clusters, without significant loss of clustering quality. The algorithm offers probabilistic guarantees for the correctness of each document assignment to a cluster. Extensive experimental evaluation with up to 1 million peers and 1 million documents demonstrates the scalability and effectiveness of the algorithm.",1558-2191,,10.1109/TKDE.2011.120,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871622,Distributed clustering;text clustering.,Clustering algorithms;Peer to peer computing;Probabilistic logic;Frequency estimation;Indexing;Computational modeling,,,,8,,36,,9-Jun-11,,,IEEE,IEEE Journals
Clustering Technique on Search Engine Dataset Using Data Mining Tool,使用數據挖掘工具的搜索引擎數據集聚類技術,M. E. Ahmed; P. Bansal,"Dept. of Comput. Sci. & Eng., itm Univ., Gurgaon, India; Dept. of Comput. Sci. & Eng., itm Univ., Gurgaon, India",2013 Third International Conference on Advanced Computing and Communication Technologies (ACCT),10-Jun-13,2013,,,86,89,"Unlabeled document collections are becoming increasingly common and mining such databases becomes a major challenge. It is a major issue to retrieve good websites from the larger collections of websites. As the number of available Web pages grows, it is become more difficult for users finding documents relevant to their interests. Clustering is the classification of a data set into subsets (clusters), so that the data in each subset share some common trait - often proximity according to some defined distance measure. By clustering we improve the quality of websites by grouping similar websites in groups. This paper addresses the applications of data mining tool Weka by applying k means clustering to find clusters from huge data sets and find the attributes that govern optimization of search engines.",2327-0659,978-0-7695-4941-5,10.1109/ACCT.2013.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6524279,Dataset;Websites;Data mining;Weka;k-means,Clustering algorithms;Data mining;Search engines;Educational institutions;Communications technology;Computer science;Databases,data mining;optimisation;pattern classification;search engines;Web sites,search engine dataset;data mining tool;Websites;Web pages;distance measure;k means clustering technique;huge data sets;optimization,,4,,10,,10-Jun-13,,,IEEE,IEEE Conferences
A New Gradient Based Character Segmentation Method for Video Text Recognition,一種新的基於梯度的視頻文本字符分割方法,P. Shivakumara; S. Bhowmick; B. Su; C. L. Tan; U. Pal,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Comput. Vision & Pattern Recognition Unit, Indian Stat. Unit, Kolkata, India",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,126,130,"The current OCR cannot segment words and characters from video images due to complex background as well as low resolution of video images. To have better accuracy, this paper presents a new gradient based method for words and character segmentation from text line of any orientation in video frames for recognition. We propose a Max-Min clustering concept to obtain text cluster from the normalized absolute gradient feature matrix of the video text line image. Union of the text cluster with the output of Canny operation of the input video text line is proposed to restore missing text candidates. Then a run length algorithm is applied on the text candidate image for identifying word gaps. We propose a new idea for segmenting characters from the restored word image based on the fact that the text height difference at the character boundary column is smaller than that of the other columns of the word image. We have conducted experiments on a large dataset at two levels (word and character level) in terms of recall, precision and f-measure. Our experimental setup involves 3527 characters of English and Chinese, and this dataset is selected from TRECVID database of 2005 and 2006.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065289,Video document analysis;Word segmentation;Video character extraction;Gradient features;Video character recognition,Text recognition;Character recognition;Image segmentation;Accuracy;Optical character recognition software;Image recognition,image segmentation;object recognition;pattern clustering;text analysis;video signal processing,gradient based character segmentation method;video text recognition;max-min clustering concept;absolute gradient feature matrix;text cluster;Canny operation;run length algorithm;text candidate image;word gap identification,,21,,21,,3-Nov-11,,,IEEE,IEEE Conferences
Scalable Recursive Top-Down Hierarchical Clustering Approach with Implicit Model Selection for Textual Data Sets,隱式模型選擇的文本數據集可擴展遞歸自上而下的層次聚類方法,M. Muhr; V. Sabol; M. Granitzer,"Knowledge Relationship Discovery Know-Center Graz, Graz, Austria; Knowledge Relationship Discovery Know-Center Graz, Graz, Austria; Knowledge Relationship Discovery Know-Center Graz, Graz, Austria",2010 Workshops on Database and Expert Systems Applications,30-Sep-10,2010,,,15,19,"Automatic generation of taxonomies can be useful for a wide area of applications. In our application scenario a topical hierarchy should be constructed reasonably fast from a large document collection to aid browsing of the data set. The hierarchy should also be used by the InfoSky projection algorithm to create an information landscape visualization suitable for explorative navigation of the data. We developed an algorithm that applies a scalable, recursive, top-down clustering approach to generate a dynamic concept hierarchy. The algorithm recursively applies a workflow consisting of preprocessing, clustering, cluster labeling and projection into 2D space. Besides presenting and discussing the benefits of combining hierarchy browsing with visual exploration, we also investigate the clustering results achieved on a real world data set.",2378-3915,978-1-4244-8049-4,10.1109/DEXA.2010.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5591979,topic hierarchy;information landscape;growing k-means;model selection;vector space model,Clustering algorithms;Heuristic algorithms;Labeling;Internet;Encyclopedias;Projection algorithms,pattern clustering;text analysis,scalable recursive top down hierarchical clustering approach;textual data set;automatic generation;document collection;InfoSky projection algorithm;information landscape visualization;visual exploration,,5,,16,,30-Sep-10,,,IEEE,IEEE Conferences
A spectral clustering approach for online and streaming applications,在線和流媒體應用程序的頻譜聚類方法,A. Robles-Kelly; R. Wei,"Data61-CSIRO, Tower A, 7 London Circuit, Canberra ACT 2601, Australia; Data61-CSIRO, Tower A, 7 London Circuit, Canberra ACT 2601, Australia",2017 International Joint Conference on Neural Networks (IJCNN),3-Jul-17,2017,,,3904,3911,"In this paper, we present a spectral clustering method for online and streaming applications. Here, we note that the rank of the coefficients of the eigenvector of the graph Laplacian govern, together with the weights of the adjacency matrix, the assignment of the data to clusters. Thus, we adopt a sampling without replacement strategy, where, at each sampling step, we select those data instances which are most relevant to the clustering process. To do this, we ?sparsify??the eigenvector making use of a Minorisation-Maximisation approach. This not only allows to cluster the data under consideration after the sampling has been effected, but also permits the optimisation in hand to be performed making use of a gradient descent approach with a closed form iterate. Moreover, the method presented here is quite general in nature and can be employed in other settings which hinge in an L-0 regularised penalty function. We discuss the use of our approach for the assessment of node centrality and document binarisation. We also illustrate the utility of our method for purposes of background subtraction and compare our results with those yielded by alternatives elsewhere in the literature.",2161-4407,978-1-5090-6182-2,10.1109/IJCNN.2017.7966348,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966348,,Laplace equations;Cost function;Eigenvalues and eigenfunctions;Streaming media;Clustering methods;Encoding,eigenvalues and eigenfunctions;gradient methods;graph theory;matrix algebra;pattern clustering,spectral clustering method;data streaming applications;online applications;eigenvector coefficients;graph Laplacian;adjacency matrix;minorisation-maximisation approach;gradient descent approach;closed form iterate;L-0 regularised penalty function;node centrality assessment;document binarisation;background subtraction,,,,44,,3-Jul-17,,,IEEE,IEEE Conferences
Developing Horizon Scanning Methods for the Discovery of Scientific Trends,開發地平線掃描方法以發現科學趨勢,M. Karasalo; J. Schubert,Swedish Defence Research Agency; Swedish Defence Research Agency,2019 International Conference on Document Analysis and Recognition (ICDAR),3-Feb-20,2019,,,1055,1062,"In this application-oriented paper, we develop a methodology and a system for horizon scanning of scientific literature to discover scientific trends. Literature within a broadly defined field is automatically clustered and ranked based on topic and scientific impact, respectively. A method for determining the optimal number of clusters for the established Gibbs sampling Dirichlet multinomial mixture model (GSDMM) algorithm is proposed along with a method for deriving descriptive and distinctive words for the discovered clusters. Furthermore, we propose a ranking methodology based on citation statistics to identify significant contributions within the discovered subject areas.",2379-2140,978-1-7281-3014-9,10.1109/ICDAR.2019.00172,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978144,horizon scanning;scientometrics;Gibbs sampling;Dirichlet multinomial model;entropy;clustering;HSTOOL,Entropy;Clustering algorithms;Market research;Ranking (statistics);Bibliometrics;Convergence;Mixture models,citation analysis;Markov processes;Monte Carlo methods;pattern clustering,horizon scanning methods;application-oriented paper;scientific literature;broadly defined field;scientific impact;discovered clusters;ranking methodology;discovered subject areas;scientific trends discovery;Gibbs sampling Dirichlet multinomial mixture model algorithm;GSDMM algorithm;citation statistics,,1,,7,,3-Feb-20,,,IEEE,IEEE Conferences
Enhancing Web Search Using Query-Based Clusters and Labels,使用基於查詢的群集和標籤增強Web搜索,R. Qumsiyeh; Y. Ng,"Comput. Sci. Dept., Brigham Young Univ., Provo, UT, USA; Comput. Sci. Dept., Brigham Young Univ., Provo, UT, USA",2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),23-Dec-13,2013,1,,159,164,"Current web search engines, such as Google, Bing, and Yahoo!, rank the set of documents S retrieved in response to a user query and display the URL of each document D in S with a title and a snippet, which serves as an abstract of D. Snippets, however, are not as useful as they are designed for, which is supposed to assist its users to quickly identify results of interest, if they exist. These snippets fail to (i) provide distinct information and (ii) capture the main contents of the corresponding documents. Moreover, when the intended information need specified in a search query is ambiguous, it is very difficult, if not impossible, for a search engine to identify precisely the set of documents that satisfy the user's intended request without requiring additional inputs. Furthermore, a document title is not always a good indicator of the content of the corresponding document. All of these design problems can be solved by our proposed query-based cluster and labeler, called QCL. QCL generates concise clusters of documents covering various subject areas retrieved in response to a user query, which saves the user's time and effort in searching for specific information of interest without having to browse through the documents one by one. Experimental results show that QCL is effective and efficient in generating high-quality clusters of documents on specific topics with informative labels.",,978-0-7695-5145-6,10.1109/WI-IAT.2013.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690009,,Quantum cascade lasers;Web search;Engines;Google;Clustering algorithms;Vectors;Frequency measurement,Internet;pattern clustering;query processing;search engines,Web search enhancement;query-based clusters;query-based labels;search query;search engine;QCL user query,,4,,14,,23-Dec-13,,,IEEE,IEEE Conferences
Automated borders detection and adaptive segmentation for binary document images,二進製文檔圖像的自動邊界檢測和自適應分割,D. X. Le; G. R. Thoma; H. Wechsler,"Nat. Libr. of Med., Bethesda, MD, USA; NA; NA",Proceedings of 13th International Conference on Pattern Recognition,6-Aug-02,1996,3,,737,741 vol.3,"This paper describes two new and effective algorithms: one for detecting the page borders for documents available as binary images, and the other an adaptive segmentation algorithm using a bottom-up approach for segmenting binary images into blocks. The borders detection algorithm relies upon the classification of blank/textual/non-textual rows and columns, objects segmentation, and an analysis of projection profiles and crossing counts. Segmentation, done by an adaptive smearing technique, is different from all previous bottom-up approaches because any decisions on merging and/or separating are based on the estimated font information in binary document images.",1051-4651,0-8186-7282-X,10.1109/ICPR.1996.547266,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547266,,Image segmentation;Merging;Libraries;Biomedical imaging;Books;Clustering algorithms;Text analysis;Computer science;Detection algorithms;Image converters,image segmentation,automated borders detection;adaptive segmentation;binary document images;page borders;objects segmentation;projection profiles;crossing counts;adaptive smearing technique;estimated font information,,16,,11,,6-Aug-02,,,IEEE,IEEE Conferences
Conceptual clustering in information retrieval,信息檢索中的概念聚類,S. K. Bhatia; J. S. Deogun,"Dept. of Math. & Comput. Sci., Missouri Univ., St. Louis, MO, USA; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",6-Aug-02,1998,28,3,427,436,"Clustering is used in information retrieval systems to enhance the efficiency and effectiveness of the retrieval process. Clustering is achieved by partitioning the documents in a collection into classes such that documents that are associated with each other are assigned to the same cluster. This association is generally determined by examining the index term representation of documents or by capturing user feedback on queries on the system. In cluster-oriented systems, the retrieval process can be enhanced by employing characterization of clusters. In this paper, we present the techniques to develop clusters and cluster characterizations by employing user viewpoint. The user viewpoint is elicited through a structured interview based on a knowledge acquisition technique, namely personal construct theory. It is demonstrated that the application of personal construct theory results in a cluster representation that can be used during query as well as to assign new documents to the appropriate clusters.",1941-0492,,10.1109/3477.678640,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=678640,,Information retrieval;Computer science;Feedback;Knowledge acquisition;Mathematics;Pattern matching,knowledge acquisition;information retrieval systems;systems analysis;feedback,conceptual clustering;information retrieval;index term representation;user feedback;knowledge acquisition;cluster representation,,49,,27,,6-Aug-02,,,IEEE,IEEE Journals
Reverse mapping of referral links from storage hierarchy for Web documents,Web文檔存儲層次結構中反向鏈接的反向映射,Chen Ding; Chi-Hung Chi; V. Tam,"Dept. of Comput., Nat. Univ. of Singapore, Singapore; NA; NA",Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000,6-Aug-02,2000,,,216,219,"Due to the lack of back-pointers, information about the referral parent(s) of a given Web page is not usually available to Web surfers. This significantly reduces the effectiveness of Web surfing and information discovery. We propose a mechanism to locate the referral parent(s) for a given fragment of a Web document (in terms of an URL address) from the client side. The basic idea is to explore the hierarchical hyperlink structure of a Web document fragment from its storage path directory information that is embedded in its URL. Two mechanisms were proposed: exhaustibility first search (EFS) and generalization first search (GFS). Results showed that through direct mapping of the storage hierarchy path to the URL address and then performing reverse tracing along the hyperlink structure, the chance of discovering a referral parent ranged from 41% to 51%. Compared to EFS, GFS was found to reduce the average search space from 26 pages to 4 pages.",1082-3409,0-7695-0909-6,10.1109/TAI.2000.889873,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889873,,Uniform resource locators;Web pages;Databases;Indexing;Information retrieval;Web sites;Search engines;Text analysis;Internet;Clustering algorithms,information resources;information retrieval,reverse mapping;referral links;storage hierarchy;Web documents;referral parent;Web page;Web surfing;information discovery;hierarchical hyperlink structure;storage path directory information;exhaustibility first search;generalization first search;URL address,,,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Egocentric Daily Activity Recognition via Multitask Clustering,通過多任務聚類進行以自我為中心的日常活動識別,Y. Yan; E. Ricci; G. Liu; N. Sebe,"Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy",IEEE Transactions on Image Processing,9-Jun-15,2015,24,10,2984,2995,"Recognizing human activities from videos is a fundamental research problem in computer vision. Recently, there has been a growing interest in analyzing human behavior from data collected with wearable cameras. First-person cameras continuously record several hours of their wearers' life. To cope with this vast amount of unlabeled and heterogeneous data, novel algorithmic solutions are required. In this paper, we propose a multitask clustering framework for activity of daily living analysis from visual data gathered from wearable cameras. Our intuition is that, even if the data are not annotated, it is possible to exploit the fact that the tasks of recognizing everyday activities of multiple individuals are related, since typically people perform the same actions in similar environments, e.g., people working in an office often read and write documents). In our framework, rather than clustering data from different users separately, we propose to look for clustering partitions which are coherent among related tasks. In particular, two novel multitask clustering algorithms, derived from a common optimization problem, are introduced. Our experimental evaluation, conducted both on synthetic data and on publicly available first-person vision data sets, shows that the proposed approach outperforms several single-task and multitask learning methods.",1941-0042,,10.1109/TIP.2015.2438540,"MIUR Cluster Project Active Ageing at Home; EC Project xLiMe; Agency for Science, Technology and Research, Singapore, through the Human-Centered Cyberphysical Systems Grant; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113851,Egocentric Activity Recognition;Multi-task Learning;Activity of Daily Living Analysis;Egocentric activity recognition;multi-task learning;activity of daily living analysis,Cameras;Optimization;Videos;Clustering algorithms;Algorithm design and analysis;Visualization;Linear programming,computer vision;image recognition;learning (artificial intelligence);optimisation;pattern clustering;video cameras;video signal processing,egocentric daily activity recognition;human activity recognition;computer vision;human behavior analysis;wearable cameras;first-person cameras;heterogeneous data;multitask clustering framework;daily living analysis;visual data;common optimization problem;synthetic data;first-person vision data sets,"Actigraphy;Activities of Daily Living;Humans;Image Interpretation, Computer-Assisted;Movement;Pattern Recognition, Automated;Photography;Reproducibility of Results;Sensitivity and Specificity;Video Recording",104,,46,,1-Jun-15,,,IEEE,IEEE Journals
A proposal of extended cosine measure for distance metric learning in text classification,一種用於文本分類中距離度量學習的擴展餘弦量度的建議,K. Mikawa; T. Ishida; M. Goto,"Dep. of Creative Science and Engineering, Waseda University, Tokyo, Japan; Media Network Center, Waseda University, Tokyo, Japan; Dep. of Creative Science and Engineering, Waseda University, Tokyo, Japan","2011 IEEE International Conference on Systems, Man, and Cybernetics",21-Nov-11,2011,,,1741,1746,"This paper discusses a new similarity measure between documents on a vector space model from the view point of distance metric learning. The documents are represented by points in the vector space by using the information of frequencies of words appearing in each document. The similarity measure between two different documents is useful to recognize the relationship and can be applied to classification or clustering of the data. Usually, the cosine similarity and the Euclid distance have been used in order to measure the similarity between points in the Euclidean space. However, these measures do not take the correlation among words which appear in documents into consideration on an application of the vector space model to document analysis. Generally speaking, many words which appear in documents have correlation to one another depending on the sentence structures, topics and subjects. Therefore, it is effective to build a suitable metric measure taking the correlation of words into consideration on the vector space in order to improve the performance of document classification and clustering. This paper presents a new effective method to acquire a distance measure on the document vector space based on an extended cosine measure. In addition, the way of distance metric learning is proposed to acquire the proper metric from the view point of supervised learning. The effectiveness of our proposal is clarified by simulation experiments for the text classification problems of the customer review which is posted on the web site and the newspaper article.",1062-922X,978-1-4577-0653-0,10.1109/ICSMC.2011.6083923,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083923,metric learning;extended cosine measure;vector space model;similarity measure;text mining,Q measurement;Vectors;Presses;Biological system modeling,data structures;electronic publishing;learning (artificial intelligence);pattern classification;pattern clustering;text analysis;Web sites,extended cosine measure;distance metric learning;text classification;similarity measure;vector space model;document representation;word processing;data classification;data clustering;document classification;document clustering;Web site;newspaper article,,9,,17,,21-Nov-11,,,IEEE,IEEE Conferences
An Unsupervised Method for Detecting Style Breaches in a Document,一種無監督的文檔樣式洩露檢測方法,M. Elamine; S. Mechti; L. H. Belguith,"MIRACL Laboratory, FSEGS, University of Sfax, Sfax, Tunisia; LARODEC Laboratory, ISG of Tunis; MIRACL Laboratory, FSEGS, University of Sfax",2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA),16-Mar-20,2019,,,1,6,"In this paper, we propose an unsupervised method for identifying style breaches in a given document, also known as intrinsic plagiarism detection. In fact, plagiarism is one of the major challenges in various domains. Supervised learning techniques fail to capture the stylistic changes in a text effectively and this is mainly due to the static segmentation of the text. For this reason, we present in this paper our proposed method for intrinsic plagiarism detection, we experimented with the unsupervised algorithm Kmeans and the similarity measure Cosine. Our results are comparable to best systems presented in PAN@CLEF competitive conference.",2161-5330,978-1-7281-5052-9,10.1109/AICCSA47632.2019.9035264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035264,Plagiarism detection;style-breach;supervised classification;unsupervised clustering;writing style inconsistencies,Plagiarism;Task analysis;Writing;Clustering algorithms;Microsoft Windows;Supervised learning;Feature extraction,pattern clustering;text analysis;unsupervised learning,intrinsic plagiarism detection;unsupervised algorithm Kmeans;style breaches detection;document;static segmentation;similarity measure cosine,,,,22,,16-Mar-20,,,IEEE,IEEE Conferences
Distance distributions for Mat矇rn cluster processes with application to network performance analysis,Matmontrn集群過程的距離分佈及其在網絡性能分析中的應用,J. Tang; G. Chen; J. P. Coon; D. E. Simmons,"Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, UK; Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, UK; Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, UK; Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, UK",2017 IEEE International Conference on Communications (ICC),31-Jul-17,2017,,,1,6,"In this work, we analyze the distance statistics corresponding to points in a Mat矇rn cluster (offspring points) and points that do not belong to that cluster (non-offspring points). We first derive the probability density function (PDF) of the distance between an offspring point and a non-offspring point of a Mat矇rn cluster. We then formulate the probability generating functional based on this PDF. Since many wireless networks (e.g., device-to-device (D2D) networks and cognitive radio systems) exhibit device clustering, this formalism enables us to efficiently formulate and evaluate expressions that describe the interference statistics and connection probability in clustered networks. We validate our theoretical analysis with numerical simulations, and illustrate that traditional methods of evaluating similar performance metrics (based on point process statistics instead of distance statistics) are unsuitable for use in such complex scenarios.",1938-1883,978-1-4673-8999-0,10.1109/ICC.2017.7997055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997055,Interference;Mat矇rn cluster process;distance probability density function;connection probability,Probability density function;Interference;Probability;Portable document format;Wireless networks;Performance analysis,cognitive radio;mobile radio;numerical analysis;probability;radiofrequency interference;wireless channels,distance distributions;Matern cluster process;network performance analysis;distance statistics;nonoffspring points;probability density function;wireless networks;device-to-device networks;D2D networks;cognitive radio systems;interference statistics;connection probability;clustered networks;numerical simulations;point process statistics,,4,,16,,31-Jul-17,,,IEEE,IEEE Conferences
Hyperspectral Imaging for Ink Mismatch Detection,用於墨水不匹配檢測的高光譜成像,Z. Khan; F. Shafait; A. Mian,"Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,877,881,"Ink mismatch detection provides important clues to forensic document examiners by identifying whether a particular handwritten note was written with a specific pen, or to show that some part (e.g. signature) of a note is written with a different ink as compared to the rest of the note. In this paper, we show that a hyper spectral image (HSI) of handwritten notes can discriminate between inks that are visually similar in appearance. For this purpose, we develop the first ever hyper spectral image database of handwritten notes in various blue and black inks, comprising a total of 70 hyper spectral images each in 33 bands of the visible spectrum. In an unsupervised clustering scheme, the spectral responses of inks fall into separate clusters to allow segmentation of two different inks in a questioned document. The same method fails to segment inks correctly when applied to RGB scans of these documents, since the inks are very hard to distinguish in the visible spectral range. HSI overcomes the shortcomings of RGB and allows better discrimination between inks. We further evaluate which subset of bands from HSI is most useful for the purpose of ink mismatch detection. We hope that these findings will stimulate the use of HSI in document analysis research, especially for questioned document examination.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.179,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628744,hyperspectral;ink;forensics,Ink;Hyperspectral imaging;Accuracy;Image segmentation;Writing;Imaging,document image processing;image colour analysis;image segmentation,hyperspectral imaging;ink mismatch detection;forensic document examiners;HSI;handwritten notes;hyper spectral image database;unsupervised clustering scheme;spectral responses;RGB scans;the visible spectral range;segmentation method,,23,,15,,15-Oct-13,,,IEEE,IEEE Conferences
Distributed multimedia information systems-imaging and networking issues,分佈式多媒體信息系統-成像和聯網問題,R. B. Johnson,"Dept. of Electr. & Electron. Eng., Bristol Univ., UK",IEE Colloquium on Document Image Processing and Multimedia Environments,6-Aug-02,1995,,,9月1日,9月6日,"Distributed multimedia information systems (DMIS) are the emerging technology which provide its users with the facility to exchange compound documents comprising of text, images, graphics and sound. They mainly consist of clusters of workstations linked via networks. These systems cater for store-and-forward and/or real-time information. Recent developments in distributed computing and applications oriented toward image processing require high speed networks to transfer large amounts of data with minimum delay. This paper reviews some high speed networks and some image compression strategies for coping with the large amount of data.",,,10.1049/ic:19951190,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=498882,,Multimedia communication;Multimedia systems;Multimedia computing;Broadband communication;Information systems;Image processing,multimedia communication;multimedia systems;multimedia computing;broadband networks;information systems;image processing,multimedia communication;multimedia computing;distributed multimedia information systems;networking;imaging;DMIS;compound document exchange;text;images;graphics;sound;document handling;workstation cluster network;store-and-forward;real-time information;distributed computing;image processing;high speed network;image compression;broadband network,,,,,,6-Aug-02,,,IET,IET Conferences
An Unsupervised Approach for Keyphrase Extraction Using Within-Collection Resources,使用集合內資源提取關鍵詞的無監督方法,T. Li; L. Hu; J. Chu; H. Li; L. Chi,"College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China",IEEE Access,13-Sep-19,2019,7,,126088,126097,"It is hard to select and read suitable documents due to the rapidly growing number of scholarly documents. Keyphrases can be considered as the gist of a document so that a researcher can select the documents that they want using keyphrase queries. However, there are also many scholarly documents without any keyphrases tagged by the authors or other researchers. Automatic keyphrase extraction can help researchers to quickly extract keyphrases. This paper proposed an unsupervised approach for keyphrase extraction using graph-based ranking and topic-based clustering under the assumption that we only use the within-collection resources. We use graph-based ranking to describe the relevance between two words and topic-based clustering to embed semantical information into words. In this paper, we assume that each word has its own meaning, and each meaning can be considered as a topic, though we know nothing about these meanings. We use topic-based clustering to assign the ?correct meaning??to the ?correct word?? In addition, by taking the relevance among phrases into consideration and only using within-collection resources, we can use the graph-based ranking in our approach. The edges in a graph that are built for phrases can describe the hidden relevance between two phrases, and the weights that are set for edges can measure the connection between two phrases. Then, after using the position feature, our approach consists of an enhanced graph-based ranking and a topic-based clustering. The experiments are run on four datasets: KDD, WWW, GSN and ACM. The results indicate that our approach has better performance than the state-of-the-art methods.",2169-3536,,10.1109/ACCESS.2019.2938213,Project of Jilin Province Development and Reform Commission; Key Technology Innovation Cooperation Project of Government and University for the Whole Industry Demonstration; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819880,Phrase extraction;graph-based ranking;topic-based clustering;within-collection resource;NLP,Data mining;Clustering algorithms;Feature extraction;Encyclopedias;Electronic publishing;Internet,graph theory;pattern clustering;query processing;text analysis;unsupervised learning,enhanced graph-based ranking;correct word;correct meaning;topic-based clustering;graph-based ranking;automatic keyphrase extraction;keyphrase queries;scholarly documents;within-collection resources;unsupervised approach,,,,24,CCBY,29-Aug-19,,,IEEE,IEEE Journals
Unsupervised HMM Adaptation Using Page Style Clustering,使用頁面樣式聚類的無監督HMM適應,H. Cao; R. Prasad; S. Saleem; P. Natarajan,"BBN Technol., Cambridge, MA, USA; BBN Technol., Cambridge, MA, USA; BBN Technol., Cambridge, MA, USA; BBN Technol., Cambridge, MA, USA",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,1091,1095,"In this paper we present an innovative two-stage adaptation approach for handwriting recognition that is based on clustering of similar pages in the training data. In our approach, we first perform page clustering on training data using features such as contour slope, pen pressure, writing velocity, and stroke sparseness. Next, we adapt the writer-independent hidden Markov models (HMMs) to each cluster in the training data. While decoding a test page, we first determine the cluster the test page belongs to and then decode the page with the model associated with that cluster. Experimental results with the two-stage adaptation show significant gains on a held-out validation set.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277484,,Hidden Markov models;Testing;Handwriting recognition;Training data;Loudspeakers;Maximum likelihood linear regression;Writing;Speech recognition;Maximum likelihood decoding;Error analysis,decoding;handwriting recognition;hidden Markov models;image coding;pattern clustering;unsupervised learning,unsupervised HMM adaptation;hidden Markov model;page style clustering;handwriting recognition;decoding;training data,,7,,14,,2-Oct-09,,,IEEE,IEEE Conferences
FSSOM: One novel SOM clustering algorithm based on feature selection,FSSOM：一種基於特徵選擇的新穎SOM聚類算法,Ming Liu; Yuan-Chao Liu; Xiao-Long Wang,"School of Computer Science and Technology, Harbin Institute of Technology, 150001, china; School of Computer Science and Technology, Harbin Institute of Technology, 150001, china; School of Computer Science and Technology, Harbin Institute of Technology, 150001, china",2008 International Conference on Machine Learning and Cybernetics,5-Sep-08,2008,1,,429,435,"In order to reduce dimension number of feature space and improve clustering precision, a novel SOM clustering algorithm based on feature selection-FSSOM is provided in this paper. This algorithm first evaluates importance and distinguishing ability of each feature, and only selects features which can efficiently improve clustering precision to construct feature space. Then, it computes kullback-leibler divergence of different co-occurring feature vector, which is gotten from large scale training corpus, to reflect the similarity of different feature. This algorithm considers the influences of similar features and uses it in self-organizing-mapping algorithm. It can make latently similar documents into same cluster. The experiment results demonstrate that because of adjusting the similar featurespsila weights, enlarging feature adjusting range, it can efficiently improve clustering precision and reduce training time.",2160-1348,978-1-4244-2095-7,10.1109/ICMLC.2008.4620444,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620444,Feature Selection;Self-Organizing-Mapping;Kullback-Leibler Divergence,Neurons;Clustering algorithms;Feature extraction;Algorithm design and analysis;Distance measurement;Euclidean distance;Machine learning,feature extraction;pattern clustering;self-organising feature maps,feature selection;clustering precision;clustering algorithm;cooccurring feature vector;large scale training corpus;self-organizing-mapping algorithm,,,,14,,5-Sep-08,,,IEEE,IEEE Conferences
MR-VSM: Map Reduce based vector Space Model for user profiling-an empirical study on News data,MR-VSM：基於Map Reduce的向量空間模型用於用戶配置文件-新聞數據的實證研究,A. Gautam; P. Bedi,"Department of Computer Science, University of Delhi, India; Department of Computer Science, University of Delhi, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",28-Sep-15,2015,,,355,360,"Velocity of data generation has increased over a period of decade which is expected to further increase exponentially with the passage of time. To mine the useful nuggets of information, satisfying a large community of users it is preferred to capture the interest of the user, i.e., to create a user profile, and then filter the content according to his taste. A user may traverse through a large number of documents, requiring a user profiling technique to support the scalability of growing number of documents. This paper proposes a novel technique of user profiling - Map Reduce based Vector Space Model (MR-VSM). MR-VSM is a technique for user profiling where the user interacts with data rich in text and volume. MR-VSM implements traditional VSM to use Map Reduce, a parallel programming paradigm to increase the computational efficiency and support scalability of documents. It works by parallelizing the task of creating a term-document class of VSM by using TF-IDF to create term vector. For experimental study this paper makes use of the News dataset which is rich in text and volume and is collected from the web using RSS feeds. The proposed system creates user profile by taking into consideration the News item read by the user and creating a term vector for each news item read. Resulting user profile is set of Top-n terms. To test the computational efficiency and scalability of MR-VSM for growing number of news items read by user, MR-VSM is made to run on a cluster of Hadoop for 12,000, 24,000 and 48000 news items. VSM is also run for 1,500 news items to show the computational efficiency of the proposed approach. It is observed that for MR-VSM computational time for user profiling and scalability of news item read by the user are improved with the increase in the number of nodes in a Hadoop cluster.",,978-1-4799-8792-4,10.1109/ICACCI.2015.7275635,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275635,Information Retrieval;User Profiling;Vector Space Model;Map Reduce;Hadoop,Scalability;Feeds;Computational efficiency;Computational modeling;Filtering;Informatics;Databases,document handling;information resources;information retrieval;parallel programming;pattern clustering,MR-VSM;Mapreduce based vector space model;user profiling;data generation;parallel programming;computational efficiency;documents scalability;term-document class;TF-IDF;term vector;news dataset;RSS feeds;news item;top-n terms;Hadoop cluster;information retrieval,,5,,12,,28-Sep-15,,,IEEE,IEEE Conferences
A Privacy-Preserving Multi-Keyword Ranked Search Over Encrypted Data in Hybrid Clouds,混合雲中加密數據的隱私保護多關鍵字排名搜索,H. Dai; Y. Ji; G. Yang; H. Huang; X. Yi,"Nanjing University of Post and Telecommunication, Nanjing, China; Nanjing University of Post and Telecommunication, Nanjing, China; Nanjing University of Post and Telecommunication, Nanjing, China; Nanjing University of Post and Telecommunication, Nanjing, China; Royal Melbourne Institute of Technology University, Melbourne, VIC, Australia",IEEE Access,9-Jan-20,2020,8,,4895,4907,"With the rapid development of cloud computing services, more and more individuals and enterprises prefer to outsource their data or computing to clouds. In order to preserve data privacy, the data should be encrypted before outsourcing and it is a challenge to perform searches over encrypted data. In this paper, we propose a privacy-preserving multi-keyword ranked search scheme over encrypted data in hybrid clouds, which is denoted as MRSE-HC. The keyword dictionary of documents is clustered into balanced partitions by a bisecting $k$ -means clustering based keyword partition algorithm. According to the partitions, the keyword partition based bit vectors are adopted for documents and queries which are utilized as the index of searches. The private cloud filters out the candidate documents by the keyword partition based bit vectors, and then the public cloud uses the trapdoor to determine the result in the candidates. On the basis of the MRSE-HC scheme, an enhancement scheme EMRSE-HC is proposed, which adds complete binary pruning tree to further improve search efficiency. The security analysis and performance evaluation show that MRSE-HC and EMRSE-HC are privacy-preserving multi-keyword ranked search schemes for hybrid clouds and outperforms the existing scheme FMRS in terms of search efficiency.",2169-3536,,10.1109/ACCESS.2019.2963096,National Natural Science Foundation of China; China Postdoctoral Science Foundation; Natural Science Foundation of Anhui Province; Natural Science Foundation of Anhui Province; Nanjing University of Posts and Telecommunications; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946623,Hybrid cloud;multi-keyword ranked search;privacy-preserving;searchable encryption,Cloud computing;Indexes;Data models;Outsourcing;Clustering algorithms;Encryption,cloud computing;cryptography;data privacy;document handling;outsourcing;pattern clustering;search problems,private cloud;public cloud;MRSE-HC scheme;privacy-preserving multikeyword ranked search;search scheme;hybrid clouds;encrypted data;cloud computing services;data privacy;keyword partition based bit vectors;K -means clustering based keyword partition algorithm;data outsourcing;EMRSE-HC;document keyword dictionary,,2,,34,CCBY,31-Dec-19,,,IEEE,IEEE Journals
Label the many with a few: Semi-automatic medical image modality discovery in a large image collection,標記很多：在大型圖像集中發現半自動醫學圖像模態,S. Vajda; D. You; S. K. Antani; G. R. Thoma,"Lister Hill National Center for Biomedical Communications, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD, USA; University of Michigan Health System, 1500 E Medical Center Dr, Ann Arbor, USA; Lister Hill National Center for Biomedical Communications, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD, USA; Lister Hill National Center for Biomedical Communications, National Library of Medicine, National Institutes of Health, 8600 Rockville Pike, Bethesda, MD, USA",2014 IEEE Symposium on Computational Intelligence in Healthcare and e-health (CICARE),15-Jan-15,2014,,,167,173,"In this paper we present a fast and effective method for labeling images in a large image collection. Image modality detection has been of research interest for querying multimodal medical documents. To accurately predict the different image modalities using complex visual and textual features, we need advanced classification schemes with supervised learning mechanisms and accurate training labels. Our proposed method, on the other hand, uses a multiview-approach and requires minimal expert knowledge to semi-automatically label the images. The images are first projected in different feature spaces, and are then clustered in an unsupervised manner. Only the cluster representative images are labeled by an expert. Other images from the cluster ?inherit??the labels from these cluster representatives. The final label assigned to each image is based on a voting mechanism, where each vote is derived from different feature space clustering. Through experiments we show that using only 0.3% of the labels was sufficient to annotate 300,000 medical images with 49.95% accuracy. Although, automatic labeling is not as precise as manual, it saves approximately 700 hours of manual expert labeling, and may be sufficient for next-stage classifier training. We find that for this collection accuracy improvements are feasible with better disparate feature selection or different filtering mechanisms.",,978-1-4799-4527-6,10.1109/CICARE.2014.7007850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007850,,Labeling;Biomedical imaging;Visualization;Feature extraction;X-ray imaging;Computed tomography;Manuals,document image processing;feature extraction;feature selection;image classification;image retrieval;learning (artificial intelligence);medical image processing;pattern clustering,filtering mechanisms;feature selection;accuracy improvement;next-stage classifier training;feature space clustering;voting mechanism;cluster representative image labelling;unsupervised mechanism;image clustering;feature spaces;image projection;multiview-approach;training labels;supervised learning mechanisms;classification schemes;complex textual features;complex visual features;multimodal medical document query;image modality detection;semiautomatic image labeling;large-image collection;semiautomatic medical image modality discovery,,3,,24,,15-Jan-15,,,IEEE,IEEE Conferences
txtKnot ??Text clustering based concept hierarchy to generalize from different text sources,txtKnot基於文本聚類的概念層次結構，可以從不同的文本源進行概括,D. Jayasinghe; S. Hettiarachchi; S. Abeywickrama; C. Ketteepearachchi; D. Alahakoon; S. Matharage; U. Gunasinghe,"Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka; Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka; Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka; Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka; Clayton School of IT, Monash University, Australia; Clayton School of IT, Monash University, Australia; Clayton School of IT, Monash University, Australia",2010 Fifth International Conference on Information and Automation for Sustainability,17-Feb-11,2010,,,239,243,"Living in the modern technology dependent world, we heavily rely on electronically stored data and information, to come up with sound and timely decisions. Considering the entire information technology world, there exists an unimaginable volume of data which contains a lot of information which is relevant to various kinds of fields. But the problem emerges when we are interested to find out about a particular subject. This is due to its scattered nature of relevant and non-relevant data. Therefore it is fair to say that there exists a critical need for a system which could create an ordered structure that provides a way of modeling the underlying relationships of data elements which will ultimately result in a much easier process of decision making. txtKnot is all about solving the above problem by generating a meaningful hierarchy of concepts from a set of unsorted text documents, thus enabling the visualization of relationships that exist within the set of documents. It consists of four main components namely, Data Extraction Module, Data Pre-processor Module, Text Clustering Module and Concept Hierarchy Generation Module. These four components are integrated together in order to fulfill the main objective of providing an easy to use method of organizing, visualizing, searching and filtering of the huge amount of electronically available unsorted textual data.",2151-1810,978-1-4244-8552-9,10.1109/ICIAFS.2010.5715666,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715666,Text clustering;growing self-organizing maps;semantic relation graph;hierarchical systems;neural networks;knowledge discovery;unsupervised learning,Data mining;Databases;Semantics;Clustering algorithms;Knowledge based systems;Artificial neural networks;Algorithm design and analysis,data mining;pattern clustering;text analysis,txtKnot;electronically stored data;information technology;relationship visualization;data extraction module;data pre-processor module;text clustering module;concept hierarchy generation module;data mining,,,,17,,17-Feb-11,,,IEEE,IEEE Conferences
An Efficient Clustering Algorithm for Small Text Documents,小文本文檔的高效聚類算法,Y. Liu; J. Cai; J. Yin; Z. Huang,"Sun Yat-Sen University, China; Sun Yat-Sen University, China; Sun Yat-Sen University, China; Sun Yat-Sen University, China",2006 Seventh International Conference on Web-Age Information Management Workshops,11-Dec-06,2006,,,16,16,"Clustering text documents into different category groups is an important problem. The size of desired clusters is an important requirement for a clustering solution. In this paper, we present an efficient clustering algorithm called RTC based on the spherical k-means algorithm for small text documents. In RTC, we present a new initial centers choice method based on the density and farthest distance strategies. Based on the first variations adjustment of Ping-Pong algorithm, we also present a new partition adjustment method, which is guided by the set of border objects of clusters. We test the algorithm performance based on the Chinese natural language platform. The experimental results show that RTC outperforms the spherical k-means and bisecting k-means in clustering accuracy and Ping-Pong both in clustering accuracy and clustering time. Especially, in the clustering time aspect, RTC sometimes is 5 times faster than Ping- Pong.",,0-7695-2705-1,10.1109/WAIMW.2006.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027176,,Clustering algorithms;Partitioning algorithms;Sun;Testing;Natural languages;Computer science;Laboratories;Information security;Euclidean distance;Refining,,,,1,,11,,11-Dec-06,,,IEEE,IEEE Conferences
Using Earth Mover's Distance in the Bag-of-Visual-Words Model for Mathematical Symbol Retrieval,在視覺詞袋模型中使用地球移動器的距離進行數學符號檢索,S. Marinai; B. Miotti; G. Soda,"Dipt. di Sist. e Inf., Univ. di Firenze, Florence, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Florence, Italy; Dipt. di Sist. e Inf., Univ. di Firenze, Florence, Italy",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,1309,1313,"In this paper, the Earth Mover's Distance (EMD) is used as a similarity measure in the mathematical symbol retrieval task. The approach is based on the Bag-of-Visual-Words model. In our case the features extracted from each symbol are clustered by means of Self-Organizing Maps (SOM) and then occurrences of features in the clusters are accumulated in a vector of visual words. The comparison between the latter vectors is performed with the EMD which naturally allows to incorporate the topological organization of SOM clusters in the distance computation. The proposed approach is experimentally tested in a mathematical symbol retrieval task and compared with the cosine similarity and with some variants that have been recently proposed.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.263,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065522,Bag of Visual Words;Earth Mover's Distance;Self Organizing Map,Shape;Vectors;Context;Euclidean distance;Earth;Indexing;Feature extraction,distance measurement;feature extraction;information retrieval;mathematics computing;pattern clustering;self-organising feature maps;topology;vectors,earth mover distance;bag-of-visual-words model;mathematical symbol retrieval;EMD;similarity measure;feature extraction;self-organizing maps;visual words;topological organization;SOM clusters;distance computation;cosine similarity,,7,,11,,3-Nov-11,,,IEEE,IEEE Conferences
A New Method for Character Segmentation from Multi-oriented Video Words,一種基於多方位視頻詞的字符分割新方法,N. Sharma; P. Shivakumara; U. Pal; M. Blumenstein; C. L. Tan,"Griffith Univ., Gold Coast, NSW, Australia; Univ. of Malaya (UM), Kuala Lumpur, Malaysia; CVPR Unit, Indian Stat. Inst., Kolkata, India; Griffith Univ., Gold Coast, NSW, Australia; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,413,417,"This paper presents a two-stage method for multi-oriented video character segmentation. Words segmented from video text lines are considered for character segmentation in the present work. Words can contain isolated or non-touching characters, as well as touching characters. Therefore, the character segmentation problem can be viewed as a two stage problem. In the first stage, text cluster is identified and isolated (non-touching) characters are segmented. The orientation of each word is computed and the segmentation paths are found in the direction perpendicular to the orientation. Candidate segmentation points computed using the top distance profile are used to find the segmentation path between the characters considering the background cluster. In the second stage, the segmentation results are verified and a check is performed to ascertain whether the word component contains touching characters or not. The average width of the components is used to find the touching character components. For segmentation of the touching characters, segmentation points are then found using average stroke width information, along with the top and bottom distance profiles. The proposed method was tested on a large dataset and was evaluated in terms of precision, recall and f-measure. A comparative study with existing methods reveals the superiority of the proposed method.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628655,Video Document Analysis;Video Character Segmentation;Multi-oriented Document Processing;Video Character Recognition;Piece-wise Linear Segmentation Line (PLSL),Image segmentation;Noise measurement;Optical character recognition software;Character recognition;Educational institutions;Electronic mail;Image resolution,image segmentation;optical character recognition;text detection;video signal processing,bottom distance profiles;average stroke width information;background cluster;top distance profile;candidate segmentation points;text cluster identification;touching character segmentation;nontouching characters;isolated character segmentation;video text lines;two-stage method;multioriented video words;multioriented video character segmentation,,8,,10,,15-Oct-13,,,IEEE,IEEE Conferences
OCEAN: 2 1/2D Interactive Visual Data Mining of Text Documents,海洋：文本文檔的2 1 / 2D交互式可視數據挖掘,C. Jacquemin; H. Folch; S. Nugier,"LIMSI-CNRS & University Paris, France; LIMSI-CNRS, Univ. Paris, Orsay; NA",Tenth International Conference on Information Visualisation (IV'06),24-Jul-06,2006,,,383,388,OCEAN is a tool for a posteriori visual data mining that uses the output of a text miner to help users better explore a document space. Clustered documents are transformed into a hierarchical 3D representation analog to reconfigurable disk trees. An intermediary document representation allows for interface customization and offers a generic approach to 3D structuring of text miner outputs. An interactive exploration is offered that avoids tree manipulation and provides local 2 frac12D view of the disks. A user evaluation is reported that outlines the benefit of controlled document hierarchy traversal and 2 frac12D document layout,2375-0138,0-7695-2602-0,10.1109/IV.2006.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1648289,,Oceans;Data mining;Space exploration;Data visualization;Research and development;Text mining;Clustering algorithms;Navigation;Web pages;Nonhomogeneous media,data mining;data visualisation;interactive systems;text analysis;user interfaces,OCEAN;interactive visual data mining;text document;3D representation analog;reconfigurable disk tree;interface customization;2 frac12D document layout;text miner;controlled document hierarchy traversal,,1,,16,,24-Jul-06,,,IEEE,IEEE Conferences
Thematic text clustering for domain specific language model adaptation,主題文本聚類，以適應特定領域的語言模型,Z. Valsan; M. Emele,"Tangible User Interface Group, Sony Corporate Labs. Eur., Stuttgart, Germany; Tangible User Interface Group, Sony Corporate Labs. Eur., Stuttgart, Germany",2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721),2-Aug-04,2003,,,513,518,"We propose a new approach for thematic text clustering. The text clusters are used to generate domain specific language models in order to address the problem of language model adaptation. The method relies on a new discriminative n-gram based term selection process (n>l), which reduces the influence of the corpus inhomogeneity, and outputs only semantically focused n-grams as being the most representative key terms in the corpus. These key terms are then used to automatically cluster the whole document collection and generate LM out of these text clusters. Different key term selection methods are evaluated using perplexity as a measure. Automatically computed clusters are compared with manually assigned labelling according to genre information. The results of these experimental studies are presented and discussed. Compared to the manual clustering a significant performance improvement between 21.87 % and 53.12 % is observed depending on the chosen key term selection method.",,0-7803-7980-2,10.1109/ASRU.2003.1318493,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318493,,Domain specific languages;Adaptation model;Labeling;Natural languages;Strontium;Information retrieval;Laboratories;Europe;User interfaces;Speech recognition,speech recognition;text analysis;pattern clustering,thematic text clustering;domain specific language models;language model adaptation;discriminative n-gram process;term selection process;automatic clustering;document collection;perplexity measure,,1,,34,,2-Aug-04,,,IEEE,IEEE Conferences
Mixture models for co-occurrence and histogram data,共現和直方圖數據的混合模型,T. Hofmann; J. Puzicha,"Artificial Intelligence Lab., MIT, Cambridge, MA, USA; NA",Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170),6-Aug-02,1998,1,,192,194 vol.1,"Modeling and predicting co-occurrences of events is a fundamental problem of unsupervised learning. We develop a general statistical framework for analyzing co-occurrence data based on probabilistic clustering by mixture models. More specifically, we discuss three models which pursue different modeling goals and which differ in the way they define the probabilistic partitioning of the observations. Adopting the maximum likelihood principle, annealed EM algorithms are derived for parameter estimation. From the class of potential applications in pattern recognition and data analysis, we have chosen document retrieval, language modeling, and unsupervised texture segmentation to test and evaluate the proposed algorithms.",1051-4651,0-8186-8512-3,10.1109/ICPR.1998.711113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711113,,Histograms;Data analysis;Partitioning algorithms;Predictive models;Unsupervised learning;Maximum likelihood estimation;Annealing;Clustering algorithms;Parameter estimation;Pattern recognition,unsupervised learning;probability;pattern clustering;maximum likelihood estimation;simulated annealing;image segmentation,mixture models;co-occurrence;histogram data;unsupervised learning;general statistical framework;probabilistic clustering;probabilistic partitioning;maximum likelihood principle;annealed EM algorithms;data analysis;document retrieval;language modeling;unsupervised texture segmentation,,,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Improving image clustering using sparse text and the wisdom of the crowds,利用稀疏文本和人群智慧改善圖像聚類,A. Ma; A. Flenner; D. Needell; A. G. Percus,"Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711; Physics and Computational Sciences, Naval Air Warfare Center, China Lake, CA 93555; Department of Mathematical Sciences, Claremont McKenna College, Claremont, CA 91711; Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711","2014 48th Asilomar Conference on Signals, Systems and Computers",27-Apr-15,2014,,,1555,1557,"We propose a method to improve image clustering using sparse text and the wisdom of the crowds. In particular, we present a method to fuse two different kinds of document features, image and text features, and use a common dictionary or ?wisdom of the crowds??as the connection between the two different kinds of documents. With the proposed fusion matrix, we use topic modeling via non-negative matrix factorization to cluster documents.",1058-6393,978-1-4799-8297-4,10.1109/ACSSC.2014.7094725,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7094725,,Sparse matrices;Internet;Measurement;Encyclopedias;Electronic publishing;Dictionaries,feature extraction;image classification;matrix decomposition;text analysis;visual databases;word processing,image clustering;sparse text;image feature;text feature;wisdom-of-the crowds;fusion matrix;nonnegative matrix factorization;cluster documents,,2,,9,,27-Apr-15,,,IEEE,IEEE Conferences
Frequent term based peer-to-peer text clustering,基於頻繁術語的對等文本聚類,Qing He; Tingting Li; Fuzhen Zhuang; Zhongzhi Shi,"The Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, China; The Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, China; The Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, China; The Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, China",2010 Third International Symposium on Knowledge Acquisition and Modeling,29-Nov-10,2010,,,352,355,"Text clustering is an important technology for automatically structuring large document collections. It is much more valuable in peer-to-peer networks. The high dimensionality of documents means much more communication could be saved if each node could get the approximate clustering result by distributed algorithm instead of transferring them into a center and do the clustering. Most of the existing text clustering algorithms in unstructured peer-to-peer networks are based on K-means algorithm. A problem of those algorithms is that the clustering quality may decreased with the increase of the network size. In this paper, we propose a text clustering algorithm based on frequent term sets for peer-to-peer networks. It requires relatively lower communication volume while achieving a clustering result whose quality will not be affected by the size of the network. Moreover, it gives a term set describing each cluster, which makes it possible for people to have a clear comprehension for the clustering result, and facilitates the users to find resource in the network or manage the local documents in accordance with the whole network.",,978-1-4244-8007-4,10.1109/KAM.2010.5646177,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646177,distributed data mining;peer-to-peer;text clustering;frequent term set,Copper,data mining;pattern clustering;peer-to-peer computing;text analysis,frequent term sets;peer-to-peer text clustering;K-means algorithm,,,,17,,29-Nov-10,,,IEEE,IEEE Conferences
Contextual image search with keyword and image input,使用關鍵字和圖像輸入進行上下文圖像搜索,M. Shiyamala; G. Kalaiarasi,"Department of Computer Science and Engineering, Dhanalakshmi Srinivasan college of Engineering and Technology, Mamallapuram, Chennai, Tamilnadu, India; Department of Computer Science and Engineering Dhanalakshmi Srinivasan college of Engineering and Technology, Mamallapuram, Chennai, Tamilnadu, India",International Conference on Information Communication and Embedded Systems (ICICES2014),9-Feb-15,2014,,,1,5,"In this paper, we propose a novel image search scheme is contextual image search with keyword input. It is different from conventional image search schemes. it consist of three step process, first one is context extraction to distinguish the image entities of the same name, second step is conceptualization to convert context into a list of weighted concepts using wikification. Wikification is process, which links the noun phrases in plain text content to the corresponding articles third step is tri stage image clustering to process the conceptualized contexts. Those three stages are Tag context clustering, Text context clustering and Expansion clustering. Where Tag context clustering clusters with most reliable signal using HAC (Hierarchical Agglomerative Clustering), Text context clustering uses the text content of all images to form the cluster using HAC algorithm and Expansion clustering using inverse document frequency. Finally the web images are clustered. The contextual image search based on keyword system presents a separate interface (e.g., text input box) to allow users to submit a query; this system enables users to find the images based on different name entity images. Though the keyword based contextual image search provides result it doesn't find the duplicate images. Thus the conceptual image search with image input concept is proposed to find the duplicate images. Contextual image search with input image, the users can select an clustered image as the search query from Web pages or other documents they are reading. Using MPEG-7 descriptors to ?annotate??an image automatically. Finally it determine the duplicate images on search result.",,978-1-4799-3834-6,10.1109/ICICES.2014.7033765,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033765,Image Search;Clustering;Conceptualization;Contextual query identification,Context;Visualization;Search engines;Semantics;Feature extraction;Educational institutions;Web pages,image coding;image retrieval;Internet;pattern clustering;Web sites,contextual image search;keyword input;image input;three step process;context extraction;wikification;noun phrases;tristage image clustering;conceptualized contexts;tag context clustering;text context clustering;expansion clustering;HAC;hierarchical agglomerative clustering;Web images;name entity images;conceptual image search;search query;Web pages;MPEG-7 descriptors,,,,13,,9-Feb-15,,,IEEE,IEEE Conferences
Automatic web page logo detection (Logo matching using semantic relevance feedback),自動網頁徽標檢測（使用語義相關性反饋進行徽標匹配）,S. Balan; P. Ponmuthuramalingam,"Department of Computer Science, Government Arts College (Autonomous), Coimbatore, Tamilnadu, India; Department of Computer Science, Government Arts College (Autonomous) Coimbatore, Tamilnadu, India","2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)",26-Jun-17,2016,,,1888,1893,"In the past few decades many information's are stored in World Wide Web as web pages and documents, it is increased by huge amount of data so the problem persist for searching a particular document. There are various document image retrieval image methods are available such as classification, clustering and graph techniques are designed to detect the problems in document image retrieval system. This research is concerned with study and analysis of focusing document image retrieval and logo matching in journal database. There are various national and international journals are available in online to representing their logo in document. Retrieval framework is effective not only in improving retrieval performance in a given query session, but also utilizes the knowledge learnt from previous queries to reduce the number of iterations in following queries. This paper describes methods and techniques for document image retrieval analysis, logo matching and survey includes the current state of research problems in information retrieval, document image retrieval, relevance feedback and logo based text retrieval.",,978-1-5090-4620-1,10.1109/SCOPES.2016.7955774,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955774,Document Image Retrieval;Logo Retrieval;Textual Search;Semantic Relevance Feedback,Feature extraction;Image retrieval;Semantics;Image recognition;Shape;Signal processing algorithms;Indexing,electronic publishing;graph theory;image classification;image matching;image retrieval;Internet;learning (artificial intelligence);object detection;pattern clustering;relevance feedback;text analysis,automatic Web page logo detection;logo matching;semantic relevance feedback;World Wide Web;document searching;document image retrieval methods;classification;clustering;graph techniques;journal database;international journals;query session;knowledge learning;image retrieval analysis;logo based text retrieval,,,,36,,26-Jun-17,,,IEEE,IEEE Conferences
PICCIL: Interactive Learning to Support Log File Categorization,PICCIL：交互式學習以支持日誌文件分類,D. Loewenstern; Sheng Ma; A. Salahshour,"IBM T.J. Watson Research & AC; IBM TJ. Watson Res. & AC, Hawthorne, NY; IBM TJ. Watson Res. & AC, Hawthorne, NY",Second International Conference on Autonomic Computing (ICAC'05),6-Sep-05,2005,,,311,312,"Motivated by the real-world application of categorizing system log messages into defined situation categories, this paper describes an interactive text categorization method, PICCIL, that leverages supervised machine learning to reduce the burden of assigning categories to documents in large finite data sets but, by coupling human expertise to the machine learning, does so without sacrificing accuracy. PICCIL uses keywords and keyword rules both to preclassify documents and to assist in the manual process of grouping and reviewing documents. The reviewed documents, in turn, are used to refine the keyword rules iteratively to improve subsequent grouping and document review. We apply PICCIL to the problem of assigning semantic situation labels to the entries of a catalog of log events to support on-line labeling of log events",,0-7695-2276-9,10.1109/ICAC.2005.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1498078,,Text categorization;Machine learning;Labeling;Supervised learning;Humans;Clustering methods,cataloguing;classification;document handling;file organisation;learning (artificial intelligence);message passing,PICCIL;interactive learning;log file categorization;system log message categorization;interactive text categorization;supervised machine learning;category assignment;finite data set;keyword rules;document classification;document grouping,,1,,10,,6-Sep-05,,,IEEE,IEEE Conferences
A color classification algorithm,顏色分類算法,K. Iwata; G. Marcu,"Graphica Computer Corp., Tokyo, Japan; Graphica Computer Corp., Tokyo, Japan",Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93),6-Aug-02,1993,,,726,729,"The color classification process requires that a color image be partitioned into a set of uniform colored regions. A method of automatic classification of a color picture, based on the advantages of a high-speed binary tree splitting algorithm and completed with a stopping criterion derived from color cluster detection methods, is presented. The histogram of distance of the current color point to the center of the investigated cluster is analyzed to stop the binary splitting algorithm. The color space was divided and labeled accordingly to the closest color cluster or region corresponding to a color of the color set. A dynamic dividing method of the color space was used. A post-processing algorithm makes it possible to filter out the unwanted transition colors by absorption and erosion.<>",,0-8186-4960-7,10.1109/ICDAR.1993.395634,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=395634,,Classification algorithms;Clustering algorithms;Partitioning algorithms;Classification tree analysis;Binary trees;Histograms;Image color analysis;Algorithm design and analysis;Filters;Absorption,image colour analysis;image classification;image segmentation;tree data structures,image partitioning;colour space labelling;colour filtering;color classification algorithm;uniform colored regions;high-speed binary tree splitting algorithm;stopping criterion;color cluster detection methods;histogram;closest color cluster;dynamic dividing method;post-processing algorithm;unwanted transition colors;absorption;erosion,,1,,5,,6-Aug-02,,,IEEE,IEEE Conferences
ClRank: A Method for Keyword Extraction from Web Pages Using Clustering and Distribution of Nouns,ClRank：一種使用名詞的聚類和分佈從網頁中提取關鍵字的方法,M. Rezaei; N. Gali; P. Fr瓣nti,"Sch. of Comput., Univ. of Eastern Finland, Joensuu, Finland; Sch. of Comput., Univ. of Eastern Finland, Joensuu, Finland; Sch. of Comput., Univ. of Eastern Finland, Joensuu, Finland",2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),4-Feb-16,2015,1,,79,84,"Text analysis of a web page is more difficult than the analysis of the text of normal document due to the presence of additional information, such as HTML structure, styling codes, irrelevant text, and presence of hyperlinks. In this paper, we propose an unsupervised method to extract keywords from a web page. The method extracts unigram nouns by applying part of speech tagging on the text. It then clusters the nouns based on their semantic similarity. It selects a number of keywords from the highest scored clusters. Experimental results show that our method outperforms state-of-the-art TextRank by 13 % in precision, 6 % in recall, and 10 % in F-measure.",,978-1-4673-9618-9,10.1109/WI-IAT.2015.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396783,web mining;keywords extraction;clustering;semantic analysis,Web pages;Semantics;HTML;Tagging;Clustering algorithms;Mice;Speech,information retrieval;Internet;pattern clustering;text analysis;unsupervised learning,ClRank method;keyword extraction;Web pages;clustering;text analysis;noun distribution;unsupervised learning method;part-of-speech tagging;semantic similarity;unigram noun extraction;TextRank,,1,,34,,4-Feb-16,,,IEEE,IEEE Conferences
An Adaptive Hash-Based Text Deduplication for ADS-B Data-Dependent Trajectory Clustering Problem,ADS-B數據相關的軌跡聚類問題的基於哈希的自適應文本重複數據刪除,T. Tran; D. Pham; Q. Duong; A. Mai,"ICT Department, John von Neumann Institute, Ho Chi Minh, Vietnam; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Air Traffic Management Research Institute, Singapore, Singapore; ICT Department, John von Neumann Institute, Ho Chi Minh, Vietnam; ICT Department, John von Neumann Institute, Ho Chi Minh, Vietnam",2019 IEEE-RIVF International Conference on Computing and Communication Technologies (RIVF),16-May-19,2019,,,1,6,"The Automatic Dependent Surveillance-Broadcast (ADS-B) protocol is equipped in aircraft as an alternative to secondary radar. This emerging technology produces such a prospective type of data to effectively broadcast the aircraft's status (location, velocity, etc.,) in a specific area, which is very useful in air traffic management (ATM). However, there is still a limited number of advanced studies from machine learning/data mining perspectives relying on this kind of data in ATM research. On the other hand, Locality Sensitive Hashing (LSH) is a data mining technique often used to find similar items in the data with high-dimension properties. It is thus relatively suitable for handling with trajectories data to group similar flight paths. From these factors, we reveal in this paper an adaptive LSH- based algorithm, used in near-duplicated documents detection, for the problem of clustering the nearest trajectories by representing the trajectories as a bag-of-words used popularly in text mining. To illustrate our proposed method, an experiment is designed and carried out in thirty successive days, employing the raw ADS-B data collected from FlightAware for the case of Changi International Airport, Singapore. The evaluation based on Silhouette score shows promising results of measuring the clustering performance.",2162-786X,978-1-5386-9313-1,10.1109/RIVF.2019.8713722,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713722,ADS-B;Trajectory Clustering;Air Traffic Control;Locality Sensitive Hashing,Trajectory;Aircraft;Clustering algorithms;Airports;Radar;Data mining;Indexes,air traffic;aircraft;airports;data mining;learning (artificial intelligence);pattern clustering;surveillance;text analysis,adaptive hash-based text deduplication;aircraft;secondary radar;air traffic management;data mining technique;text mining;machine learning;ATM;locality sensitive hashing;LSH;ADS-B;automatic dependent surveillance-broadcast protocol;data-dependent trajectory clustering problem;flight path;airport;Silhouette score,,,,23,,16-May-19,,,IEEE,IEEE Conferences
Improving Search Result Clustering by Enriching Snippets with Word2Vec Model,通過使用Word2Vec模型豐富代碼段來改善搜索結果的聚類,Y. Nan; L. Yaping; L. Qing,"Dept. of Comput. Sci. & Technol., Renmin Univ. of China, Beijing, China; Dept. of Comput. Sci. & Technol., Renmin Univ. of China, Beijing, China; Dept. of Comput. Sci. & Technol., Renmin Univ. of China, Beijing, China",2017 14th Web Information Systems and Applications Conference (WISA),9-Apr-18,2017,,,33,37,"Search Result Clustering (SRC) is an approach to solve the problems of web search engines under user's broad or ambiguous queries and no clues to find exact information in a long returned list. SRC groups the returned list and outputs a semantic structured organization to help users to find the desired information quickly. In a general way, the search engine results consist of concise information(snippet) about matching page documents. Because the snippet is short and carries few information, the clustering performance based on traditional TF-IDF weight is very low. An effective way to solve this problem is to enrich snippets according the semantic relationship from external text corpus. In this paper, we propose a new snippet enriching approach throughout word2vec model. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various Natural Language Processing(NLP) tasks. We propose to use the top-n similar words in word2vec model to enrich snippets and we still use traditional TF-IDF weights schema to select features. In order to demonstrate the effectiveness of our method, we design intensive experiments to evaluate new method and baseline methods. The result of the analysis shows that proposed method outperforms baseline approach significantly.",,978-1-5386-4806-3,10.1109/WISA.2017.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8332582,search result clustering;snippet;word embedding,Semantics;Testing;Task analysis;Search engines;Neural networks;Web search;Clustering algorithms,information retrieval;Internet;learning (artificial intelligence);natural language processing;pattern clustering;query processing;search engines;text analysis,search engine results;snippet enriching approach;word2vec model;web search engines;desired information;concise information;snippets;returned list;SRC;search result clustering;Natural Language Processing tasks;NLP tasks;TF-IDF weights schema,,,,21,,9-Apr-18,,,IEEE,IEEE Conferences
A comparison of SOM based document categorization systems,基於SOM的文檔分類系統的比較,X. Luo; A. N. Zincir-Heywood,"Fac. of Comput. Sci., Dalhousie Univ., Halifax, NS, Canada; Fac. of Comput. Sci., Dalhousie Univ., Halifax, NS, Canada","Proceedings of the International Joint Conference on Neural Networks, 2003.",26-Aug-03,2003,3,,1786,1791 vol.3,"This paper describes the development and evaluation of two unsupervised learning mechanisms for solving the automatic document categorization problem. Both mechanisms are based on a hierarchical structure of self-organizing feature maps. Specifically, one architecture is based on the vector space model whereas the other one is based on a code-books model. Results show that the latter architecture performs better than the first one which is based on the quality of the returned clusters.",1098-7576,0-7803-7898-9,10.1109/IJCNN.2003.1223678,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1223678,,Information retrieval;Frequency;Unsupervised learning;Organizing;Vocabulary;Clustering algorithms;Computer science;Computer architecture;Fingerprint recognition;Nearest neighbor searches,self-organising feature maps;unsupervised learning;pattern clustering;classification,document categorization systems;self-organizing feature maps;unsupervised learning;hierarchical structure;vector space model;information retrieval systems;pattern clustering,,5,,17,,26-Aug-03,,,IEEE,IEEE Conferences
Tag-Weighted Dirichlet Allocation,標籤加權狄利克雷分配,S. Li; G. Huang; R. Tan; R. Pan,"Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China; Sch. of Inf. Sci. & Technol., Sun Yat-sen Univ., Guangzhou, China",2013 IEEE 13th International Conference on Data Mining,3-Feb-14,2013,,,438,447,"In the past two decades, there has been a huge amount of document data with rich tag information during the evolution of the Internet, which can be called semi-structured data. These semi-structured data contain both unstructured features (e.g., plain text) and metadata, such as tags in html files or author and venue information in research articles. It's of great interest to model such kind of data. Most previous works focused on modeling the unstructured data. Some other methods have been proposed to model the unstructured data with specific tags. To build a general model for semi-structured documents remains an important problem in terms of both model fitness and efficiency. In this paper, we propose a novel method to model the tagged documents by a so-called Tag-Weighted Dirichlet Allocation (TWDA). TWDA is a framework that leverages both the tags and words in each document to infer the topic components for the documents. This allows not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining (e.g., classification, clustering, and recommendations). Moreover, TWDA can automatically infer the probabilistic weights of tags for each document, that can be used to predict the tags in one document. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. The experimental results show the effectiveness, efficiency and robustness of our TWDA approach by comparing it with the state-of-the-art methods on four corpora in document modeling, tags prediction and text classification.",2374-8486,978-0-7695-5108-1,10.1109/ICDM.2013.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729528,Tag-Weighted;Dirichlet allocation;topic model;variational inference,Data models;Vectors;Resource management;Mathematical model;Predictive models;Probabilistic logic;Analytical models,data mining;expectation-maximisation algorithm;inference mechanisms;parameter estimation;pattern classification;pattern clustering;probability;recommender systems;text analysis,tag-weighted Dirichlet allocation;tag information;plain text;metadata;HTML files;author information;venue information;research articles;semistructured document data;model fitness;model efficiency;tagged document modelling;TWDA framework;document tag leveraging;document word leveraging;topic components;document-topic distribution learning;topic-word distribution learning;tag-topic distribution inference;text mining;text clustering;text recommendations;probabilistic weights;variational inference method;EM algorithm;model parameter estimation;tag prediction;text classification,,8,,30,,3-Feb-14,,,IEEE,IEEE Conferences
Fuzzy Learning Vector Quantization with Size and Shape Parameters,具有尺寸和形狀參數的模糊學習矢量量化,C. Borgelt; A. Nurnberger; R. Kruse,"Dept. of Knowledge Process. & Language Eng., Magdeburg Otto-von-Guericke-Univ.; Dept. of Knowledge Process. & Language Eng., Magdeburg Otto-von-Guericke-Univ.; Dept. of Knowledge Process. & Language Eng., Magdeburg Otto-von-Guericke-Univ.","The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.",20-Jun-05,2005,,,195,200,"We study an extension of fuzzy learning vector quantization that draws on ideas from the more sophisticated approaches to fuzzy clustering, enabling us to find fuzzy clusters of ellipsoidal shape and differing size with a competitive learning scheme. This approach may be seen as a kind of online fuzzy clustering, which can have advantages w.r.t. the execution time of the clustering algorithm. We demonstrate the usefulness of our approach by applying it to document collections, which are, in general, difficult to cluster due to the high number of dimensions and the special distribution characteristics of the data",1098-7584,0-7803-9159-4,10.1109/FUZZY.2005.1452392,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1452392,,Vector quantization;Shape;Clustering algorithms;Fuzzy sets;Covariance matrix;Partitioning algorithms;Knowledge engineering;Maximum likelihood estimation;Web pages,document handling;fuzzy set theory;learning (artificial intelligence);pattern clustering;vector quantisation,fuzzy learning vector quantization;size parameter;shape parameter;ellipsoidal shape;competitive learning;online fuzzy clustering;document collections,,1,,31,,20-Jun-05,,,IEEE,IEEE Conferences
A Novel Method to Predict Query Performance Based on Cluster Score,基於聚類得分的查詢性能預測新方法,W. Wang; D. Peng,"Bus. Intell. Lab., Univ. of Sci. & Technol. of China, Hefei; Bus. Intell. Lab., Univ. of Sci. & Technol. of China, Hefei",2008 International Symposium on Knowledge Acquisition and Modeling,30-Dec-08,2008,,,637,640,"Predicting query performance has been recently recognized by the information retrieval community as a crucial issue in information retrieval systems. In this paper, we present a novel method for predicting query performance by computing cluster score. For a fixed query, cluster score quantifies and reflects the correlation between retrieved document collections and each query term and the distribution of this correlation simultaneously. Experiments demonstrate that cluster score significantly and consistently correlates with query performance in a variety of TREC test collections. We compare cluster score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that cluster score performs better than, or at least as well as clarity score.",,978-0-7695-3488-6,10.1109/KAM.2008.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732905,Information Retrieval;Query Performance Prediction;Cluster Score,Information retrieval;Testing;Search engines;Robustness;Frequency estimation;Knowledge acquisition;Predictive models;Metasearch;History;Feedback,query processing,query performance prediction;cluster score;information retrieval,,,,10,,30-Dec-08,,,IEEE,IEEE Conferences
Object-oriented clustering of VHR panchromatic images using a nonparametric bayesian model embeded with a latent scene,使用嵌入潛在場景的非參數貝葉斯模型對VHR全色圖像進行面向對象的聚類,Y. Shu; H. Tang; J. Li; J. Yue,"State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, 100875, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, 100875, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, 100875, China; College of Resources Science & Technology, Beijing Normal University, 100875, China",2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS,27-Jan-14,2013,,,1497,1500,"LDA model has successfully been used to analyzing satellite images. However there are two cucial problems: (1) the number of clusters needs being given in advance, and (2) all documents share a Dirichlet prior. To solve the problems, a novel model include multiple LDAs with variable topics are proposed to cluster satellite images. Each LDA in the model is dedicated to model one kind of natural scene in satellite images. Gibbs sampling method is used to discover natural scenes and learning model parameters. The effect on number of topic estimation is analyzed and then the result of our model is compared with other models. The results indicate that the proposed algorithm outperforms the other comparing models in our experiment.",2153-7003,978-1-4799-1114-1,10.1109/IGARSS.2013.6723070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6723070,LDA;scene understanding;image clustering,Abstracts;Correlation;Indexing;Optical imaging;Optical sensors;Resource management,geophysical image processing;geophysical techniques;object-oriented methods;remote sensing,Gibbs sampling method;natural scene;cluster satellite images;satellite images;LOA model;latent scene;nonparametric Bayesian model;VHR panchromatic images;object-oriented clustering,,1,,10,,27-Jan-14,,,IEEE,IEEE Conferences
Efficient similarity-based alignment of temporally-situated graph nodes with Apache Spark,基於時間的圖形節點與Apache Spark的基於相似度的高效對齊,H. Naacke; K. Li; B. Amann; O. Cur矇,"Sorbonne Universit矇,CNRS, Laboratoire d?Informatique de Paris,F-75005; Sorbonne Universit矇,CNRS, Laboratoire d?Informatique de Paris,F-75005; Sorbonne Universit矇,CNRS, Laboratoire d?Informatique de Paris,F-75005; LIGM (UMR 8049) CNRS, ENPC, ESIEE, UPEM,Marne-la-Vall矇e,France",2019 IEEE International Conference on Big Data (Big Data),24-Feb-20,2019,,,4793,4798,"Topic evolution networks are widely used to represent the evolution of research topics in scientific document archives. These networks might contain thousands of topics and alignment edges which are computed by comparing millions of topic pairs with some similarity function. In this work, we are addressing the problem of computing a very large number cosine-based topic alignments on top of Apache Spark. We present the native map-reduce implementation proposed by Spark and a more efficient implementation which is tuned for alignment computation. Both implementations are evaluated on three real-world datasets.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9005483,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005483,,Big Data;Cluster computing;Document handling;Graph theory;Parallel processing,cluster computing;document handling;graph theory;parallel processing,Apache Spark;temporally-situated graph nodes;topic evolution networks;alignment edges;similarity-based alignment;scientific document archive;cosine-based topic alignments;map-reduce implementation,,,,12,,24-Feb-20,,,IEEE,IEEE Conferences
A Feature Reduction Technique for Improved Web Page Clustering,一種改進的網頁聚類的特徵約簡技術,E. A. Mohamed; S. R. El-beltagy; S. El-Gamal,"Computer Science Department, Faculty of Computers and Information, Cairo University, 5 Tharwat Street, Orman, Giza 12613, Egypt. ehab@mail.usa.com; Computer Science Department, Faculty of Computers and Information, Cairo University, 5 Tharwat Street, Orman, Giza 12613, Egypt. samhaa@computer.org; Computer Science Department, Faculty of Computers and Information, Cairo University, 5 Tharwat Street, Orman, Giza 12613, Egypt. s.elgamal@fci-cu.edu.eg",2006 Innovations in Information Technology,12-Feb-07,2006,,,1,5,"This paper presents a new approach for text feature reduction that can be used to speed up Web page clustering. The technique is based on using a classified corpus in order to build a dictionary that captures the importance of various terms in different categories. The dictionary is then used to translate an input document's feature vector into a smaller one. Two experiments carried out in order to evaluate this technique are also presented. The evaluation results show that when used, the presented technique results in much faster and more accurate clustering, than when it is not. They also show that despite being simpler, the presented technique can give results comparable to those of currently widely used feature reduction techniques",,1-4244-0673-0,10.1109/INNOVATIONS.2006.301930,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4085445,,Web pages;Dictionaries;Large scale integration;Frequency;Indexing;Independent component analysis;Information retrieval;Matrices;Computer science;Noise reduction,Internet;pattern classification;pattern clustering;text analysis,Web page clustering;text feature reduction;classified corpus;dictionary,,1,,11,,12-Feb-07,,,IEEE,IEEE Conferences
Information navigation by clustering and summarizing query results,通過對查詢結果進行聚類和匯總來進行信息導航,D. G. Roussinov; M. J. McQuaid,"Sch. of Inf. Studies, Syracuse Univ., NY, USA; NA",Proceedings of the 33rd Annual Hawaii International Conference on System Sciences,6-Aug-02,2000,,,9 pp. vol.1,,"We have explored and evaluated a novel approach to information seeking grounded in the idea of summarizing query results through automated document clustering. The user starts with a natural language description of the needed information and navigates the information space through the interaction of the system. We implemented a prototype allowing searches of a significant portion of the entire World Wide Web. In a laboratory experiment, subjects searched the WWW for answers to a given set of questions. Our results indicate that our prototype improved search peformance, presumably through better understanding of query results. In addition, we analyzed interaction patterns and the effects of such parameters as subject skills and task peculiarities.",,0-7695-0493-0,10.1109/HICSS.2000.926689,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=926689,,Navigation;Prototypes;World Wide Web;Internet;Search engines;Humans;Laboratories;Electrical capacitance tomography;Ear;Pattern analysis,information resources;query processing,information navigation;clustering;query results summarizing;automated document clustering;natural language description;World Wide Web,,3,,28,,6-Aug-02,,,IEEE,IEEE Conferences
Unsupervised Topic Detection based on 2D Vector Space model using Apriori Algorithm and NLP,基於Apriori算法和NLP的2D向量空間模型的無監督主題檢測,M. George,"Department of Information Technology, Dubai Municipality, Dubai, UAE",2018 Thirteenth International Conference on Digital Information Management (ICDIM),26-Sep-19,2018,,,279,283,"Topic modelling is an approach in data mining, use machine learning methods to discover patterns in large amount of unstructured text. It takes a collection of documents and group the words into clusters of words that we call Bag of words, and identify topics by using process of similarity. Topic modelling provides us with methods to organize, understand and summarize large collections of textual information. There are a lot of approaches have been exposed for Topic modelling, the most in use are Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) and explicit semantic analysis (ESA). In our study we describing an approach to refine Topic detection based on 2d vector space model VSM by using Apriori algorithm along with Natural language processing, to form a better connected terms in vector space for clean engagement with the query.",,978-1-5386-5244-2,10.1109/ICDIM.2018.8846982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846982,Vector space model;Apriori;Unsupervised Topic Models;Natural language processing,Natural language processing;Itemsets;Clustering algorithms;Semantics;Two dimensional displays;Data models,data mining;document handling;information retrieval;learning (artificial intelligence);natural language processing;pattern clustering;text analysis,vector space;Apriori algorithm;explicit semantic analysis;latent dirichlet allocation;latent semantic analysis;bag of words;NLP;data mining;machine learning methods;unsupervised topic detection;topic modelling,,,,23,,26-Sep-19,,,IEEE,IEEE Conferences
Machine learning of engineering diagnostic knowledge from unstructured verbatim text descriptions,從非結構化逐字描述中機器學習工程診斷知識,Yinghao Huang; Y. L. Murphey; Yao Ge,"Dept. of Electrical and Computer Engineering, University of Michigan-Dearborn, 48128, USA; Dept. of Electrical and Computer Engineering, University of Michigan-Dearborn, 48128, USA; Information Technology and Services, Ford Motor Company, Dearborn, Michigan, USA",2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM),16-Sep-13,2013,,,46,52,"This paper presents our research in text mining for discovering important engineering fault diagnostic knowledge from unstructured and verbatim text descriptions. In particular we focus on developing machine learning algorithms for detecting documents that contain descriptions of systematic failures and root causes to the faults. We developed a machine algorithm based on entropy analysis to extract an A-word list, a list of words that are important to characterize the documents of interests, a vector space model to represent features of important documents, and a constraint based k-means clustering algorithm to generate high purity clusters for use in detecting important documents. We applied the algorithms to automotive diagnostic text data, which are unstructured and verbatim descriptions by customers and technicians that contain many typos and self-invented terms. We were able to reduce a list of 2183 words to a list of 137 important words. The classification system generated by these machine learning algorithms showed high recall and accuracy in detecting important diagnostic descriptions.",,978-1-4673-5895-8,10.1109/CIDM.2013.6597216,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597216,machine learning;text mining;engineering diagnostics;important document detection,Vehicles;Entropy;Clustering algorithms;Machine learning algorithms;Training data;Vectors;Automotive engineering,automotive engineering;data mining;entropy;failure analysis;fault diagnosis;learning (artificial intelligence);pattern clustering;text analysis;word processing,engineering diagnostic knowledge;unstructured verbatim text descriptions;text mining;engineering fault diagnostic knowledge;machine learning algorithms;systematic failures;machine algorithm;entropy analysis;A-word list extraction;vector space model;constraint based k-means clustering algorithm;document detection;automotive diagnostic text data;classification system,,1,,15,,16-Sep-13,,,IEEE,IEEE Conferences
Unsupervised Feature Learning for Writer Identification and Writer Retrieval,用於作者識別和作者檢索的無監督特徵學習,V. Christlein; M. Gropp; S. Fiel; A. Maier,"Pattern Recognition Lab., Friedrich-Alexander-Univ. Erlangen-Nurnberg, Erlangen, Germany; Pattern Recognition Lab., Friedrich-Alexander-Univ. Erlangen-Nurnberg, Erlangen, Germany; Comput. Vision Lab., Tech. Univ. Wien, Vienna, Austria; Pattern Recognition Lab., Friedrich-Alexander-Univ. Erlangen-Nurnberg, Erlangen, Germany",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR),29-Jan-18,2017,1,,991,997,"Deep Convolutional Neural Networks (CNN) have shown great success in supervised classification tasks such as character classification or dating. Deep learning methods typically need a lot of annotated training data, which is not available in many scenarios. In these cases, traditional methods are often better than or equivalent to deep learning methods. In this paper, we propose a simple, yet effective, way to learn CNN activation features in an unsupervised manner. Therefore, we train a deep residual network using surrogate classes. The surrogate classes are created by clustering the training dataset, where each cluster index represents one surrogate class. The activations from the penultimate CNN layer serve as features for subsequent classification tasks. We evaluate the feature representations on two publicly available datasets. The focus lies on the ICDAR17 competition dataset on historical document writer identification (Historical-WI). We show that the activation features we trained without supervision are superior to descriptors of state-of-the-art writer identification methods. Additionally, we achieve comparable results in the case of handwriting classification using the ICFHR16 competition dataset on historical Latin script types (CLaMM16).",2379-2140,978-1-5386-3586-5,10.1109/ICDAR.2017.165,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270096,unsupervised feature learning;writer identification;writer retrieval;deep learning;document analysis,Training;Task analysis;Encoding;Support vector machines;Feature extraction;Kernel;Machine learning,document image processing;feature extraction;feedforward neural nets;handwriting recognition;handwritten character recognition;image classification;optical character recognition;unsupervised learning,writer retrieval;unsupervised feature learning;handwriting classification;historical document writer identification;publicly available datasets;penultimate CNN layer;training dataset;surrogate class;deep residual network;CNN activation features;annotated training data;deep learning methods;character classification;supervised classification tasks;Convolutional Neural Networks,,9,,42,,29-Jan-18,,,IEEE,IEEE Conferences
Research on keywords Extraction of Chinese documents based on TEXT-NET,基於TEXT-NET的中文文檔關鍵詞提取研究,GangLiu; Zhouwei Zhai,"School of Electronic and Engineering, Beijing University of Posts and Telecommunications, China; School of Electronic and Engineering, Beijing University of Posts and Telecommunications, China",2011 International Conference on Electric Information and Control Engineering,27-May-11,2011,,,6074,6077,"A keywords Extraction algorithm of Chinese documents based on TEXT-NET is proposed. By using Semantic similarity computation of Howmet theory, a text is mapped a TEXT-NET, and then combined with complex network theory and statistical methods to extract keywords. Experimental results show that the recall and precision rate has increased compared with small-world model method and statistical methods, and this method is more flexible because it is independent of fields.",,978-1-4244-8039-5,10.1109/ICEICE.2011.5777961,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777961,chinese information processing;semantic similarity;complex network;keywords extraction;text network;clustering coefficient;average path length,Semantics;Complex networks;Feature extraction;Computers;Frequency measurement;Computational modeling;Analytical models,complex networks;natural language processing;statistical analysis;text analysis,Chinese documents;TEXT-NET;keywords extraction algorithm;semantic similarity computation;Howmet theory;complex network theory;statistical methods;small-world model method,,,,15,,27-May-11,,,IEEE,IEEE Conferences
Information retrieval on mind maps - what could it be good for?,思維導圖上的信息檢索-有什麼用？,J. Beel; B. Gipp; J. Stiller,"Otto-von-Guericke University, Computer Science/ITI/VLBA-Lab, Magdeburg, Germany; Otto-von-Guericke University, Computer Science/ITI/VLBA-Lab, Magdeburg, Germany; University of Wolfenb羹ttel, Karl Scharfenberg Faculty, Germany","2009 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing",28-Dec-09,2009,,,1,4,"Mind maps are used by millions of people. In this paper we present how information retrieval on mind maps could be used to enhance expert search, document summarization, keyword based search engines, document recommender systems and determining word relatedness. For instance, words in a mind map could be used for creating a skill profile of the mind maps' author and hence enhance expert search. This paper is a research-in-progress paper which means no research results are presented but only ideas.",,978-963-9799-76-9,10.4108/ICST.COLLABORATECOM2009.8298,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5364172,data mining;information retrieval;mind maps;expert seach;document clustering;document classification,Information retrieval;Data mining;Search engines;Computer science;Recommender systems;Displays;Employee rights;Databases;Electronic mail;Decision making,data mining;information retrieval;recommender systems,information retrieval;mind maps;document summarization;keyword based search engines;document recommender systems;expert search;data mining,,6,,39,,28-Dec-09,,,IEEE,IEEE Conferences
Text categorization using the semi-supervised fuzzy c-means algorithm,使用半監督模糊c均值算法的文本分類,M. Benkhalifa; A. Bensaid; A. Mouradi,"Sch. of Sci. & Eng, AlAkhawayn, Morocco; NA; NA",18th International Conference of the North American Fuzzy Information Processing Society - NAFIPS (Cat. No.99TH8397),6-Aug-02,1999,,,561,565,"Text categorization (TC) is the automated assignment of text documents to predefined categories based on document contents. TC has become very important in the information retrieval area, where information needs have tremendously increased with the rapid growth of textual information sources such as the Internet. We compare, for text categorization, two partially supervised (or semi-supervised) clustering algorithms: the Semi-Supervised Agglomerative Hierarchical Clustering (ssAHC) algorithm (A. Amar et al., 1997) and the Semi-Supervised Fuzzy-c-Means (ssFCM) algorithm (M. Amine et al., 1996). This (semi-supervised) learning paradigm falls somewhere between the fully supervised and the fully unsupervised learning schemes, in the sense that it exploits both class information contained in labeled data (training documents) and structure information possessed by unlabeled data (test documents) in order to produce better partitions for test documents. Our experiments, make use of the Reuters 21578 database of documents and consist of a binary classification for each of the ten most populous categories of the Reuters database. To convert the documents into vector form, we experiment with different numbers of features, which we select, based on an information gain criterion. We verify experimentally that ssFCM both outperforms and takes less time than the Fuzzy-c-Means (FCM) algorithm. With a smaller number of features, ssFCM's performance is also superior to that of ssAHC's. Finally ssFCM results in improved performance and faster execution time as more weight is given to training documents.",,0-7803-5211-4,10.1109/NAFIPS.1999.781756,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=781756,,Text categorization;Clustering algorithms;Unsupervised learning;Partitioning algorithms;Testing;Spatial databases;Classification algorithms;Supervised learning;Information retrieval;Internet,text analysis;information retrieval;learning (artificial intelligence);fuzzy set theory;knowledge based systems,text categorization;semi-supervised fuzzy c-means algorithm;automated assignment;text documents;predefined categories;document contents;information retrieval;textual information sources;Internet;Semi-Supervised Agglomerative Hierarchical Clustering algorithm;Semi-Supervised Fuzzy-c-Means algorithm;class information;labeled data;training documents;structure information;unlabeled data;Reuters 21578 database;binary classification;information gain criterion,,14,,15,,6-Aug-02,,,IEEE,IEEE Conferences
An interactive classification of Web documents by self-organizing maps and search engines,通過自組織地圖和搜索引擎對Web文檔進行交互式分類,K. Hatano; R. Sano; Y. Duan; K. Tanaka,"Graduate Sch. of Sci. & Technol., Kobe Univ., Japan; NA; NA; NA",Proceedings. 6th International Conference on Advanced Systems for Advanced Applications,6-Aug-02,1999,,,35,42,"We propose an effective classification view mechanism for hypertext data such as Web documents based on Kohonen's self-organizing map (SOM) and search engines. Web documents collected by search engines are automatically classified by SOM and the obtained SOMs are incrementally modified according to the interaction between users and SOMs. At present, various search engines are designed to retrieve Web documents. When we use search engines to retrieve Web documents we get a lot of answers and have to examine each Web document. Therefore, in order to make up for search engines, we need a function to classify Web documents corresponding to the user's point of view and their purposes. Furthermore, we cannot retrieve pertinent Web documents by conventional search engines when a specific topic is described by more than one Web document. To solve these problems, we exploited a content-based clustering system for Web documents. In this system, Web documents are automatically clustered by their feature vectors produced from Web documents or minimal subgraphs consisting of multiple Web documents, and their overview maps are dynamically generated by SOM. Furthermore, we propose a method by which an obtained SOM is modified by user's interaction such as feedback operations.",,0-7695-0084-6,10.1109/DASFAA.1999.765734,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765734,,Self organizing feature maps;Search engines;Feedback;World Wide Web;Electronic mail;Web sites;Uniform resource locators;Navigation;Joining processes;Information retrieval,information resources;Internet;relevance feedback;search engines;classification;hypermedia;self-organising feature maps,Web document classification;self-organizing maps;search engines;classification view mechanism;hypertext;document retrieval;content-based clustering system;feature vectors;minimal subgraphs;SOM;relevance feedback,,6,,14,,6-Aug-02,,,IEEE,IEEE Conferences
Ensembling Document and Graph Vectors for Improving Topic Modeling of Microblogs,整合文檔和圖形向量以改善微博主題建模,A. N. Sumanth; M. Anirudh; M. Ramesh; G. S. Kumar; G. P. Sajeev,"Amrita Vishwa Vidyapeetham,Dept of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Dept of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Dept of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Dept of Computer Science and Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Dept of Computer Science and Engineering,Amritapuri,India",2020 12th International Conference on Computational Intelligence and Communication Networks (CICN),3-Nov-20,2020,,,448,453,"Microblogging is a popular social networking service in which users interact and post messages. Also, the users are provided with facility to frequently engage in various discussions relating to their areas of interests, fields they want to explore, and people they want to connect with. Topic modeling is a mechanism to detect the category to which a group of words belong to. Employing topic modeling in microblogs is a quite productive way of organizing the content. However, topic modeling of microblogs is relatively an unexplored area. A prime challenge is to automatically segregate the data into the abstract topics that they belong to, so that a user need not manually search for posts of his/her delight, but can easily move to the particular topic which provides the sets of related records. This paper proposes a novel topic modeling method by combining graph based document representations and Bag of Words (BoW) representation in order to classify the microblog data into the topic that it belongs to. Our proposed system is validated through experiments, using real-world data. Our method performs well and helps to efficiently designate documents into its topics.",2472-7555,978-1-7281-9393-9,10.1109/CICN49253.2020.9242613,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9242613,Topic Modeling;Vectorization;Dependency Graph;Clustering,Social networking (online);Heuristic algorithms;Computational modeling;Blogs;Clustering algorithms;Data models;Performance analysis,graph theory;social networking (online);text analysis,microblog data;Bag of Words representation;graph based document representations;topic modeling method;abstract topics;unexplored area;post messages;social networking service;microblogging;graph vectors;ensembling document,,,,20,,3-Nov-20,,,IEEE,IEEE Conferences
Distributional Similarity Model for Multi-modality Clustering in Social Media,社交媒體中多模式聚類的分佈相似模型,D. C. M. Sze; T. Fu; F. Chung; R. W. P. Luk,"Hong Kong Polytech. Univ., Hong Kong; Hong Kong Polytech. Univ., Hong Kong; Hong Kong Polytech. Univ., Hong Kong; Hong Kong Polytech. Univ., Hong Kong",2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops,14-Jan-08,2007,,,268,271,"User generated content (UGC) has become the fastest growing sector of the WWW. Data mining from UGC presents challenges not typically found in text mining from documents. UGC can be semi-structured and its content can be very short and informal, containing relatively little content similar to a chat or an email conversation. In addition UGC can be viewed as a multi-modality data. These characteristics pose big challenges and research questions for scholars to cope with. To cluster UGC data, we can construct multiple contingency tables of modalities and employ the multi-way distributional clustering (MDC) algorithm. However, by considering a contingency table which summarizes the co-occurrence statistics of two modalities, it is not robust to represent the information entropy between two modalities in UGC data. In this paper, we propose a novel similarity measurement, called distributional similarity model (DSM), to solidify the graph model in the MDC algorithm to deal with the unique characteristics of the UGC data.",,0-7695-3028-1,10.1109/WI-IATW.2007.105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4427586,Social Media AnalysisMulti-Modality ClusteringDistributional Features,User-generated content;Clustering algorithms;Intelligent agent;World Wide Web;Data mining;Text mining;Robustness;Information entropy;Solid modeling;Machine learning algorithms,data mining;Internet;user interfaces,distributional similarity model;multi-modality clustering;social media;user generated content;data mining;text mining;email conversation;multi-way distributional clustering,,4,,7,,14-Jan-08,,,IEEE,IEEE Conferences
Useful ToPIC: Self-Tuning Strategies to Enhance Latent Dirichlet Allocation,有用的主題：增強潛在狄利克雷分配的自我調整策略,S. Proto; E. Di Corso; F. Ventura; T. Cerquitelli,"Dipt. di Autom. e Inf., Politec. di Torino, Turin, Italy; Dipt. di Autom. e Inf., Politec. di Torino, Turin, Italy; Dipt. di Autom. e Inf., Politec. di Torino, Turin, Italy; Dipt. di Autom. e Inf., Politec. di Torino, Turin, Italy",2018 IEEE International Congress on Big Data (BigData Congress),11-Sep-18,2018,,,33,40,"TToPIC (Tuning of Parameters for Inference of Concepts) is a distributed self-tuning engine whose aim is to cluster collections of textual data into correlated groups of documents through a topic modeling methodology (i.e., LDA). ToPIC includes automatic strategies to relieve the end-user of the burden of selecting proper values for the overall analytics process. ToPIC's current implementation runs on Apache Spark, a state-of-the-art distributed computing framework. As a case study, ToPIC has been validated on three real collections of textual documents characterized by different distributions. The experimental results show the effectiveness and efficiency of the proposed solution in analyzing collections of documents without tuning algorithm parameters and in discovering cohesive and well-separated groups of documents with a similar topic.",,978-1-5386-7232-7,10.1109/BigDataCongress.2018.00012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457728,Text mining;Parameter-free technique;topic detection;LDA;data weighting function;Big Data framework,Entropy;Computational modeling;Data models;Tuning;Clustering algorithms;Probabilistic logic;Engines,cluster computing;distributed processing;document handling,Apache Spark;textual documents;self-Tuning strategies;TToPIC;distributed self-tuning engine;cluster collections;textual data;topic modeling methodology;distributed computing framework;latent Dirichlet allocation;tuning of parameters for inference of concepts,,1,,20,,11-Sep-18,,,IEEE,IEEE Conferences
Automatic classification of securities using hierarchical clustering of the 10-Ks,使用10-K的分層聚類對證券進行自動分類,H. Yang; H. J. Lee; S. Cho; E. Cho,"Department of Industrial Engineering, Seoul National University, Seoul, Korea; Department of Industrial Engineering, Seoul National University, Seoul, Korea; Department of Industrial Engineering, Seoul National University, Seoul, Korea; 4327 Ravensworth Rd, Annandale, VA 22003, USA",2016 IEEE International Conference on Big Data (Big Data),6-Feb-17,2016,,,3936,3943,"Industry classification has been rigorously utilized in academic research and business analytics. The existing classification schemes, however, have been constructed and maintained manually by domain experts, which require exhaustive time and human effort while vulnerable to subjectivity. Hence, the existing classification systems do not properly reflect the fast-changing trends of the firms and the capital market. As a remedy to such shortcomings, this paper proposes a new classification scheme, Business Text Industry Classification (BTIC), namely, that automatically clusters securities based on the textual information from the corporate disclosures. BTIC exploits the business section of the Form 10-Ks, in which firms provide their self-identities in a rich context. We employ doc2vec for document embedding and apply Ward's hierarchical clustering method to categorize securities into BTIC groups. Evaluation results using 12 financial ratios commonly found in financial research show that BTIC performs just as good as SIC and GICS in terms of inter- and intra-industry homogeneity, especially for the higher level of clustering. Given that, we claim that BTIC outperforms SIC and GICS in four aspects: process automation, objectivity, clustering flexibility, and result interpretability.",,978-1-4673-9005-7,10.1109/BigData.2016.7841069,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841069,10-K;Industry classification;Doc2vec;Hierarchical clustering;Capital market research;SIC;GICS,Industries;Security;Silicon carbide;Companies;Context;Government,pattern classification;pattern clustering;securities trading,automatic classification;securities;10-Ks;business text industry classification;BTIC;automatic clustering;textual information;corporate disclosures;business section;self-identities;doc2vec;Ward hierarchical clustering;financial ratios;inter-industry homogeneity;intra-industry homogeneity,,5,,26,,6-Feb-17,,,IEEE,IEEE Conferences
Measuring similarity of interests for clustering Web-users,衡量網絡用戶的興趣相似度,Jitian Xiao; Yanchun Zhang; Xiaohua Jia; Tianzhu Li,"Dept. of Math. & Comput., Univ. of Southern Queensland, Toowoomba, Qld., Australia; NA; NA; NA",Proceedings 12th Australasian Database Conference. ADC 2001,7-Aug-02,2001,,,107,114,"There has been an increased demand for understanding of Web-users due to the Web development and the increased number of Web-based applications. Informative knowledge extracted from Web user access patterns has been used for many applications, such as the prefetching of pages between clients and proxies. This paper presents an approach for measuring similarity of interests among Web users, based on the interest items collected from Web user's access logs. A matrix-based algorithm is then developed to cluster Web users such that the users in the same cluster are closely related with respect to the similarity measure. As an application example, a Web document prefetching technique is proposed that utilises the similarity measure and clusters obtained. Experiments have been conducted and the results have shown that our clustering method is capable of clustering Web users with similar interests, and the prefetching method is practical.",1530-0919,0-7695-0966-5,10.1109/ADC.2001.904471,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904471,,Prefetching;Navigation;Computer science;Application software;Clustering algorithms;Clustering methods;Web page design;Web pages;Cities and towns;Australia,Internet;information resources;information retrieval;storage management;information needs,Web user clustering;Web-based applications;Web user access patterns;user interest similarity measurement;matrix-based algorithm;Web document prefetching technique;experiments;Internet,,18,,11,,7-Aug-02,,,IEEE,IEEE Conferences
Learning taxonomic relations from a set of text documents,從一組文本文檔中學習分類關係,M. Paukkeri; A. P. Garc穩a-Plaza; S. Pessala; T. Honkela,"Aalto University School of Science and Technology, Adaptive Informatics Research Centre, P.O. Box 15400, FI-00076, Finland; NLP & IR Group, E.T.S.I. Inform獺tica, UNED, 28040, Madrid, Spain; Aalto University School of Science and Technology, Adaptive Informatics Research Centre, P.O. Box 15400, FI-00076, Finland; Aalto University School of Science and Technology, Adaptive Informatics Research Centre, P.O. Box 15400, FI-00076, Finland",Proceedings of the International Multiconference on Computer Science and Information Technology,6-Jan-11,2010,,,105,112,"This paper presents a methodology for learning taxonomic relations from a set of documents that each explain one of the concepts. Three different feature extraction approaches with varying degree of language independence are compared in this study. The first feature extraction scheme is a language-independent approach based on statistical keyphrase extraction, and the second one is based on a combination of rule-based stemming and fuzzy logic-based feature weighting and selection. The third approach is the traditional tf-idf weighting scheme with commonly used rule-based stemming. The concept hierarchy is obtained by combining Self-Organizing Map clustering with agglomerative hierarchical clustering. Experiments are conducted for both English and Finnish. The results show that concept hierarchies can be constructed automatically also by using statistical methods without heavy language-specific preprocessing.",2157-5533,978-83-60810-27-9,10.1109/IMCSIT.2010.5679865,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5679865,,Ontologies;Feature extraction;Encyclopedias;Internet;Electronic publishing;Taxonomy,feature extraction;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;self-organising feature maps;statistical analysis;text analysis,taxonomic relations learning;text documents;feature extraction approaches;language independent approach;statistical keyphrase extraction;rule based stemming;fuzzy logic based feature weighting;tf-idf weighting scheme;self-organizing map clustering;agglomerative hierarchical clustering;statistical methods,,1,,35,,6-Jan-11,,,IEEE,IEEE Conferences
An architecture for streaming coclustering in high speed hardware,高速硬件中的流式群集的體系結構,J. Byrnes; R. Rohwer,"HNC Software, LLC, Fair Isaac Corp., San Diego, CA, USA; HNC Software, LLC, Fair Isaac Corp., San Diego, CA, USA",2006 IEEE Aerospace Conference,24-Jul-06,2006,,,10 pp.,,"We seek to learn the semantics of a data stream at optical line speed. We focus on text data, but the techniques developed should apply to broad modalities of network data wherever appropriate features can be computed rapidly enough. We consider a custom hardware system designed to categorize documents based on feature clusters and document clusters that have been learned offline on standard general-purpose computers, and we present a technique for extending this system to permit online learning from arbitrarily large data sets",1095-323X,0-7803-9545-X,10.1109/AERO.2006.1656051,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656051,,Hardware;Computer architecture;Vocabulary;Computer networks;Intelligent networks;Clustering algorithms;Software algorithms;Biomedical optical imaging;High speed optical techniques;Optical fiber networks,data mining;feature extraction;pattern clustering;text analysis,streaming coclustering;high speed hardware;data stream semantics;text data;network data;document categorization;feature clusters;document clusters;online learning,,2,,16,,24-Jul-06,,,IEEE,IEEE Conferences
Text Document Classification and Pattern Recognition,文本文檔分類和模式識別,Q. Wu; E. Fuller; C. Zhang,"Dept. of Math., West Virginia Univ., Morgantown, WV, USA; Dept. of Math., West Virginia Univ., Morgantown, WV, USA; Dept. of Math., West Virginia Univ., Morgantown, WV, USA",2009 International Conference on Advances in Social Network Analysis and Mining,4-Sep-09,2009,,,405,410,"In this extended abstract, a novel approach is proposed for text pattern recognition. Instead of the traditional models which are mainly based on the frequency of keywords for text document classification, we introduce a new graph theory model which is constructed based on both information about frequency and position of keywords. We applied this new idea to the detection of fraudulent emails written by the same person, and plagiarized publications. The results on these case studies show that this new method performs much better than traditional methods.",,978-0-7695-3689-7,10.1109/ASONAM.2009.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231807,Pattern Recognition;Text Pattern;Similarity measures;Data mining;Text analysis;Graph model,Pattern recognition;Data mining;Mathematics;Graph theory;Internet;Clustering algorithms;Testing;Frequency estimation;Social network services;Pattern analysis,electronic mail;graph theory;pattern classification;text analysis,text document classification;text pattern recognition;graph theory model;fraudulent email detection;keyword frequency;keyword position;plagiarized publications,,4,,19,,4-Sep-09,,,IEEE,IEEE Conferences
Entropy-based pattern matching for document image compression,基於熵的模式匹配，用於文檔圖像壓縮,Qin Zhang; J. M. Danskin,"Dept. of Comput. Sci., Dartmouth Coll., Hanover, NH, USA; NA",Proceedings of 3rd IEEE International Conference on Image Processing,6-Aug-02,1996,2,,221,224 vol.2,"In this paper, we introduce a pattern matching algorithm used in document image compression. This pattern matching algorithm uses the cross entropy between two patterns as the criterion for a match. We use a physical model which is based on the finite resolution of the scanner (spatial sampling error) to estimate the probability values used in cross entropy calculation. Experimental results show this pattern matching algorithm compares favorably to previous algorithms.",,0-7803-3259-8,10.1109/ICIP.1996.560734,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=560734,,Pattern matching;Image coding;Entropy;Clustering algorithms;Libraries;Laboratories;Computer science;Educational institutions;Spatial resolution;Image sampling,image matching;data compression;image coding;entropy;image resolution;image sampling,entropy-based pattern matching;document image compression;cross entropy;finite resolution;spatial sampling error;probability,,11,,7,,6-Aug-02,,,IEEE,IEEE Conferences
Improving Topic Extraction in Chinese Documents Using Word Sense Disambiguation,利用詞義消歧改善中文文檔中的主題提取,H. Song; T. Yao,"Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China","2009 Fourth International Conference on Innovative Computing, Information and Control (ICICIC)",17-Feb-10,2009,,,1106,1109,This paper reports experiments on topic extraction in Chinese documents using a feature set enriched with Word Sense Disambiguation (WSD) as semantic information. The results of these experiments suggest that incorporating WSD information into Chinese topic extraction tasks may yield improvements over models which do not use WSD information.,,978-1-4244-5544-7,10.1109/ICICIC.2009.243,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5412195,,Data mining;Clustering algorithms;Computer science;Morphology;Machine learning;Natural languages;Error analysis;Ontologies;Feature extraction;Testing,natural language processing;support vector machines;text analysis,Chinese topic extraction task;Chinese documents;word sense disambiguation;feature set;semantic information;support vector machine,,1,,14,,17-Feb-10,,,IEEE,IEEE Conferences
A Mixture Approach for Multi-Label Document Classification,多標籤文檔分類的混合方法,S. Tsai; J. Jiang; S. Lee,"Dept. of Electr. Eng., Nat. Sun Yat-Sen Univ., Kaohsiung, Taiwan; Dept. of Electr. Eng., Nat. Sun Yat-Sen Univ., Kaohsiung, Taiwan; Dept. of Electr. Eng., Nat. Sun Yat-Sen Univ., Kaohsiung, Taiwan",2010 International Conference on Technologies and Applications of Artificial Intelligence,20-Jan-11,2010,,,387,391,"Multi-label classification learning concerns the determination of categories in the situation where one pattern may belong to more than one category. In this paper we propose a mixture approach, named FSMLKNN, which combines Fuzzy Similarity Measure (FSM) and Multi-Label K-Nearest Neighbor (MLKNN) for multi-label document classification. One of the problems associated with KNN-like approaches is its demanding computational cost in finding the K nearest neighbors from all training patterns. For FSMLKNN, FSM is used as an efficient clustering approach before MLKNN is applied. For a document pattern, its K nearest neighbors are only calculated from the closest cluster having the highest fuzzy similarity to the document pattern. Experimental results show that our proposed method can maintain a good performance and achieve a high efficiency simultaneously.",2376-6824,978-1-4244-8668-7,10.1109/TAAI.2010.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695481,Multi-label classification;fuzzy similarity measure;K-Nearest Neighbor algorithm (KNN),Training;Nearest neighbor searches;Testing;Text categorization;Machine learning;Algorithm design and analysis;Decision trees,classification;document handling;fuzzy set theory;learning (artificial intelligence),multilabel document classification;multilabel classification learning;FSMLKNN;fuzzy similarity measure;multilabel K-nearest neighbor;clustering approach,,1,,21,,20-Jan-11,,,IEEE,IEEE Conferences
Feature Selection using k-Medoid Algorithm for Categorization of Hadith Translation in English,基於k-Medoid算法的特徵選擇對英語中的聖訓進行分類,F. A. Setiawati; Q. U. Safitri; A. F. Huda; A. Saepulloh; W. Darmalaksana,"UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Mathematics Department,Bandung,Indonesia; UIN Sunan Gunung Djati,Hadist Department,Bandung,Indonesia",2019 IEEE 5th International Conference on Wireless and Telematics (ICWT),3-Feb-20,2019,,,1,5,"The problem of document classification is the number of features that are very large. the number of features depends on the number of terms or vocabulary used. Obviously, for every document, it contains only a small number of words in a vocabulary. So that will cause the number of elements zero. Therefore, a method is proposed to select some features that can represent all features. the method used is to cluster the vocabulary. representatives of each cluster of clustered results are used as a feature for each document in the categorization process. the categorization process is done by the k-Neirest Neighbor (k-NN) and Nearest Centroid (NC) algorithms. The data used is the translation of English hadith. with this method, it is expected that computation time will be faster and categorization result will be better (accuracy more precise).",,978-1-7281-4796-3,10.1109/ICWT47785.2019.8978221,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978221,Feature Selection;Clustering;k-Medoid;Classification;k-Nearest Neighbor;Neirest Centroid,,document handling;feature selection;language translation;nearest neighbour methods;pattern classification,feature selection;k-medoid algorithm;hadith translation;document classification;categorization process;English hadith;nearest centroid algorithms;k-nearest neighbor algorithm,,,,17,,3-Feb-20,,,IEEE,IEEE Conferences
A framework for medical text mining using a feature weighted clustering algorithm,使用特徵加權聚類算法的醫學文本挖掘框架,A. Chakrabarty; S. Roy,"Department of MCA, Future Institute of Engineering and Management, Kolkata, India; Department of MCA, Future Institute of Engineering and Management, Kolkata, India",2013 1st International Conference on Emerging Trends and Applications in Computer Science,23-Dec-13,2013,,,135,139,"Text categorization is the task of deciding whether a document belongs to a set of pre specified classes of documents. Categorization of documents is challenging, as the number of discriminating words can be huge. Many existing text classification algorithms simply do not work with these many number of words. Traditional text classification algorithm uses all training samples for classification, thereby increasing the storage requirements and calculation complexity as the number of features increase. Mining medical records for relationships between living factors and the symptoms of a disease is an important task, however there has been relatively little research into this area. The proposed work evolves a text classification algorithm where al l cluster centers are taken as training samples there by reducing the sample size and introduces a weight factor to indicate the different importance of each training sample. A similarity measure function is used to classify a new patient document, based on the measure. Experiments on real life data show that the proposed algorithm outperforms the state of art classification algorithms such as k-nearest neighbor.",,978-1-4673-5250-5,10.1109/ICETACS.2013.6691410,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691410,Text categorization;minimum spanning tree;clustering;cosine based similarity;weight factor;liver disease,Classification algorithms;Clustering algorithms;Text categorization;Diseases;Training;Liver;Biomedical imaging,data mining;electronic health records;text analysis,calculation complexity;storage requirements;text classification algorithms;documents categorization;text categorization;feature weighted clustering algorithm;medical text mining,,1,,18,,23-Dec-13,,,IEEE,IEEE Conferences
Cluster analysis using the GSOM: Patterns in epidemiology,使用GSOM進行聚類分析：流行病學模式,D. De Silva; D. Alahakoon; S. Dharmage,"Cognitive and Connectionist Systems Lab, Monash University, Australia; Cognitive and Connectionist Systems Lab, Monash University, Australia; School of Population Health, University of Melbourne, Australia",2007 Third International Conference on Information and Automation for Sustainability,17-Jun-08,2007,,,63,69,"The growing self organizing map (GSOM) has proven Itself as an adaptive and robust knowledge discovery algorithm with applications in diverse disciplines. The unrestricted network topology of the learning phase of GSOM enables clustering of data vectors based on the notion of similarity. The hierarchical clustering capability of the GSOM allows drill down examination of clusters at different levels of correlation. In addition to static patterns in the form of cluster groups, the feature map can also contain dynamic information of the dataset. There is continuous experimenting on enhancing the functionality of GSOM as a comprehensive data analysis and knowledge discovery application. This paper documents such extensions along with their application in knowledge discovery. The extensions were applied to a population based sample of children born to families in which either parents or/and siblings have had asthma, eczema or hay fever. The dataset contained time variant information on rash development and allergy diagnosis by skin prick testing to different allergens.",2151-1810,978-1-4244-1899-2,10.1109/ICIAFS.2007.4544781,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544781,,Pattern analysis;Clustering algorithms;Robustness;Smoothing methods;Algorithm design and analysis;Organizing;Network topology;Data analysis;Skin;Testing,data analysis;data mining;diseases;pattern clustering;self-organising feature maps;unsupervised learning,cluster analysis;GSOM;epidemiology;growing self organizing map;knowledge discovery algorithm;network topology;data analysis;data mining;unsupervised clustering algorithms,,2,,7,,17-Jun-08,,,IEEE,IEEE Conferences
Camera-based degraded character segmentation into individual components,基於攝像機的降級字符分割成單個組件,C. Mancas-Thillou; M. Mancas; B. Gosselin,"Fac. Polytechnique de Mons, Belgium; Fac. Polytechnique de Mons, Belgium; Fac. Polytechnique de Mons, Belgium",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,755,759 Vol. 2,"In this article, we present a novel fully automatic character segmentation for camera-based images. This is a top-down approach inspired by the human visual system: the high level step is based on color clustering while the low level one on the phase of log-Gabor filters. First, this latter step deals with remaining touching characters after the color clustering approach and then takes into account the neighborhood information to address the problem of broken characters. Results are very encouraging and the method should be fast enough to be used on embedded platforms like personal digital assistants.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575646,,Degradation;Image segmentation;Image databases;Text recognition;Humans;Visual system;Filters;Personal digital assistants;Optical character recognition software;Optical distortion,image segmentation;image colour analysis;character recognition;pattern clustering;visual databases;Gabor filters,automatic character segmentation;camera-based image;human visual system;color clustering;log-Gabor filter;personal digital assistant,,2,,8,,16-Jan-06,,,IEEE,IEEE Conferences
A Robust Wavelet Transform Based Technique for Video Text Detection,基於魯棒小波變換的視頻文本檢測技術,P. Shivakumara; T. Q. Phan; C. L. Tan,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2009 10th International Conference on Document Analysis and Recognition,2-Oct-09,2009,,,1285,1289,"In this paper, we propose a new method based on wavelet transform, statistical features and central moments for both graphics and scene text detection in video images. The method uses wavelet single level decomposition LH, HL and HH subbands for computing features and the computed features are fed to k means clustering to classify the text pixel from the background of the image. The average of wavelet subbands and the output of k means clustering helps in classifying true text pixel in the image. The text blocks are detected based on analysis of projection profiles. Finally, we introduce a few heuristics to eliminate false positives from the image. The robustness of the proposed method is tested by conducting experiments on a variety of images of low contrast, complex background, different fonts, and size of text in the image. The experimental results show that the proposed method outperforms the existing methods in terms of detection rate, false positive rate and misdetection rate.",2379-2140,978-1-4244-4500-4,10.1109/ICDAR.2009.83,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5277693,wavelet subbands;statistical features;k-means clsutering;text detection,Robustness;Wavelet transforms;Image edge detection;Layout;Wavelet analysis;Graphics;Pixel;Text analysis;Image analysis;Image recognition,computer graphics;image classification;object detection;pattern clustering;statistical analysis;text analysis;video signal processing;wavelet transforms,robust wavelet transform;video image text detection technique;statistical feature;computer graphics;wavelet single level decomposition;k means-clustering;text pixel;image classification;projection profile analysis;image LH text detection;image HL text detection;image HH text detection,,32,,15,,2-Oct-09,,,IEEE,IEEE Conferences
Designing efficient distributed neural classifiers: application to handwritten digit recognition,設計高效的分佈式神經分類器：在手寫數字識別中的應用,A. Ribert; Y. Lecourtier; A. Ennaji,"Fac. des Sci., Rouen Univ., Mont-Saint-Aignan, France; NA; NA",Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318),6-Aug-02,1999,,,265,268,"Describes an automatic method for building distributed neural classifiers for pattern recognition. The methodology is based on the detection of reliable regions in the representation space, i.e. clusters exclusively composed of patterns from the same class. This detection is performed using a hierarchical clustering method associated with the supervised information provided by a professor. The proposed methodology consists of associating each of these regions with a multilayer perceptron (MLP) which has to recognise elements that are inside its region while rejecting all others. Experimental results for a real problem (handwritten digit recognition) reveal an interesting generalisation behaviour of the distributed classifier in comparison to the k-nearest neighbour algorithm as well as a single MLP.",,0-7695-0318-7,10.1109/ICDAR.1999.791775,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=791775,,Handwriting recognition;Neural networks;Identity-based encryption;Nonhomogeneous media;Multi-layer neural network;Multilayer perceptrons;Pattern recognition;Buildings;Humans;Spatial databases,multilayer perceptrons;handwritten character recognition;pattern clustering;generalisation (artificial intelligence);pattern classification,distributed neural classifiers;handwritten digit recognition;reliable region detection;representation space;hierarchical clustering method;supervised information;multilayer perceptron;generalisation behaviour;k-nearest neighbour algorithm,,,,11,,6-Aug-02,,,IEEE,IEEE Conferences
Extracting Features of Paper-made Objects Recognized from Origami Books Based on Design Knowledge,基於設計知識的摺紙書籍識別紙質對象特徵提取,H. Shimanuki; J. Kato; T. Watanabe,"Nagoya University, Japan; Nagoya University, Japan; Nagoya University, Japan",Ninth International Conference on Document Analysis and Recognition (ICDAR 2007),5-Nov-07,2007,2,,1203,1207,"This paper proposes an approach to extracting features of origami works which are recognized from facing page images of instructions described as origami drill books. The origami work is presented as a set effaces (polygons). First, in order to associate geometric shapes of objects with knowledge, an origami data model based on design knowledge is proposed. Therefore, the set of faces is classified into clusters which indicate a certain meaning in origami design. The cluster is called ""origami molecule "". Next, the relationships among the molecules are defined. The relationships are not only connected relationships but also symmetrical relationships and equivalence relationships. By using these relationships, it is possible to comprehend twin parts of origami work, for example, two forefeet of the four-limbed creature. Finally, a method for extraction of the design knowledge from crease patterns is proposed.",2379-2140,978-0-7695-2822-9,10.1109/ICDAR.2007.4377106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4377106,,Feature extraction;Books;Data mining;Shape;Image recognition;Data structures;Graphics;Informatics;Information science;Data models,art;computational geometry;feature extraction;image classification;object recognition;pattern clustering,feature extraction;paper-made object recognition;origami drill book;origami design knowledge;origami data model;image classification;pattern clustering;origami molecule;object geometric shape,,,,9,,5-Nov-07,,,IEEE,IEEE Conferences
Extraction of Significant Video Summaries by Dendrogram Analysis,通過樹狀圖分析提取重要的視頻摘要,S. Benini; A. Bianchetti; R. Leonardi; P. Migliorati,"DEA-SCL, University of Brescia, Via Branze 38, I-25123, Brescia, Italy; DEA-SCL, University of Brescia, Via Branze 38, I-25123, Brescia, Italy; DEA-SCL, University of Brescia, Via Branze 38, I-25123, Brescia, Italy; DEA-SCL, University of Brescia, Via Branze 38, I-25123, Brescia, Italy",2006 International Conference on Image Processing,20-Feb-07,2006,,,133,136,"In the current video analysis scenario, effective clustering of shots facilitates the access to the content and helps in understanding the associated semantics. This paper introduces a cluster analysis on shots which employs dendrogram representation to produce hierarchical summaries of the video document. Vector quantization codebooks are used to represent the visual content and to group the shots with similar chromatic consistency. The evaluation of the cluster codebook distortions, and the exploitation of the dependency relationships on the dendrograms, allow to obtain only a few significant summaries of the whole video. Finally the user can navigate through summaries and decide which one best suites his/her needs for eventual post-processing. The effectiveness of the proposed method is demonstrated by testing it on a collection of video-data from different kinds of programmes. Results are evaluated in terms of metrics that measure the content representational value of the summarization technique.",2381-8549,1-4244-0480-0,10.1109/ICIP.2006.312377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106484,Vector quantization;clustering methods,Vector quantization;Nonlinear distortion;Clustering methods;Performance analysis;Navigation;Testing;Graph theory;Tree graphs;Distortion measurement;Multimedia databases,feature extraction;image representation;vector quantisation;video coding,video summary extraction;dendrogram representation;video document;vector quantization codebook;visual content representation;chromatic consistency;cluster codebook distortion,,17,,9,,20-Feb-07,,,IEEE,IEEE Conferences
Using Wikipedia for Co-clustering Based Cross-Domain Text Classification,使用Wikipedia進行基於共同域的跨域文本分類,P. Wang; C. Domeniconi; J. Hu,"Dept. of Comput. Sci., George Mason Univ., Fairfax, VA; Dept. of Comput. Sci., George Mason Univ., Fairfax, VA; Microsoft Res. Asia, Beijing",2008 Eighth IEEE International Conference on Data Mining,10-Feb-09,2008,,,1085,1090,"Traditional approaches to document classification requires labeled data in order to construct reliable and accurate classifiers. Unfortunately, labeled data are seldom available, and often too expensive to obtain. Given a learning task for which training data are not available, abundant labeled data may exist for a different but related domain. One would like to use the related labeled data as auxiliary information to accomplish the classification task in the target domain. Recently, the paradigm of transfer learning has been introduced to enable effective learning strategies when auxiliary data obey a different probability distribution. A co-clustering based classification algorithm has been previously proposed to tackle cross-domain text classification. In this work, we extend the idea underlying this approach by making the latent semantic relationship between the two domains explicit. This goal is achieved with the use of Wikipedia. As a result, the pathway that allows to propagate labels between the two domains not only captures common words, but also semantic concepts based on the content of documents. We empirically demonstrate the efficacy of our semantic-based approach to cross-domain classification using a variety of real data.",2374-8486,978-0-7695-3502-9,10.1109/ICDM.2008.136,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4781229,Cross-domain text classification;Co-clustering;Transfer learning;Wikipedia,Wikipedia;Text categorization;Training data;Probability distribution;Classification algorithms;Dictionaries;Bridges;Data mining;Computer science;Asia,classification;learning (artificial intelligence);pattern clustering;statistical distributions;text analysis;Web sites,co-clustering-based cross-domain text classification;wikipedia;document classification;learning strategy;probability distribution;latent semantic relationship,,19,,16,,10-Feb-09,,,IEEE,IEEE Conferences
Ontology Based Clustering for Improving Genomic IR,基於本體的聚類算法可改善基因組IR,J. Wen; Z. Li; X. Hu,"National University of Defence Technology, China; Beihang University, China; Drexel University, USA",Twentieth IEEE International Symposium on Computer-Based Medical Systems (CBMS'07),25-Jun-07,2007,,,225,230,"Recent work has shown that ontology is useful to improve the performance of information retrieval, especially in biomedical literatures. The method of ontology-based can solve synonym problems. In this paper, we propose a new frame for genomic information retrieval based on UMLS. In our frame, genomic information retrieval includes three processes: first, documents were indexed based UMLS, which means documents were represented by concepts, besides, the concept weight was re-calculated combined with similarity between concepts. Second, documents were clustered using fuzzy c-means method. At last cluster language model is utilized for information retrieval. Our method can solve partly synonymy and polysemy problems. The new method is evaluated on TREC 2004/05 genomics track collections. Experiments show that the retrieval performance is greatly improved by the new method compared with the basic language model.",1063-7125,0-7695-2905-4,10.1109/CBMS.2007.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4262654,Genomic information retrieval;concept index;cluster language model,Ontologies;Genomics;Bioinformatics;Information retrieval;Unified modeling language;Optical computing;Indexing;Biomedical measurements;Computer science;Biomedical engineering,biology computing;fuzzy systems;genetics;information retrieval;ontologies (artificial intelligence),ontology;clustering;genomic information retrieval;UMLS;fuzzy c-means method;synonymy;polysemy;TREC 2004/05 genomics track collections,,4,,12,,25-Jun-07,,,IEEE,IEEE Conferences
Resource Provisioning for Cyber?Physical?Social System in Cloud-Fog-Edge Computing Using Optimal Flower Pollination Algorithm,基於最佳花授粉算法的雲霧邊緣計算中的網絡虛擬系統資源配置,V. Porkodi; A. R. Singh; A. R. W. Sait; K. Shankar; E. Yang; C. Seo; G. P. Joshi,"Department of Information Technology, College of Engineering and Computer Science, Lebanese French University, Erbil, Iraq; School of Computing, Kalasalingam Academy of Research and Education, Virudhunagar, India; Center of Documents, Archives and Communication, King Faisal University, Al-Hasa, Saudi Arabia; Department of Computer Applications, Alagappa University, Karaikudi, India; School of Software, Soongsil University, Seoul, South Korea; Department of Convergence Science, Kongju National University, Gongju, South Korea; Department of Computer Science and Engineering, Sejong University, Seoul, South Korea",IEEE Access,11-Jun-20,2020,8,,105311,105319,"The rise of cyber-physical-social systems (CPSS) as a novel paradigm has revolutionized the relationship among humans, computers and physical environment. The key technologies to design CPSS directly related to multi-disciplinary technologies including cyber-physical systems (CPS) and cyber-social systems (CSS). Unfortunately, the design of CPSS is not an easier process because of the network heterogeneity, complex hardware and software entities. At the same time, fog computing is emerged as an expansion of cloud computing which efficiently addresses the abovementioned issue. Resource provisioning is a main technology involved in fog computing. This paper devises a novel fuzzy clustering with flower pollination algorithm called FCM-FPA as a resource provisioning model for fog computing. At the earlier stage, the resource attributes are standardized and normalized. Next, the fuzzy clustering with FPA is developed for partitioning the resources and the scalability of resource searching has been minimized. At last, the presented resource provisioning algorithm based on optimized fuzzy clustering has been devised. The performance of the proposed FCM-FPA model has been tested using a set of two benchmark Iris and Wine dataset. The experimental outcome ensured that the FCM-FPA model has shown proficient results over the compared methods by offering maximum user satisfaction and effective resource provisioning.",2169-3536,,10.1109/ACCESS.2020.2999734,Institute for Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107138,CPSS;big data;resource provisioning;clustering;flower pollination algorithm,Task analysis;Edge computing;Intelligent sensors;Computational modeling;Urban areas;Clustering algorithms,cloud computing;cyber-physical systems;fuzzy set theory;pattern clustering;resource allocation,user satisfaction;complex hardware;multidisciplinary technologies;physical environment;CPSS;cyber-physical-social systems;optimal flower pollination algorithm;cloud-fog-edge computing;FCM-FPA model;optimized fuzzy clustering;resource provisioning algorithm;resource searching;resource attributes;resource provisioning model;cloud computing;fog computing;software entities,,2,,22,CCBY,3-Jun-20,,,IEEE,IEEE Journals
Feature Selection and Term Weighting,特徵選擇和術語加權,A. Algarni; N. Tairan,"Coll. of Comput. Sci., King Khalid Univ., Abha, Saudi Arabia; Coll. of Comput. Sci., King Khalid Univ., Abha, Saudi Arabia",2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),20-Oct-14,2014,1,,336,339,"Term-based approaches can extract many features in text documents, but most include noise. Many popular text-mining techniques have been adapted to reduce noisy information from extracted features but still contains some noises features. However, the noise features are extracted from the same training documents that good features extracted from. Therefore, the main problem is that some training documents contain large a mount of noises data. If we can reduce the noises data in the training documents that would help to reduce noises in extracted features. Moreover, we believe that remove some of training documents (documents that contains noises data more than useful data) can help to improve the effectiveness of the classifier. Using the advantages of clustering method can help to reduce the affect of noises data. The main problem of clustering is defined to be that of finding groups of similar projects in the data. In this paper we introduce the methodology that using clustering algorithm to group training data before use it. Also we tested our theory that not all training documents are useful to train the classifier.",,978-1-4799-4143-8,10.1109/WI-IAT.2014.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927562,Data mining;Text mining;Information retrieval,Training;Feature extraction;Noise;Information retrieval;Frequency measurement;Text categorization,data mining;data reduction;feature extraction;feature selection;pattern classification;pattern clustering;text analysis,classifier;group training data;clustering algorithm;noise data reduction;training documents;noisy information reduction;feature extraction;text documents;text-mining techniques;term weighting approach;feature selection,,,,23,,20-Oct-14,,,IEEE,IEEE Conferences
Document space dimension reduction by nonlinear Hebbian neural network,非線性Hebbian神經網絡減少文檔空間維數,L. Skovajsova; I. Mokris,NA; NA,2009 7th International Symposium on Applied Machine Intelligence and Informatics,19-May-09,2009,,,89,91,"This paper deals with information retrieval of text documents, and their clustering into some other feature space. The aim of this paper is to reduce the dimension of the document space by the nonlinear Hebbian neural network. As can be seen from the results, not only dimension reduction of document space is made, but also clustering of these documents into clusters. We used here the nonlinear Hebbian neural network, which is feed-forward neural network with unsupervised learning.",,978-1-4244-3801-3,10.1109/SAMI.2009.4956615,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4956615,,Neural networks;Information retrieval;Feedforward neural networks;Matrix decomposition;Principal component analysis;Indexing;Strontium;Sparse matrices;Feedforward systems;Unsupervised learning,feedforward neural nets;Hebbian learning;information retrieval;text analysis;unsupervised learning,document space dimension reduction;nonlinear Hebbian neural network;information retrieval;text documents;unsupervised learning;feedforward neural network,,,,20,,19-May-09,,,IEEE,IEEE Conferences
Towards automatic generation of query taxonomy: a hierarchical query clustering approach,邁向自動生成查詢分類法：分層查詢聚類方法,Shui-Lung Chuang; Lee-Feng Chien,"Inst. of Inf. Sci., Acad. Sinica, Taipei, Taiwan; NA","2002 IEEE International Conference on Data Mining, 2002. Proceedings.",10-Mar-03,2002,,,75,82,"Most previous work on automatic query clustering generated a flat, un-nested partition of query terms. In this work, we discuss the organization of query terms into a hierarchical structure and construct a query taxonomy in an automatic way. The proposed approach is designed based on a hierarchical agglomerative clustering algorithm to hierarchically group similar queries and generate cluster hierarchies using a novel cluster partition technique. The search processes of real-world search engines are combined to obtain highly ranked Web documents as the feature source for each query term. Preliminary experiments show that the proposed approach is effective for obtaining thesaurus information for query terms, and is also feasible for constructing a query taxonomy which provides a basis for in-depth analysis of users' search interests and domain-specific vocabulary on a larger scale.",,0-7695-1754-4,10.1109/ICDM.2002.1183888,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183888,,Taxonomy;Search engines;Information science;Thesauri;Vocabulary;Classification tree analysis;Clustering algorithms;Performance analysis;Terminology;Marine vehicles,search engines;information retrieval;thesauri;information needs,automatic query taxonomy generation;hierarchical query clustering approach;hierarchical agglomerative clustering algorithm;cluster partition technique;search engines;highly ranked Web documents;thesaurus information;user search interests;domain-specific vocabulary,,4,,12,,10-Mar-03,,,IEEE,IEEE Conferences
CDW: A text clustering model for diverse versions discovery,CDW：用於各種版本發現的文本集群模型,R. Xiao; L. Kong; Y. Zhang; M. Wang,"Department of Machine Intelligence, Peking University, Beijing 100871, China; Department of Machine Intelligence, Peking University, Beijing 100871, China; Department of Machine Intelligence, Peking University, Beijing 100871, China; Ucap Corporation, DongGuan, Guangdong, China",2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-Sep-11,2011,2,,1113,1117,"The development of information technology brings numerous online news and events to our daily life. One big problem of such information explosion is, many times there are diverse descriptions for one incident which make people confused. Although previous researches have provided various algorithms to detect and track events, few of them focus on uncovering the diversified versions of an event. In this paper, we propose a novel algorithm which is capable of discovering different versions of one event according to the news reports. We map documents to the topic layer to get the information of each topic. Then we extract the highly-differentiated words of each topic to cluster the documents. Compared with previous work, the accuracy of our algorithm is much higher. Experiments conducted on two data sets show that our algorithm is effective and outperforms various related algorithms, including classical methods such as K-means and LDA.",,978-1-61284-181-6,10.1109/FSKD.2011.6019733,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019733,,Clustering algorithms;Feature extraction;Tuning;DVD;Semantics;Event detection;Sun,information technology;pattern clustering;text analysis,CDW;text clustering model;diverse versions discovery;information technology development;information explosion;map documents;K-means;LDA,,1,,16,,15-Sep-11,,,IEEE,IEEE Conferences
Efficient and adaptive Web replication using content clustering,使用內容集群的高效和自適應Web複製,Yan Chen; Lili Qiu; Weiyu Chen; Luan Nguyen; R. H. Katz,"Comput. Sci. Div., Univ. of California, Berkeley, CA, USA; NA; NA; NA; NA",IEEE Journal on Selected Areas in Communications,4-Aug-03,2003,21,6,979,994,"Recently, there has been an increasing deployment of content distribution networks (CDNs) that offer hosting services to Web content providers. In this paper, we first compare the uncooperative pulling of Web contents used by commercial CDNs with the cooperative pushing. Our results show that the latter can achieve comparable users' perceived performance with only 4%-5% of replication and update traffic compared with the former scheme. Therefore, we explore how to efficiently push content to CDN nodes. Using trace-driven simulation, we show that replicating content in units of URLs can yield 60%-70% reduction in clients' latency, compared with replicating in units of Websites. However, it is very expensive to perform such a fine-grained replication. To address this issue, we propose to replicate content in units of clusters, each containing objects which are likely to be requested by clients that are topologically close. To this end, we describe three clustering techniques and use various topologies and several large Web server traces to evaluate their performance. Our results show that the cluster-based replication achieves performance close to that of the URL-based scheme, but only at 1%-2% of computation and management cost. In addition, by adjusting the number of clusters, we can smoothly trade off management and computation cost for better client performance. To adapt to changes in users' access patterns, we also explore incremental clustering that adaptively adds new documents to the existing content clusters. We examine both offline and online incremental clustering, where the former assumes access history is available while the latter predicts access pattern based on the hyperlink structure. Our results show that the offline clustering yields performance close to that of the complete re-clustering at much lower overhead. The online incremental clustering and replication cut down the retrieval cost by 4.6 times compared with random and by 8 times compared with no replication. Therefore it is especially useful to improve document availability during flash crowds.",1558-0008,,10.1109/JSAC.2003.814608,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1217282,,Computational efficiency;Telecommunication traffic;Traffic control;Uniform resource locators;Delay;Topology;Web server;History;Costs;Availability,Internet;Web sites;file servers;replicated databases;telecommunication traffic;digital simulation;network topology,adaptive Web replication;content distribution networks;hosting services;Web content providers;content clustering;cooperative pushing;update traffic;replication traffic;CDN nodes;trace-driven simulation;Websites;fine-grained replication;Web server traces;performance evaluation;cluster-based replication;URL-based scheme;computation cost;management cost;users access patterns;offline incremental clustering;online incremental clustering;access history;hyperlink structure;re-clustering;overhead;document availability;network topology,,67,,33,,4-Aug-03,,,IEEE,IEEE Journals
Information visualization for collaborative computing,用於協作計算的信息可視化,H. Chen; J. Nunamaker; R. Orwig; O. Titkova,"Arizona Univ., Tucson, AZ, USA; NA; NA; NA",Computer,6-Aug-02,1998,31,8,75,82,"Information technology continues to generate increasing amounts of data, most of which is useless without scalable methods to collect, analyze, process, and understand it. Visualization is a promising approach to such systemization because it lets users see underlying processes and guide process simulations interactively. However, visualization must be combined with some way to make repositories of text documents more manageable, providing users with a flexible, interactive environment in which to access them. The article describes a prototype tool that addresses these problems for GroupSystems, an electronic meeting system developed at the University of Arizona and installed at more than 1,500 business, government, and university settings. The tool automatically categorizes information, statistically clusters similar documents, and displays the organized document set graphically, providing more at-a-glance information than a typical text based display. Users can thus more easily browse document collections. The tool uses text analysis techniques that aim to identify descriptors and develop an unambiguous internal representation of a document.",1558-0814,,10.1109/2.707620,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707620,,Collaboration;Data visualization;Displays;Information technology;Information analysis;Computational modeling;Environmental management;Prototypes;Government;Text analysis,groupware;data visualisation;document handling;interactive systems;information retrieval,information visualization;collaborative computing;information technology;scalable methods;underlying processes;process simulations;text documents;flexible interactive environment;prototype tool;GroupSystems;electronic meeting system;university settings;organized document set;at-a-glance information;text based display;document collections;text analysis techniques;unambiguous internal representation,,19,,10,,6-Aug-02,,,IEEE,IEEE Magazines
Concept analysis and web clustering using combinatorial topology,使用組合拓撲的概念分析和Web聚類,T. Y. Lin; A. Sutojo; J. Hsu,"San Jose State University, CA, USA; San Jose State University, CA, USA; San Jose State University, CA, USA",Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06),15-Jan-07,2006,,,412,416,"The collection of the concepts that are discussed in a document set can be represented by a geometric structure, called simplical complex, of combinatorial topology. A simplex is a high-frequency keyword set that co-occurs closely which, we believe, carries a concept in the document set. The collection of all these simplexes that forms the simplical complex represents the structure of these concepts. Based on the topological structure of this complex, the documents are clustered. Several clustering schemes are presented. Our initial experiments, as expected, do support the theory",2375-9259,0-7695-2792-2,10.1109/ICDMW.2006.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4063662,,Topology;Data mining;Association rules;Computer science;Web sites;Clustering methods;Singular value decomposition;Matrix decomposition;Spine;Mathematical analysis,combinatorial mathematics;data mining;Internet;vocabulary,concept analysis;Web clustering;combinatorial topology;high-frequency keyword set,,3,,5,,15-Jan-07,,,IEEE,IEEE Conferences
Multi-view Clustering of Visual Words Using Canonical Correlation Analysis for Human Action Recognition,使用典範相關分析對視覺單詞進行多視圖聚類以識別人類動作,B. Saghafi; D. Rajan,"Centre For Multimedia & Network Technol., Nanyang Technol. Univ., Singapore, Singapore; Centre For Multimedia & Network Technol., Nanyang Technol. Univ., Singapore, Singapore",2010 Ninth International Conference on Machine Learning and Applications,4-Feb-11,2010,,,661,666,"In this paper we propose a novel approach for introducing semantic relations into the bag-of-words framework for recognizing human actions. We represent visual words in two different views: the original features and the document co-occurrence representation. The latter view conveys semantic relations but is large, sparse and noisy. We use canonical correlation analysis between the two views to find a subspace in which the words are more semantically distributed. We apply k-means clustering in the computed space to find semantically meaningful clusters and use them as the semantic visual vocabulary. Incorporating the semantic visual vocabulary the features are quantized to form more discriminative histograms. Eventually the histograms are classified using an SVM classifier. We have tested our approach on KTH action dataset and achieved promising results.",,978-1-4244-9211-4,10.1109/ICMLA.2010.102,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708901,Canonical Correlation Analysis;Multi-view;Clustering;Bag-of-words;Human Action Recognition,Semantics;Visualization;Feature extraction;Vocabulary;Correlation;Accuracy;Histograms,correlation methods;gesture recognition;information retrieval;natural language processing;pattern clustering;support vector machines;vocabulary,human action recognition;canonical correlation analysis;k-means clustering;semantic visual vocabulary;discriminative histogram;SVM classifier;KTH action dataset;bag of word;document cooccurrence representation,,,,22,,4-Feb-11,,,IEEE,IEEE Conferences
A K-means Approach Based on Concept Hierarchical Tree for Search Results Clustering,基於概念層次樹的K均值搜索結果聚類方法,P. Jiang; C. Zhang; G. Guo; Z. Niu; D. Gao,"Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China",2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery,28-Dec-09,2009,1,,380,386,"Search results clustering aims to facilitate users' information retrieval process and query refinement by online grouping similar documents returned from the search engine. It has stringent requirements on performance and meaningful cluster labels. Thus, most existing clustering algorithms such as K-means and agglomerative hierarchical clustering cannot be directly applied to the task of online search results clustering. In this paper, we propose a K-means approach based on concept hierarchical tree to cluster search results. This algorithm not only over comes weaknesses of the classic K-means method: the results produced depend on the initial seeds and the parameter k is often unknown, but also satisfies the requirements of online search results clustering. Our method utilizes the semantic relation among documents by mapping terms to concepts in the concept hierarchical tree, which can be constructed by WordNet. We have developed a meta-search and clustering system based on our approach, followed by using an impersonal and repeatable evaluation solution. Experimental results indicate that our proposed algorithm is effective and suitable in performing the task of clustering search results.",,978-0-7695-3735-1,10.1109/FSKD.2009.658,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358569,,Clustering algorithms;Search engines;Web search;Fuzzy systems;Computer science;Information retrieval;Metasearch;Filters;Web sites;Rail transportation,pattern clustering;query formulation;search engines;trees (mathematics),K-means approach;concept hierarchical tree;information retrieval process;query refinement;search engine;online search results clustering;WordNet;meta-search,,4,,15,,28-Dec-09,,,IEEE,IEEE Conferences
Towards an authorship analysis of two religious documents,對兩個宗教文獻的作者分析,H. Hadjadj; H. Sayoud,"Faculty of Electronics and Informatics. USTHB University, USTHB University, Algiers, Algeria; Faculty of Electronics and Informatics. USTHB University, USTHB University, Algiers, Algeria","2016 8th International Conference on Modelling, Identification and Control (ICMIC)",5-Jan-17,2016,,,369,373,"In this paper, we try to make an author identification of two ancient Arabic religious books dating from the 6th century: The holy Quran and the Hadith. The authorship identification process is achieved through four phases which are: documents collection, text preprocessing, features extraction and classification model building. Thus, two series of experiments are undergone and commented. The first experiment deals with authorship identification of the two books using a Manhattan centroid distance and SMO-SVM classifier. Whereas, in the second experiment a Hierarchical Clustering is employed to distinguish the different segments belonging to the two books. For that purpose, three types of original NLP features are combined before the classification process. The results show good authorship identification performances with an accuracy of 100%. In fact, all the results of this investigation correspond to a clear authorship distinction between the two religious books.",,978-0-9567157-7-7,10.1109/ICMIC.2016.7804139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804139,Authorship analysis;Natural language processing;Author identification;Religious books;Quran;Hadith;Text Classifiaction,Support vector machines;Feature extraction;Training;Classification algorithms;Testing;Algorithm design and analysis;Conferences,feature extraction;humanities;natural language processing;pattern classification;pattern clustering;support vector machines;text analysis,authorship analysis;religious document;Arabic religious book identification;holy Quran;Hadith;document collection;text preprocessing;feature extraction;classification model building;Manhattan centroid distance;SMO-SVM classifier;hierarchical clustering;NLP feature,,,,18,,5-Jan-17,,,IEEE,IEEE Conferences
Improving Image Decomposition Method of the 3-MRC Coding of Scanned Compound Document Images,掃描複合文檔圖像的3-MRC編碼的改進圖像分解方法,G. Lakhani,"Texas Tech Univ., Lubbock, TX","2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing",20-Jan-09,2008,,,289,296,"This paper discusses image decomposition problem of the 3-layer MRC model based coding of scanned (noisy) document images. A widely-used approach for document decomposition is to divide the document image into blocks and split the pixel histogram of each block into two halves by minimizing the sum of variance of its pixels with the mean of the halves. We propose to split a block by minimizing the variance of one half with its minimum pixel and the variance of the other half with its maximum pixel. Our goal is to increase the gap between the two halves by avoiding splitting of any cluster of pixels into both halves. It should help reduce complexity of the generated mask. Moreover, we do not decompose a block if it has no edge points, again to reduce the mask complexity. We also implement a noise reduction heuristic in the mask layer to correct placement of transition pixels. We provide simple analysis and evaluate block energy in terms of the DCT coefficients of the resulting FG/BG layer blocks. Experimental results show that code size of the mask layer of our test images, obtained using proposed processing is reduced to nearly half of the mask obtained by a straight-forward 3-MRC implementation.",,978-0-7695-3476-3,10.1109/ICVGIP.2008.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756084,,Image decomposition;Image coding;Pixel;Noise reduction;Image reconstruction;Entropy;Computer vision;Computer graphics;Image processing;Histograms,computational complexity;document image processing;image coding;image denoising;image resolution;image segmentation,image decomposition method;3-MRC coding;scanned compound document images;document decomposition;pixel histogram;noise reduction;DCT coefficients,,,,10,,20-Jan-09,,,IEEE,IEEE Conferences
A framework for the selective dissemination of XML documents based on inferred user profiles,一個基於推斷的用戶概要文件選擇性分發XML文檔的框架,I. Stanoi; G. Mihaila; Sriram Padmanabhan,"IBM Thomas J. Watson Res. Center, NY, USA; IBM Thomas J. Watson Res. Center, NY, USA; IBM Thomas J. Watson Res. Center, NY, USA",Proceedings 19th International Conference on Data Engineering (Cat. No.03CH37405),21-Jan-04,2003,,,531,542,"As the amount of data available online and the number of pervasive applications that take advantage of it increase, systems that support selective dissemination of information are becoming more popular. At the same time, XML is becoming the standard for document exchange over the Internet. A key capability of emerging information dissemination systems is therefore the effective filtering of a continuous stream of XML data items according to user preferences. Here we propose a model for information dissemination that integrates profile inference with data dissemination and takes advantage of the structured content in XML documents. Starting from the assumption that explicitly stating one's information interests is an inconvenient and error-prone process, we aim to automatically construct user profiles. We do this by clustering items previously deemed valuable by the user according to a novel similarity measure that takes advantage of the semantic content of XML. Furthermore, we index the profiles from all users into a multilevel index structure whose nodes naturally will be a close match to subject areas present in the document collection. Such an approach is both intuitive and efficient since the indexing structure is not primarily affected by an increasing number of users. To support our claims we experimentally validate our method and report on its effectiveness and efficiency.",,0-7803-7665-X,10.1109/ICDE.2003.1260819,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260819,,XML;Internet;Information filtering;Information filters;Indexing;Pressing;Deductive databases;Bandwidth;Traffic control;Monitoring,document handling;XML;Internet;information filters;information retrieval;information dissemination,information dissemination;XML document;user profile;document exchange standard;Internet;information filtering;XML data item;semantic content,,11,,23,,21-Jan-04,,,IEEE,IEEE Conferences
Scalable spectral clustering with cosine similarity,具有餘弦相似度的可擴展頻譜聚類,G. Chen,"Department of Mathematics and Statistics, San J籀se State University, San J籀se, California, 95192??103",2018 24th International Conference on Pattern Recognition (ICPR),29-Nov-18,2018,,,314,319,"We propose a unified scalable computing framework for three versions of spectral clustering - Normalized Cut (Shi and Malik, 2000), the Ng-Jordan-Weiss (NJW) algorithm (2001), and Diffusion Maps (Coifman and Lafon, 2006), in the setting of cosine similarity. We assume that the input data is either sparse (e.g., as a document-term frequency matrix) or of only a few hundred dimensions (e.g., for small images or data obtained through PCA). We show that in such cases, spectral clustering can be implemented solely based on efficient operations on the data matrix such as elementwise manipulation, matrix-vector multiplication and low-rank SVD, thus entirely avoiding the weight matrix. Our algorithm is simple to implement, fast to run, accurate and robust to outliers. We demonstrate its superior performance through extensive experiments which compare our scalable algorithm with the plain implementation on several benchmark data sets.",1051-4651,978-1-5386-3788-3,10.1109/ICPR.2018.8546193,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546193,,Clustering algorithms;Sparse matrices;Matrix decomposition;Principal component analysis;Symmetric matrices;Task analysis;Laplace equations,matrix algebra;pattern clustering;singular value decomposition,scalable spectral clustering;cosine similarity;unified scalable computing framework;Ng-Jordan-Weiss algorithm;document-term frequency matrix;data matrix;matrix-vector multiplication;weight matrix;scalable algorithm;benchmark data sets;normalized cut;diffusion maps;PCA,,1,,18,,29-Nov-18,,,IEEE,IEEE Conferences
Using Web Clustering for Web Communities Mining and Analysis,使用Web群集進行Web社區的挖掘和分析,Y. Zhang; G. Xu,"Centre for Appl. Inf., Victoria Univ., Melbourne, VIC; Centre for Appl. Inf., Victoria Univ., Melbourne, VIC",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,20,31,"Due to the inherent correlation among Web objects and the lack of a uniform schema of Web documents, Web community mining and analysis has become an important area for Web data management and analysis. The research of Web communities spans a number of research domains such as Web mining, Web search, clustering and text retrieval. In this talk we will present some recent studies on this topic, which cover finding relevant Web pages based on linkage information, discovering user access patterns through analyzing Web log files, co-clustering Web objects and investigating social networks from Web data. The algorithmic issues and related experimental studies will be addressed. Some research directions are also to be discussed.",,978-0-7695-3496-1,10.1109/WIIAT.2008.415,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740419,,Web pages;Data analysis;Clustering algorithms;Web search;Information analysis;Databases;Knowledge management;Internet;Intelligent agent;Couplings,data mining;information retrieval;Internet;pattern clustering;text analysis;Web sites,Web communities mining;Web communities analysis;Web clustering;Web documents;Web data management;Web search;text retrieval;Web pages;social networks;Web log files,,4,,21,,6-Jan-09,,,IEEE,IEEE Conferences
A new geometric approach to latent topic modeling and discovery,潛在主題建模和發現的新幾何方法,W. Ding; M. H. Rohban; P. Ishwar; V. Saligrama,"Department of Electrical and Computer Engineering, Boston University, MA, USA; Department of Electrical and Computer Engineering, Boston University, MA, USA; Department of Electrical and Computer Engineering, Boston University, MA, USA; Department of Electrical and Computer Engineering, Boston University, MA, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing",21-Oct-13,2013,,,5568,5572,"A new geometrically-motivated algorithm for topic modeling is developed and applied to the discovery of latent ?topics??in text and image ?document??corpora. The algorithm is based on robustly finding and clustering extreme-points of empirical cross-document word-frequencies that correspond to novel words unique to each topic. In contrast to related approaches that are based on solving non-convex optimization problems using suboptimal approximations, locally-optimal methods, or heuristics, the new algorithm is convex, has polynomial complexity, and has competitive qualitative and quantitative performance compared to the current state- of-the-art approaches on synthetic and real-world datasets.",2379-190X,978-1-4799-0356-6,10.1109/ICASSP.2013.6638729,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638729,Topic modeling;nonnegative matrix factorization (NMF);extreme points;subspace clustering,Integrated circuits;Abstracts;Support vector machines;Logic gates;Nominations and elections;Games,approximation theory;data mining;document image processing;optimisation;pattern clustering;text analysis,polynomial complexity;locally-optimal method;suboptimal approximation;nonconvex optimization problem;empirical cross-document word-frequency;image document corpora;geometrically-motivated algorithm;latent topic discovery;latent topic modeling,,2,,16,,21-Oct-13,,,IEEE,IEEE Conferences
An improved sentiment analysis of online movie reviews based on clustering for box-office prediction,基於聚類的票房預測的在線電影評論情感情緒分析,P. Nagamma; H. R. Pruthvi; K. K. Nisha; N. H. Shwetha,"Department Of Information Technology, National Institute of Technology, Karnataka, India; Department Of Information Technology, National Institute of Technology, Karnataka, India; Department Of Information Technology, National Institute of Technology, Karnataka, India; Department Of Information Technology, National Institute of Technology, Karnataka, India","International Conference on Computing, Communication & Automation",6-Jul-15,2015,,,933,937,"With the rapid development of E-commerce, more online reviews for products and services are created, which form an important source of information for both sellers and customers. Research on sentiment and opinion mining for online review analysis has attracted increasingly more attention because such study helps leverage information from online reviews for potential economic impact. The paper discusses applying sentiment analysis and machine learning methods to study the relationship between the online reviews for a movie and the movies box office revenue performance. The paper shows that a simplified version of the sentiment-aware autoregressive model can produce very good accuracy for predicting the box office sale using online review data. Document level sentiment analysis is used which consists of Term Frequency (TF) and Inverse Document Frequency (IDF) values as features along with Fuzzy Clustering which results in positive and negative sentiments. This lead to the creation of a simpler model which could be more efficient to train and use. In addition, a classification model is created using Support Vector Machine (SVM) Classifier for predicting the trend of the box office revenue from the review sentiment.",,978-1-4799-8890-7,10.1109/CCAA.2015.7148530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148530,,Motion pictures;Accuracy;Sentiment analysis;Predictive models;Mathematical model;Support vector machines;Data mining,learning (artificial intelligence);pattern classification;support vector machines;text analysis,improved sentiment analysis;online movie review analysis;box-office prediction;e-commerce;opinion mining;machine learning methods;sentiment-aware autoregressive model;document level sentiment analysis;IDF;term frequency values;inverse document frequency values;TF;fuzzy clustering;support vector machine classifier;SVM classifier,,14,,8,,6-Jul-15,,,IEEE,IEEE Conferences
Writer profiling using handwriting copybook styles,使用手寫抄寫本樣式進行作家分析,Sungsoo Yoon; Seungseok Choi; Sung-Hyuk Cha; C. C. Tappert,"Sch. of Comput. Sci. & Inf. Syst., Pace Univ., Pleasantville, NY, USA; Sch. of Comput. Sci. & Inf. Syst., Pace Univ., Pleasantville, NY, USA; Sch. of Comput. Sci. & Inf. Syst., Pace Univ., Pleasantville, NY, USA; Sch. of Comput. Sci. & Inf. Syst., Pace Univ., Pleasantville, NY, USA",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,600,604 Vol. 2,"Handwriting originates from a particular copybook style such as Palmer or Zaner-Bloser that one learns in childhood. Since questioned document examination plays an important investigative and forensic role in many types of crime, it is important to develop a system that helps objectively identify a questioned document's handwriting style. We proposed a handwriting analysis system that can assist a document examiner in the identification of the writer's handwriting style and therefore of his/her origin or nationality. We collected 33 English alphabet copybook styles from 18 countries. Here, we extend the analysis using several data mining techniques to discover important information that can be gleaned from a handwriting copybook style image database, e.g., the most information bearing alphabet characters for the purpose of copybook style identification and the relationship between geographical regions and similarity based clusters of copybook styles.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.256,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575615,Handwriting analysis;handwriting copybook style;writer profiling,Forensics;Image analysis;Information analysis;Image databases;Data analysis;Data mining;Text analysis;Handwriting recognition;Clustering algorithms;Computer science,document image processing;handwriting recognition,writer profiling;handwriting copybook styles;document examination;handwriting analysis system;copybook style identification,,,,16,,16-Jan-06,,,IEEE,IEEE Conferences
Comparative study of techniques for brain tumor segmentation,腦腫瘤分割技術的比較研究,H. Tulsani; S. Saxena; M. Bharadwaj,"Electronics and Communication Department, Ambedkar Institute Of Advanced Communication Technologies And Research, New Delhi-110031, India; Electronics and Communication Department, Ambedkar Institute Of Advanced Communication Technologies And Research, New Delhi-110031, India; Electronics and Communication Department, Ambedkar Institute Of Advanced Communication Technologies And Research, New Delhi-110031, India",IMPACT-2013,22-May-14,2013,,,117,120,"In this paper, we present a comparative study of various techniques which have been proposed for segmentation of brain tumors in MRI data. Three different techniques are discussed in this paper. These include morphological watershed segmentation, K-Means and Fuzzy C-means clustering. In watershed technique, marker is used for tumor segmentation. Clustering is a technique for reducing the number of objects in the data set. K-Means and Fuzzy C-Means clustering algorithms are discussed in this paper. K-Means used an objective function for clustering while Fuzzy C-Means comes under the category of soft segmentation technique. Simulation are dome in MATLAB 2013a and results for the techniques are discussed.",,978-1-4799-1205-6,10.1109/MSPCT.2013.6782100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782100,,Portable document format;IEEE Xplore,biomedical MRI;brain;fuzzy set theory;image segmentation;mathematical morphology;medical image processing;pattern clustering;tumours,brain tumor segmentation;MRI;morphological watershed segmentation;fuzzy C-means clustering algorithm;K-means clustering algorithm;soft segmentation technique,,2,,11,,22-May-14,,,IEEE,IEEE Conferences
Similarity score for information filtering thresholds,信息過濾閾值的相似性評分,Jun Lai; B. Soh,"Dept. of Comput. Sci. & Comput. Eng., Latrobe Univ., Melbourne, Vic., Australia; Dept. of Comput. Sci. & Comput. Eng., Latrobe Univ., Melbourne, Vic., Australia","IEEE International Symposium on Communications and Information Technology, 2004. ISCIT 2004.",11-Apr-05,2004,1,,216,221 vol.1,"The rapid growth of on-line information has led to the development of many techniques for information filtering. The tremendous growth in the amount of information available and the number of visitors to Web sites in recent years poses some key challenges for information filtering and retrieval. Web visitors not only expect high quality and relevant information, but also wish that the information be presented in as efficient a way as possible. The traditional filtering methods, however, only consider the relevant values of document. These conventional methods fail to consider the efficiency of document retrieval. In this paper, we propose a new algorithm to calculate an index called document similarity score based on elements of the document. Using the index, document profile is derived. Any documents with the similarity score above a given threshold are clustered. Using these pre-clustered documents, information filtering and retrieval can be made more efficient. Experimental results clearly show our proposed method tremendously improves the efficiency of information filtering and retrieval.",,0-7803-8593-4,10.1109/ISCIT.2004.1412482,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1412482,,Information filtering;Information filters;Information retrieval;Search engines;Clustering algorithms;Electronic mail;Crawlers;Web sites;Books;Conference proceedings,information filtering;search engines;Web sites,information filtering thresholds;pre-clustered documents;information retrieval;on-line information;Web sites;document retrieval efficiency;document similarity score,,,,9,,11-Apr-05,,,IEEE,IEEE Conferences
Multinomial Self Organizing Maps,多項式自組織圖,F. Allouti; M. Nadif; B. Otjacques,"LIPADE, UFR MI, University of Paris Descartes, 45, rue des Saints P癡res, 75270, Paris, France; LIPADE, UFR MI, University of Paris Descartes, 45, rue des Saints P癡res, 75270, Paris, France; Public Research Center - Gabriel Lippmann, Informatics, Systems and Collaboration Department, 41, Rue du Brill, L-4422 Belvaux, Luxembourg",2010 10th International Conference on Intelligent Systems Design and Applications,13-Jan-11,2010,,,621,626,"Co-occurrence data matrices arise frequently in various important applications such as a document clustering. By considering a multinomial mixture model, we present a new probabilistic Self-Organizing Map (SOM) for clustering and visualizing this kind of data. Contrary to SOM, our proposed learning algorithm optimizes an objective function. Its performances are evaluated by using Monte Carlo simulations and real datasets.",2164-7151,978-1-4244-8136-1,10.1109/ISDA.2010.5687194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687194,Porbabilistic formalism;Clustering,Clustering algorithms;Probabilistic logic;Data visualization;Topology;Neurons;Algorithm design and analysis;Markov processes,data visualisation;learning (artificial intelligence);matrix algebra;Monte Carlo methods;pattern clustering;self-organising feature maps,multinomial self organizing maps;cooccurrence data matrices;document clustering;multinomial mixture model;data clustering;data visualization;learning algorithm;Monte Carlo simulations;real datasets,,,,16,,13-Jan-11,,,IEEE,IEEE Conferences
Research and implementation of hot topic detection system based on web,基於Web的熱點話題檢測系統的研究與實現,B. Zhu; Y. Yu; C. Li; H. Wang,"Information Engineering School, Communication, University of China; Information Engineering School, Communication, University of China; Information Engineering School, Communication, University of China; Key Laboratory of Media Audio & Video (Communication University of China), Ministry of Education",2017 3rd IEEE International Conference on Computer and Communications (ICCC),26-Mar-18,2017,,,1504,1509,"To alleviate the problem of ?topic drift??in hot topic on web, the hot topic detection system is implemented. Our system contains the parts as follow. First, news data are collected from Web and cut into the Chinese word segmentation; second, the word weight is calculated by the improved word frequency-inverse document frequency formula (TF-IDF); at last, the Single-Pass clustering algorithm is used for text clustering. The TF-IDF formula is improved, which contains the position information of the word. Experiments show that the hot topic can be effectively identified by this system.",,978-1-5090-6352-9,10.1109/CompComm.2017.8322791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322791,hot topic;web crawler;improved TF-IDF;clustering,Crawlers;Task analysis;Clustering algorithms;Instruction sets;Databases;Portals;Uniform resource locators,information retrieval;Internet;pattern clustering;text analysis,hot topic detection system;topic drift;Chinese word segmentation;word weight;Web;word frequency-inverse document frequency formula;single-pass clustering algorithm;text clustering;TF-IDF formula,,,,14,,26-Mar-18,,,IEEE,IEEE Conferences
A New Search Results Clustering Algorithm Based on Formal Concept Analysis,基於形式概念分析的搜索結果聚類新算法,Y. Zhang; B. Feng; Y. Xue,"Sch. of Electron. & Inf. Eng., Xian Jiaotong Univ., Xian; Sch. of Electron. & Inf. Eng., Xian Jiaotong Univ., Xian; Sch. of Electron. & Inf. Eng., Xian Jiaotong Univ., Xian",2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,5-Nov-08,2008,2,,356,360,"Organizing Web search results into a hierarchy of topics and subtopics facilitates browsing the collection and locating results of interest. In this paper, we propose a new method based on formal concept analysis (FCA) to build a two-level hierarchy for retrieved search results of a query. After formal concepts are extracted using FCA, anew algorithm is proposed to extract concepts most relevant to the query and a two-level hierarchy is built and presented to the user. Evaluating the quality of the resulting clusters is a non-trivial task. Two improved objective metrics of clustering quality, ANMI@K and ANCE@K, are proposed in this paper. We compare our method with three other search results clustering (SRC) algorithms: Suffix tree clustering (STC), Lingo, and Vivisimo, using a comprehensive set of documents obtained from the Open Directory Project hierarchy as benchmark. In addition to comparison based on objective measures, we also subjectively analyze the properties of cluster labels produced by different SRC algorithms. The experimental results show that our method outperforms the other three SRC algorithms, and is helpful for browsing and locating the results of interests.",,978-0-7695-3305-6,10.1109/FSKD.2008.140,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666138,search results clustering;formal concept analysis;ANMI@K;ANCE@K,Clustering algorithms;Algorithm design and analysis;Lattices;Web search;Fuzzy systems;Information analysis;Knowledge engineering;Organizing;Information retrieval;Singular value decomposition,Internet;query formulation,search results clustering algorithm;formal concept analysis;Web search;query;information retrieval;Suffix tree clustering;Lingo;Vivisimo,,2,,12,,5-Nov-08,,,IEEE,IEEE Conferences
Recall estimation for rare topic retrieval from large corpuses,召回估計，從大型語料庫中檢索稀有主題,P. Bommannavar; A. Kolcz; A. Rajaraman,"Twitter, Inc.; Twitter, Inc.; Stanford University",2014 IEEE International Conference on Big Data (Big Data),8-Jan-15,2014,,,825,834,"The problem of finding documents pertaining to a particular topic finds application in a variety of scenarios. Indeed, the demand for topically pertinent documents has led to myriad companies offering services to find and deliver them (perhaps along with sentiment analysis or clustering) to customers for any topics of interest. The methodologies used to uncover relevant documents range from manually curated keyword filters to trained classification models. Any serious topical analysis requires a sound understanding of key metrics behind the retrieval process, two of the most important being precision and recall. While precision can be easily and inexpensively measured by sampling from classified documents and utilizing (paid) human computation to mark incorrectly classified instances, it is not as straightforward to use the same approach for measuring recall. With most topics occurring relatively sparsely, an unbiased sampling approach becomes prohibitively expensive. In this paper, we introduce a recall measurement procedure requiring only relatively few human judgements. The technique makes use of pairs of sufficiently independent classifiers and the paper provides a detailed discussion of how such classifier pairs can be constructed in practice, with a focus on social media classifiers. We report the performance of the proposed method with simple keyword filters as well as with classifiers of progressive levels of complexity and show that under reasonable conditions, recall can be estimated to within 0.10 absolute error and 15% relative error, and often closer with a reduction of cost by a factor of as much as 1000x as compared with unbiased sampling.",,978-1-4799-5666-1,10.1109/BigData.2014.7004312,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004312,recall estimation;classifier evaluation;social media;Twitter;mechanical turk;human evaluation,Estimation;Measurement;Labeling;Media;Twitter;Joints;Companies,document handling;information retrieval;pattern classification;pattern clustering,recall estimation;rare topic retrieval;pertinent documents;sentiment analysis;clustering;keyword filters;classification models;topical analysis;classified documents;unbiased sampling;recall measurement procedure;social media classifiers,,1,,16,,8-Jan-15,,,IEEE,IEEE Conferences
A semantic similarity approach based on web resources,基於網絡資源的語義相似度方法,M. Karthiga; P. Kalaivaani; S. Sankarananth,"Dept of CSE, Kongu Engineering College, Perundurai, Tamilnadu, India; Dept of CSE, Kongu Engineering College, Perundurai, Tamilnadu, India; Dept of EEE, Excel college of Engineering, Nammakkal, Tamilnadu, India",2013 International Conference on Information Communication and Embedded Systems (ICICES),25-Apr-13,2013,,,226,231,"The ability to accurately judge the semantic similarity is important in various tasks on the web such as extracting the relation, document clustering, and automatic metadata extraction. An empirical method is proposed to provide a semantic wise search that uses in one hand, a technical English dictionary and on the other hand, a page count based metric and a text snippet based metric retrieved from a web search engine for two words. To identify the numerous semantic relations between the words, a novel pattern extraction algorithm and a pattern clustering algorithm is proposed. The page counts based co-occurrence measures and lexical pattern clusters extracted from snippets is learned using support vector machines. Integrate the page count, text snippet and dictionary based metric to accurately measure the semantic similarity search compared to normal search.",,978-1-4673-5788-3,10.1109/ICICES.2013.6508365,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508365,Web search;information extraction;natural language processing;user generated content;snippet,Semantics;Dictionaries;Measurement;Web search;Engines;Search engines;Vectors,dictionaries;document handling;pattern clustering;search engines;support vector machines,semantic similarity approach;Web resources;relation extraction;document clustering;automatic metadata extraction;semantic wise search;technical English dictionary;page count based metric;text snippet based metric;Web search engine;numerous semantic relations;pattern clustering algorithm;lexical pattern clusters;snippets;support vector machines;semantic similarity search,,,,25,,25-Apr-13,,,IEEE,IEEE Conferences
A Million-Plus Neuron Model of the Hippocampal Dentate Gyrus: Critical Role for Topography in Determining Spatiotemporal Network Dynamics,海馬齒狀回的百萬加神經元模型：地形在確定時空網絡動力學中的關鍵作用,P. J. Hendrickson; G. J. Yu; D. Song; T. W. Berger,"Department of Biomedical Engineering, Center for Neural Engineering, University of Southern California, Los Angeles, CA, USA; University of Southern California; University of Southern California; University of Southern California",IEEE Transactions on Biomedical Engineering,18-Dec-15,2016,63,1,199,209,"Goal: This paper describes a million-plus granule cell compartmental model of the rat hippocampal dentate gyrus, including excitatory, perforant path input from the entorhinal cortex, and feedforward and feedback inhibitory input from dentate interneurons. Methods: The model includes experimentally determined morphological and biophysical properties of granule cells, together with glutamatergic AMPA-like EPSP and GABAergic GABAA-like IPSP synaptic excitatory and inhibitory inputs, respectively. Each granule cell was composed of approximately 200 compartments having passive and active conductances distributed throughout the somatic and dendritic regions. Modeling excitatory input from the entorhinal cortex was guided by axonal transport studies documenting the topographical organization of projections from subregions of the medial and lateral entorhinal cortex, plus other important details of the distribution of glutamatergic inputs to the dentate gyrus. Information contained within previously published maps of this major hippocampal afferent were systematically converted to scales that allowed the topographical distribution and relative synaptic densities of perforant path inputs to be quantitatively estimated for inclusion in the current model. Results: Results showed that when medial and lateral entorhinal cortical neurons maintained Poisson random firing, dentate granule cells expressed, throughout the million-cell network, a robust nonrandom pattern of spiking best described as a spatiotemporal ?clustering.??To identify the network property or properties responsible for generating such firing ?clusters,??we progressively eliminated from the model key mechanisms, such as feedforward and feedback inhibition, intrinsic membrane properties underlying rhythmic burst firing, and/or topographical organization of entorhinal afferents. Conclusion: Findings conclusively identified topographical organization of inputs as the key element responsible for generating a spatiotemporal distribution of clustered firing. These results uncover a functional organization of perforant path afferents to the dentate gyrus not previously recognized: topography-dependent clusters of granule cell activity as ?functional units??or ?channels??that organize the processing of entorhinal signals. This modeling study also reveals for the first time how a global signal processing feature of a neural network can evolve from one of its underlying structural characteristics.",1558-2531,,10.1109/TBME.2015.2445771,Office of Naval Research; U.S. NIBIB; U.S. NIH; University of Southern California Center for High-Performance Computing and Communications; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7124441,hippocampus;dentate gyrus;entorhinal cortex;neuroanatomy;topography;topographical organization;mathematical model;model;compartmental model;cluster analysis;spatio-temporal pattern;dynamics;spike;rhythmicity;gamma;synchrony;oscillations;Cluster analysis;compartmental model;dentate gyrus (DG);dynamics;entorhinal cortex (EC);gamma;hippocampus;mathematical model;model;neuroanatomy;oscillations;rhythmicity;spatiotemporal pattern;spike;synchrony;topographical organization;topography,Firing;Brain modeling;Organizations;Surfaces;Mathematical model;Nerve fibers,biomembrane transport;brain models;neural nets;neurophysiology;random processes;spatiotemporal phenomena;stochastic processes,spatiotemporal network dynamics;million-plus granule cell compartmental model;rat hippocampal dentate gyrus;feedforward inhibitory input;feedback inhibitory input;dentate interneurons;morphological properties;biophysical properties;glutamatergic AMPA-like EPSP synaptic excitatory inputs;GABAergic GABAA-like IPSP synaptic excitatory inputs;GABAergic GABAA-like IPSP inhibitory inputs;passive conductances;active conductances;somatic region;dendritic region;axonal transport;topographical organization;medial entorhinal cortex;lateral entorhinal cortex;glutamatergic inputs;major hippocampal afferent;topographical distribution;synaptic densities;perforant path inputs;medial entorhinal cortical neurons;lateral entorhinal cortical neurons;Poisson random firing;dentate granule cells;nonrandom spiking pattern;spatiotemporal clustering;network property;firing clusters;intrinsic membrane properties;rhythmic burst firing;entorhinal afferents;spatiotemporal distribution;perforant path afferents;topography-dependent clusters;granule cell activity;functional units;entorhinal signal processing;global signal processing feature;neural network;structural characteristics,"Animals;Brain Mapping;Cluster Analysis;Computer Simulation;Dentate Gyrus;Models, Neurological;Neurons;Rats",18,,89,,16-Jun-15,,,IEEE,IEEE Journals
Dual Fuzzy-Possibilistic Co-clustering for Document Categorization,用於文檔分類的雙重模糊可能性聯合聚類,W. Tjhi; L. Chen,NA; NA,Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007),31-Mar-08,2007,,,259,264,"In this paper, we introduce a new algorithm called Dual Fuzzy-possibilistic Co-clustering (DFPC) for docu- ment categorization. The proposed algorithm offers several advantages. Firstly, the combined fuzzy and possibilistic cluster memberships in DFPC can provide realistic repre- sentation of document clusters. Secondly, as a co-clustering algorithm, DFPC can categorize high-dimensional datasets effectively. Thirdly, the possibilistic clustering element of the algorithm makes it robust to outliers. We detail the for- mulation of DFPC, and empirically demonstrate its effec- tiveness in categorizing benchmark document datasets.",2375-9259,978-0-7695-3019-2,10.1109/ICDMW.2007.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4476677,,Clustering algorithms;Robustness;Data mining;Data engineering;Conferences;Context modeling;Constraint optimization;Fuzzy sets;TV;Matrix decomposition,,,,1,,16,,31-Mar-08,,,IEEE,IEEE Conferences
Managing Textual Data Semantically In Relational Databases,在關係數據庫中語義管理文本數據,W. M. S. Yafooz; S. Z. Abdin; S. A. Fahad,"Faculty of Computer Information Technology, Faculty of Computer and Mathematical Sciences UiTM Shah Alam, Al-Madinah International University, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences UiTM, Shah Alam, Malaysia; Faculty of Computer & Information Technology, Al-Madinah International University, Shah Alam, Malaysia",2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE),18-Nov-18,2018,,,1,5,"The massive volume of data in databases, web pages, and document files usually causes information to be disorganized and unclear for the user. Therefore, information in such an environment can be classified into three forms: structured, semistructured, or unstructured. Structured information is the best form of information because it facilitates the acquisition and comprehension of knowledge. Relational Database Management System (RDBMS) has a robust structure that manages, organizes and retrieves data. There are many attempts have been made in order to deal with such data. These attempts can be categorized into three groups: within a database schema, by a developed data model within the database, or by query-based techniques in database. Nonetheless, RDBMS contain massive amount of unstructured data such as textual data.. This paper proposed Textual Virtual Schema Model (TVSM). TVSM is conducted to perform semantic textual data linking and clustering and is embedded in the relational database structure (schema). In addition, linking and converting the unstructured information to structured data. Quality improvement of textual data clusters. Achievement of high query processing efficiency in retrieving data clusters. TVSM initially developed to assist researchers, developers, and database administrators who are concerned on unstructured information management, information extraction, multi-document clustering, information retrieval, query processing efficiency, personal information management, question answering, information integration, news tracking, and news summarization.",,978-1-5386-4838-4,10.1109/ICSCEE.2018.8538426,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538426,database;textual documents;clustering;unstructured data,Data models;Relational databases;Semantics;Information retrieval;Data mining;Conferences,data models;Internet;knowledge acquisition;pattern clustering;query processing;relational databases;text analysis,information retrieval;personal information management;web pages;RDBMS;TVSM;textual data clusters;database administrators;information extraction;multidocument clustering;relational database management system;data model;query processing;textual virtual schema model;knowledge acquisition,,,,14,,18-Nov-18,,,IEEE,IEEE Conferences
Incremental hierarchical discriminant regression for online image classification,在線圖像分類的增量層次判別回歸,Juyang Weng; Wey-Shiuan Hwang,"Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; NA",Proceedings of Sixth International Conference on Document Analysis and Recognition,7-Aug-02,2001,,,476,480,"This paper presents an incremental algorithm for image classification problems. Virtual labels are automatically formed by clustering in the output space. These virtual labels are used for the process of deriving discriminating features in the input space. This procedure is performed recursively in a coarse-to-fine fashion resulting in a tree, called incremental hierarchical discriminating regression (IHDR) method. Embedded in the tree is a hierarchical probability distribution model used to prune unlikely cases. A sample size dependent negative-log-likelihood (NLL) metric is introduced to deal with large-sample size cases, small-sample size cases, and unbalanced-sample size cases, measured among different internal nodes of the IHDR algorithm. We report the experimental results of the proposed algorithm for an OCR classification problem and an image orientation classification problems.",,0-7695-1263-1,10.1109/ICDAR.2001.953835,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=953835,,Image classification;Image databases;Spatial databases;Information retrieval;Decision trees;Optical character recognition software;Principal component analysis;Linear discriminant analysis;Neural networks;Computer science,image classification;statistical analysis;online operation;pattern clustering;trees (mathematics),incremental hierarchical discriminant regression;online image classification;virtual labels;clustering;discriminating features;recursive coarse-to-fine feature derivation;tree;IHDR method;hierarchical probability distribution model;unlikely case pruning;sample size dependent negative-log-likelihood metric;NLL metric;OCR;image orientation classification problems,,6,,5,,7-Aug-02,,,IEEE,IEEE Conferences
Multi-template GAT correlation for character recognition with a limited quantity of data,多模板GAT相關性用於有限數量的數據字符識別,T. Wakahara,"Fac. of Comput. & Inf. Sci., Hosei Univ., Tokyo, Japan",Eighth International Conference on Document Analysis and Recognition (ICDAR'05),16-Jan-06,2005,,,824,828 Vol. 2,"This paper addresses the problem of how to construct a robust character classifier when statistical pattern recognition techniques fail because of a limited quantity of data. The key ideas are two ways. One is to adopt a distortion-tolerant shape matching technique. Here, we use an affine-invariant matching technique of global affine transformation (GAT) correlation to absorb linear distortion between grayscale images. The other is to prepare multiple templates for dealing with nonlinear distortion or topologically different shapes. For this purpose, K-means clustering is applied to a given limited data in a simple gradient feature space. Recognition experiments using the handwritten numeral database IPTP CDROMIB show that the proposed method achieves a much higher recognition rate of 97.2% as compared to that of 85.8% obtained by the conventional, simple correlation matching with a single template per category.",2379-2140,0-7695-2420-6,10.1109/ICDAR.2005.166,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1575660,,Character recognition;Shape;Image databases;Spatial databases;Handwriting recognition;Pattern recognition;Nonlinear distortion;Gray-scale;Robustness;Optical character recognition software,handwritten character recognition;image matching;image classification;pattern clustering;correlation methods;gradient methods;affine transforms;visual databases,multitemplate GAT correlation;character recognition;character classifier;shape matching;affine-invariant matching;global affine transformation;linear distortion;grayscale image;nonlinear distortion;K-means clustering;gradient feature space;handwritten numeral database;IPTP CDROMIB;correlation matching,,2,,11,,16-Jan-06,,,IEEE,IEEE Conferences
Ukiyo-e Rakkan Retrieval System,浮世繪Rakkan檢索系統,L. Li; C. Panichkriangkrai; K. Hachimura,"Ritsumeikan Global Innovation Res. Organ., Ritsumeikan Univ., Kusatsu, Japan; Grad. Sch. of Sci. & Eng., Ritsumeikan Univ., Kusatsu, Japan; Coll. of Inf. Sci. & Eng., Ritsumeikan Univ., Kusatsu, Japan",2013 12th International Conference on Document Analysis and Recognition,15-Oct-13,2013,,,150,154,"Rakkan is a Japanese word for the signature of the artist for a completed painting. In this paper, we propose a system that helps the users to identify and retrieve rakkan images of ukiyo-e paintings. The users can select the rakkan region from a digitalized ukiyo-e image via a graphical user interface. The selected rakkan area is binarized using k-means clustering in L*a*b* color space and normalized for retrieval process as a query image. Histogram of oriented gradients and cross-correlation are used for retrieval. The proposed system can be adapted to ukiyo-e authorship identification and ukiyo-e retrieval system for ukiyo-e study and archiving.",2379-2140,978-0-7695-4999-6,10.1109/ICDAR.2013.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628602,ukiyo-e;rakkan;authorship identification;image retrieval,Feature extraction;Image color analysis;Educational institutions;Painting;Databases;Art;Histograms,art;authorisation;graphical user interfaces;handwriting recognition;image retrieval;natural language processing;pattern clustering,ukiyo-e Rakkan image retrieval system;Japanese word;artist signature;ukiyo-e paintings;digitalized ukiyo-e image;graphical user interface;k-means clustering;color space;query image;histogram of oriented gradients;cross-correlation;ukiyo-e authorship identification;archiving,,,,7,,15-Oct-13,,,IEEE,IEEE Conferences
Kapsama katsayisi tabanli k羹meleme ile belge 繹zetleme,Kapsama katsayisi tabanli k羹meleme ile belge ucezeme,M. ?akir; E. ?elebi,"Bilgisayar M羹hendisli?i B繹l羹m羹, Uluslararasi Kibris 羹niversitesi, Turkey; Bilgisayar M羹hendisli?i B繹l羹m羹, Uluslararasi Kibris 羹niversitesi, Turkey",2011 IEEE 19th Signal Processing and Communications Applications Conference (SIU),23-Jun-11,2011,,,186,189,"Multimedia documents, especially the text documents are occupying an important time in our daily life. Reading the summery of documents will allow us to spend less time and understand the document faster. A summary consist of the subset sentences of documents and they needs to cover the main content of document as a brief. In this paper, we have proposed an automatic, language independent text summarization system, that is based on Cover Coefficient-Based Clustering Methodology (C3M) With our methodology C3M extracts the summary sentences from the similarity between the document sentences where their features extracted by Natural Language Processing (NLP) methods such as Term Frequency (TF) and Feature Extraction. Performance evaluations evaluated on Turkish document summaries were approved by 10 human evaluators such as research assistants and graduate students. We have evaluated our machine summaries with using Precision, Recall and Rouge-N evaluation methods. Performance evaluation results shows that our system results are better than most of similar systems.",2165-0608,978-1-4577-0463-5,10.1109/SIU.2011.5929618,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5929618,,Conferences;Feature extraction;Signal processing;Natural language processing;Artificial intelligence;Humans;HTML,natural language processing;pattern clustering;text analysis,multimedia document summarization;cover coefficient based clustering methodology;automatic language independent text summarization system;natural language processing methods;term frequency;feature extraction performance evaluations;Turkish document summaries;precision evaluation methods;recall evaluation methods;Rouge-N evaluation methods,,,,6,,23-Jun-11,,,IEEE,IEEE Conferences
Socket cloning for cluster-based web servers,基於集群的Web服務器的套接字克隆,Yiu-Fai Sit; Cho-Li Wang; F. Lau,"Dept. of Comput. Sci. & Inf. Syst., Univ. of Hong Kong, China; Dept. of Comput. Sci. & Inf. Syst., Univ. of Hong Kong, China; Dept. of Comput. Sci. & Inf. Syst., Univ. of Hong Kong, China",Proceedings. IEEE International Conference on Cluster Computing,6-Jan-03,2002,,,333,340,"Cluster-based web server is a popular solution to meet the demand of the ever-growing web traffic. However existing approaches suffer from several limitations to achieve this. Dispatcher-based systems either can achieve only coarse-grained load balancing or would introduce heavy load to the dispatcher Mechanisms like cooperative caching consume much network resources when transferring large cache objects. In this paper, we present a new network support mechanism, called Socket Cloning (SC), in which an opened socket can be migrated efficiently between cluster nodes. With SC, the processing of HTTP requests can be moved to the node that has a cached copy of the requested document, thus bypassing any object transfer between peer servers. A prototype has been implemented and tests show that SC incurs less overhead than all the mentioned approaches. In trace-driven benchmark tests, our system outperforms these approaches by more than 30% with a cluster of twelve web server nodes.",,0-7695-1745-5,10.1109/CLUSTR.2002.1137762,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1137762,,Sockets;Cloning;Web server;Peer to peer computing;Telecommunication traffic;Load management;Cooperative caching;Network servers;Prototypes;Testing,file servers;Internet;resource allocation,web server;cluster-based;network support mechanism;Socket Cloning;peer servers;trace-driven benchmark;web hosting,,2,,31,,6-Jan-03,,,IEEE,IEEE Conferences
Correlation clustering,相關聚類,N. Bansal; A. Blum; S. Chawla,"Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA","The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.",28-Feb-03,2002,,,238,247,"We consider the following clustering problem: we have a complete graph on n vertices (items), where each edge (u, /spl upsi/) is labeled either + or - depending on whether a and /spl upsi/ have been deemed to be similar or different. The goal is to produce a partition of the vertices (a clustering) that agrees as much as possible with the edge labels. That is, we want a clustering that maximizes the number of + edges within clusters, plus the number of - edges between clusters (equivalently, minimizes the number of disagreements: the number of - edges inside clusters plus the number of + edges between clusters). This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data, and the goal is to partition the current set of documents in a way that correlates with f as much as possible; it can also be viewed as a kind of ""agnostic learning"" problem. An interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter, as in measures such as k-median or min-sum or min-max clustering. Instead, in our formulation, the optimal number of clusters could be any value between 1 and n, depending on the edge labels. We look at approximation algorithms for both minimizing disagreements and for maximizing agreements. For minimizing disagreements, we give a constant factor approximation. For maximizing agreements we give a PTAS. We also show how to extend some of these results to graphs with edge labels in [-1, +1], and give some results for the case of random noise.",0272-5428,0-7695-1822-2,10.1109/SFCS.2002.1181947,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1181947,,Clustering algorithms;Approximation algorithms;Computer science;Training data;Databases,probability;randomised algorithms;approximation theory,correlation clustering;complete graph;edge labels;pairwise similarity function;min-max clustering;min-sum clustering;approximation algorithms,,76,,20,,28-Feb-03,,,IEEE,IEEE Conferences
Dimension reduction in text document retrieval by Hebbian neural network and nonlinear activation functions,通過Hebbian神經網絡和非線性激活函數進行文本文檔檢索的降維,L. Skovajsov獺; I. Mokri禳,"Institute of Informatics, Slovak Academy of Sciences, Slovakia; Institute of Informatics, Slovak Academy of Sciences, Slovakia",2012 4th IEEE International Symposium on Logistics and Industrial Informatics,4-Oct-12,2012,,,59,64,"The paper deals with utilization of neural networks for information retrieval. It is focused on reduction of text document space by Hebbian neural networks. The Hebbian neural network with Oja learning rule with linear activation function reduces term space into much lower dimension and gives good results for text document dimension reduction and retrieval. The aim of this paper is to try to increase the retrieval evaluation by F-measure that applies different nonlinear activation functions to the output layer of network. Results show better F-measure when applying other nonlinear activation functions instead of applying classical linear activation function. The results were verified on the collection of 50 documents and 100 terms, where documents were clustered into five different clusters. For each dimension the Precision, Recall and F-measure were computed and the results were depicted graphically.",2156-8804,978-1-4673-4519-4,10.1109/LINDI.2012.6319462,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319462,,Neural networks;Vectors;Information retrieval;Training;Semantics;Electronic mail;Organizations,Hebbian learning;information retrieval;neural nets;nonlinear functions;text analysis;transfer functions,dimension reduction;text document retrieval;Hebbian neural network;nonlinear activation functions;information retrieval;text document space reduction;Oja learning rule;linear activation function;term space reduction;F-measure;precision;recall,,,,17,,4-Oct-12,,,IEEE,IEEE Conferences
A HMM_based hot topic lifecycle prediction model,基於HMM_的熱門話題生命週期預測模型,Ruifang Liu; Jun Wang; Meng Zhang,"School of Information and Communication Engineering, Beijing University of Posts and Telecommunication, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunication, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunication, 100876, China",2011 International Conference on Electric Information and Control Engineering,27-May-11,2011,,,2881,2884,"Web documents can be clustered into topics with topic detection and tracking(TDT) technologies. With the topics' data collected by TDT system, it is found that the lifecycle of topics has 4 stages. In the paper a HMM-based state prediction model for topics is proposed. Some topics with similar lifecycles share a same model, several models are trained with history data of topics, these models are used for new topic state prediction. Experiment results show the performance of the Forward Probability Prediction Algorithm, and the comparison with other method is analyzed. It will be useful for the department who concern the hot topic monitoring.",,978-1-4244-8039-5,10.1109/ICEICE.2011.5777416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5777416,hot topic prediction;topic detection and tracking;prediction model;HMM,Hidden Markov models;Internet;Predictive models;Data models;Monitoring;Event detection;Fuzzy systems,document handling;pattern clustering,HMM_based hot topic lifecycle prediction model;web documents;topic detection and tracking;TDT;forward probability prediction algorithm,,,,,,27-May-11,,,IEEE,IEEE Conferences
A Novel Interactive Image Recommendation System,一種新穎的交互式圖像推薦系統,Q. Bo; J. Peng,"Inst. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China; Inst. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China",2010 International Forum on Information Technology and Applications,11-Nov-10,2010,2,,248,251,"With the repaid development of internet technology, image documents have become an important information source. It is hard for us to retrieve certain images from all available ones. In this paper, we propose an interactive image recommendation system, which firstly uses color histogram feature or Gabor texture feature to express image contents, then a kernel based K-meanse is utilized to cluster images into multiple classes by their visual features, finally based on a sample and hyperbolic techniques, images are recommended and displayed. The experimental results demonstrate that the proposed system can recommend and display the similar images from the same class efficiently, when users click on the images they are interested in.",,978-1-4244-7622-0,10.1109/IFITA.2010.289,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5634828,Image Recommendation;similarity-preserving image projection,Image color analysis;Kernel;Histograms;Feature extraction;Visualization;Gabor filters;Eigenvalues and eigenfunctions,content-based retrieval;document image processing;feature extraction;image colour analysis;image retrieval;image texture;pattern clustering;recommender systems,interactive image recommendation system;Internet technology;image documents;color histogram feature;Gabor texture feature;k-means kernel;image retrieval;hyperbolic technique,,1,,6,,11-Nov-10,,,IEEE,IEEE Conferences
Clustering Arabic Tweets for Sentiment Analysis,聚類阿拉伯推文進行情感分析,D. Abuaiadah; D. Rajendran; M. Jarrar,"Centre for Bus., Waikato Inst. of Technol., New Zealand; Centre for Bus., Waikato Inst. of Technol., New Zealand; Comput. Sci. Dept., Birzeit Univ., Birzeit, Palestinian Authority",2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA),12-Mar-18,2017,,,449,456,"The focus of this study is to evaluate the impact of linguistic preprocessing and similarity functions for clustering Arabic Twitter tweets. The experiments apply an optimized version of the standard K-Means algorithm to assign tweets into positive and negative categories. The results show that root-based stemming has a significant advantage over light stemming in all settings. The Averaged Kullback-Leibler Divergence similarity function clearly outperforms the Cosine, Pearson Correlation, Jaccard Coefficient and Euclidean functions. The combination of the Averaged Kullback-Leibler Divergence and root-based stemming achieved the highest purity of 0.764 while the second-best purity was 0.719. These results are of importance as it is contrary to normal-sized documents where, in many information retrieval applications, light stemming performs better than root-based stemming and the Cosine function is commonly used.",2161-5330,978-1-5386-3581-0,10.1109/AICCSA.2017.162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308321,Clustering algorithms;K-means;Sentiment analysis;Arabic stemmers;Arabic tweets,Clustering algorithms;Classification algorithms;Sentiment analysis;Pragmatics;Twitter;Standards,information retrieval;pattern clustering;sentiment analysis;social networking (online),sentiment analysis;linguistic preprocessing;similarity functions;positive categories;negative categories;light stemming;Arabic tweet clustering;averaged Kullback-Leibler divergence similarity function;root-based stemming,,1,,31,,12-Mar-18,,,IEEE,IEEE Conferences
GATE: Classification and clustering of text for semi-vowel/j/-morphophonemic approach,GATE：半元音/ j /變聲方法的文本分類和聚類,K. S. Reddy; K. Sasanka; S. Prasanna; R. Venkatesan,"CSE, SRC-Sastra University, Kumbakonam; CSE, SRC-Sastra University, Kumbakonam; CSE, SRC-Sastra University, Kumbakonam; CSE, SRC-Sastra University, Kumbakonam","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)",4-Aug-16,2016,,,1,7,"In recent years, many successful machine learning applications have been developed. Classification & Clustering is one such. This application is cross-disciplinary, now that it is based on data mining algorithms on the technical side and on graphemes and morphophonemic on the linguistic side. It will thus map the correspondence between grapheme ?y??and related phonemes via morphemes in a given context. The grapheme ?y??is often realized as an approximant /j/ (a consonant phoneme), as in ?yacht??or as vowel / i / as in ?racy?? or a diphthong / ai / as in ?sky?? etc. The objective, that is to say, of this text analysis is to map the various articulators//phonemic realizations of the grapheme ?y?? This experiment will thus help the study the occurrence of ?y??in different positions in words, word initially, finally or elsewhere, as in these examples. The training data (or corpus) chosen for this experimentation is a set of English literary texts [Harry Potter part 1, part 2, The complete works of William Shakespeare By William Shakespeare, The Adventures of Sherlock Holmes By Arthur Conan Doyle, (A Christmas carol) By Charles Dickens's. As for the tools, the alphabet recognizer is used for retrieving information and categorizing the data as noun, adjective, etc. Then, GATE Developer, the Text engineering tool is used to analyze the dataset and to derive the output with statistical data of global distribution ?y??as in all the input documents. This project is particularly relevant because it offers, as will be demonstrated, a solution to problems due to lack of one-to-one correspondence between spelling and pronunciation in English, as for instance, in the context of language pedagogy. Glossary of technical terms (of both linguistics & computing) is appended, now that the paper is inter-disciplinary.",,978-1-5090-1277-0,10.1109/ICCPCT.2016.7530234,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530234,Clustering and Classification;machine learning;Text mining;GATE;grapheme;morpheme;phoneme;morphophonemic;mapping;NLP,Logic gates;Pragmatics;Computers;Text mining;Classification algorithms;Clustering algorithms,classification;computational linguistics;data mining;learning (artificial intelligence);pattern clustering;statistical analysis;text analysis,text classification;text clustering;semivowel/j/-morphophonemic approach;machine learning;data mining;graphemes;morphemes;text analysis;articulator;phonemic realization;alphabet recognizer;GATE developer;text engineering tool;statistical data;language pedagogy,,,,27,,4-Aug-16,,,IEEE,IEEE Conferences
Extractive Text Summarization Technique Using Fuzzy C-Means Clustering Algorithm,基於模糊C-均值聚類算法的文本提取摘要技術,S. Gani; J. Uddin; M. I. Mobin,"BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh; BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh; BRAC University,Department of Computer Science and Engineering,Dhaka,Bangladesh","2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)",16-Mar-20,2019,,,1,6,"Text summarization process has become one of the significant research areas for years owing to cope up with the astounding increase of virtual textual materials. Text summarization is the process to keep the relevant important information of the original text in a shorter version with the main ideas of the original text. There are two main classifications of text summarization process Extractive and Abstractive text summarization, Extractive summarization processes by using most important fragments of existing words, phrases or sentences from the original document, A sentence based mode) using Fuzzy C-Means clustering has been proposed in this research. Six best key features including a new feature ?Sentence Highlighter Feature??have been introduced for the sentence scoring. Performance of the proposed FCM mode) is evaluated by ROUGE, which has been gauged with the precision, recall and f-measure. The result shows that this FCM model extractive techniques with a less outline repetition and profundity of data.",,978-1-7281-3060-6,10.1109/IC4ME247184.2019.9036642,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036642,Sentence Extraction;Clustering;Summarization,Clustering algorithms;Mathematical model;Feature extraction;Silicon;Partitioning algorithms;Chemicals;Data mining,fuzzy set theory;pattern clustering;text analysis,Extractive text summarization technique;Fuzzy C-Means clustering algorithm;Extractive summarization processes;Sentence Highlighter Feature;FCM model extractive techniques;virtual textual materials;sentence scoring;f-measure;Abstractive text summarization,,,,19,,16-Mar-20,,,IEEE,IEEE Conferences
A new approach for clustering alarm sequences in mobile operators,移動運營商中警報序列聚類的新方法,S. Sozuer; C. Etemoglu; E. Zeydan,"OBSS, Istanbul, Turkey; T羹rk Telekom Labs, Istanbul, Turkey; T羹rk Telekom Labs, Istanbul, Turkey",NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium,4-Jul-16,2016,,,1055,1060,"Telecom Networks produce huge amount of daily alarm logs. These alarms usually arrive from different regions and network equipments of mobile operators at different times. In a typical network operator, Network Operations Centers (NOCs) constantly monitor those alarms in a central location and try to fix issues raised by intelligent warning systems by performing a trouble ticketing based management system. In order to automate rule findings, different sequential rule mining algorithms can be exploited. However, the number of sequential rules and alarm correlations that can be generated by using these algorithms can overwhelm the NOC administrators since some of those rules are neither utilized nor reduced appropriately by the non-customized sequential rule mining algorithms. Therefore, additional efficient and intelligent rule identification techniques need to be developed depending on the characteristic of the data. In this paper, two new metrics that is inspired from document classification approaches are proposed in order to increase the accuracy of the sequential alarm rules. This approach utilizes new definition of identifying transactions as alarm features and clustering the alarms by their occurrences in built transactions. Experimental evaluations demonstrate that up to 61% accuracy improvements can be achieved through utilizing the proposed appropriate metrics compared to a sequential rule mining algorithm.",2374-9709,978-1-5090-0223-8,10.1109/NOMS.2016.7502960,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502960,telecommunication;network management;clustering;alarm correlation;rule mining,Data mining;Clustering algorithms;Correlation;Measurement;Databases;Mobile computing;Mobile communication,alarm systems;correlation methods;data mining;mobile radio;pattern clustering;telecommunication computing;telecommunication network management,alarm sequence clustering;mobile operators;telecom networks;daily alarm logs;network equipments;network operator;Network Operations Centers;intelligent warning systems;trouble ticketing based management system;rule findings;alarm correlations;NOC administrators;noncustomized sequential rule mining algorithms;intelligent rule identification techniques;sequential alarm rule accuracy;transaction identification,,7,,29,,4-Jul-16,,,IEEE,IEEE Conferences
An Efficient Linear Text Segmentation Algorithm Using Hierarchical Agglomerative Clustering,基於層次聚類的高效線性文本分割算法,J. Wu; J. C. R. Tseng; W. Tsai,"Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Chung Hua Univ., Hsinchu, Taiwan; Dept. of Multimedia & Game Sci., Yu-Da Univ., Miaoli, Taiwan",2011 Seventh International Conference on Computational Intelligence and Security,12-Jan-12,2011,,,1081,1085,"Linear text segmentation aims at dividing a long text into several topical segments. It is beneficial to many natural language processing tasks, such as information retrieval and document summarization. In this article, an efficient linear text segmentation algorithm based on hierarchical agglomerative clustering is presented. The proposed linear text segmentation algorithm is implemented without auxiliary knowledge base, parameter setting, and user involvement. Experimental results show that the proposed linear text segmentation algorithm not only provides linear time computational complexity, but also provides comparable segmentation accuracy with several well-known linear text segmentation algorithms.",,978-1-4577-2008-6,10.1109/CIS.2011.240,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6128290,text segmentation;hierarchical agglomerative clustering;computational intelligence;NLP application,Algorithm design and analysis;Clustering algorithms;Accuracy;Computational complexity;Heuristic algorithms;Merging;Error probability,computational complexity;knowledge based systems;natural language processing;pattern clustering;text analysis,efficient linear text segmentation algorithm;hierarchical agglomerative clustering;natural language processing task;auxiliary knowledge base;linear time computational complexity;segmentation accuracy,,2,,12,,12-Jan-12,,,IEEE,IEEE Conferences
Improving the clustering performance of the scanning n-tuple method by using self-supervised algorithms to introduce subclasses,通過使用自監督算法引入子類來提高掃描n元組方法的聚類性能,G. Tambouratzis,"Inst. for Language & Speech Process., Athens, Greece",IEEE Transactions on Pattern Analysis and Machine Intelligence,7-Aug-02,2002,24,6,722,733,"The scanning n-tuple technique (as introduced by Lucas and Amiri, 1996) is studied in pattern recognition tasks, with emphasis placed on methods that improve its recognition performance. We remove potential edge effect problems and optimize the parameters of the scanning n-tuple method with respect to memory requirements, processing speed and recognition accuracy for a case study task. Next, we report an investigation of self-supervised algorithms designed to improve the performance of the scanning n-tuple method by focusing on the characteristics of the pattern space. The most promising algorithm is studied in detail to determine its performance improvement and the consequential increase in the memory requirements. Experimental results using both small-scale and real-world tasks indicate that this algorithm results in an improvement of the scanning n-tuple classification performance.",1939-3539,,10.1109/TPAMI.2002.1008380,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1008380,,Character recognition;Pattern recognition;Neural networks;Clustering algorithms;Optimization methods;Algorithm design and analysis;Handwriting recognition;Delay;Random access memory;Digital circuits,pattern clustering;handwritten character recognition;document image processing;unsupervised learning;performance evaluation,clustering performance;scanning n-tuple method;self-supervised algorithms;subclasses;pattern recognition;edge effect problems;handwritten character recognition;chain coding;memory requirements;processing speed;case study;experimental results,,8,,15,,7-Aug-02,,,IEEE,IEEE Journals
Multilabel Text Categorization Based on Fuzzy Relevance Clustering,基於模糊關聯聚類的多標籤文本分類,S. Lee; J. Jiang,"Department of Electrical Engineering , National Sun Yat-Sen University, Kaohsiung, Taiwan; Department of Electrical Engineering , National Sun Yat-Sen University, Kaohsiung, Taiwan",IEEE Transactions on Fuzzy Systems,25-Nov-14,2014,22,6,1457,1471,"We propose a fuzzy based method for multilabel text classification in which a document can belong to one or more than one category. In text categorization, the number of the involved features is usually huge, causing the curse of the dimensionality problem. Besides, a category can be a nonconvex region, which is a union of several overlapping or disjoint subregions. An automatic classification system, thus, may suffer from large memory requirements or poor performance. By incorporating fuzzy techniques, our proposed method can overcome these issues. A fuzzy relevance measure is adopted to transform high-dimensional documents to low-dimensional fuzzy relevance vectors to avoid the curse of dimensionality problem. A clustering technique is used to divide the relevance space into a collection of subregions which are then combined to make up individual categories. This allows complex and nonconvex regions to be created. A number of experiments are presented to show the effectiveness of the proposed method in both performance and speed.",1941-0034,,10.1109/TFUZZ.2013.2294355,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679223,Clustering;dimensionality reduction;fuzzy relevance;multilabel learning;text classification,Vectors;Training;Testing;Text categorization;Equations;Principal component analysis;Transforms,fuzzy set theory;pattern clustering;text analysis,multilabel text categorization;fuzzy relevance clustering;fuzzy based method;curse-of-dimensionality problem;fuzzy techniques;fuzzy relevance measure;fuzzy relevance vectors;clustering technique,,24,,78,,6-Dec-13,,,IEEE,IEEE Journals
Empirical Analysis of the Rank Distribution of Relevant Documents in Web Search,網絡搜索中相關文檔等級分佈的實證分析,S. Jiang; S. Zilles; R. Holte,"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB",2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,6-Jan-09,2008,1,,208,213,"This paper proposes an empirical approach for analyzing the rank distribution of relevant documents in Web search. From a methodological point of view a new transaction log analysis method is proposed; the relevance of documents is studied over transaction sessions rather than single transactions. From a practical point of view the paper provides insights about the actual rank distribution of relevant documents in Web search, with several consequences for the design of applications related to Web search.",,978-0-7695-3496-1,10.1109/WIIAT.2008.200,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4740451,transaction log analysis,Web search;Search engines;Statistics;Intelligent agent;Probability distribution;Distributed computing;Clustering algorithms;Design methodology;Search methods,document handling;Internet;search engines,rank distribution;relevant documents;Web search;transaction log analysis method,,5,,12,,6-Jan-09,,,IEEE,IEEE Conferences
Combining fuzzy clustering and fuzzy inferencing in information retrieval,模糊聚類與模糊推理相結合的信息檢索,D. H. Kraft; J. Chen; A. Mikulcic,"Dept. of Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA; NA; NA",Ninth IEEE International Conference on Fuzzy Systems. FUZZ- IEEE 2000 (Cat. No.00CH37063),6-Aug-02,2000,1,,375,380 vol.1,"We present an integrated approach to information retrieval which combines fuzzy clustering and fuzzy inference techniques in order to achieve optimal retrieval performance. To capture the relationships among index terms, fuzzy logic rules are used. We adapt several fuzzy clustering methods to the task of clustering documents with respect to the index terms. The clusters generated provide a basis for building the fuzzy logic rules. The clusters can also be used to form hyperlinks between documents. The fuzzy logic rules are applied with fuzzy inference to derive useful modifications of the initial query, which will guide the search for relevant documents. Alternative ways to use the fuzzy clusters are explored in this work as well. Our method combines fuzzy clustering and fuzzy inference with traditional relevance feedback approach for retrieval. The advantage of this approach is the emphasis on semantic information which relates the terms through the fuzzy clusters and fuzzy rules. A series of experiments have been conducted in order to validate this approach; a description of those experiments along with the results are presented.",1098-7584,0-7803-5877-5,10.1109/FUZZY.2000.838689,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=838689,,Information retrieval;Fuzzy logic;Databases;Clustering algorithms;Uncertainty;Engines;Fuzzy set theory;Frequency measurement;Computer science;Clustering methods,information retrieval;fuzzy logic;inference mechanisms;pattern recognition;vocabulary,fuzzy clustering;fuzzy inferencing;information retrieval;fuzzy logic;index terms;query process;semantic information,,10,,21,,6-Aug-02,,,IEEE,IEEE Conferences
Meta latent semantic analysis,元潛在語義分析,M. Simina; C. Barbu,"Dept. of CIS, Loyola Univ., New Orleans, LA, USA; NA","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",7-Mar-05,2004,4,,3720,3724 vol.4,"Meta latent semantic analysis (MLSA) is a novel approach to automated document analysis and indexing which relies on symbolic ontologies to further enhance the traditional probabilistic latent semantic analysis (LSA) of documents. While LSA is able to discover clusters of related terms and documents in a given collection of documents, the proposed MLSA is able to meta-cluster such clusters by taking into account existing symbolic ontologies relevant for the analyzed collections of documents. Such an approach can be successfully used to improve the performance of fast LSA by random projection.",1062-922X,0-7803-8566-7,10.1109/ICSMC.2004.1400922,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1400922,,Information retrieval;Indexing;Ontologies;Text analysis;Computational Intelligence Society;Information analysis;User interfaces;Singular value decomposition;Sampling methods,semantic networks;document handling;ontologies (artificial intelligence);indexing,meta latent semantic analysis;automated document analysis;symbolic ontologies;traditional probabilistic latent semantic analysis;meta-cluster;random projection,,,,7,,7-Mar-05,,,IEEE,IEEE Conferences
Semantic Expansion of Tweet Contents for Enhanced Event Detection in Twitter,推文內容的語義擴展，以增強Twitter中的事件檢測,O. Ozdikis; P. Senkul; H. Oguztuzun,"Dept. of Comput. Eng., Middle East Tech. Univ., Ankara, Turkey; Dept. of Comput. Eng., Middle East Tech. Univ., Ankara, Turkey; Dept. of Comput. Eng., Middle East Tech. Univ., Ankara, Turkey",2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,4-Feb-13,2012,,,20,24,"This paper aims to enhance event detection methods in a micro-blogging platform, namely Twitter. The enhancement technique we propose is based on lexico-semantic expansion of tweet contents while applying document similarity and clustering algorithms. Considering the length limitations and idiosyncratic spelling in Twitter environment, it is possible to take advantage of word similarities and to enrich texts with similar words. The semantic expansion technique we implement is based on syntagmatic and paradigmatic relationships between words, extracted from their co-occurrence statistics. As our technique does not depend on an existing ontology or a lexicon database such as Word Net, it should be applicable for any language. The proposed technique is applied on a tweet set collected for three days from the users in Turkey. The results indicate earlier detection of events and improvements in accuracy.",,978-1-4673-2497-7,10.1109/ASONAM.2012.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425790,Event Detection;Clustering;Micro-blogging;Twitter;Tweets in Turkish;Semantics;Word Co-occurrences,Vectors;Semantics;Twitter;Event detection;Clustering algorithms;Ontologies;Radar tracking,information retrieval;pattern clustering;social networking (online);statistical analysis;text analysis,lexico-semantic expansion technique;tweet contents;event detection method enhancement;Twitter;microblogging platform;document similarity algorithm;document clustering algorithm;length limitations;idiosyncratic spelling;syntagmatic relationships;paradigmatic relationships;word similarities;cooccurrence statistics;Turkey,,44,,16,,4-Feb-13,,,IEEE,IEEE Conferences
Single-frame text super-resolution: a Bayesian approach,單幀文本超分辨率：貝葉斯方法,G. Dalley; B. Freeman; J. Marks,"Mitsubishi Electr. Res. Labs, Cambridge, MA, USA; Mitsubishi Electr. Res. Labs, Cambridge, MA, USA; Mitsubishi Electr. Res. Labs, Cambridge, MA, USA","2004 International Conference on Image Processing, 2004. ICIP '04.",18-Apr-05,2004,5,,3295,3298 Vol. 5,"We address the problem of text super-resolution: given a single image of text scanned in at low resolution from a piece of paper, return the image that is mostly likely to be generated from a noiseless high-resolution scan of the same piece of paper. In doing so, we wish to: (1) avoid introducing artifacts in the high-resolution image such as blurry edges and rounded corners, (2) recover from quantization noise and grid-alignment effects that introduce errors in the low-resolution image, and (3) handle documents with very large glyph sets such as Japanese's Kanji. Applications for this technology include improving the display of: fax documents, low-resolution scans of archival documents, and low-resolution bitmapped fonts on high-resolution output devices.",1522-4880,0-7803-8554-3,10.1109/ICIP.2004.1421818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421818,,Bayesian methods;Shape;Image resolution;Paper technology;Noise generators;Displays;Clustering algorithms;Gray-scale;Pixel;Interpolation,image resolution;Bayes methods;document image processing,single-frame text super-resolution;Bayesian approach;noiseless high-resolution scan;quantization noise;grid-alignment effect;glyph set;fax documents;archival documents;bitmapped fonts,,19,,9,,18-Apr-05,,,IEEE,IEEE Conferences
A topic model analysis of science and technology linkages: A case study in pharmaceutical industry,科技聯繫的主題模型分析：以製藥業為例,S. Ranaei; A. Suominen; O. Dedehayir,"School of Business and Management, Lappeenranta University of Technology, Finland; VTT Technical Research Centre of Finland, Espoo, Finland; Queensland University of Technology, Brisbane, Australia",2017 IEEE Technology & Engineering Management Conference (TEMSCON),3-Aug-17,2017,,,49,54,"Science and technology (S&T) linkages have been studied extensively using patent and scientific publication databases. Existing methods used to track S&T linkages, such as analysis of non-patent literature (NPL) or author-inventor matching offer a narrow window for industry level analysis of the data. This paper examines the application of a machine learning algorithm, namely Latent Dirichlet Allocation, to detect the semantic relationship between patent and scientific publication corpus. The case of ?Taxol?? a cancer drug, is used to illustrate the performance of the unsupervised algorithm in clustering documents with similar topics. In total 26 475 documents retrieved from the Europe PMC database was used a sample for the analysis. Qualitative analysis of the clusters shows that the topic clustering algorithm is valuable approach in detection of patent and publication linkage.",,978-1-5090-1114-8,10.1109/TEMSCON.2017.7998353,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7998353,technology management;science and technology;machine learning;topic modeling;taxol,Patents;Couplings;Drugs;Machine learning algorithms;Algorithm design and analysis;Classification algorithms;Analytical models,document handling;drugs;patents;pattern clustering;pharmaceutical industry;production engineering computing;unsupervised learning,topic model analysis;science and technology linkages;pharmaceutical industry;S&T linkages;machine learning algorithm;latent Dirichlet allocation;semantic relationship;patent;scientific publication corpus;Taxol;cancer drug;unsupervised algorithm;document clustering;Europe PMC database;topic clustering algorithm;publication linkage,,,,29,,3-Aug-17,,,IEEE,IEEE Conferences
Applications of Machine learning to document classification and clustering,機器學習在文檔分類和聚類中的應用,M. S. Shaikh,"Habib University, Karachi, Pakistan",2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT),4-May-17,2017,,,1,1,"Recently, Machines learning techniques have had great successes in areas such as spam filtering, recommender systems, human speech recognition systems, handwriting recognition, news articles clustering, etc.",,978-1-5090-3310-2,10.1109/ICIEECT.2017.7916596,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916596,,Semantics;Recommender systems;Speech recognition;Handwriting recognition,,,,,,,,4-May-17,,,IEEE,IEEE Conferences
Dirichlet process mixture models for clustering i-vector data,Dirichlet過程混合模型，用於對i-vector數據進行聚類,S. Seshadri; U. Remes; O. R瓣s瓣nen,"Aalto University, Department of Signal Processing and Acoustics, Finland; Aalto University, Department of Signal Processing and Acoustics, Finland; Aalto University, Department of Signal Processing and Acoustics, Finland","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",19-Jun-17,2017,,,5470,5474,"Non-parametric Bayesian methods have recently gained popularity in several research areas dealing with unsupervised learning. These models are capable of simultaneously learning the cluster models as well as their number based on properties of a dataset. The most commonly applied models are using Dirichlet process priors and Gaussian models, called as Dirichlet process Gaussian mixture models (DPGMMs). Recently, von Mises-Fisher mixture models (VMMs) have also been gaining popularity in modelling high-dimensional unit-normalized features such as text documents and gene expression data. VMMs are potentially more efficient in modeling certain speech representations such as i-vector data when compared to the GMM-based models, as they work with unit-normalized features based on cosine distance. The current work investigates the applicability of Dirichlet process VMMs (DPVMMs) for i-vector-based speaker clustering and verification, showing that they indeed show superior performance in comparison to DPGMMs in the tasks. In addition, we introduce an implementation of the DPVMMs with variational inference that is publicly available for use.",2379-190X,978-1-5090-4117-6,10.1109/ICASSP.2017.7953202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953202,Non-parametric methods;speaker clustering;unsupervised learning;variational inference;von Mises-Fisher mixtures,Data models;Mathematical model;Mixture models;Virtual machine monitors;Estimation;Parameter estimation;Random variables,Bayes methods;Gaussian processes;mixture models;pattern clustering;speaker recognition;unsupervised learning,ivector data clustering;nonparametric Bayesian methods;unsupervised learning;dirichlet process Gaussian mixture models;DPGMM;von Mises-Fisher mixture models;VMM;speaker clustering;speaker verification,,,,35,,19-Jun-17,,,IEEE,IEEE Conferences
Multi-sentiment Modeling with Scalable Systematic Labeled Data Generation via Word2Vec Clustering,通過Word2Vec聚類與可擴展的系統化標記數據生成的多情感建模,D. Mayank; K. Padmanabhan; K. Pal,"Data Sci. Labs., Svsomos LP, Toronto, ON, Canada; Data Sci. Labs., Svsomos LP, Toronto, ON, Canada; Data Sci. Labs., Svsomos LP, Toronto, ON, Canada",2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),2-Feb-17,2016,,,952,959,"Social networks are now a primary source for news and opinions on topics ranging from sports to politics. Analyzing opinions with an associated sentiment is crucial to the success of any campaign (product, marketing, or political). However, there are two significant challenges that need to be overcome. First, social networks produce large volumes of data at high velocities. Using traditional (semi-) manual methods to gather training data is, therefore, impractical and expensive. Second, humans express more than two emotions, therefore, the typical binary good/bad or positive/negative classifiers are no longer sufficient to address the complex needs of the social marketing domain. This paper introduces a hugely scalable approach to gathering training data by using emojis as proxy for user sentiments. This paper also introduces a systematic Word2Vec based clustering method to generate emoji clusters that arguably represent different human emotions (multi-sentiment). Finally, this paper also introduces a threshold-based formulation to predicting one or two class labels (multi-label) for a given document. Our scalable multi-sentiment multi-label model produces a cross-validation accuracy of 71.55% (簣 0.22%). To compare against other models in the literature, we also trained a binary (positive vs. negative) classifier. It produces a cross-validation accuracy of 84.95% (簣 0.17%), which is arguably better than several results reported in literature thus far.",2375-9259,978-1-5090-5910-2,10.1109/ICDMW.2016.0139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836770,Sentiment Analysis;Multi-sentiment;Multi-label;Emoji;Word2Vec Clustering,Conferences;Data mining,data handling;pattern classification;pattern clustering;sentiment analysis;social networking (online),multisentiment modeling;scalable systematic labeled data generation;social networks;opinion analysis;training data;emojis cluster generation;systematic Word2Vec based clustering method;threshold-based formulation;class label prediction,,2,,19,,2-Feb-17,,,IEEE,IEEE Conferences
Towards a linguistic patterns for arabic keyphrases extraction,尋求阿拉伯語短語提取的語言模式,I. Sahmoudi; A. Lachkar,"Dept. of Electrical and Computer Engineering, L.I.S.A, E.N.S.A, USMBA, Fez, Morocco; Dept. of Electrical and Computer Engineering, L.I.S.A, E.N.S.A, USMBA, Fez, Morocco",2016 International Conference on Information Technology for Organizations Development (IT4OD),26-May-16,2016,,,1,6,"Keyphrases are defined as phrases that capture the main topic(s) discussed in a document, they offer a brief and short summary of document's content. In fact, they were integrated in many Text Mining Applications such as Document Summarization, Document Indexing, Feature Extraction for Document Clustering and Classification as new Text Representation instead of the Full-Text Representation. This paper presents new improvement of our previous Arabic keyphrases extraction system based on Suffix Tree Data Structure and named the KpST system, by adding linguistic patterns specialized in extracting Arabic noun phrases, and using adapted C-value multi-terms extraction method to calculate the keyphrase relevance and solve the sub-keyphrases problem.",,978-1-4673-7689-1,10.1109/IT4OD.2016.7479308,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479308,Keyphrases;Full Text Representation;Text Mining;Arabic Language,Pragmatics;Mice;Text mining;Cleaning;Dairy products;Feature extraction,data mining;feature extraction;indexing;linguistics;natural language processing;pattern clustering;text analysis;tree data structures,linguistic patterns;document content;text mining applications;document summarization;document indexing;feature extraction;document clustering;document classification;full-text representation;Arabic keyphrase extraction system;suffix tree data structure;KpST system;Arabic noun phrase extraction;adapted C-value multiterm extraction method,,1,,13,,26-May-16,,,IEEE,IEEE Conferences
Constrained Concept Factorization for Image Representation,約束的圖像分解概念分解,H. Liu; G. Yang; Z. Wu; D. Cai,"College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China",IEEE Transactions on Cybernetics,20-May-17,2014,44,7,1214,1224,"Matrix factorization based techniques, such as nonnegative matrix factorization and concept factorization, have attracted great attention in dimensionality reduction and data clustering. Previous studies show that both of them yield impressive results on image processing and document clustering. However, both of them are essentially unsupervised methods and cannot incorporate label information. In this paper, we propose a novel semi-supervised matrix decomposition method for extracting the image concepts that are consistent with the known label information. With this constraint, we call the new approach constrained concept factorization. By requiring that the data points sharing the same label have the same coordinate in the new representation space, this approach has more discriminating power. The experimental results on several corpora show good performance of our novel algorithm in terms of clustering accuracy and mutual information.",2168-2275,,10.1109/TCYB.2013.2287103,National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; Qian Jiang Talented Program of Zhejiang Province; Doctoral Fund of Ministry of Education New Faculty Program; National High-tech R&D Program of China (863 Program); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651834,Clustering;dimensionality reduction;nonnegative matrix factorization;semisupervised learning;Clustering;dimensionality reduction;nonnegative matrix factorization;semisupervised learning,Matrix decomposition;Linear programming;Vectors;Algorithm design and analysis;Approximation algorithms;Clustering algorithms;Data models,image representation;matrix decomposition,constrained concept factorization;image representation;nonnegative matrix factorization;dimensionality reduction;data clustering;image processing;document clustering;semisupervised matrix decomposition method,,26,,36,,1-Nov-13,,,IEEE,IEEE Journals
A Graph Lattice Approach to Maintaining and Learning Dense Collections of Subgraphs as Image Features,圖格方法用於維護和學習子圖的密集集合作為圖像特徵,E. Saund,"Palo Alto Research Center, Palo Alto",IEEE Transactions on Pattern Analysis and Machine Intelligence,21-Aug-13,2013,35,10,2323,2339,"Effective object and scene classification and indexing depend on extraction of informative image features. This paper shows how large families of complex image features in the form of subgraphs can be built out of simpler ones through construction of a graph lattice - a hierarchy of related subgraphs linked in a lattice. Robustness is achieved by matching many overlapping and redundant subgraphs, which allows the use of inexpensive exact graph matching, instead of relying on expensive error-tolerant graph matching to a minimal set of ideal model graphs. Efficiency in exact matching is gained by exploitation of the graph lattice data structure. Additionally, the graph lattice enables methods for adaptively growing a feature space of subgraphs tailored to observed data. We develop the approach in the domain of rectilinear line art, specifically for the practical problem of document forms recognition. We are especially interested in methods that require only one or very few labeled training examples per category. We demonstrate two approaches to using the subgraph features for this purpose. Using a bag-of-words feature vector we achieve essentially single-instance learning on a benchmark forms database, following an unsupervised clustering stage. Further performance gains are achieved on a more difficult dataset using a feature voting method and feature selection procedure.",1939-3539,,10.1109/TPAMI.2012.267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583199,Graph lattice;subgraph matching;document classification;line-art analysis;CMD distance;weighted voting,Junctions;Lattices;NIST;Vocabulary;Vectors;Histograms;Support vector machine classification,data structures;database indexing;document image processing;feature extraction;graph theory;image classification;image matching;image retrieval;natural scenes;pattern clustering;unsupervised learning,complex image features;inexpensive exact graph matching;graph lattice data structure;informative image feature extraction;subgraph feature space;rectilinear line art;document form recognition;labeled training;bag-of-words feature vector;single-instance learning;benchmark form database;unsupervised clustering stage;feature voting method;feature selection procedure;scene classification;object classification;scene indexing;object indexing;dense subgraph collections learning,"Algorithms;Artificial Intelligence;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Subtraction Technique",7,,28,,21-Aug-13,,,IEEE,IEEE Journals
Word-Hunter: A Gamesourcing Experience to Validate the Transcription of Historical Manuscripts,尋字獵人：驗證歷史手稿抄寫的遊戲外包經驗,J. Chen; P. Riba; A. Forn矇s; J. Mas; J. Llad籀s; J. M. Pujadas-Mora,"Comput. Sci. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Comput. Sci. Dept., Univ. Autonoma de Barcelona, Barcelona, Spain; Centre for Demographic Studies, Univ. Autonoma de Barcelona, Barcelona, Spain",2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR),20-Dec-18,2018,,,528,533,"Nowadays, there are still many handwritten historical documents in archives waiting to be transcribed and indexed. Since manual transcription is tedious and time consuming, the automatic transcription seems the path to follow. However, the performance of current handwriting recognition techniques is not perfect, so a manual validation is mandatory. Crowdsourcing is a good strategy for manual validation, however it is a tedious task. In this paper we analyze experiences based in gamification in order to propose and design a gamesourcing framework that increases the interest of users. Then, we describe and analyze our experience when validating the automatic transcription using the gamesourcing application. Moreover, thanks to the combination of clustering and handwriting recognition techniques, we can speed up the validation while maintaining the performance.",,978-1-5386-5875-8,10.1109/ICFHR-2018.2018.00098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583816,Crowdsourcing;Gamification;Handwritten documents;Performance evaluation,Games;Task analysis;Crowdsourcing;Handwriting recognition;Manuals;Systems architecture;Text recognition,computer games;document image processing;handwriting recognition;history;pattern clustering,handwriting recognition techniques;gamification;clustering;gamesourcing application;gamesourcing framework;automatic transcription;handwritten historical documents;historical manuscripts;gamesourcing experience;word-hunter,,,,18,,20-Dec-18,,,IEEE,IEEE Conferences
Forgery Detection in Hyperspectral Document Images using Graph Orthogonal Nonnegative Matrix Factorization,圖正交非負矩陣分解在高光譜文檔圖像中的偽造檢測,A. Rahiche; M. Cheriet,"?cole de Technologie Sup矇rieure (ETS),Synchromedia Laboratory,Montreal,Canada; ?cole de Technologie Sup矇rieure (ETS),Synchromedia Laboratory,Montreal,Canada",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),28-Jul-20,2020,,,2823,2831,"The analysis of inks plays a crucial role in the examination process of questioned documents. To address this issue, we propose a new approach for ink mismatch detection in Hyperspectral document (HSD) images based on a new orthogonal and graph regularized Nonnegative Matrix Factorization (NMF) model. Although some previous works have proposed orthogonality constraints to solve clustering problems in different contexts, the application of such constraints is not straightforward due to the sum-to-one assumption related to the physical nature of Hyperspectral images. In this work, we demonstrate that under some acquisition protocols, latent factors in HSD images can be constrained to be orthogonal. We also incorporate a graph regularized term to exploit the geometric information lost by the matricization of HSD images. Furthermore, we propose an efficient alternating direction method of multipliers based algorithm to solve the proposed method. Our empirical validation demonstrates the competitiveness of the proposed algorithm compared to the state-of-the-art methods. It shows a high potential to be used as a reliable tool for quick investigation ofquestioned documents.",2160-7516,978-1-7281-9360-1,10.1109/CVPRW50498.2020.00339,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150599,,Ink;Forgery;Hyperspectral imaging;Forensics;Text analysis;Cost function,document image processing;graph theory;hyperspectral imaging;image forensics;matrix decomposition,forgery detection;hyperspectral document images;ink mismatch detection;orthogonality constraints;clustering problems;sum-to-one assumption;HSD images;graph regularized term;graph orthogonal nonnegative matrix factorization;regularized nonnegative matrix factorization model;NMF model,,1,,29,,28-Jul-20,,,IEEE,IEEE Conferences
Features for word spotting in historical manuscripts,歷史手稿中的單詞發現功能,T. M. Rath; R. Manmatha,"Center for Intelligent & Inf. Retrieval, Massachusetts Univ., Amherst, MA, USA; Center for Intelligent & Inf. Retrieval, Massachusetts Univ., Amherst, MA, USA","Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.",8-Sep-03,2003,,,218,222 vol.1,"For the transition from traditional to digital libraries, the large number of handwritten manuscripts that exist pose a great challenge. Easy access to such collections requires an index, which is currently created manually at great cost. Because automatic handwriting recognizers fail on historical manuscripts, the word spotting technique has been developed: the words in a collection are matched as images and grouped into clusters which contain all instances of the same word. By annotating ""interesting"" clusters, an index that links words to the locations where they occur can be built automatically. Due to the noise in historical documents, selecting the right features for matching words is crucial. We analyzed a range of features suitable for matching words using dynamic time warping (DTW), which aligns and compares sets of features extracted from two images. Each feature's individual performance was measured on a test set. With an average precision of 72%, a combination of features outperforms competing techniques in speed and precision.",,0-7695-1960-1,10.1109/ICDAR.2003.1227662,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1227662,,Handwriting recognition;Optical character recognition software;Information retrieval;Image segmentation;Software libraries;Image analysis;Degradation;Character recognition;Indexing;Costs,image matching;handwritten character recognition;document image processing;history,historical manuscripts;traditional-digital library transition;handwritten manuscript;automatic handwriting recognizer;word spotting technique;word matching;dynamic time warping;DTW,,109,,8,,8-Sep-03,,,IEEE,IEEE Conferences
Some remarks on vector representations of legal documents,關於法律文件載體表達的一些評論,E. Schweighofer; A. Rauber; D. Merkl,"Inst. of Public Int. Law, Wien Univ., Austria; NA; NA",Proceedings 11th International Workshop on Database and Expert Systems Applications,6-Aug-02,2000,,,1087,1091,"Vector representation of legal documents is still the best way for computing classification clusters and labelling of its contents. This paper deals with the problem of diversity of legal documents making vector representation a difficult task. Extensive experiments with three text corpora of about 580 documents in three languages have shown that binary or weighted vector representation may not be sufficient. Even quite successful approaches of similarity computation have problems in identifying the best context of classification. The LabelSOM method can be seen as a very efficient tool for verification of similarity because common elements are explicitly identified. Finally, some proposals for the ""best"" vector representation are discussed: weighted vectors, feature vectors and hierarchies of vectors using XML information for identifying similar contexts.",1529-4188,0-7695-0680-1,10.1109/DEXA.2000.875162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=875162,,Law;Legal factors;Information retrieval;Neural networks;World Wide Web;Labeling;HTML;Proposals;XML;Internet,law administration;document handling;classification;self-organising feature maps;knowledge representation,vector representation;legal documents;classification clusters;content labelling;text corpora;LabelSOM method;similarity verification;feature vectors;vector hierarchies;XML information,,3,,23,,6-Aug-02,,,IEEE,IEEE Conferences
Automatic Keyword and Sentence-Based Text Summarization for Software Bug Reports,用於軟件錯誤報告的基於關鍵字和句子的自動文本摘要,S. G. Jindal; A. Kaur,"University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, New Delhi, India; University School of Information and Communication Technology, Guru Gobind Singh Indraprastha University, New Delhi, India",IEEE Access,15-Apr-20,2020,8,,65352,65370,"Text Summarization is a process which efficiently retrieves the relevant information from documents. The objective of the proposed, unsupervised approach is to summarize bug reports (software artefacts) with complete content and diversified information. The proposed approach utilizes Rapid Automatic Keyword Extraction and term frequency-inverse document frequency method to extract meaningful keywords and key-phrases with a relevant score. For sentence extraction, fuzzy C-means clustering is used to extracts sentences having high degree of membership from each cluster above a set threshold value. A rule-engine is used for sentence selection. The rules are generated with the domain knowledge and based on the extracted information by the keywords and sentences selected by the clustering method. Cohesive and coherent summary is generated by the proposed method on apache bug reports. For redundancy removal and to re-rank generated summary, hierarchical clustering is presented to enrich the extracted summary. The proposed approach is evaluated on newly constructed Apache project Bug Report Corpus (APBRC) and existing Bug Report Corpus (BRC). The results are compared on the basis of performance metrics such as precision, recall, pyramid precision and F-score. The experimental results depict that our proposed approach attains significant improvement over other baseline approaches such as BRC and LRCA. It also attains significant improvement over existing state-of-art unsupervised approaches such as Hurried, centroid and others. It extracts significant keyword phrases and sentences from each cluster to achieve full coverage and coherent summary. The results evaluated on APBRC corpus attains an average value of 78.22%, 82.18%, 80.10% and 81.66% for precision, recall, f-score and pyramid precision respectively.",2169-3536,,10.1109/ACCESS.2020.2985222,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055390,Text summarization;rapid automatic keyword extraction;fuzzy c-means;hierarchical clustering;bug reports;rule engine,Computer bugs;Feature extraction;Software;Task analysis;Data mining;Engines;Standards,information retrieval;natural language processing;pattern clustering;program debugging;text analysis,software bug reports;unsupervised approach;software artefacts;diversified information;rapid automatic keyword extraction;term frequency-inverse document frequency method;relevant score;sentence extraction;fuzzy C-means clustering;set threshold value;rule-engine;sentence selection;clustering method;coherent summary;apache bug reports;re-rank generated summary;hierarchical clustering;extracted summary;pyramid precision;f-score;sentence-based text summarization,,,,61,CCBY,2-Apr-20,,,IEEE,IEEE Journals
Modelling Implicit Content Networks to Track Information Propagation Across Media Sources to Analyze News Events,對隱式內容網絡建模以跟?跨媒體源的信息傳播以分析新聞事件,A. Joshi; R. O. Sinnott,"Sch. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia; Sch. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia",2018 IEEE 14th International Conference on e-Science (e-Science),27-Dec-18,2018,,,475,485,"With the rise of the Internet as the premier news source for billions of people around the world, the propagation of news media online now influences many critical decisions made by society every day. Fake news is now a mainstream concern. In the context of news propagation, recent works in media analysis largely focus on extracting clusters, news events, stories or tracking links or conserved sentences at aggregate levels between sources. However, the insight provided by these approaches is limited for analysis and context for end users. To tackle this, we present an approach to model implicit content networks at a semantic level that is inherent within news event clusters as seen by users on a daily basis through the generation of semantic content indexes. The approach is based on an end-to-end unsupervised machine learning system trained on real-life news data that combine together with algorithms to generate useful contextual views of the sources and the inter-relationships of news events. We illustrate how the approach is able to track conserved semantic context through the use of a combination of machine learning techniques, including document vectors, k-nearest neighbors and the use of hierarchical agglomerative clustering. We demonstrate the system by training semantic vector models on realistic real-world data taken from the Signal News dataset. We quantitatively evaluate the performance against existing state of the art systems to demonstrate the end-to-end capability. We then qualitatively demonstrate the usefulness of a news event centered semantic content index graph for end-user applications. This is evaluated with respect to the goal of generating rich contextual interconnections and providing differential background on how news media sources report, parrot and position information on ostensibly identical news events.",,978-1-5386-9156-4,10.1109/eScience.2018.00136,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588755,implicit content networks;information propagation;media analysis;news events;online news;document vectors;k-nearest neighbors;hierarchical agglomerative clustering;graphical modeling;natural language processing;machine learning;visualization,Semantics;Media;Indexes;Clustering algorithms;Internet;Training;Focusing,data mining;information resources;Internet;pattern clustering;unsupervised learning,fake news;news media online;premier news source;track information propagation;modelling implicit content networks;ostensibly identical news events;news media sources report;end-user applications;semantic content index graph;end-to-end capability;Signal News dataset;semantic vector models;conserved semantic context;real-life news data;end-to-end unsupervised machine;semantic content indexes;news event clusters;semantic level;end users;media analysis;news propagation,,,,37,,27-Dec-18,,,IEEE,IEEE Conferences
Parallel processing of XML databases,XML數據庫的並行處理,G. Z. Qadah,"Dept. of Comput. Eng., American Univ. of Sharjah, United Arab Emirates","Canadian Conference on Electrical and Computer Engineering, 2005.",3-Jan-06,2005,,,2000,2004,"Beowulf cluster is a name given to a high performance, low-cost parallel computer system made of commodity hardware and software components. It consists of a number of processing nodes, interconnected via a switch. The extensible markup language (XML) data model, on the other hand, has recently gained huge popularity because of its ability to represent a wide variety of structured (tabular-like) and semi-structured (textual-like) data. Several query languages have been proposed for the XML data model, the most-widely known is XQuery. This paper reviews the XML data model and its query language within the context of cluster/parallel computing environment. It examines several techniques for structuring and storing XML data across the different cluster nodes. It develops a number of algorithms suitable for processing a certain class of queries, namely, the containment queries, against the parallel XML database. This paper also shows that one of these algorithms, the one that takes advantage of the parallelism existing between the different documents within the XML database, is outperforming all of the other presented ones",0840-7789,0-7803-8885-2,10.1109/CCECE.2005.1557377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1557377,,Parallel processing;XML;Data models;Switches;Database languages;Clustering algorithms;Concurrent computing;High performance computing;Hardware;Software performance,multimedia databases;parallel processing;query languages;workstation clusters;XML,XML databases;parallel processing;Beowulf cluster;parallel computer system;extensible markup language;query languages;XQuery;cluster;computing;parallel computing;cluster nodes,,3,,10,,3-Jan-06,,,IEEE,IEEE Conferences
The shape of Shakespeare: visualizing text using implicit surfaces,莎士比亞的形狀：使用隱式表面可視化文本,R. M. Rohrer; D. S. Ebert; J. L. Sibert,"Dept. of Electr. Eng. & Comput. Sci., George Washington Univ., Washington, DC, USA; NA; NA",Proceedings IEEE Symposium on Information Visualization (Cat. No.98TB100258),6-Aug-02,1998,,,121,129,"Information visualization focuses on the use of visual means for exploring non-visual information. While free-form text is a rich, common source of information, visualization of text is a challenging problem since text is inherently non-spatial. The paper explores the use of implicit surface models for visualizing text. The authors describe several techniques for text visualization that aid in understanding document content and document relationships. A simple method is defined for mapping document content to shape. By comparing the shapes of multiple documents, global content similarities and differences may be noted. In addition, they describe a visual clustering method in which documents are arranged in 3D based upon similarity scoring. Documents deemed closely related blend together as a single connected shape. Hence, a document corpus becomes a collection of shapes that reflect inter-document relationships. These techniques provide methods to visualize individual documents as well as corpus meta-data. They then combine the two techniques to produce transparent clusters enclosing individual document shapes. This provides a way to visualize both local and global contextual information. Finally, they elaborate on several potential applications of these methods.",,0-8186-9093-3,10.1109/INFVIS.1998.729568,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=729568,,Visualization;Shape control;Humans;Self organizing feature maps;Neural networks;Scattering;Navigation;Information analysis;Amorphous materials;Animation,data visualisation;text analysis,text visualization;implicit surfaces;information visualization;nonvisual information;free-form text;document content;document relationships;document content mapping;shape;global content similarities;global content differences;visual clustering method;similarity scoring;3D arrangement;connected shape;document corpus;inter-document relationships;corpus meta-data;transparent clusters;local contextual information;global contextual information,,21,,26,,6-Aug-02,,,IEEE,IEEE Conferences
Dimensionality reduction approach for high dimensional text documents,高維文本文檔的降維方法,G. S. Reddy,"Faculty of Information Technology, VNR Vignana Jyothi Institute of Engineering and Technology Hyderabad, India",2016 International Conference on Engineering & MIS (ICEMIS),17-Nov-16,2016,,,1,6,"Feature dimensionality has always been one of the key challenges in text mining as it increases complexity when mining documents with high dimensionality. High dimensionality introduces sparseness, noise, and boosts the computational and space complexities. Dimensionality reduction is usually addressed by implementing either feature reduction or feature selection techniques. In this work, the problem of dimensionality reduction is addressed by achieving feature reduction through the use of a novel membership function. For feature selection, Singular value decomposition and Information gain approaches are adopted through retaining top-k features. The approach of feature reduction is compared to feature selection techniques and results prove the dimensionality reduction achieved through proposed approach is better.",,978-1-5090-5579-1,10.1109/ICEMIS.2016.7745364,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745364,feature selection;feature reduction;clustering;classification;dimensionality,Matrix decomposition;Feature extraction;Complexity theory;Text mining;Singular value decomposition;Clustering algorithms;Standards,computational complexity;data mining;feature selection;singular value decomposition;text analysis,dimensionality reduction approach;high dimensional text documents;feature dimensionality;text mining;document mining;high dimensionality;computational complexity;space complexity;feature reduction;feature selection;membership function;singular value decomposition;information gain,,4,,49,,17-Nov-16,,,IEEE,IEEE Conferences
Research on the application of page segmentation in information retrieval,頁面分割在信息檢索中的應用研究,Men Rui; Sun Yueheng; Deng Zheng; Ni Weijie,"School of Computer Science and Technology, Tianjin University, China; School of Computer Science and Technology, Tianjin University, China; School of Computer Science and Technology, Tianjin University, China; School of Computer Science and Technology, Tianjin University, China","Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)",14-May-12,2011,,,295,298,"Existing search engines index web pages as a whole and use them for information retrieval, which leads to irrelevant documents being returned to users. This paper proposes a new indexing approach for solving this problem by 1) using VIPS algorithm for page segmentation, 2) filtering out the function blocks through several heuristic rules, 3) clustering feature blocks into different sub-documents and indexing them respectively. For three user queries, the initial results retrieved from Google are compared with the search results of improved indexing system, which shows that our approach gets a higher performance in terms of precision and F-measure.",,978-1-4577-1701-7,10.1109/TMEE.2011.6199201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6199201,page segmentation;information retrieval;indexing approach;VIPS algorithm,Web pages;Visualization;Indexing;Google;Particle separators;Search engines,document handling;feature extraction;indexing;information retrieval;pattern clustering;search engines;Web sites,page segmentation;information retrieval;search engines index Web pages;VIPS algorithm;function blocks filtering;heuristic rules;feature blocks clustering;subdocuments;user queries;Google;indexing system;F-measure,,,,7,,14-May-12,,,IEEE,IEEE Conferences
Efficient Semantic Indexing for Image Retrieval,圖像檢索的有效語義索引,C. Pulla; S. Karthik; C. V. Jawahar,"CVIT, Int. Inst. of Inf. Technol., Hyderabad, India; CVIT, Int. Inst. of Inf. Technol., Hyderabad, India; CVIT, Int. Inst. of Inf. Technol., Hyderabad, India",2010 20th International Conference on Pattern Recognition,7-Oct-10,2010,,,3276,3279,"Semantic analysis of a document collection can be viewed as an unsupervised clustering of the constituent words and documents around hidden or latent concepts. This has shown to improve the performance of visual bag of words in image retrieval. However, the enhancement in performance depends heavily on the right choice of number of semantic concepts. Most of the semantic indexing schemes are also computationally costly. In this paper, we employ a bipartite graph model (BGM) for image retrieval. BGM is a scalable data structure that aids semantic indexing in an efficient manner. It can also be incrementally updated. BGM uses tf-idf values for building a semantic bipartite graph. We also introduce a graph partitioning algorithm that works on the BGM to retrieve semantically relevant images from a database. We demonstrate the properties as well as performance of our semantic indexing scheme through a series of experiments. We also compare our methods with incremental pLSA.",1051-4651,978-1-4244-7541-4,10.1109/ICPR.2010.801,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597497,,Semantics;Indexing;Bipartite graph;Image retrieval;Feature extraction;Computational modeling,document image processing;graph theory;image retrieval;indexing;pattern clustering,semantic indexing;image retrieval;unsupervised clustering;bipartite graph model;BGM;scalable data structure;tf-idf values;graph partitioning algorithm,,1,,16,,7-Oct-10,,,IEEE,IEEE Conferences
A combination approach for enhancing automated traceability: (NIER track),增強自動跟?能力的組合方法：（NIER跟?）,X. Chen; J. Hosking; J. Grundy,"University of Auckland, Auckland, New Zealand; University of Auckland, Auckland, New Zealand; Swinburne University of Technology, Melbourne, Australia",2011 33rd International Conference on Software Engineering (ICSE),10-Oct-11,2011,,,912,915,"Tracking a variety of traceability links between artifacts assists software developers in comprehension, efficient development, and effective management of a system. Traceability systems to date based on various Information Retrieval (IR) techniques have been faced with a major open research challenge: how to extract these links with both high precision and high recall. In this paper we describe an experimental approach that combines Regular Expression, Key Phrases, and Clustering with IR techniques to enhance the performance of IR for traceability link recovery between documents and source code. Our preliminary experimental results show that our combination technique improves the performance of IR, increases the precision of retrieved links, and recovers more true links than IR alone.",1558-1225,978-1-4503-0445-0,10.1145/1985793.1985943,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032550,clustering;key phrases;regular expression;traceability,Unified modeling language;Documentation;Information retrieval;Clustering algorithms;Software;Large scale integration;Thesauri,information retrieval;program diagnostics;software engineering,automated traceability system;information retrieval;regular expression;key phrases;document clustering;traceability link recovery,,3,,12,,10-Oct-11,,,IEEE,IEEE Conferences
A domain cluster interface for WWW search,用於WWW搜索的域群集界面,H. Shimamura; H. Takano; T. Kamba; Y. Koseki,"C&C Media Res. Labs., NEC Corp, Kanagawa, Japan; NA; NA; NA",Proceedings. 3rd Asia Pacific Computer Human Interaction (Cat. No.98EX110),6-Aug-02,1998,,,318,323,"Because of the recent explosive increase in the number of WWW documents, directory services are becoming indispensable. In the keyword search function of most directory services, search results are displayed as a URL list ordered by importance as calculated by the system, but the order sometimes does not have any meaning to the user since the calculation algorithm is a black box. In addition, it is difficult to find useful documents from a long list. To solve this problem, the authors have developed a new WWW search system that clusters the documents in the search results by organization name, derived from its URL domain name. The system displays the clusters in a hierarchical tree view form.",,0-8186-8347-3,10.1109/APCHI.1998.704449,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=704449,,World Wide Web;Uniform resource locators;Keyword search;National electric code;Explosives;Web sites;Explosions;Internet;Displays;Read only memory,Internet;online front-ends;information retrieval;user interfaces,domain cluster interface;WWW search;WWW documents;directory services;keyword search function;URL list;URL domain name;hierarchical tree view,,2,,14,,6-Aug-02,,,IEEE,IEEE Conferences
Text mining based approach for intrusion detection,基於文本挖掘的入侵檢測方法,N. Mangathayaru; G. R. Kumar; G. Narsimha,"Faculty of Information Technology VNR VJIET, Hyderabad, India; Faculty of Information Technology VNR VJIET, Hyderabad, India; Faculty of CSE JNTU, Hyderabad, India",2016 International Conference on Engineering & MIS (ICEMIS),17-Nov-16,2016,,,1,5,"Intrusion detection is classified as NP-Hard in the literature even today. Also supervised learning also termed classification, when performed on high dimensional documents has problem from the noise or outliers, which make the text classification inaccurate and leads to reduced accuracy by classifiers. We discuss the feature reduction methods which we adopted to achieve dimensionality reduction. In the Feature Extraction process, the high dimensional text documents are projected onto their corresponding low dimensional representation in feature space through using algebraic rules and transformations. The objective is to find optimal transformation matrix corresponding the input high dimensional document feature matrix. This objective is achieved in this thesis by using the concept of feature clustering and through clustering the features into a optimal set of clusters by designing a novel fuzzy membership function. The membership function designed retains the original distribution of words in the documents which is the importance of this approach.",,978-1-5090-5579-1,10.1109/ICEMIS.2016.7745351,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745351,Intrusion Detection;Dimensionality Reduction;Fuzzy Feature Clustering;Fuzzy Distribution;Process System Call Feature Matrix,,computational complexity;data mining;data reduction;feature extraction;fuzzy set theory;learning (artificial intelligence);matrix algebra;pattern classification;pattern clustering;security of data;text analysis,text mining;intrusion detection;NP-hard problem;supervised learning;text classification;feature reduction;dimensionality reduction;feature extraction;high dimensional text documents;algebraic rules;optimal transformation matrix;high dimensional document feature matrix;feature clustering;fuzzy membership function,,7,,51,,17-Nov-16,,,IEEE,IEEE Conferences
A Confidence-Based Hierarchical Feature Clustering Algorithm for Text Classification,基於信心的文本分類層次特徵聚類算法,J. Jiang; K. Yin; S. Lee,NA; NA; NA,The 2007 International Conference on Intelligent Pervasive Computing (IPC 2007),22-Jan-08,2007,,,161,164,"In this paper, we propose a novel feature reduction ap- proach to group words hierarchically into clusters which can then be used as new features for document classifica- tion. Initially, each word constitutes a cluster. We calculate the mutual confidence between any two different words. The pair of clusters containing the two words with the highest mutual confidence are combined into a new cluster. This process of merging is iterated until all the mutual confi- dences between the un-processed pair of words are smaller than a predefined threshold or only one cluster exists. In this way, a hierarchy of word clusters is obtained. The user can decide the clusters, from a certain level, to be used as new features for document classification. Experimental re- sults have shown that our method can perform better than other methods.",,978-0-7695-3006-2,10.1109/IPC.2007.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4438416,,Classification algorithms;Clustering algorithms;Text categorization;Feature extraction;Merging;Clustering methods;Pervasive computing;Text mining;Software libraries;Gain measurement,,,,,,9,,22-Jan-08,,,IEEE,IEEE Conferences
Extended Subtree: A New Similarity Function for Tree Structured Data,擴展子樹：樹結構數據的新相似性函數,A. Shahbazi; J. Miller,"Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada",IEEE Transactions on Knowledge and Data Engineering,21-Mar-14,2014,26,4,864,877,"Although several distance or similarity functions for trees have been introduced, their performance is not always satisfactory in many applications, ranging from document clustering to natural language processing. This research proposes a new similarity function for trees, namely Extended Subtree (EST), where a new subtree mapping is proposed. EST generalizes the edit base distances by providing new rules for subtree mapping. Further, the new approach seeks to resolve the problems and limitations of previous approaches. Extensive evaluation frameworks are developed to evaluate the performance of the new approach against previous proposals. Clustering and classification case studies utilizing three real-world and one synthetic labeled data sets are performed to provide an unbiased evaluation where different distance functions are investigated. The experimental results demonstrate the superior performance of the proposed distance function. In addition, an empirical runtime analysis demonstrates that the new approach is one of the best tree distance functions in terms of runtime efficiency.",1558-2191,,10.1109/TKDE.2013.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487504,Tree classification;tree clustering;tree comparison;tree distance function;tree similarity;tree structured data,Runtime;Complexity theory;Entropy;Indexes;Support vector machines;Distance measurement,data structures;document handling;natural language processing;pattern clustering,extended subtree;similarity function;tree structured data;document clustering;natural language processing;EST;subtree mapping;synthetic labeled data sets;distance functions,,8,,43,,27-Mar-13,,,IEEE,IEEE Journals
"Automatic identification, clustering and reporting of recurrent faults in electric distribution feeders",自動識別，聚集和報告配電饋線中的經常性故障,K. Manivinnan; C. L. Benner; B. D. Russell; J. A. Wischkaemper,"The Department of Electrical and Computer Engineering, Texas A&M University, College Station, Texas, USA; The Department of Electrical and Computer Engineering, Texas A&M University, College Station, Texas, USA; The Department of Electrical and Computer Engineering, Texas A&M University, College Station, Texas, USA; The Department of Electrical and Computer Engineering, Texas A&M University, College Station, Texas, USA",2017 19th International Conference on Intelligent System Application to Power Systems (ISAP),19-Oct-17,2017,,,1,6,"Latent power line conditions, such as vegetation intrusion and apparatus that have failed or are in the process of failing can cause recurring fault events. Many such conditions are influenced by other factors such as wind and moisture, and therefore cause fault events only intermittently. These conditions are difficult to detect and locate with conventional technologies. Fault current and arcing from recurrent faults can cause further damage to already weak apparatus, ultimately leading to a catastrophic failure, at which time there may be more consequential damage to apparatus, including burned-down lines. For more than a decade, Texas A&M researchers have instrumented dozens of feeders using sensitive, high-fidelity waveform recorders to document numerous apparatus failure conditions, including multiple instances in which failing apparatus and other factors have caused recurring faults and momentary interruptions, spread over significant periods of time, without causing sustained outages. A series of related faults can escape notice when an unmonitored, pole-mount recloser is the interrupting device, unless customers report momentary interruptions, and experience indicates this often does not happen. Even if customers report individual momentary interruptions, the utility may not recognize that the multiple interruptions are related to each other, particularly if time intervals between operations are sufficiently long for operator memories to fade. Awareness of recurrent fault conditions would enable utilities to make timely, proactive repairs, thus avoiding additional faults and interruptions, as well as potentially preventing more catastrophic failures (e.g., equipment damage, downed conductors, fires). This paper describes an on-line, automated method to mine, cluster and report recurrent faults to utility operators in a near real-time fashion. This paper also documents one of multiple real-world examples where the methodology described in this paper was successfully used by utilities to locate and fix problematic components and prevent further faults.",,978-1-5090-4000-1,10.1109/ISAP.2017.8071426,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071426,Incipient fault detection;waveform analytics;situational awareness;fault anticipation;condition based maintenance;hierarchical clustering;recurrent faults,Circuit faults;Clustering algorithms;Fault diagnosis;Feature extraction;Fault currents;Classification algorithms;Power systems,failure analysis;fault currents;fault diagnosis;maintenance engineering;power distribution faults;power distribution lines,automatic identification;electric distribution feeders;vegetation intrusion;arcing;catastrophic failure;Texas A&M researchers;high-fidelity waveform recorders;document numerous apparatus failure conditions;interrupting device;individual momentary interruptions;equipment damage;pole-mount recloser;recurrent fault clustering;repairs;fault current;power line conditions,,4,,21,,19-Oct-17,,,IEEE,IEEE Conferences
Using semantic similarity matrix for defining operations involved in NTSO for clustering 20NewsGroups,使用語義相似度矩陣定義NTSO中涉及的操作以對20NewsGroups進行聚類,T. Jo,"School of Computer and Information Engineering, Inha University, Incheon, 451-702, South Korea",IEEE Congress on Evolutionary Computation,27-Sep-10,2010,,,1,6,"In this research, we propose the similarity matrix based version of NTSO as the approach to the text clustering. For using one of traditional approaches to text clustering, documents should be encoded into numerical vectors; encoding so causes the two main problems: the huge dimensionality and the sparse distribution. In order to solve the problems, in this research, we propose to encode documents into string vectors and use the NTSO (Neural Text Self Organization) as the string vector based neural network for the text clustering. By encoding documents into another form, we attempt to avoid the two main problems, completely. As the empirical validation, the proposed approach will be compared with others with respect to the clustering performance and speed.",1941-0026,978-1-4244-6911-6,10.1109/CEC.2010.5586335,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586335,,Semantics;Clustering algorithms;Encoding;Artificial neural networks;Training;Finite element methods;Text categorization,matrix algebra;neural nets;pattern clustering;text analysis;vectors,semantic similarity matrix;NTSO;20NewsGroups;text clustering;numerical vector;string vector;neural text self organization;neural network,,,,15,,27-Sep-10,,,IEEE,IEEE Conferences
Layout Analysis for Historical Manuscripts Using Sift Features,使用篩選功能對歷史手稿進行佈局分析,A. Garz; R. Sablatnig; M. Diem,"Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria; Comput. Vision Lab., Vienna Univ. of Technol., Vienna, Austria",2011 International Conference on Document Analysis and Recognition,3-Nov-11,2011,,,508,512,"We propose a layout analysis method for historical manuscripts that relies on the part-based identification of layout entities. A layout entity -- such as letters of the text, initials or headings -- is composed of a set of characteristic segments or structures, which is dissimilar for distinct classes in the manuscripts under consideration. This fact is exploited in order to segment a manuscript page into homogeneous regions. Historical documents traditionally involve challenges such as uneven writing support and varying shapes of characters, fluctuating text lines, changing scripts and writing styles, and variance in the layout itself. Hence, a part-based detection of layout entities is proposed using a multi-stage algorithm for the localization of the entities, based on interest points. Results show that the proposed method is able to locate initials, headings and text areas in ancient manuscripts containing stains, tears and partially faded-out ink sufficiently well.",2379-2140,978-0-7695-4520-2,10.1109/ICDAR.2011.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065363,Sift;part-based;layout analysis;document layout;handwritten;historical manuscripts,Layout;Writing;Shape;Noise;Robustness;Clustering algorithms;Support vector machines,character recognition;document image processing;text analysis,layout analysis;historical manuscript;SIFT features;part-based identification;layout entity;writing styles;multistage algorithm;scale invariant feature transform,,24,,18,,3-Nov-11,,,IEEE,IEEE Conferences
Finer Granularity Clustering for Opinion Mining,意見挖掘的更細粒度聚類,Y. Luo; G. Lin; Y. Fu,"Dept. of Software, Univ. of Electron. Sci. & Technol. of China, Chengdu, China; Dept. of Software, Univ. of Electron. Sci. & Technol. of China, Chengdu, China; Dept. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, China",2009 Second International Symposium on Computational Intelligence and Design,31-Dec-09,2009,1,,68,71,"The boom of opinion-rich resources such as online review Websites, discussion groups, personal blogs and forums on the Web has attracted many research efforts on opinion mining. Positive and negative opinions represented in review documents are helpful information for governments to improve their services, for companies to market their products, and for customers to purchase their commodities. In this paper, we introduce a new approach that employs finer granularity clustering for opinions extraction and clustering for the calculation of their sentiment orientation of opinions. The experimental result shows that the approach is qualitatively quite useful when used to analyze the netizens' opinions to hot topics from some Websites.",,978-0-7695-3865-5,10.1109/ISCID.2009.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370398,opinion mining;Text mining;Semantic Orientation;finer granularity clustering,Data mining;Consumer electronics;Government;Information retrieval;Humans;Natural languages;Computational intelligence;Computer science;Design engineering;Blogs,data mining;text analysis;Web sites,finer granularity clustering;opinion mining;opinion-rich resources;online review Websites;discussion groups;personal blogs;forums,,1,,9,,31-Dec-09,,,IEEE,IEEE Conferences
Clustering Web Search Results Based on Interactive Suffix Tree Algorithm,基於交互式後綴樹算法的Web搜索結果聚類,Y. Wang; W. Zuo; T. Peng; F. He; H. Hu,"Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun",2008 Third International Conference on Convergence and Hybrid Information Technology,18-Nov-08,2008,2,,851,857,"Clustering is an effective way to organize Web search results, which allows users to navigate into relevant documents quickly. Traditional clustering techniques are inadequate to Chinese search results and do not generated clusters with highly readable names. In this paper, we propose a new method to clustering Web search results which is based on interactive suffix tree algorithm (ISTC). This method uses phrase extracted from the snippets as characteristics of clustering. In the course of interaction with users, it only returns cluster label to users in the first tier. When users want to make further interaction, users can select a document which they are interested in for the second clustering instead of the traditional recursive clustering. ISTC can also be applied to Chinese and English information processing which avoids the recursive algorithm for achieving linear time complexity and improving the efficiency of search engine. Experimental results verify our methodpsilas feasibility and effectiveness.",,978-0-7695-3407-7,10.1109/ICCIT.2008.108,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4682352,Interactive clustering;Suffix Tree;ISTC,Web search;Clustering algorithms;Search engines;Information processing;Information technology;Helium;Educational institutions;Computer science;Navigation;Visualization,information retrieval;search engines,Web search results clustering;interactive suffix tree algorithm;phrase extraction;information processing;linear time complexity;search engine,,2,,16,,18-Nov-08,,,IEEE,IEEE Conferences
Parallel Processing System for Marathi Content Generation,馬拉地語內容生成的並行處理系統,S. R. Vispute; S. Patil; S. Sangale; A. Padwal; A. Ukarde,"Dept. Of Comput. Eng., PCCOE, Pune, India; Dept. Of Comput. Eng., PCCOE, Pune, India; Dept. Of Comput. Eng., PCCOE, Pune, India; Dept. Of Comput. Eng., PCCOE, Pune, India; Dept. Of Comput. Eng., PCCOE, Pune, India",2015 International Conference on Computing Communication Control and Automation,16-Jul-15,2015,,,575,579,"The objective of the present work is to design a HADOOP based parallel Marathi content retrieval system using clustering technique to get the efficient and optimized result than existing systems. The system also focuses on providing the personalized documents in Marathi language to the end user based on their interests identified from the browsing history and using time session mechanism for re ranking of documents to find more interested document from accessed pages. Several authors have presented their work for content retrieval using different categorization techniques such as Na簿ve Bayes, Neural Networks, Support Vector Machines (SVM), LINGO, Suffix Tree Clustering (STC) and for different languages such as Tamil, Arabic, Polish, and Marathi. Increasing the number of input documents severely affects the processing times and accuracy. The system uses HADOOP based clustering technique to distribute the task of clustering over multiple Datanodes and executing parallel in the HADOOP framework to get the optimized and efficient result of clustering of Marathi documents.",,978-1-4799-6892-3,10.1109/ICCUBEA.2015.118,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155913,HADOOP;Clustering;categorization;Personalization,Clustering algorithms;Classification algorithms;Support vector machines;Text categorization;History;Accuracy;Data models,fuzzy set theory;information retrieval;natural language processing;neural nets;parallel processing;support vector machines,Marathi document;Datanodes;HADOOP based clustering technique;Polish;Arabic;Tamil;STC;suffix tree clustering;LINGO;SVM;support vector machine;Naive Bayes neural networks;time session mechanism;Marathi language;HADOOP based parallel Marathi content retrieval system;Marathi content generation;parallel processing system,,1,,19,,16-Jul-15,,,IEEE,IEEE Conferences
Summarizing Documents by Measuring the Importance of a Subset of Vertices within a Graph,通過測量圖形中頂點子集的重要性來匯總文檔,S. Chen; M. Huang; Z. Lu,NA; NA; NA,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,13-Oct-09,2009,1,,269,272,"This paper presents a novel method of generating extractive summaries for multiple documents. Given a cluster of documents, we firstly construct a graph where each vertex represents a sentence and edges are created according to the asymmetric relationship between sentences. Then we develop a method to measure the importance of a subset of vertices by adding a super-vertex into the original graph. The importance of such a super-vertex is quantified as super-centrality, a quantitative measure for the importance of a subset of vertices within the whole graph. Finally, we propose a heuristic algorithm to find the best summary. Our method is evaluated with extensive experiments. The comparative results show that the proposed method outperforms other methods on several datasets.",,978-0-7695-3801-3,10.1109/WI-IAT.2009.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5286065,document summarization;centrality;graph;diversity,Intelligent agent;Computer science;Heuristic algorithms;Conferences;Laboratories;Information science;Biotechnology;Libraries;USA Councils;Social network services,,,,3,,12,,13-Oct-09,,,IEEE,IEEE Conferences
An Auto-tuning Solution to Data Streams Clustering in OpenCL,OpenCL中數據流群集的自動調整解決方案,J. Fang; A. L. Varbanescu; H. Sips,"Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands; Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands; Parallel & Distrib. Syst. Group, Delft Univ. of Technol., Delft, Netherlands",2011 14th IEEE International Conference on Computational Science and Engineering,31-Oct-11,2011,,,587,594,"Due to its applicability to numerous types of data, including telephone records, web documents, and click streams, the data stream model has recently attracted attention. For analysis of such data, it is crucial to process the data in a single pass, or a small number of passes, using little memory. This paper provides an OpenCL implementation for data streams clustering, and then presents several optimizations for it, which make it more efficient in terms of memory usage. In order to maximize performance for different problem sizes and architectures, we also propose an auto-tuning solution. Experimental results show that our fully optimized implementation can perform 2.1x and 1.4x faster than the native OpenCL implementation on NVIDIA GTX480 and AMD HD5870, respectively, it can also achieve 1.4x to 3.3x speedup relative to the original CUDA implementation solution on GTX480.",,978-1-4577-0974-6,10.1109/CSE.2011.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062935,Clustering;Data Streams;OpenCL;Performance Optimizations;Auto-tuning,Optimization;Graphics processing unit;Mathematical model;Equations;Bandwidth;Memory management;Analytical models,data models;open systems;optimisation;pattern clustering,auto-tuning solution;data stream clustering;telephone record;Web document;click stream;data stream model;OpenCL implementation;NVIDIA GTX480;AMD HD5870;CUDA implementation solution;GTX480,,6,,23,,31-Oct-11,,,IEEE,IEEE Conferences
A novel clustering based fuzzy approach for character segmentation in handwritten odia scripts,一種基於聚類的新穎模糊方法在手寫Odia腳本中進行字符分割,S. Behera; A. Pradhan; B. Majhi,"Dept. Of Computer Science and Engineering, National Institute of Technology, Rourkela, India; Dept. Of Computer Science and Engineering, National Institute of Technology, Rourkela, India; Dept. Of Computer Science and Information Technology, Guru Ghasidas Vishwavidyalaya, Central University, Bilaspur, India",2017 Fourth International Conference on Image Information Processing (ICIIP),12-Mar-18,2017,,,1,6,"Character Segmentation is a pivotal decisive component in Natural Language Processing that seeks to dissolve an image of an array of characters into sub-images of distinct symbols. Literature survey reveals that very little work has been reported in character segmentation of handwritten Odia documents. In this paper, a novel clustering based Fuzzy schema is presented for segmentation of characters in unconstrained Odia language. The simulation based experiments of proposed methodology are conducted on transcribed Odia scripts. It is observed that the segmentation performance in terms of Success Rate and F-Measure parameters is superior to the other existing state of the art segmentation techniques.",,978-1-5090-6734-3,10.1109/ICIIP.2017.8313728,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8313728,Segmentation;Handwritten;Odia;Clustering;Fuzzy,Image segmentation;Character recognition;Information processing;Computer science;Task analysis;Optical character recognition software;Handwriting recognition,feature extraction;fuzzy set theory;handwritten character recognition;image segmentation;natural language processing;optical character recognition;pattern clustering,handwritten Odia documents;unconstrained Odia language;transcribed Odia scripts;segmentation performance;fuzzy approach;character segmentation;handwritten Odia scripts;pivotal decisive component;Natural Language Processing;clustering based fuzzy schema;success rate parameters;F-Measure parameters,,,,17,,12-Mar-18,,,IEEE,IEEE Conferences
Unsupervised Relation Extraction by Massive Clustering,大規模聚類的無監督關係提取,E. Gonz?lez; J. Turmo,"TALP Res. Center, Univ. Politec. de Catalunya, Barcelona, Spain; TALP Res. Center, Univ. Politec. de Catalunya, Barcelona, Spain",2009 Ninth IEEE International Conference on Data Mining,28-Dec-09,2009,,,782,787,"The goal of Information Extraction is to automatically generate structured pieces of information from the relevant information contained in text documents. Machine Learning techniques have been applied to reduce the cost of Information Extraction system adaptation. However, elements of human supervision strongly bias the learning process. Unsupervised learning approaches can avoid these biases. In this paper, we propose an unsupervised approach to learning for Relation Detection, based on the use of massive clustering ensembles. The results obtained on the ACE Relation Mention Detection task outperform in terms of F1 score by 5 points the state of the art of unsupervised techniques for this evaluation framework, in addition to being simpler and more flexible.",2374-8486,978-1-4244-5242-2,10.1109/ICDM.2009.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5360311,Relation Detection;Unsupervised Methods;Ensemble Clustering,Data mining;Humans;Machine learning;Costs;Unsupervised learning;Automatic testing;Proposals;Learning systems;Text mining;Adaptive systems,data mining;information retrieval;pattern clustering;text analysis;unsupervised learning,unsupervised relation extraction;massive clustering;text documents;machine learning techniques;information extraction system adaptation;unsupervised learning approach;relation detection;ACE relation mention detection;automatic content extraction,,7,,14,,28-Dec-09,,,IEEE,IEEE Conferences
Test Case Prioritization Using Requirements-Based Clustering,使用基於需求的集群對測試案例進行優先級排序,M. J. Arafeen; H. Do,"Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA; Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation",29-Jul-13,2013,,,312,321,"The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.",2159-4848,978-0-7695-4968-2,10.1109/ICST.2013.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743,regression testing;test case prioritization;requirements-based clustering;empirical study,Testing;Software;Measurement;Complexity theory;Fault detection;Educational institutions;Java,formal specification;Java;pattern clustering;program testing,test case prioritization process;requirements documents;Java programs;test case prioritization techniques;traditional code analysis information;defects source identification;software engineers;regression testing techniques;requirements engineering;requirements information;requirements-based clustering,,56,,35,,29-Jul-13,,,IEEE,IEEE Conferences
Topic Modelling Used to Improve Arabic Web Pages Clustering,用於改善阿拉伯語網頁聚類的主題建模,H. Alghamdi; A. Selamat,"Fac. of Comput., Univ. Teknol. Malaysia, Johor Bahru, Malaysia; Software Eng. Res. Group (SERG), Univ. Teknol. Malaysia, Johor Bahru, Malaysia",2015 International Conference on Cloud Computing (ICCC),9-Jul-15,2015,,,1,6,"Topic modelling main purpose is to have machine-understandable and semantic annotation to textual contents of Web.It aim to extract knowledge rather than unrelated information. In this paper, we evaluate the impact of using topic model (which intended to represent the documents like a combination of topics where each topic is a mix of vectors) in improving documents clustering results. We have compared the results of clustering using PLSA or LSA. The experiments performed on a set of common newspaper websites that have highly dimensional data and we use Purity, Mean intra-cluster distance (MICD) and Davies-Bouldin index (DBI) for clustering evaluation. Thus, we acquired favorable clustering results, especially in the context of the Arabic language as PLSA were effective in minimizing MICD, expanding purity and bringing down DBI.",,978-1-4673-6618-2,10.1109/CLOUDCOMP.2015.7149662,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149662,,Semantics;Computational modeling;Probabilistic logic;Clustering algorithms;Web pages;Indexes;Matrix decomposition,Internet;knowledge acquisition;natural language processing;pattern clustering;probability;text analysis,topic modelling;Arabic Web page clustering;semantic annotation;Web textual content;knowledge extraction;probabilistic latent semantic analysis;PLSA;latent Dirichlet allocation;LDA;mean intracluster distance;MICD;Davies-Bouldin index;DBI,,,,32,,9-Jul-15,,,IEEE,IEEE Conferences
A rough set based approach to detect plagiarism,基於粗集的方法來檢測竊,M. Bhavani; M. Shashi,"Dept. of Computer science and Engineering, VIIT Visakhapatnam, India; Dept. of Computer Science and systems Engineering, Andhra University, Visakhapatnam, India",TENCON 2009 - 2009 IEEE Region 10 Conference,22-Jan-10,2009,,,1,8,"Plagiarism is the practice of claiming, or implying, original authorship or incorporating material from someone else's written or creative work, in whole or in part, into one's own without adequate acknowledgement. Unlike cases of forgery, in which the authenticity of the writing, document, or some other kind of object, itself is in question, plagiarism is concerned with the issue of false attribution. Plagiarism has become a significant problem in the student community due to the fact that the wide accessability of digitalized information in the WWW. It has become difficult task for the teachers as well as adjudicators to catch the cheaters. There are many tools which are either internet based or pc based to detect plagiarism and both are having advantages and disadvantages. To detect plagiarism there is a need to find the extent of similarity between a pair of text documents for providing access to topically relevant documents on one hand and for identifying document replication on the other hand. In this paper the details of a Rough Set based Document Ranking system (RSDRS) developed by the authors are presented. The terms associated with related concepts are grouped together to form equivalence classes by clustering the terms of the vocabulary. The query passage and the documents are represented as rough sets using these equivalence classes of terms and further partitioned into families of rough sets in higher level approximation spaces which impose partial ordering on the families of documents with reference to the query passage. Documents falling in the same family are ordered in accordance with their similarity to the query to form the relevance ranking of the documents.",2159-3450,978-1-4244-4546-2,10.1109/TENCON.2009.5395873,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5395873,Plagiarism;Rough set;clustering;Term frequency,Plagiarism;Data privacy;Cryptography;Data mining;Testing;Data security;Databases;Usability;Clustering algorithms;Cryptographic protocols,copyright;Internet;rough set theory;text analysis,plagiarism detection;authenticity;Internet;topically relevant documents;document replication identification;rough set based document ranking system;query passage;text documents,,1,,21,,22-Jan-10,,,IEEE,IEEE Conferences
Ontology Based Semantic Measures in Document Similarity Ranking,文檔相似度排序中基於本體的語義測度,U. K. Sridevi; N. Nagaveni,"Dept. of Appl. Sci., Sri Krishna Coll. of Engg & Tech, Coimbatore, India; Dept. of Math., Coimbatore Inst. of Technol., Coimbatore, India",2009 International Conference on Advances in Recent Technologies in Communication and Computing,17-Nov-09,2009,,,482,486,"Work has shown that ontologies are useful to improve the performance of retrieval. In this paper, we present a new distance measure using ontologies. Ontology based correlation analysis is implemented to find the relations between the terms. Combining the ontology based correlation analysis and the traditional vector space model, the document similarity is calculated. Our results show that ontology based distance measure makes better relevance measure. The proposed method has been evaluated on USGS Science directory collection. Preliminary experiments results show that our method may generate relevant document in the top rank.",,978-1-4244-5104-3,10.1109/ARTCom.2009.144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329275,Ontology;Information Retrieval;Annotation;Semantic Search;Correlation,Ontologies;Image segmentation;Clustering algorithms;Color;Computer vision;Image recognition;Pattern recognition;Machine learning;Artificial intelligence;Communications technology,correlation theory;document handling;information retrieval;ontologies (artificial intelligence);vectors,ontology;semantic measures;document similarity ranking;information retrieval;distance measure;correlation analysis;vector space model;USGS Science directory collection,,,,12,,17-Nov-09,,,IEEE,IEEE Conferences
Distorted Replicas: Intelligent Replication Schemes to Boost I/O Throughput in Document-Stores,扭曲的副本：智能複制方案可提高文檔存儲中的I / O吞吐量,K. Jouini,"MARS Res. Lab., Univ. of Sousse, Sousse, Tunisia",2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA),12-Mar-18,2017,,,25,32,"NoSQL databases commonly use an aggregate data model, where data that is expected to be accessed together is packed in a single clump and stored in a single node. This aggregate-orientation is essential to running on a cluster as it avoids cross-nodes joins and writes. An important downside of the aggregate data model is that it severely limits the ways data can be efficiently explored and processed, especially as NoSQL systems are being increasingly used by complex data-driven applications, mixing heterogeneous data access patterns.Replication is ubiquitous in NoSQL systems. In this work we propose a new replication scheme termed distorted replicas. Rather than being physically identical, distorted replicas are logically identical: they restructure replicated data in different ways, but keep the fundamental property of being constructible from one another. By doing so, distorted replicas provide new ways for exploring data, while still ensuring high availability. Experiments conducted in this paper show that even basic distortion schemes allow substantial performance improvements.",2161-5330,978-1-5386-3581-0,10.1109/AICCSA.2017.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308258,Aggregate Data Model;Document-Stores;Replication;Physical Design,Servers;Data models;Aggregates;Throughput;Databases;Electronic mail;Data aggregation,data aggregation;data models;document handling;NoSQL databases;relational databases;replicated databases;storage management,aggregate data model;single clump;single node;aggregate-orientation;cross-nodes;NoSQL systems;complex data-driven applications;distorted replicas;basic distortion schemes;intelligent replication schemes;document-stores;NoSQL databases;heterogeneous data access pattern mixing,,,,21,,12-Mar-18,,,IEEE,IEEE Conferences
Inference for probabilistic unsupervised text clustering,概率無監督文本聚類的推理,L. Rigouste; O. Cappe; F. Yvon,"Ecole Nationale Superieure des Telecommun., Paris; Ecole Nationale Superieure des Telecommun., Paris; Ecole Nationale Superieure des Telecommun., Paris","IEEE/SP 13th Workshop on Statistical Signal Processing, 2005",8-May-06,2005,,,387,392,"In this article, we investigate the use of a simple probabilistic model for unsupervised document clustering in large collections of texts. The model consists of a mixture of multinomial distributions over the word counts, each component corresponding to a different theme. The expectation-maximization (EM) algorithm is the basic tool used for inference. After introducing the model and experimental framework (corpus and evaluation measures), we discuss the importance of initialization and illustrate the difficulty caused by the lack of supervision information. We propose some ideas to solve this problem, one of the most efficient method being based on vocabulary reduction, and finally compare those heuristics with other inference processes, such as Gibbs sampling",2373-0803,0-7803-9403-8,10.1109/SSP.2005.1628626,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1628626,,Parameter estimation;Vocabulary;Electronic mail;Clustering algorithms;Sampling methods;Inference algorithms;Availability;Web pages;Text mining;Performance analysis,expectation-maximisation algorithm;inference mechanisms;pattern clustering;probability;text analysis;unsupervised learning;vocabulary,unsupervised document clustering;probabilistic model;text collection;multinomial distribution;expectation-maximization algorithm;vocabulary reduction;inference process,,5,,6,,8-May-06,,,IEEE,IEEE Conferences
Collaborative Co-clustering across Multiple Social Media,跨多個社交媒體的協作聯合集群,F. Wang; S. Lin; P. S. Yu,"Univ. of Illinois at Chicago, Chicago, IL, USA; Univ. of Illinois at Chicago, Chicago, IL, USA; Univ. of Illinois at Chicago, Chicago, IL, USA",2016 17th IEEE International Conference on Mobile Data Management (MDM),21-Jul-16,2016,1,,142,151,"Combining multiple source information can often lead to improved performance on the learning task. Information from different sources could potentially compensate the missing information in a single source. However, designing an effective combining scheme is not always straightforward in practice. This paper aims to combine information from multiple social media websites to enhance the co-clustering performance of two types of objects (social media objects and users) in one social network, since users could leave footprints across different social media websites, such as Twitter, Foursquare, etc. Data generated from multiple heterogeneous sources can be casted in a multi-view setting. Specifically, we construct the relationship matrix as relationship view and features of each individual object from different sources as different feature views. In previous works, features besides relationship matrix were added to co-clustering in the following manners: different features are taken indiscriminately, those features act as hard constraints to force final co-clusters agree with these constraints. A co-regularized collaborative co-clustering model (Co-CoClust) is proposed to simultaneously perform co-clustering on relationship view and clustering on multiple feature views. In this framework, features from different sources are treated discriminately since they are divided into separate views and utilized based on the distance from relationship matrix. Co-regularization technique is introduced to impose a common constraint between co-clustering and clusterings such that the relationship matrix and feature matrix are unified. By alternating minimization, results of co-clustering and clustering from different views are iteratively optimized. Therefore, co-clustering results are improved by leveraging multiple source information. The proposed algorithm proves its effectiveness in social media datasets and traditional document-word datasets.",2375-0324,978-1-5090-0883-4,10.1109/MDM.2016.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517788,co-clustering;social media;multi-view learning,Media;Twitter;Collaboration;Linear programming;Optimization;Bipartite graph,pattern clustering;social networking (online),collaborative co-clustering;social media;learning task;social media objects;social media users;relationship matrix;heterogeneous sources;coregularized collaborative coclustering model,,5,,24,,21-Jul-16,,,IEEE,IEEE Conferences
Scalable video summarization of cultural video documents in cross-media space based on data cube approach,基於數據立方體方法的跨媒體空間中文化視頻文檔的可擴展視頻摘要,K. R. Perez-Daniel; M. N. Miyatake; J. Benois-Pineau; S. Maabout; G. Sargent,"SEPI, ESIME Culhuacan, National Polytechnic Institute, IPN, Mexico City, Mexico; SEPI, ESIME Culhuacan, National Polytechnic Institute, IPN, Mexico City, Mexico; LaBRI, University of Bordeaux I, France; LaBRI, University of Bordeaux I, France; LaBRI, University of Bordeaux I, France",2014 12th International Workshop on Content-Based Multimedia Indexing (CBMI),10-Jul-14,2014,,,1,6,Video summarization has been a core problem to manage the growing amount of content in multimedia databases. An efficient video summary should display an overview of the video content and most of existing approaches fulfil this goal. However the information does not allow user to get all details of interest selectively and progressively. This paper proposes a scalable video summarization approach which provides multiple views and levels of details. Our method relies on the usage of cross media space and consensus clustering method. A video document is modelled as a data cube where the level of details is refined over nonconsensual features of the space. The method is designed for weakly structured content such as cultural documentaries and was tested on the INA corpus of cultural archives.,1949-3991,978-1-4799-3990-9,10.1109/CBMI.2014.6849824,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849824,Video summarization;Scalability;Cross-media space;Consensus clustering;Data cube,Navigation;Visualization;Image color analysis;Cultural differences;Scalability;Data models;Multimedia communication,cultural aspects;document handling;multimedia databases;records management;video signal processing,scalable video summarization;cultural video documents;cross-media space;data cube approach;multimedia databases;cultural archives,,2,,16,,10-Jul-14,,,IEEE,IEEE Conferences
Traceability between business process and software component using Probabilistic Latent Semantic Analysis,使用概率潛在語義分析的業務流程和軟件組件之間的可追溯性,F. Revindasari; R. Sarno; A. Solichah,"Informatics Department, Faculty of Information Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Department, Faculty of Information Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Department, Faculty of Information Technology, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia",2016 International Conference on Informatics and Computing (ICIC),24-Apr-17,2016,,,245,250,"Business process and software component has relationship on business process execution in the organization or company. Changes in business process affecting the software component. A Method is needed to determine traceability of artifacts between process on business process and software component. The purpose of traceability is to trace the difference between business process of accompany and its software component through the artifacts. The artifacts in business process are identified by sequence of process, while the artifacts in software component are in the form of modules. In the proposed method, there are two main stage, namely modelling of process on business process and software component and document clustering using Probabilistic Latent Semantic Analysis (PLSA). In the modelling phase, process on business process and software component are grouped into documents. Then, the documents are processed by separating document into words. In the document clustering, documents are calculated using PLSA. It can be concluded that the document clustering can be done with recall 100% and precision 59%.",,978-1-5090-1648-8,10.1109/IAC.2016.7905723,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905723,traceability;business process;software component;PLSA;cosine similarity,Business;Software;Probability;Probabilistic logic;Semantics;Context;Informatics,business data processing;document handling;natural language processing;pattern clustering;probability;program diagnostics,probabilistic latent semantic analysis;business process execution;artifacts traceability;software component;document clustering;PLSA;recall;precision,,2,,17,,24-Apr-17,,,IEEE,IEEE Conferences
Knowledge acquisition from documents with both fixed and free formats,從固定和自由格式的文檔中獲取知識,S. Hirasawa; W. W. Chu,"Dept. of Industrial & Manage. Syst. Eng., Waseda Univ., Japan; NA","SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",17-Nov-03,2003,5,,4694,4699 vol.5,"Based on techniques in information retrieval, we discuss the methods for knowledge acquisition from the documents composed of both fixed and free formats. The documents with the fixed format imply items with those selected from the sentences, words, symbols, or numbers, while the documents with free format are with the usual text. In this paper, starting with the item-document matrix and term-document matrix used for the representation of a document set, we propose a new method for knowledge acquisition taking simultaneously into account of both fixed and free formats. A method based on the probabilistic latent semantic indexing (PLSI) model is used for clustering a set of documents. The proposed method is applied to a document set given by the questionnaires of students taken for the purpose of faculty development. We show the effectiveness of the proposed method compared to the conventional method.",1062-922X,0-7803-7952-7,10.1109/ICSMC.2003.1245725,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245725,,Knowledge acquisition;Knowledge engineering;Indexing;Information retrieval;Matrix decomposition;Computer industry;Knowledge management;Engineering management;Systems engineering and theory;Computer science,knowledge acquisition;information retrieval;programming language semantics;probability,knowledge acquisition;free format;information retrieval;fixed format;item-document matrix;term-document matrix;probabilistic latent semantic indexing model;students questionnaires;clustering,,1,,7,,17-Nov-03,,,IEEE,IEEE Conferences
An enhanced approach on handling missing values using bagging k-NN imputation,使用裝袋k-NN插補處理缺失值的增強方法,V. Kumutha; S. Palaniammal,"Dept. of Computer Science, D. J. Academy for Managerial Excellence, Coimbatore, Tamil nadu, India; Dept. of Science & Humanities, Sri Krishna College of Technology, Coimbatore, Tamil nadu, India",2013 International Conference on Computer Communication and Informatics,21-Feb-13,2013,,,1,8,"Researchers in the database community have aroused great interest in handling high dimensional data sets for the past decades. Today's business captures inundate sets of data which includes digital documents, web pages-customer databases, hyper-spectral imagery, social networks, gene arrays, proteomics data, neurobiological signals, high dimensional dynamical systems, sensor networks, financial transactions and traffic statistics thereby generating massive high dimensional datasets. DNA microarray paves methods in identifying different expression levels of thousands of genes during biological process. The problem with microarrays is to measure gene expression from thousands of genes (features) from only tens of hundreds of samples. Microarray data often contain several missing values that may affect subsequent analysis. In this paper, a novel approach on imputation using k-NN with bagging method is proposed to handle missing value. The experimental result shows that the proposed method outperforms other methods in terms of distance and density of clusters. The proposed approach has enhanced the performance of traditional k-NN impute using bagging method.",,978-1-4673-2907-1,10.1109/ICCCI.2013.6466301,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466301,clustering;microarray;missing value;bagging,Clustering algorithms;Gene expression;Bagging;Correlation;Classification algorithms;Databases;Computers,biology computing;data handling;genetics;lab-on-a-chip;learning (artificial intelligence);pattern clustering,missing values handling;bagging k-NN imputation;database community;high dimensional data sets;business;digital documents;Web pages-customer databases;hyper-spectral imagery;social networks;gene arrays;proteomics data;neurobiological signals;high dimensional dynamical systems;sensor networks;financial transactions;traffic statistics;DNA microarray;biological process;genes expression levels;microarray data;bagging method;cluster density;cluster distance,,3,,37,,21-Feb-13,,,IEEE,IEEE Conferences
MCBDist: A Novel Markov-Chain-Based Measure of Distance among Webpages,MCBDist：一種基於馬爾可夫鏈的新穎的網頁之間距離度量,Z. Xiong; G. Wu,"Department of Computer Science, Shantou University, Shantou, Guangdong Province, China. witmmx@yahoo.com.cn; Department of Computer Science, Shantou University, Shantou, Guangdong Province, China. witmmx@yahoo.com.cn","2008 IEEE International Conference on Networking, Sensing and Control",20-May-08,2008,,,1577,1582,"In the Web server cluster adopting content-aware request dispatching, the persistent connection feature of HTTP/1.1 may bring extra cost. If the webpages, that are likely to be visited together by users, are grouped into webpage cluster appropriately, and the webpage cluster is viewed as document distribution unit, such cost may decrease. How to measure the distance among webpages is a key problem of webpage clustering. In this paper, we propose a novel Markov-chain-based measure of distance among webpages, called MCBDist. It not only considers the temporal correlation of users' visit, but also considers the path of users' visit. In addition, for the dimension of the transition probability matrix is usually very large, transition probability matrix compression is used to deduce the computational complexity. Finally, we give an example to illustrate the effectiveness of MCBDist.",,978-1-4244-1685-1,10.1109/ICNSC.2008.4525472,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4525472,,Costs;Web server;Dispatching;File servers;Network servers;Splicing;File systems;Computational complexity;Routing;Delay,computational complexity;data compression;document handling;Internet;Markov processes;matrix algebra;probability;workstation clusters,Markov-chain-based measure;Web page distance measure;Web server cluster;content-aware request dispatching;Web page clustering;document distribution unit;temporal correlation;transition probability matrix compression;computational complexity,,,,14,,20-May-08,,,IEEE,IEEE Conferences
Detecting hot topics in technology news streams,在技??術新聞流中檢測熱門話題,Bo You; Ming Liu; Bing-Quan Liu; Xiao-Long Wang,"Intelligient Technology and Natural Language Processing Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, 150001, China; Intelligient Technology and Natural Language Processing Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, 150001, China; Intelligient Technology and Natural Language Processing Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, 150001, China; Intelligient Technology and Natural Language Processing Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, 150001, China",2012 International Conference on Machine Learning and Cybernetics,24-Nov-12,2012,5,,1968,1974,"Detecting hot topics with a fine granularity in technology news streams is an interesting and important problem given the large amount of reports and a relatively narrow range of topics. In this paper, a three-phase method is proposed. In the first phase, the document topic distribution vector is generated and keywords are extracted for each document using topic model pachinko allocation. In the second phase, the documents are clustered based on the document topic distribution vector obtained from the previous phase using affinity propagation. And in the last phase, actual events denoted by combinations of keywords within each cluster are found out using frequent pattern mining algorithms. We evaluate our approach on a collection of technology news reports from various sites in a fixed time period. T he results show that this method is effective.",2160-1348,978-1-4673-1487-9,10.1109/ICMLC.2012.6359678,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359678,Hot topic;Technology news streams;Topic model;Document clustering;Frequent pattern mining,Abstracts;Merging,data mining;document handling;information resources,hot topics detection;technology news streams;document topic distribution vector;topic model pachinko allocation;affinity propagation;pattern mining,,,,21,,24-Nov-12,,,IEEE,IEEE Conferences
World image matching as a technique for degraded text recognition,世界圖像匹配作為降級文本識別技術,J. J. Hull; S. Khoubyari; T. K. Ho,"Dept. of Comput. Sci., State Univ. of New York, Buffalo, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Buffalo, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Buffalo, NY, USA","Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems",6-Aug-02,1992,,,665,668,A technique is presented that determines equivalences between word images in a passage of text. A clustering procedure is applied to group visually similar words. Initial hypotheses for the identities of words are then generated by matching the word groups to language statistics that predict the frequency at which certain words will occur. This is followed by a recognition step that assigns identifications to the images in the clusters. This paper concentrates on the clustering algorithm. A clustering technique is presented and its performance on a running text of 1062 word images is determined. It is shown that the clustering algorithm can correctly locate groups of short function words with better than a 95 percent correct rate.<>,,0-8186-2915-0,10.1109/ICPR.1992.201864,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=201864,,Image matching;Degradation;Text recognition;Image recognition;Character recognition;Statistics;Dictionaries;Clustering algorithms;Error correction;Natural languages,document image processing;image recognition,degraded text recognition;equivalences;word images;clustering procedure;word groups;language statistics;recognition step;clustering algorithm;short function words,,6,,8,,6-Aug-02,,,IEEE,IEEE Conferences
Learning local image descriptors for word spotting,學習本地圖像描述符以發現單詞,S. Sudholt; L. Rothacker; G. A. Fink,"Department of Computer Science, Technische Universit瓣t Dortmund University, 44221, Germany; Department of Computer Science, Technische Universit瓣t Dortmund University, 44221, Germany; Department of Computer Science, Technische Universit瓣t Dortmund University, 44221, Germany",2015 13th International Conference on Document Analysis and Recognition (ICDAR),23-Nov-15,2015,,,651,655,"The Bag-of-Features paradigm has enjoyed great success in computer vision as well as document image analysis applications. By far the most common approach here is to power the Bag-of-Features pipeline with SIFT descriptors which are then clustered into a visual vocabulary using Lloyd's algorithm. In contrast to using handcrafted descriptors, many researches have started to use descriptors that have been learned from data. While descriptor learning is common in other computer vision tasks, there has been little work on learning descriptors for document analysis purposes. In this work we propose a descriptor learning pipeline designed for word spotting. Evaluation results on the well known George Washington database demonstrate that word-spotting results can effectively be improved by learning specialized local image descriptors.",,978-1-4799-1805-8,10.1109/ICDAR.2015.7333842,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333842,,Lead;Shape,computer vision;document image processing;learning (artificial intelligence);transforms;visual databases,learning local image descriptors;word spotting;Bag-of-Features;computer vision;document image analysis applications;Bag-of-Features pipeline;SIFT descriptors;visual vocabulary;Lloyd algorithm;handcrafted descriptors;descriptor learning;document analysis;George Washington database,,4,,15,,23-Nov-15,,,IEEE,IEEE Conferences
Automatic text categorization using a system of high-precision and high-recall models,使用高精度和高召回率模型系統進行自動文本分類,D. Li; Y. L. Murphey,"Dept. of Electrical and Computer Engineering, University of Michigan-Dearborn, 48128, USA; Dept. of Electrical and Computer Engineering, University of Michigan-Dearborn, 48128, USA",2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM),15-Jan-15,2014,,,373,380,"This paper presents an automatic text document categorization system, HPHR. HPHR contains high precision, high recall and noise-filtered text categorization models. The text categorization models are generated through a suite of machine learning algorithms, a fast clustering algorithm that efficiently and effectively group documents into subcategories, and a text category generation algorithm that automatically generates text subcategories that represent high precision, high recall and noise-filtered text categorization models from a given set of training documents. The HPHR system was evaluated on documents drawn from two different applications, vehicle fault diagnostic documents, which are in a form of unstructured and verbatim text descriptions, and Reuters corpus. The performance of the proposed system, HPHR, on both document collections showed superiority over the systems commonly used in text document categorization.",,978-1-4799-4518-4,10.1109/CIDM.2014.7008692,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008692,,Clustering algorithms;Text categorization;Vectors;Machine learning algorithms;Training data;Algorithm design and analysis;Training,data mining;learning (artificial intelligence);pattern clustering;text analysis,automatic text document categorization system;high-precision and high-recall models;HPHR;machine learning algorithms;clustering algorithm;vehicle fault diagnostic documents;Reuters corpus;text mining,,1,,16,,15-Jan-15,,,IEEE,IEEE Conferences
Text Modeling for Real-Time Document Categorization,用於實時文檔分類的文本建模,J. Byrnes; R. Rohwer,"HNC Software, LLC Fair Isaac Corporation 3661 Valley Centre Drive San Diego, CA 92130 858-369-8193, JohnByrnes@FairIsaac.com; HNC Software, LLC, Fair Isaac Corp., San Diego, CA",2005 IEEE Aerospace Conference,19-Dec-05,2005,,,1,11,"We report on experiments in adapting document categorization techniques to provide for implementation in high-speed hardware. Because resources are scarce, it is important to have a small set of robust and maximally informative variables over which learning can occur. We generate variables using information-theoretic clustering. The resulting performance is on par with general-purpose computing implementations which are able to take advantage of large amounts of time and memory. We conclude that custom high-speed hardware for document categorization can be made very accurate. We also believe that some of the strengths of information-theoretic data analysis techniques are brought out",1095-323X,0-7803-8870-4,10.1109/AERO.2005.1559610,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559610,,Hardware;Robustness;Mutual information;TCPIP;Data analysis;Biographies;Labeling;Routing;Internet;Protocols,classification;document handling,text modeling;real-time document categorization;high-speed hardware;information-theoretic clustering;information-theoretic data analysis,,8,,14,,19-Dec-05,,,IEEE,IEEE Conferences
A taxonomy based semantic similarity of documents using the cosine measure,使用餘弦測度的基於分類的文檔語義相似度,A. Madylova; S. G. Oguducu,"Dept. of Comput. Eng., Istanbul Tech. Univ., Istanbul, Turkey; Dept. of Comput. Eng., Istanbul Tech. Univ., Istanbul, Turkey",2009 24th International Symposium on Computer and Information Sciences,23-Oct-09,2009,,,129,134,"In this paper, we present a new method for calculating semantic similarities between documents. This method is based on cosine similarity calculation between concept vectors of documents obtained from a taxonomy of words that captures IS-A relations. The calculation of semantic similarities between documents is a very time consuming task, since it is necessary first to calculate semantic similarities between each pair of words that appear on different documents. In this paper, we present a new method to calculate semantic similarities between documents which results in faster computational time. Both a taxonomy based semantic similarity and cosine similarity are employed. First, the concept vectors of documents are obtained by extending the terms in the document vectors with their corresponding IS-A concepts. Cosine similarity is then calculated between those concept vectors of documents. Thus, the overall similarity between documents is a combination of cosine similarity and semantic similarity. The proposed semantic similarity is tested in document clustering problem. The experimental results show that our method achieves a good performance.",,978-1-4244-5021-3,10.1109/ISCIS.2009.5291865,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291865,,Taxonomy;Search engines;Frequency;Recommender systems;Computational complexity;Testing;Web sites;World Wide Web;Internet;Information retrieval,document handling,taxonomy-based semantic similarity;documents similarity;cosine measure;cosine similarity calculation;concept vectors;IS-A concepts,,18,,23,,23-Oct-09,,,IEEE,IEEE Conferences
Knowledge Map Construction Using Text Mining and Artificial Bee Colony Algorithm,基於文本挖掘和人工蜂群算法的知識圖構建,T. Chieh-Yuan; J. Wei-Zhong,"Dept. of Ind. Eng. & Manage., Yuan Ze Univ., Chungli, Taiwan; Dept. of Ind. Eng. & Manage., Yuan Ze Univ., Chungli, Taiwan",2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),22-Jun-18,2018,1,,382,387,"With the rapid development of the information technology, information overload becomes a serious problem during the information acquisition process. To relieve this difficulty, knowledge map is a systematic approach to reveal the underlying relationships between abundant knowledge sources. However, few studies focused on how to optimize the coordinates of knowledge items in the map to help users easily understand complicated relatedness among knowledge topics. To bridge this gap, this paper proposes a novel knowledge map construction approach using text mining and artificial bee colony (ABC) algorithm. First, the textural documents related to a certain domain are represented as a term vector in m-dimensional space with the term frequency-inverse document frequency (TF-IDF) analysis. Second, hierarchical clustering is applied to identify important topics. Third, high-dimensional relationships among knowledge items are transformed into a 2-dimensional space optimized by the ABC algorithm. A set of experiments shows that setting appropriate number of clusters is important for visual perception. In addition, a practical example in topic trend analysis using the proposed approach is demonstrated at the end of this paper.",0730-3157,978-1-5386-2667-2,10.1109/COMPSAC.2018.00060,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377685,"Knowledge map, Text mining, Hierarchical clustering, Artificial bee colony algorithm.",Clustering algorithms;Databases;Visualization;Stress;Text mining;Genetic algorithms,data mining;information retrieval;optimisation;pattern clustering;text analysis,ABC algorithm;text mining;artificial bee colony algorithm;information technology;information acquisition process;knowledge topics;textural documents;term frequency-inverse document frequency analysis;knowledge sources;knowledge map construction approach;visual perception,,,,15,,22-Jun-18,,,IEEE,IEEE Conferences
A Latent Semantic Approach to XML Clustering by Content and Structure Based on Non-negative Matrix Factorization,基於非負矩陣分解的按內容和結構進行XML聚類的潛在語義方法,G. Costa; R. Ortale,"ICAR, Rende, Italy; ICAR, Rende, Italy",2013 12th International Conference on Machine Learning and Applications,10-Apr-14,2013,1,,179,184,"Non-negative matrix factorization is intensively used in text clustering. We investigate its exploitation in the XML domain for clustering XML documents by structure and content into topically homogeneous groups. Non-negative matrix factorization is performed through an alternating least squares method, which incorporates expedients to attenuate the burden of large-scale factorizations. This is especially relevant when massive text-centric XML corpora are processed. Empirical evidence from a comparative evaluation on real-world XML corpora reveals that our approach overcomes several state-of-the-art competitors in effectiveness.",,978-0-7695-5144-9,10.1109/ICMLA.2013.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784608,,XML;Semantics;Vegetation;Encyclopedias;Electronic publishing;Internet,least squares approximations;matrix decomposition;pattern clustering;text analysis;XML,latent semantic approach;XML clustering;nonnegative matrix factorization;text clustering;XML documents;alternating least squares method,,7,,29,,10-Apr-14,,,IEEE,IEEE Conferences
Geodesic distance based multi-document summarization,基於測地距離的多文檔摘要,H. Ma; Q. He; Z. Shi,"Institute of Computing Technology, Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Sciences, China",2008 International Conference on Natural Language Processing and Knowledge Engineering,2-May-09,2008,,,1,6,"This paper presents a novel extractive approach which takes advantage of geodesic distance for sentence similarity computation to multi-document summarization task. Based on geodesic distance between every two sentences, the text relationship map is constructed. Sentences with higher degree in the map are selected and grouped into clusters. Finally, sentences with highest degree of each cluster are chosen to form the final summary. Experiments on DUC data are performed and the ROUGE evaluation results demonstrate that the proposed approach can be an effective way for multi-document summarization.",,978-1-4244-4515-8,10.1109/NLPKE.2008.4906785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906785,Geodesic distance;Cosine angle distance;Sentence similarity;Manifold;Multi-document Summarization,Computers;Geophysics computing;Data mining;Distance measurement;Helium;Performance evaluation;Text mining;Extraterrestrial measurements;Level measurement;Indexing,document handling,geodesic distance;multidocument summarization;sentence similarity computation;relationship map;ROUGE evaluation,,,,15,,2-May-09,,,IEEE,IEEE Conferences
Theses cluster based on bilingual and synonymous keyword sets using mutual information,論文基於使用互信息的雙語和同義詞關鍵字集,Chung-Yi Huang; Rung-Ching Chen,"Department of Information Management, Chaoyang University of Technology, Taichung 41349, Taiwan; Department of Computer Science and Engineering, National Chung-Hsing University, Taichung 402, Taiwan",2009 International Conference on Machine Learning and Cybernetics,25-Aug-09,2009,5,,2999,3004,"Searching published papers is a required activity for the researching process. Since articles are presented in various languages, it makes precise queries hard to achieve. In this paper, we propose an automatic theses clustering method based on bilingual and synonymous keyword sets which includes Chinese and English keywords. We also provide a clustering computation to speedup operation. First, the system automatically generates bilingual and synonymous keyword sets, and then based on bilingual and synonymous keyword sets, clustering the theses. The method not only solves the weakness of using digital dictionaries to solve clustering problems, but also makes error problem, the query by bilingual and synonymous keywords, be restricted. The system was implemented by a clustering computation technology to solve traditional documents clustering systems performance problems. Through many computer processes, the system not only can save a lot of time, but also can attain high availability and load balancing effectiveness. Primary experiments prove that the system makes the theses clustering work effectively.",2160-1348,978-1-4244-3702-3,10.1109/ICMLC.2009.5212598,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212598,Document clustering;Keyword set;Bilingual and synonymous keyword,Mutual information;Dictionaries;Databases;Natural languages;Wireless networks;Frequency;Wireless LAN;Classification tree analysis;Machine learning;Cybernetics,data mining;dictionaries;text analysis;word processing,mutual information;automatic theses clustering method;bilingual keyword;synonymous keyword sets;error problem;digital dictionary,,,,16,,25-Aug-09,,,IEEE,IEEE Conferences
Robust text extraction in mixed-type binary documents,混合類型二進製文檔中的可靠文本提取,A. Nikolaidis; C. Strouthopoulos,"Department of Informatics and Communications, Technological Educational Institute of Serres, Terma Magnisias, 62124, GREECE; Department of Informatics and Communications, Technological Educational Institute of Serres, Terma Magnisias, 62124, GREECE",2008 IEEE 10th Workshop on Multimedia Signal Processing,5-Nov-08,2008,,,393,398,"Text extraction from documents is an essential preprocessing stage of applications such as OCR (optical character recognition), document image compression, storage and retrieval. Although many different techniques have been proposed to date, they usually assume that text orientation and size is fixed throughout the document image. Our work faces the problem of varying orientation and size, which is often the case in practice, either because of the nature of the original document or due to imposed distortions. Our algorithm first identifies marks using a suitable contour following technique. A PCA (principal component analyzer) is afterwards employed in order to determine the principal axes of each mark, and a nearest-neighbor technique is used to find the shortest distances between marks. A feature vector is formed based on mark dimensions and distances between them, which is then fed into a SOFM (self-organizing feature map) in order to divide the marks into homogeneous clusters. A set of fuzzy rules is formed using all cluster weights and variances. Finally, a fuzzy classification scheme identifies each mark as a character or a non-character. The technique was tested on a variety of mixed-type documents and it proved to be quite fast and accurate.",,978-1-4244-2294-4,10.1109/MMSP.2008.4665110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665110,,Neurons;Distance measurement;Feature extraction;Optical character recognition software;Classification algorithms;Pattern recognition;Principal component analysis,fuzzy set theory;principal component analysis;self-organising feature maps;text analysis,robust text extraction;mixed-type binary documents;optical character recognition;document image compression;text orientation;principal component analysis;self organizing feature map;homogeneous clusters;fuzzy rules;fuzzy classification,,,,15,,5-Nov-08,,,IEEE,IEEE Conferences
Robust Output Modeling in Bag-of-Features HMMs for Handwriting Recognition,功能強大的HMM中用於手寫識別的魯棒輸出建模,L. Rothacker; G. A. Fink,"Dept. of Comput. Sci., Tech. Univ. Dortmund Univ., Dortmund, Germany; Dept. of Comput. Sci., Tech. Univ. Dortmund Univ., Dortmund, Germany",2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR),16-Jan-17,2016,,,199,204,"Bag-of-Features HMMs have been successfully applied to handwriting recognition and word spotting. In this paper we extend our previous work and present methods for modeling sequences of Bag-of-Features representations with Hidden Markov Models. We will discuss our previous approach that uses a pseudo-discrete model. Afterwards, we present a novel semi-continuous integration. The method is effective for probabilistic text clustering and is suitable for statistically modeling the characteristics of Bag-of-Features representations extracted from document images. Furthermore, its statistical expectation-maximization estimation can directly be integrated in Baum-Welch HMM training. In our experiments we present competitive results on the IfN/ENIT word recognition benchmark and state-of-the-art results for word spotting on the George Washington benchmark. Our evaluation gives insights into the properties of the models from the perspectives of modern as well as historic document analysis.",2167-6445,978-1-5090-0981-7,10.1109/ICFHR.2016.0047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814063,Bag-of-Features;Hidden Markov Models;handwriting recognition;word spotting,Hidden Markov models;Visualization;Mathematical model;Handwriting recognition;Vocabulary;Adaptation models;Computational modeling,document image processing;estimation theory;expectation-maximisation algorithm;handwriting recognition;hidden Markov models;image representation;pattern clustering;probability;statistical analysis;text analysis,robust output modeling;handwriting recognition;Bag-of-Features HMMs;word spotting;Bag-of-Features representations;hidden Markov models;pseudodiscrete model;semicontinuous integration;probabilistic text clustering;document images;statistical expectation-maximization estimation;Baum-Welch HMM training;IfN/ENIT word recognition benchmark;George Washington benchmark;historic document analysis,,2,,20,,16-Jan-17,,,IEEE,IEEE Conferences
Concept decomposition by fuzzy k-means algorithm,模糊K-均值算法的概念分解,J. Dobsa; B. J. Basic,"Fac. of Organ. & Informatics, Zagreb Univ., Croatia; NA",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),27-Oct-03,2003,,,684,688,"The method of latent semantic indexing (LSI) is an information retrieval technique using a low-rank singular value decomposition (SVD) of the term-document matrix. Although the LSI method has empirical success, it suffers from the lack of interpretation for the low-rank approximation and, consequently, the lack of controls for accomplishing specific tasks in information retrieval. A method introduced by Dhillon and Modha is an improvement in that direction. It uses centroids of clusters or so called concept decomposition for lowering the rank of the term-document matrix. We focus on improvements of that method using fuzzy k-means algorithm. Also, we compare the precision of information retrieval for the two above methods.",,0-7695-1932-6,10.1109/WI.2003.1241296,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241296,,Large scale integration;Information retrieval;Matrix decomposition;Clustering algorithms;Singular value decomposition;Informatics;Indexing;Data mining;Approximation algorithms,information retrieval;singular value decomposition;approximation theory;fuzzy logic;indexing;document handling,latent semantic indexing;LSI;information retrieval technique;singular value decomposition;SVD;term-document matrix;low-rank approximation;concept decomposition;fuzzy k-means algorithm,,2,,10,,27-Oct-03,,,IEEE,IEEE Conferences
"Extrapolation, Design and Implementation of a Bangla Web Document Amendable Text Synopsis",Bangla Web文檔可修改文本簡介的外推，設計和實現,M. M. Rahman; Z. Sultana,"International Islamic University Chittagong,Department of Computer Science and Engineering,Chittagong,Bangladesh,4318; International Islamic University Chittagong,Department of Computer Science and Engineering,Chittagong,Bangladesh,4318",2019 International Conference on Sustainable Technologies for Industry 4.0 (STI),16-Apr-20,2019,,,1,4,"With nearly 200 million speakers, Bengali is one of the ten most widely spoken languages in the globe. Growing internet resources show a definite need for apps in the Bengali language, recovery systems and automatic text summary. It is hard to design a scheme for producing summaries of human quality, so many scientists have concentrated on the extraction of sentences or paragraphs, which is a kind of summary. With the assistance of machine-supported technology, an smart method is implemented in this study job to summarize the Bangla texts. This scheme can be commonly used to effectively summarize Bangla text that helps to quickly obtain the focal or central components of the papers. A excellent instance of a summary scheme is the convention search engine such as Google, representing a compressed search results description. Other examples include summarizing news for Bangla SMS or WAP, news subscriptions addressed by keywords, etc. An algorithm is suggested in this work Bangla Summarization System to extract the Bangla overview that operates on four passes. On the input Bangla phrases and word frequency calculation, the first two passes conduct tokenization and the next two passes conduct phrase scoring and summary generation.",,978-1-7281-6099-3,10.1109/STI47673.2019.9068073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068073,Text summarization;Extrapolation;Natural language processing;Morphology;Synopsis,Search engines;Industries;Extrapolation;Google;Semantics;Clustering algorithms,electronic messaging;Internet;natural language processing;search engines;text analysis,summary generation;Bangla web document amendable text synopsis;internet resources;Bengali language;recovery systems;automatic text summary;human quality;paragraphs;machine-supported technology;smart method;Bangla text;focal components;central components;summary scheme;convention search engine;compressed search results description;work Bangla Summarization System;input Bangla;phrase scoring,,,,14,,16-Apr-20,,,IEEE,IEEE Conferences
Generating a Topic Hierarchy from Dialect Texts,從方言文本生成主題層次結構,W. D. Smet; M. Moens,"K.U. Leuven, Belgium; K.U. Leuven, Belgium",18th International Workshop on Database and Expert Systems Applications (DEXA 2007),24-Sep-07,2007,,,249,253,"We built a system for the automatic creation of a text- based topic hierarchy, meant to be used in a geographically defined community. This poses two main problems. First, the appearance of both standard language and a community-related dialect, demanding that dialect words should be as much as possible corrected to standard words, and second, the automatic hierarchic clustering of texts by their topic. The problem of correcting dialect words is dealt with by performing a nearest neighbor search over a dynamic set of known words, using a set of transition rules from dialect to standard words, which are learned from a parallel corpus. We solve the clustering problem by implementing a hierarchical co-clustering algorithm that automatically generates a topic hierarchy of the collection and simultaneously groups documents and words into clusters.",2378-3915,978-0-7695-2932-5,10.1109/DEXA.2007.149,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4312895,,Cities and towns;Nearest neighbor searches;Clustering algorithms;Dictionaries;Databases;Expert systems;Application software;Computer science;Text recognition;Document handling,natural language processing;text analysis,dialect texts;text-based topic hierarchy;geographically defined community;standard language;community-related dialect;dialect words;automatic hierarchic clustering;hierarchical coclustering algorithm,,4,,3,,24-Sep-07,,,IEEE,IEEE Conferences
Cluster-based adaptive information retrieval,基於聚類的自適應信息檢索,J. N. Bhuyan; J. S. Deogun; V. V. Raghavan,"Dept. of Comput. Sci., Tuskegee Univ., AL, USA; NA; NA",Proceedings of the Twenty-Fourth Annual Hawaii International Conference on System Sciences,6-Aug-02,1991,i,,307,316 vol.1,The paper discusses the issues involved in the design of a complete information retrieval system based on user-oriented clustering schemes. Clusters are constructed taking into account the users' perception of similarity between documents. The system accumulates feed-back from the users and employs it to construct user-oriented clusters. An optimization function to improve the effectiveness of the clustering process is developed. A retrieval process based on the clustering scheme is described. The system developed is experimentally validated and compared with existing systems.<>,,,10.1109/HICSS.1991.183900,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=183900,,Information retrieval;Indexing;Computer science;Pattern recognition;Feedback,adaptive systems;information retrieval,document classification;co-relevant;document similarity;relevance feedback;cluster based adaptive information retrieval;information retrieval system;user-oriented clustering schemes;optimization function,,1,,30,,6-Aug-02,,,IEEE,IEEE Conferences
Querying and clustering Web pages about persons and organizations,查詢和聚類有關個人和組織的網頁,S. Ye; T. -. Chua; J. R. Kei,"Sch. of Comput., Nat. Univ. of Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore",Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003),27-Oct-03,2003,,,344,350,"One of the most frequent Web surfing tasks is to search for names of persons and organizations. Such names are often not distinctive, commonly occurring, and nonunique. Thus, a single name may be mapped to several entities. We describe a methodology to cluster the Web pages returned by the search engine so that pages belonging to different entities are clustered into different groups. The algorithm uses a combination of named entities, link-based and structure-based information as features to partition the document set into direct and indirect pages using a decision model. It then uses the distinct direct pages as seeds to cluster the document set into different clusters. The algorithm has been found to be effective for Web-based applications.",,0-7695-1932-6,10.1109/WI.2003.1241214,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1241214,,Web pages;Search engines;Clustering algorithms;Partitioning algorithms;Internet;Tellurium;Home computing;Books;Resumes;Biographies,search engines;Internet;query formulation;pattern clustering,Web page clustering;Web surfing;search engine;decision model;Internet;query formulation;statistical analysis,,,,18,,27-Oct-03,,,IEEE,IEEE Conferences
Cluster Matching for Discrete Data with Multiple Domains without Alignment of Information,無需信息對齊即可對具有多個域的離散數據進行集群匹配,S. Bollam; J. Kandi,"Mallareddy Institute of Engineering and Technology,Department of IT,Hyderabad,India; Mallareddy Institute of Engineering and Technology,Department of IT,Hyderabad,India",2019 International Conference on Emerging Trends in Science and Engineering (ICESE),11-Sep-20,2019,1,,1,4,"Outsourcing statistics toward a third party managerial manipulate, when be accomplished within cloud computing, offers increase to protection uncertainties. The facts cooperation may additionally arise because of assaults by way of additional user along with nodes inside the cloud. Consequently, excessive safety procedures be requisite to defend records in the cloud. Nevertheless, engaged safety method should as well recall the optimization of the facts recovery moment in time. Within this term paper, we endorse Division along with Replication of Data inside the Cloud for Optimal Performance along with Security so as to together strategies the safety as well as overall performance issue. In this planned method, we separate a report keen on fragments, in addition to duplicate the split facts in excess of the cloud nodes. every of the nodes stores simplest a single fragment of a exacting facts report that guarantees with the intention of level during case of a success assault, no significant statistics is discovered in the direction of the attacker. Furthermore, the nodes store the fragments, be divided among positive space through way of graph T-coloring toward limit an assailant of guess the places of the fragments. In addition, the planned method does no longer depend on the conventional cryptographic techniques for the statistics safety; thus relieve the structure of computationally exclusive methods. We demonstrate with the intention of the possibility to find in addition to cooperation every the nodes store the fragments of a single document is very low down. We additionally evaluate the presentation of the planned method with ten different frameworks. The better stages of safety with mild overall presentation in the clouds become discovered.",,978-1-7281-1871-0,10.1109/ICESE46178.2019.9194632,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194632,Optimal Performance;Fragmentation;T-coloring;Computationally Exclusive,Cloud computing;Safety;Security;Reliability;Ions;Elasticity;Throughput,cloud computing;data protection;graph colouring;outsourcing;pattern clustering;pattern matching;security of data,cluster matching;discrete data;cloud computing;protection uncertainties;facts cooperation;facts recovery moment;success assault;graph T-coloring,,,,8,,11-Sep-20,,,IEEE,IEEE Conferences
Classification and clustering of information objects based on fuzzy neighborhood system,基於模糊鄰域系統的信息對象分類聚類,S. Miyamoto; Y. Endo; S. Hayakawa; E. Kataoka,"Dept. of Risk Eng., Tsukuba Univ., Ibaraki, Japan; Dept. of Risk Eng., Tsukuba Univ., Ibaraki, Japan; NA; NA","2005 IEEE International Conference on Systems, Man and Cybernetics",10-Jan-06,2005,4,,3210,3215 Vol. 4,"Supervised and unsupervised classification given a family of fuzzy neighborhood on a set of information objects to be retrieved is considered. An information object implies any type of objects to be retrieved, e.g., documents, keywords, images, and Web pages. We do not distinguish between terms and documents as in traditional setting of the vector space model. Instead, information link is used and the concept of fuzzy neighborhood is introduced. Classification rules based on the nearest neighbor, K nearest neighbor, and fuzzy K nearest neighbor are proposed. Agglomerative clustering algorithms are moreover developed on the basis of similarity measures defined on the neighborhood. Illustrative examples are given.",1062-922X,0-7803-9298-1,10.1109/ICSMC.2005.1571640,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1571640,Fuzzy neighborhood;supervised classification;fuzzy K nearest neighborhood;agglomerative clustering,Fuzzy systems;Information retrieval;Image retrieval;Web pages;Nearest neighbor searches;Indexing;Fuzzy sets;Clustering algorithms;Navigation;Data mining,information retrieval;classification;fuzzy set theory,fuzzy neighborhood system;supervised classification;unsupervised classification;information object classification;information object clustering;vector space model;information link;classification rule;fuzzy K nearest neighbor;agglomerative clustering algorithm,,,,9,,10-Jan-06,,,IEEE,IEEE Conferences
The discovery of burst topic and its intermittent evolution in our real world,突發主題的發現及其在現實世界中的間歇性演變,S. Tang; Y. Zhang; H. Wang; M. Chen; F. Wu; Y. Zhuang,"Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China; Institute of Artificial Intelligence, College of Computer Science, Zhejiang University, Hangzhou 310027, China",China Communications,1-Apr-13,2013,10,3,1,12,"Nowadays, a considerably large number of documents are available over many online news sites (e.g., CNN and NYT). Therefore, the utilization of these online documents, for example, the discovery of a burst topic and its evolution, is a significant challenge. In this paper, a novel topic model, called intermittent Evolution LDA (iELDA) is proposed. In iELDA, the time-evolving documents are divided into many small epochs. iELDA utilizes the detected global topics as priors to guide the detection of an emerging topic and keep track of its evolution over different epochs. As a natural extension of the traditional Latent Dirichlet Allocation (LDA) and Dynamic Topic Model (DTM), iELDA has an advantage: it can discover the intermittent recurring pattern of a burst topic. We apply iELDA to real-world data from NYT; the results demonstrate that the proposed iELDA can appropriately capture a burst topic and track its intermittent evolution as well as produce a better predictive ability than other related topic models.",1673-5447,,10.1109/CC.2013.6488826,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488826,LDA;time series;iterative clustering model,Predictive models;Data models;Context modeling;Cluster approximation;Iterative methods;Resource management,document handling,predictive ability;intermittent recurring pattern;real-world data;dynamic topic model;latent Dirichlet allocation;natural extension;epochs;global topics detection;time-evolving documents;iELDA;intermittent evolution LDA;online documents;NYT;CNN;online news sites;burst topic discovery,,,,,,1-Apr-13,,,IEEE,IEEE Magazines
Fuzzy clustering of lecture videos based on topic modeling,基於主題建模的演講視頻模糊聚類,S. Basu; Y. Yu; R. Zimmermann,"National University of Singapore, Singapore 117417; National Institute of Informatics, Tokyo 101-8430, Japan; National University of Singapore, Singapore 117417",2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI),30-Jun-16,2016,,,1,6,"Lecture videos constitute an important part of the e-learning paradigm. These online video-lectures contain multimedia materials aimed at explaining complex concepts in a more effective way. The videos are mostly grouped by their subjects. However, often there are overlaps between the subjects, e.g. Mathematics and Statistics. Hence, educational content-wise, some of the lecture videos can belong to more than one subject. When they are labeled by only one subject, students searching for the content of the lecture might miss some of these videos. To solve this problem, we aim to provide a clustering of these lecture videos based on their educational content rather than their titles so that such lectures will not be missed out based on the subject labels. Our novel algorithm uses topic modeling on video transcripts generated by automatic captions to extract the contents of these videos. We choose representative text documents for each of the clusters from the Wikipedia. Then we calculate a similarity between the topics extracted from the videos and those of the representative documents of the clusters. Finally we apply fuzzy clustering based on these similarity values and provide a lecture-content based clustering for these lecture videos. The initial results are plausible and confirm the effectiveness of the proposed scheme.",1949-3991,978-1-4673-8695-1,10.1109/CBMI.2016.7500264,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500264,,Videos;Encyclopedias;Electronic publishing;Internet;Mathematics;Multimedia communication,computer aided instruction;pattern clustering;video signal processing,fuzzy clustering;lecture videos;topic modeling;e-learning paradigm;online video lectures;multimedia materials;complex concepts;educational content wise;automatic captions;representative text documents;Wikipedia;lecture-content,,14,,19,,30-Jun-16,,,IEEE,IEEE Conferences
A comparative study of topic models for topic clustering of Chinese web news,中文網絡新聞主題聚類的主題模型比較研究,Yonghui Wu; Yuxin Ding; X. Wang; Jun Xu,"Harbin Institute of Technology, China; Key Laboratory of Network Oriented Intelligent Computation, Harbin Institute of Technology, Shenzhen Graduate School, China; Harbin Institute of Technology, China; Key Laboratory of Network Oriented Intelligent Computation, Harbin Institute of Technology, Shenzhen Graduate School, China",2010 3rd International Conference on Computer Science and Information Technology,7-Sep-10,2010,5,,236,240,"Topic model is an increasing useful tool to analyze the semantic level meanings and capture the topical features. However, there is few research about the comparative study of the topic models. In this paper, we describe our comparative study of three topic models in the extrinsic application of topic clustering. The topic model distance is defined on the converged parameters of topic models, which is used in the topic clustering. Then, the topic models are compared using the clustering result of the corresponding topic distance matrix. A series of comparative experiments are carried on a corpus containing 5033 web news from 30 topics using the cosine distance as the base-line. Web page collections with different number of topics and documents are used in experiments. The experiment results show that topic clustering using topic distance achieves a better precision and recall in the data set containing related topics. The topic clustering using topic distance benefits from the topic features captured by topic models. The complex topic model does provide further help than the simple topic model in topic clustering.",,978-1-4244-5540-9,10.1109/ICCSIT.2010.5564723,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564723,comparative study;topic model;clustering;distance measure,Artificial neural networks,information retrieval;Internet;pattern clustering;search engines,Chinese Web news;semantic level meanings;topic models;topic clustering;topic distance matrix;cosine distance;Web page collections;data set;topic features,,1,,,,7-Sep-10,,,IEEE,IEEE Conferences
